Debblobbed:

* drivers/char/drm/nv40_graph.c: Deblobbed *_ctx_prog.
* drivers/char/drm/nv50_graph.c: Deblobbed *_ctx_voodoo.

diff --git a/drivers/char/drm/Kconfig b/drivers/char/drm/Kconfig
index 610d6fd..6b98529 100644
--- a/drivers/char/drm/Kconfig
+++ b/drivers/char/drm/Kconfig
@@ -105,3 +105,10 @@ config DRM_SAVAGE
 	help
 	  Choose this option if you have a Savage3D/4/SuperSavage/Pro/Twister
 	  chipset. If M is selected the module will be called savage.
+
+config DRM_NOUVEAU
+	tristate "Nvidia video cards"
+	depends on DRM
+	help
+	  Choose this for nvidia open source 3d driver
+
diff --git a/drivers/char/drm/Makefile b/drivers/char/drm/Makefile
index 1283ded..2c652ba 100644
--- a/drivers/char/drm/Makefile
+++ b/drivers/char/drm/Makefile
@@ -6,14 +6,26 @@ drm-objs    :=	drm_auth.o drm_bufs.o drm_context.o drm_dma.o drm_drawable.o \
 		drm_drv.o drm_fops.o drm_ioctl.o drm_irq.o \
 		drm_lock.o drm_memory.o drm_proc.o drm_stub.o drm_vm.o \
 		drm_agpsupport.o drm_scatter.o ati_pcigart.o drm_pci.o \
-		drm_sysfs.o drm_hashtab.o drm_sman.o drm_mm.o
+		drm_sysfs.o drm_hashtab.o drm_sman.o drm_mm.o drm_object.o \
+		drm_fence.o drm_ttm.o drm_bo.o drm_bo_move.o drm_bo_lock.o \
+		drm_modes.o drm_crtc.o drm_edid.o
 
 tdfx-objs   := tdfx_drv.o
 r128-objs   := r128_drv.o r128_cce.o r128_state.o r128_irq.o
 mga-objs    := mga_drv.o mga_dma.o mga_state.o mga_warp.o mga_irq.o
 i810-objs   := i810_drv.o i810_dma.o
 i830-objs   := i830_drv.o i830_dma.o i830_irq.o
-i915-objs   := i915_drv.o i915_dma.o i915_irq.o i915_mem.o
+i915-objs   := i915_drv.o i915_dma.o i915_irq.o i915_mem.o i915_fence.o i915_buffer.o i915_init.o intel_display.o intel_lvds.o intel_crt.o intel_i2c.o intel_modes.o intel_sdvo.o intel_tv.o intel_fb.o
+nouveau-objs := nouveau_drv.o nouveau_state.o nouveau_fifo.o nouveau_mem.o \
+		nouveau_object.o nouveau_irq.o nouveau_notifier.o nouveau_swmthd.o \
+		nouveau_sgdma.o nouveau_dma.o nouveau_bo.o nouveau_fence.o \
+		nv04_timer.o \
+		nv04_mc.o nv40_mc.o nv50_mc.o \
+		nv04_fb.o nv10_fb.o nv40_fb.o \
+		nv04_fifo.o nv10_fifo.o nv40_fifo.o nv50_fifo.o \
+		nv04_graph.o nv10_graph.o nv20_graph.o \
+		nv40_graph.o nv50_graph.o \
+		nv04_instmem.o nv50_instmem.o
 radeon-objs := radeon_drv.o radeon_cp.o radeon_state.o radeon_mem.o radeon_irq.o r300_cmdbuf.o
 sis-objs    := sis_drv.o sis_mm.o
 savage-objs := savage_drv.o savage_bci.o savage_state.o
@@ -25,6 +37,7 @@ radeon-objs += radeon_ioc32.o
 mga-objs    += mga_ioc32.o
 r128-objs   += r128_ioc32.o
 i915-objs   += i915_ioc32.o
+nouveau-objs   += nouveau_ioc32.o
 endif
 
 obj-$(CONFIG_DRM)	+= drm.o
@@ -35,6 +48,7 @@ obj-$(CONFIG_DRM_MGA)	+= mga.o
 obj-$(CONFIG_DRM_I810)	+= i810.o
 obj-$(CONFIG_DRM_I830)	+= i830.o
 obj-$(CONFIG_DRM_I915)  += i915.o
+obj-$(CONFIG_DRM_NOUVEAU)   += nouveau.o
 obj-$(CONFIG_DRM_SIS)   += sis.o
 obj-$(CONFIG_DRM_SAVAGE)+= savage.o
 obj-$(CONFIG_DRM_VIA)	+=via.o
diff --git a/drivers/char/drm/ati_pcigart.c b/drivers/char/drm/ati_pcigart.c
index c533d0c..f27f791 100644
--- a/drivers/char/drm/ati_pcigart.c
+++ b/drivers/char/drm/ati_pcigart.c
@@ -43,7 +43,6 @@ static int drm_ati_alloc_pcigart_table(struct drm_device *dev,
 						gart_info->table_mask);
 	if (gart_info->table_handle == NULL)
 		return -ENOMEM;
-
 	return 0;
 }
 
@@ -149,7 +148,7 @@ int drm_ati_pcigart_init(struct drm_device *dev, struct drm_ati_pcigart_info *ga
 		page_base = (u32) entry->busaddr[i];
 
 		for (j = 0; j < (PAGE_SIZE / ATI_PCIGART_PAGE_SIZE); j++) {
-			switch(gart_info->gart_reg_if) {
+			switch (gart_info->gart_reg_if) {
 			case DRM_ATI_GART_IGP:
 				*pci_gart = cpu_to_le32((page_base) | 0xc);
 				break;
@@ -173,7 +172,7 @@ int drm_ati_pcigart_init(struct drm_device *dev, struct drm_ati_pcigart_info *ga
 	mb();
 #endif
 
-      done:
+done:
 	gart_info->addr = address;
 	gart_info->bus_addr = bus_address;
 	return ret;
diff --git a/drivers/char/drm/drm.h b/drivers/char/drm/drm.h
index 38d3c6b..4d1b6b6 100644
--- a/drivers/char/drm/drm.h
+++ b/drivers/char/drm/drm.h
@@ -190,6 +190,7 @@ enum drm_map_type {
 	_DRM_AGP = 3,		  /**< AGP/GART */
 	_DRM_SCATTER_GATHER = 4,  /**< Scatter/gather memory for PCI DMA */
 	_DRM_CONSISTENT = 5,	  /**< Consistent memory for PCI DMA */
+	_DRM_TTM = 6
 };
 
 /**
@@ -471,6 +472,7 @@ struct drm_irq_busid {
 enum drm_vblank_seq_type {
 	_DRM_VBLANK_ABSOLUTE = 0x0,	/**< Wait for specific vblank sequence number */
 	_DRM_VBLANK_RELATIVE = 0x1,	/**< Wait for given number of vblanks */
+	_DRM_VBLANK_FLIP = 0x8000000,	/**< Scheduled buffer swap should flip */
 	_DRM_VBLANK_NEXTONMISS = 0x10000000,	/**< If missed, wait for next vblank */
 	_DRM_VBLANK_SECONDARY = 0x20000000,	/**< Secondary display controller */
 	_DRM_VBLANK_SIGNAL = 0x40000000	/**< Send signal instead of blocking */
@@ -573,6 +575,494 @@ struct drm_set_version {
 	int drm_dd_minor;
 };
 
+#define DRM_FENCE_FLAG_EMIT                0x00000001
+#define DRM_FENCE_FLAG_SHAREABLE           0x00000002
+#define DRM_FENCE_FLAG_WAIT_LAZY           0x00000004
+#define DRM_FENCE_FLAG_WAIT_IGNORE_SIGNALS 0x00000008
+#define DRM_FENCE_FLAG_NO_USER             0x00000010
+
+/* Reserved for driver use */
+#define DRM_FENCE_MASK_DRIVER              0xFF000000
+
+#define DRM_FENCE_TYPE_EXE                 0x00000001
+
+struct drm_fence_arg {
+	unsigned int handle;
+	unsigned int fence_class;
+	unsigned int type;
+	unsigned int flags;
+	unsigned int signaled;
+	unsigned int error;
+	unsigned int sequence;
+	unsigned int pad64;
+	uint64_t expand_pad[2]; /*Future expansion */
+};
+
+/* Buffer permissions, referring to how the GPU uses the buffers.
+ * these translate to fence types used for the buffers.
+ * Typically a texture buffer is read, A destination buffer is write and
+ *  a command (batch-) buffer is exe. Can be or-ed together.
+ */
+
+#define DRM_BO_FLAG_READ        (1ULL << 0)
+#define DRM_BO_FLAG_WRITE       (1ULL << 1)
+#define DRM_BO_FLAG_EXE         (1ULL << 2)
+
+/*
+ * All of the bits related to access mode
+ */
+#define DRM_BO_MASK_ACCESS	(DRM_BO_FLAG_READ | DRM_BO_FLAG_WRITE | DRM_BO_FLAG_EXE)
+/*
+ * Status flags. Can be read to determine the actual state of a buffer.
+ * Can also be set in the buffer mask before validation.
+ */
+
+/*
+ * Mask: Never evict this buffer. Not even with force.
+ * This type of buffer is only available to root and must be manually
+ * removed before buffer manager shutdown or lock.
+ * Flags: Acknowledge
+ */
+#define DRM_BO_FLAG_NO_EVICT    (1ULL << 4)
+
+/*
+ * Mask: Require that the buffer is placed in mappable memory when validated.
+ * If not set the buffer may or may not be in mappable memory when validated.
+ * Flags: If set, the buffer is in mappable memory.
+ */
+#define DRM_BO_FLAG_MAPPABLE    (1ULL << 5)
+
+/* Mask: The buffer should be shareable with other processes.
+ * Flags: The buffer is shareable with other processes.
+ */
+#define DRM_BO_FLAG_SHAREABLE   (1ULL << 6)
+
+/* Mask: If set, place the buffer in cache-coherent memory if available.
+ *       If clear, never place the buffer in cache coherent memory if validated.
+ * Flags: The buffer is currently in cache-coherent memory.
+ */
+#define DRM_BO_FLAG_CACHED      (1ULL << 7)
+
+/* Mask: Make sure that every time this buffer is validated,
+ *       it ends up on the same location provided that the memory mask
+ *       is the same.
+ *       The buffer will also not be evicted when claiming space for
+ *       other buffers. Basically a pinned buffer but it may be thrown out as
+ *       part of buffer manager shutdown or locking.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_NO_MOVE     (1ULL << 8)
+
+/* Mask: Make sure the buffer is in cached memory when mapped.  In conjunction
+ * with DRM_BO_FLAG_CACHED it also allows the buffer to be bound into the GART
+ * with unsnooped PTEs instead of snooped, by using chipset-specific cache
+ * flushing at bind time.  A better name might be DRM_BO_FLAG_TT_UNSNOOPED,
+ * as the eviction to local memory (TTM unbind) on map is just a side effect
+ * to prevent aggressive cache prefetch from the GPU disturbing the cache
+ * management that the DRM is doing.
+ *
+ * Flags: Acknowledge.
+ * Buffers allocated with this flag should not be used for suballocators
+ * This type may have issues on CPUs with over-aggressive caching
+ * http://marc.info/?l=linux-kernel&m=102376926732464&w=2
+ */
+#define DRM_BO_FLAG_CACHED_MAPPED    (1ULL << 19)
+
+
+/* Mask: Force DRM_BO_FLAG_CACHED flag strictly also if it is set.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_FORCE_CACHING  (1ULL << 13)
+
+/*
+ * Mask: Force DRM_BO_FLAG_MAPPABLE flag strictly also if it is clear.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_FORCE_MAPPABLE (1ULL << 14)
+#define DRM_BO_FLAG_TILE           (1ULL << 15)
+
+/*
+ * Memory type flags that can be or'ed together in the mask, but only
+ * one appears in flags.
+ */
+
+/* System memory */
+#define DRM_BO_FLAG_MEM_LOCAL  (1ULL << 24)
+/* Translation table memory */
+#define DRM_BO_FLAG_MEM_TT     (1ULL << 25)
+/* Vram memory */
+#define DRM_BO_FLAG_MEM_VRAM   (1ULL << 26)
+/* Up to the driver to define. */
+#define DRM_BO_FLAG_MEM_PRIV0  (1ULL << 27)
+#define DRM_BO_FLAG_MEM_PRIV1  (1ULL << 28)
+#define DRM_BO_FLAG_MEM_PRIV2  (1ULL << 29)
+#define DRM_BO_FLAG_MEM_PRIV3  (1ULL << 30)
+#define DRM_BO_FLAG_MEM_PRIV4  (1ULL << 31)
+/* We can add more of these now with a 64-bit flag type */
+
+/*
+ * This is a mask covering all of the memory type flags; easier to just
+ * use a single constant than a bunch of | values. It covers
+ * DRM_BO_FLAG_MEM_LOCAL through DRM_BO_FLAG_MEM_PRIV4
+ */
+#define DRM_BO_MASK_MEM         0x00000000FF000000ULL
+/*
+ * This adds all of the CPU-mapping options in with the memory
+ * type to label all bits which change how the page gets mapped
+ */
+#define DRM_BO_MASK_MEMTYPE     (DRM_BO_MASK_MEM | \
+				 DRM_BO_FLAG_CACHED_MAPPED | \
+				 DRM_BO_FLAG_CACHED | \
+				 DRM_BO_FLAG_MAPPABLE)
+
+/* Driver-private flags */
+#define DRM_BO_MASK_DRIVER      0xFFFF000000000000ULL
+
+/*
+ * Don't block on validate and map. Instead, return EBUSY.
+ */
+#define DRM_BO_HINT_DONT_BLOCK  0x00000002
+/*
+ * Don't place this buffer on the unfenced list. This means
+ * that the buffer will not end up having a fence associated
+ * with it as a result of this operation
+ */
+#define DRM_BO_HINT_DONT_FENCE  0x00000004
+/*
+ * Sleep while waiting for the operation to complete.
+ * Without this flag, the kernel will, instead, spin
+ * until this operation has completed. I'm not sure
+ * why you would ever want this, so please always
+ * provide DRM_BO_HINT_WAIT_LAZY to any operation
+ * which may block
+ */
+#define DRM_BO_HINT_WAIT_LAZY   0x00000008
+/*
+ * The client has compute relocations refering to this buffer using the
+ * offset in the presumed_offset field. If that offset ends up matching
+ * where this buffer lands, the kernel is free to skip executing those
+ * relocations
+ */
+#define DRM_BO_HINT_PRESUMED_OFFSET 0x00000010
+
+
+#define DRM_BO_INIT_MAGIC 0xfe769812
+#define DRM_BO_INIT_MAJOR 1
+#define DRM_BO_INIT_MINOR 0
+#define DRM_BO_INIT_PATCH 0
+
+
+struct drm_bo_info_req {
+	uint64_t mask;
+	uint64_t flags;
+	unsigned int handle;
+	unsigned int hint;
+	unsigned int fence_class;
+	unsigned int desired_tile_stride;
+	unsigned int tile_info;
+	unsigned int pad64;
+	uint64_t presumed_offset;
+};
+
+struct drm_bo_create_req {
+	uint64_t flags;
+	uint64_t size;
+	uint64_t buffer_start;
+	unsigned int hint;
+	unsigned int page_alignment;
+};
+
+
+/*
+ * Reply flags
+ */
+
+#define DRM_BO_REP_BUSY 0x00000001
+
+struct drm_bo_info_rep {
+	uint64_t flags;
+	uint64_t proposed_flags;
+	uint64_t size;
+	uint64_t offset;
+	uint64_t arg_handle;
+	uint64_t buffer_start;
+	unsigned int handle;
+	unsigned int fence_flags;
+	unsigned int rep_flags;
+	unsigned int page_alignment;
+	unsigned int desired_tile_stride;
+	unsigned int hw_tile_stride;
+	unsigned int tile_info;
+	unsigned int pad64;
+	uint64_t expand_pad[4]; /*Future expansion */
+};
+
+struct drm_bo_arg_rep {
+	struct drm_bo_info_rep bo_info;
+	int ret;
+	unsigned int pad64;
+};
+
+struct drm_bo_create_arg {
+	union {
+		struct drm_bo_create_req req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_handle_arg {
+	unsigned int handle;
+};
+
+struct drm_bo_reference_info_arg {
+	union {
+		struct drm_bo_handle_arg req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_map_wait_idle_arg {
+	union {
+		struct drm_bo_info_req req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_op_req {
+	enum {
+		drm_bo_validate,
+		drm_bo_fence,
+		drm_bo_ref_fence,
+	} op;
+	unsigned int arg_handle;
+	struct drm_bo_info_req bo_req;
+};
+
+
+struct drm_bo_op_arg {
+	uint64_t next;
+	union {
+		struct drm_bo_op_req req;
+		struct drm_bo_arg_rep rep;
+	} d;
+	int handled;
+	unsigned int pad64;
+};
+
+
+#define DRM_BO_MEM_LOCAL 0
+#define DRM_BO_MEM_TT 1
+#define DRM_BO_MEM_VRAM 2
+#define DRM_BO_MEM_PRIV0 3
+#define DRM_BO_MEM_PRIV1 4
+#define DRM_BO_MEM_PRIV2 5
+#define DRM_BO_MEM_PRIV3 6
+#define DRM_BO_MEM_PRIV4 7
+
+#define DRM_BO_MEM_TYPES 8 /* For now. */
+
+#define DRM_BO_LOCK_UNLOCK_BM       (1 << 0)
+#define DRM_BO_LOCK_IGNORE_NO_EVICT (1 << 1)
+
+struct drm_bo_version_arg {
+	uint32_t major;
+	uint32_t minor;
+	uint32_t patchlevel;
+};
+
+struct drm_mm_type_arg {
+	unsigned int mem_type;
+	unsigned int lock_flags;
+};
+
+struct drm_mm_init_arg {
+	unsigned int magic;
+	unsigned int major;
+	unsigned int minor;
+	unsigned int mem_type;
+	uint64_t p_offset;
+	uint64_t p_size;
+};
+
+struct drm_mm_info_arg {
+	unsigned int mem_type;
+	uint64_t p_size;
+};
+
+
+/*
+ * Drm mode setting
+ */
+#define DRM_DISPLAY_INFO_LEN 32
+#define DRM_OUTPUT_NAME_LEN 32
+#define DRM_DISPLAY_MODE_LEN 32
+#define DRM_PROP_NAME_LEN 32
+
+#define DRM_MODE_TYPE_BUILTIN	(1<<0)
+#define DRM_MODE_TYPE_CLOCK_C	((1<<1) | DRM_MODE_TYPE_BUILTIN)
+#define DRM_MODE_TYPE_CRTC_C	((1<<2) | DRM_MODE_TYPE_BUILTIN)
+#define DRM_MODE_TYPE_PREFERRED	(1<<3)
+#define DRM_MODE_TYPE_DEFAULT	(1<<4)
+#define DRM_MODE_TYPE_USERDEF	(1<<5)
+#define DRM_MODE_TYPE_DRIVER	(1<<6)
+
+struct drm_mode_modeinfo {
+	unsigned int clock;
+	unsigned short hdisplay, hsync_start, hsync_end, htotal, hskew;
+	unsigned short vdisplay, vsync_start, vsync_end, vtotal, vscan;
+
+	unsigned int vrefresh; /* vertical refresh * 1000 */
+
+	unsigned int flags;
+	unsigned int type;
+	char name[DRM_DISPLAY_MODE_LEN];
+};
+
+struct drm_mode_card_res {
+	uint64_t fb_id_ptr;
+	uint64_t crtc_id_ptr;
+	uint64_t output_id_ptr;
+	int count_fbs;
+	int count_crtcs;
+	int count_outputs;
+	int min_width, max_width;
+	int min_height, max_height;
+};
+
+struct drm_mode_crtc {
+	uint64_t set_outputs_ptr;
+
+	unsigned int crtc_id; /**< Id */
+	unsigned int fb_id; /**< Id of framebuffer */
+
+	int x, y; /**< Position on the frameuffer */
+
+	int count_outputs;
+	unsigned int outputs; /**< Outputs that are connected */
+
+	int count_possibles;
+	unsigned int possibles; /**< Outputs that can be connected */
+	int gamma_size;
+	int mode_valid;
+	struct drm_mode_modeinfo mode;
+};
+
+#define DRM_MODE_OUTPUT_NONE 0
+#define DRM_MODE_OUTPUT_DAC  1
+#define DRM_MODE_OUTPUT_TMDS 2
+#define DRM_MODE_OUTPUT_LVDS 3
+#define DRM_MODE_OUTPUT_TVDAC 4
+
+struct drm_mode_get_output {
+
+	uint64_t modes_ptr;
+	uint64_t props_ptr;
+	uint64_t prop_values_ptr;
+
+	int count_modes;
+	int count_props;
+	unsigned int output; /**< Id */
+	unsigned int crtc; /**< Id of crtc */
+	unsigned int output_type;
+	unsigned int output_type_id;
+
+	unsigned int connection;
+	unsigned int mm_width, mm_height; /**< HxW in millimeters */
+	unsigned int subpixel;
+	int count_crtcs;
+	int count_clones;
+	unsigned int crtcs; /**< possible crtc to connect to */
+	unsigned int clones; /**< list of clones */
+};
+
+#define DRM_MODE_PROP_PENDING (1<<0)
+#define DRM_MODE_PROP_RANGE (1<<1)
+#define DRM_MODE_PROP_IMMUTABLE (1<<2)
+#define DRM_MODE_PROP_ENUM (1<<3) // enumerated type with text strings
+#define DRM_MODE_PROP_BLOB (1<<4)
+
+struct drm_mode_property_enum {
+	uint64_t value;
+	unsigned char name[DRM_PROP_NAME_LEN];
+};
+
+struct drm_mode_get_property {
+	uint64_t values_ptr; /* values and blob lengths */
+	uint64_t enum_blob_ptr; /* enum and blob id ptrs */
+
+	unsigned int prop_id;
+	unsigned int flags;
+	unsigned char name[DRM_PROP_NAME_LEN];
+
+	int count_values;
+	int count_enum_blobs;
+};
+
+struct drm_mode_output_set_property {
+	uint64_t value;
+	unsigned int prop_id;
+	unsigned int output_id;
+};
+
+struct drm_mode_get_blob {
+	uint32_t blob_id;
+	uint32_t length;
+	uint64_t data;
+};
+
+struct drm_mode_fb_cmd {
+        unsigned int buffer_id;
+        unsigned int width, height;
+        unsigned int pitch;
+        unsigned int bpp;
+        unsigned int handle;
+	unsigned int depth;
+};
+
+struct drm_mode_mode_cmd {
+	unsigned int output_id;
+	struct drm_mode_modeinfo mode;
+};
+
+#define DRM_MODE_CURSOR_BO   0x01
+#define DRM_MODE_CURSOR_MOVE 0x02
+
+/*
+ * depending on the value in flags diffrent members are used.
+ *
+ * CURSOR_BO uses
+ *    crtc
+ *    width
+ *    height
+ *    handle - if 0 turns the cursor of
+ *
+ * CURSOR_MOVE uses
+ *    crtc
+ *    x
+ *    y
+ */
+struct drm_mode_cursor {
+	unsigned int flags;
+	unsigned int crtc;
+	int x;
+	int y;
+	uint32_t width;
+	uint32_t height;
+	unsigned int handle;
+};
+
+/*
+ * oh so ugly hotplug
+ */
+struct drm_mode_hotplug {
+	uint32_t counter;
+};
+
+/**
+ * \name Ioctls Definitions
+ */
+/*@{*/
+
 #define DRM_IOCTL_BASE			'd'
 #define DRM_IO(nr)			_IO(DRM_IOCTL_BASE,nr)
 #define DRM_IOR(nr,type)		_IOR(DRM_IOCTL_BASE,nr,type)
@@ -635,6 +1125,50 @@ struct drm_set_version {
 
 #define DRM_IOCTL_UPDATE_DRAW		DRM_IOW(0x3f, struct drm_update_draw)
 
+#define DRM_IOCTL_MM_INIT               DRM_IOWR(0xc0, struct drm_mm_init_arg)
+#define DRM_IOCTL_MM_TAKEDOWN           DRM_IOWR(0xc1, struct drm_mm_type_arg)
+#define DRM_IOCTL_MM_LOCK               DRM_IOWR(0xc2, struct drm_mm_type_arg)
+#define DRM_IOCTL_MM_UNLOCK             DRM_IOWR(0xc3, struct drm_mm_type_arg)
+
+#define DRM_IOCTL_FENCE_CREATE          DRM_IOWR(0xc4, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_REFERENCE       DRM_IOWR(0xc6, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_UNREFERENCE     DRM_IOWR(0xc7, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_SIGNALED        DRM_IOWR(0xc8, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_FLUSH           DRM_IOWR(0xc9, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_WAIT            DRM_IOWR(0xca, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_EMIT            DRM_IOWR(0xcb, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_BUFFERS         DRM_IOWR(0xcc, struct drm_fence_arg)
+
+#define DRM_IOCTL_BO_CREATE             DRM_IOWR(0xcd, struct drm_bo_create_arg)
+#define DRM_IOCTL_BO_MAP                DRM_IOWR(0xcf, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_UNMAP              DRM_IOWR(0xd0, struct drm_bo_handle_arg)
+#define DRM_IOCTL_BO_REFERENCE          DRM_IOWR(0xd1, struct drm_bo_reference_info_arg)
+#define DRM_IOCTL_BO_UNREFERENCE        DRM_IOWR(0xd2, struct drm_bo_handle_arg)
+#define DRM_IOCTL_BO_SETSTATUS          DRM_IOWR(0xd3, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_INFO               DRM_IOWR(0xd4, struct drm_bo_reference_info_arg)
+#define DRM_IOCTL_BO_WAIT_IDLE          DRM_IOWR(0xd5, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_VERSION          DRM_IOR(0xd6, struct drm_bo_version_arg)
+#define DRM_IOCTL_MM_INFO               DRM_IOWR(0xd7, struct drm_mm_info_arg)
+
+#define DRM_IOCTL_MODE_GETRESOURCES     DRM_IOWR(0xA0, struct drm_mode_card_res)
+#define DRM_IOCTL_MODE_GETCRTC          DRM_IOWR(0xA1, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_GETOUTPUT        DRM_IOWR(0xA2, struct drm_mode_get_output)
+#define DRM_IOCTL_MODE_SETCRTC          DRM_IOWR(0xA3, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_ADDFB            DRM_IOWR(0xA4, struct drm_mode_fb_cmd)
+#define DRM_IOCTL_MODE_RMFB             DRM_IOWR(0xA5, unsigned int)
+#define DRM_IOCTL_MODE_GETFB            DRM_IOWR(0xA6, struct drm_mode_fb_cmd)
+
+#define DRM_IOCTL_MODE_SETPROPERTY     DRM_IOWR(0xA7, struct drm_mode_output_set_property)
+#define DRM_IOCTL_MODE_GETPROPBLOB     DRM_IOWR(0xA8, struct drm_mode_get_blob)
+#define DRM_IOCTL_MODE_ATTACHMODE      DRM_IOWR(0xA9, struct drm_mode_mode_cmd)
+#define DRM_IOCTL_MODE_DETACHMODE      DRM_IOWR(0xAA, struct drm_mode_mode_cmd)
+
+#define DRM_IOCTL_MODE_GETPROPERTY     DRM_IOWR(0xAB, struct drm_mode_get_property)
+#define DRM_IOCTL_MODE_CURSOR          DRM_IOWR(0xAC, struct drm_mode_cursor)
+#define DRM_IOCTL_MODE_HOTPLUG         DRM_IOWR(0xAD, struct drm_mode_hotplug)
+
+/*@}*/
+
 /**
  * Device specific ioctls should only be in their respective headers
  * The device specific ioctl range is from 0x40 to 0x99.
diff --git a/drivers/char/drm/drmP.h b/drivers/char/drm/drmP.h
index 0764b66..58f89fa 100644
--- a/drivers/char/drm/drmP.h
+++ b/drivers/char/drm/drmP.h
@@ -56,6 +56,7 @@
 #include <linux/smp_lock.h>	/* For (un)lock_kernel */
 #include <linux/dma-mapping.h>
 #include <linux/mm.h>
+#include <linux/pagemap.h>
 #include <linux/cdev.h>
 #include <linux/mutex.h>
 #if defined(__alpha__) || defined(__powerpc__)
@@ -67,6 +68,7 @@
 #ifdef CONFIG_MTRR
 #include <asm/mtrr.h>
 #endif
+#include <asm/agp.h>
 #if defined(CONFIG_AGP) || defined(CONFIG_AGP_MODULE)
 #include <linux/types.h>
 #include <linux/agp_backend.h>
@@ -104,6 +106,7 @@ struct drm_device;
 #define DRIVER_DMA_QUEUE   0x200
 #define DRIVER_FB_DMA      0x400
 #define DRIVER_IRQ_VBL2    0x800
+#define DRIVER_MODESET     0x1000
 
 /***********************************************************************/
 /** \name Begin the DRM... */
@@ -145,9 +148,22 @@ struct drm_device;
 #define DRM_MEM_CTXLIST   21
 #define DRM_MEM_MM        22
 #define DRM_MEM_HASHTAB   23
+#define DRM_MEM_OBJECTS   24
+#define DRM_MEM_FENCE     25
+#define DRM_MEM_TTM       26
+#define DRM_MEM_BUFOBJ    27
 
 #define DRM_MAX_CTXBITMAP (PAGE_SIZE * 8)
 #define DRM_MAP_HASH_OFFSET 0x10000000
+#define DRM_MAP_HASH_ORDER 12
+#define DRM_OBJECT_HASH_ORDER 12
+#define DRM_FILE_PAGE_OFFSET_START ((0xFFFFFFFFUL >> PAGE_SHIFT) + 1)
+#define DRM_FILE_PAGE_OFFSET_SIZE ((0xFFFFFFFFUL >> PAGE_SHIFT) * 16)
+/*
+ * This should be small enough to allow the use of kmalloc for hash tables
+ * instead of vmalloc.
+ */
+#define DRM_FILE_HASH_ORDER 8
 
 /*@}*/
 
@@ -237,11 +253,11 @@ struct drm_device;
  */
 #define LOCK_TEST_WITH_RETURN( dev, file_priv )				\
 do {									\
-	if ( !_DRM_LOCK_IS_HELD( dev->lock.hw_lock->lock ) ||		\
-	     dev->lock.file_priv != file_priv )	{			\
+	if ( !_DRM_LOCK_IS_HELD( file_priv->master->lock.hw_lock->lock ) ||		\
+	     file_priv->master->lock.file_priv != file_priv )	{			\
 		DRM_ERROR( "%s called without lock held, held  %d owner %p %p\n",\
-			   __func__, _DRM_LOCK_IS_HELD( dev->lock.hw_lock->lock ),\
-			   dev->lock.file_priv, file_priv );		\
+			   __func__, _DRM_LOCK_IS_HELD( file_priv->master->lock.hw_lock->lock ),\
+			   file_priv->master->lock.file_priv, file_priv );		\
 		return -EINVAL;						\
 	}								\
 } while (0)
@@ -272,9 +288,10 @@ typedef int drm_ioctl_t(struct drm_device *dev, void *data,
 typedef int drm_ioctl_compat_t(struct file *filp, unsigned int cmd,
 			       unsigned long arg);
 
-#define DRM_AUTH	0x1
-#define	DRM_MASTER	0x2
-#define DRM_ROOT_ONLY	0x4
+#define DRM_AUTH        0x1
+#define DRM_MASTER      0x2
+#define DRM_ROOT_ONLY   0x4
+#define DRM_CONTROL_ALLOW 0x8 // allow ioctl to operate on control node 
 
 struct drm_ioctl_desc {
 	unsigned int cmd;
@@ -375,10 +392,15 @@ struct drm_buf_entry {
 	struct drm_freelist freelist;
 };
 
+enum drm_ref_type {
+	_DRM_REF_USE = 0,
+	_DRM_REF_TYPE1,
+	_DRM_NO_REF_TYPES
+};
+
 /** File private data */
 struct drm_file {
 	int authenticated;
-	int master;
 	pid_t pid;
 	uid_t uid;
 	drm_magic_t magic;
@@ -387,8 +409,21 @@ struct drm_file {
 	struct drm_minor *minor;
 	int remove_auth_on_close;
 	unsigned long lock_count;
+	/*
+	 * The user object hash table is global and resides in the
+	 * drm_device structure. We protect the lists and hash tables with the
+	 * device struct_mutex. A bit coarse-grained but probably the best
+	 * option.
+	 */
+	struct list_head refd_objects;
+	struct drm_open_hash refd_object_hash[_DRM_NO_REF_TYPES];
 	struct file *filp;
 	void *driver_priv;
+
+	int is_master; /* this file private is a master for a minor */
+	struct drm_master *master; /* master this node is currently associated with
+				      N.B. not always minor->master */
+	struct list_head fbs;
 };
 
 /** Wait queue */
@@ -518,6 +553,9 @@ struct drm_map_list {
 	struct drm_hash_item hash;
 	struct drm_map *map;			/**< mapping */
 	uint64_t user_token;
+	struct drm_mm_node *file_offset_node;
+	struct drm_master *master; /** if this map is associated with a specific
+				       master */
 };
 
 typedef struct drm_map drm_local_map_t;
@@ -557,6 +595,31 @@ struct drm_ati_pcigart_info {
 	int table_size;
 };
 
+#include "drm_objects.h"
+#include "drm_crtc.h"
+
+/* per-master structure */
+struct drm_master {
+	
+	struct list_head head; /**< each minor contains a list of masters */
+	struct drm_minor *minor; /**< link back to minor we are a master for */
+
+	char *unique;			/**< Unique identifier: e.g., busid */
+	int unique_len;			/**< Length of unique field */
+
+	int blocked;			/**< Blocked due to VC switch? */
+
+	/** \name Authentication */
+	/*@{ */
+	struct drm_open_hash magiclist;
+	struct list_head magicfree;
+	/*@} */
+
+	struct drm_lock_data lock;		/**< Information on hardware lock */
+
+	void *driver_priv; /**< Private structure for driver to use */
+};
+
 /**
  * DRM driver structure. This structure represent the common code for
  * a family of cards. There will one drm_device for each card present
@@ -614,6 +677,18 @@ struct drm_driver {
 	void (*set_version) (struct drm_device *dev,
 			     struct drm_set_version *sv);
 
+	/* FB routines, if present */
+	int (*fb_probe)(struct drm_device *dev, struct drm_crtc *crtc);
+	int (*fb_remove)(struct drm_device *dev, struct drm_crtc *crtc);
+	int (*fb_resize)(struct drm_device *dev, struct drm_crtc *crtc);
+
+	/* Master routines */
+	int (*master_create)(struct drm_device *dev, struct drm_master *master);
+	void (*master_destroy)(struct drm_device *dev, struct drm_master *master);
+
+	struct drm_fence_driver *fence_driver;
+	struct drm_bo_driver *bo_driver;
+
 	int major;
 	int minor;
 	int patchlevel;
@@ -630,8 +705,9 @@ struct drm_driver {
 };
 
 #define DRM_MINOR_UNASSIGNED 0
-#define DRM_MINOR_LEGACY 1
-
+#define DRM_MINOR_CONTROL 1
+#define DRM_MINOR_LEGACY 2
+#define DRM_MINOR_RENDER 3
 /**
  * DRM minor structure. This structure represents a drm minor number.
  */
@@ -641,7 +717,15 @@ struct drm_minor {
 	dev_t device;			/**< Device number for mknod */
 	struct device kdev;		/**< Linux device */
 	struct drm_device *dev;
+	/* for render nodes */
 	struct proc_dir_entry *dev_root;  /**< proc directory entry */
+	struct class_device *dev_class;
+
+	/* for control nodes - a pointer to the current master for this control node */
+	struct drm_master *master; /* currently active master for this node */
+	struct list_head master_list;
+
+	/* possibly needs a list of configured modesetting pieces */
 };
 
 /**
@@ -649,13 +733,9 @@ struct drm_minor {
  * may contain multiple heads.
  */
 struct drm_device {
-	char *unique;			/**< Unique identifier: e.g., busid */
-	int unique_len;			/**< Length of unique field */
 	char *devname;			/**< For /proc/interrupts */
 	int if_version;			/**< Highest interface version set */
 
-	int blocked;			/**< Blocked due to VC switch? */
-
 	/** \name Locks */
 	/*@{ */
 	spinlock_t count_lock;		/**< For inuse, drm_device::open_count, drm_device::buf_use */
@@ -678,18 +758,16 @@ struct drm_device {
 	atomic_t counts[15];
 	/*@} */
 
-	/** \name Authentication */
-	/*@{ */
-	struct list_head filelist;
-	struct drm_open_hash magiclist;	/**< magic hash table */
-	struct list_head magicfree;
-	/*@} */
 
 	/** \name Memory management */
 	/*@{ */
 	struct list_head maplist;	/**< Linked list of regions */
 	int map_count;			/**< Number of mappable regions */
-	struct drm_open_hash map_hash;	/**< User token hash table for maps */
+	struct drm_open_hash map_hash;       /**< User token hash table for maps */
+	struct drm_mm offset_manager;        /**< User token manager */
+	struct drm_open_hash object_hash;    /**< User token hash table for objects */
+	struct address_space *dev_mapping;  /**< For unmap_mapping_range() */
+	struct page *ttm_dummy_page;
 
 	/** \name Context handle management */
 	/*@{ */
@@ -700,7 +778,9 @@ struct drm_device {
 	struct idr ctx_idr;
 
 	struct list_head vmalist;	/**< List of vmas (for debugging) */
-	struct drm_lock_data lock;	/**< Information on hardware lock */
+
+	struct list_head filelist;
+
 	/*@} */
 
 	/** \name DMA queues (contexts) */
@@ -764,14 +844,32 @@ struct drm_device {
 	struct drm_driver *driver;
 	drm_local_map_t *agp_buffer_map;
 	unsigned int agp_buffer_token;
+
+	/* minor number for control node */
+	struct drm_minor *control;
 	struct drm_minor *primary;		/**< render type primary screen head */
 
+	struct drm_fence_manager fm;
+	struct drm_buffer_manager bm;
+
 	/** \name Drawable information */
 	/*@{ */
 	spinlock_t drw_lock;
 	struct idr drw_idr;
 	/*@} */
+
+	/* DRM mode setting */
+	struct drm_mode_config mode_config;
+};
+
+#if __OS_HAS_AGP
+struct drm_agp_ttm_backend {
+	struct drm_ttm_backend backend;
+	DRM_AGP_MEM *mem;
+	struct agp_bridge_data *bridge;
+	int populated;
 };
+#endif
 
 static __inline__ int drm_core_check_feature(struct drm_device *dev,
 					     int feature)
@@ -869,6 +967,15 @@ extern int drm_free_agp(DRM_AGP_MEM * handle, int pages);
 extern int drm_bind_agp(DRM_AGP_MEM * handle, unsigned int start);
 extern int drm_unbind_agp(DRM_AGP_MEM * handle);
 
+extern void drm_free_memctl(size_t size);
+extern int drm_alloc_memctl(size_t size);
+extern void drm_query_memctl(uint64_t *cur_used,
+			     uint64_t *low_threshold,
+			     uint64_t *high_threshold);
+extern void drm_init_memctl(size_t low_threshold,
+			    size_t high_threshold,
+			    size_t unit_size);
+
 				/* Misc. IOCTL support (drm_ioctl.h) */
 extern int drm_irq_by_busid(struct drm_device *dev, void *data,
 			    struct drm_file *file_priv);
@@ -973,7 +1080,8 @@ extern unsigned long drm_get_resource_start(struct drm_device *dev,
 					    unsigned int resource);
 extern unsigned long drm_get_resource_len(struct drm_device *dev,
 					  unsigned int resource);
-
+struct drm_map_list *drm_find_matching_map(struct drm_device *dev,
+					   drm_local_map_t *map);
 				/* DMA support (drm_dma.h) */
 extern int drm_dma_setup(struct drm_device *dev);
 extern void drm_dma_takedown(struct drm_device *dev);
@@ -985,6 +1093,7 @@ extern void drm_core_reclaim_buffers(struct drm_device *dev,
 extern int drm_control(struct drm_device *dev, void *data,
 		       struct drm_file *file_priv);
 extern irqreturn_t drm_irq_handler(DRM_IRQ_ARGS);
+extern int drm_irq_install(struct drm_device * dev);
 extern int drm_irq_uninstall(struct drm_device *dev);
 extern void drm_driver_irq_preinstall(struct drm_device *dev);
 extern void drm_driver_irq_postinstall(struct drm_device *dev);
@@ -1026,13 +1135,16 @@ extern DRM_AGP_MEM *drm_agp_allocate_memory(struct agp_bridge_data *bridge, size
 extern int drm_agp_free_memory(DRM_AGP_MEM * handle);
 extern int drm_agp_bind_memory(DRM_AGP_MEM * handle, off_t start);
 extern int drm_agp_unbind_memory(DRM_AGP_MEM * handle);
-
+extern struct drm_ttm_backend *drm_agp_init_ttm(struct drm_device *dev);
+extern void drm_agp_chipset_flush(struct drm_device *dev);
 				/* Stub support (drm_stub.h) */
+extern struct drm_master *drm_get_master(struct drm_minor *minor);
+extern void drm_put_master(struct drm_master *master);
 extern int drm_get_dev(struct pci_dev *pdev, const struct pci_device_id *ent,
 		       struct drm_driver *driver);
 extern int drm_put_dev(struct drm_device *dev);
 extern int drm_put_minor(struct drm_minor **minor);
-extern unsigned int drm_debug;
+extern unsigned int drm_debug; /* 1 to enable debug output */
 
 extern struct class *drm_class;
 extern struct proc_dir_entry *drm_proc_root;
@@ -1147,6 +1259,39 @@ extern void drm_free(void *pt, size_t size, int area);
 extern void *drm_calloc(size_t nmemb, size_t size, int area);
 #endif
 
+/*
+ * Accounting variants of standard calls.
+ */
+
+static inline void *drm_ctl_alloc(size_t size, int area)
+{
+	void *ret;
+	if (drm_alloc_memctl(size))
+		return NULL;
+	ret = drm_alloc(size, area);
+	if (!ret)
+		drm_free_memctl(size);
+	return ret;
+}
+
+static inline void *drm_ctl_calloc(size_t nmemb, size_t size, int area)
+{
+	void *ret;
+
+	if (drm_alloc_memctl(nmemb*size))
+		return NULL;
+	ret = drm_calloc(nmemb, size, area);
+	if (!ret)
+		drm_free_memctl(nmemb*size);
+	return ret;
+}
+
+static inline void drm_ctl_free(void *pt, size_t size, int area)
+{
+	drm_free(pt, size, area);
+	drm_free_memctl(size);
+}
+
 /*@}*/
 
 #endif				/* __KERNEL__ */
diff --git a/drivers/char/drm/drm_agpsupport.c b/drivers/char/drm/drm_agpsupport.c
index aefa5ac..3e1358d 100644
--- a/drivers/char/drm/drm_agpsupport.c
+++ b/drivers/char/drm/drm_agpsupport.c
@@ -68,7 +68,6 @@ int drm_agp_info(struct drm_device *dev, struct drm_agp_info *info)
 
 	return 0;
 }
-
 EXPORT_SYMBOL(drm_agp_info);
 
 int drm_agp_info_ioctl(struct drm_device *dev, void *data,
@@ -93,7 +92,7 @@ int drm_agp_info_ioctl(struct drm_device *dev, void *data,
  * Verifies the AGP device hasn't been acquired before and calls
  * \c agp_backend_acquire.
  */
-int drm_agp_acquire(struct drm_device * dev)
+int drm_agp_acquire(struct drm_device *dev)
 {
 	if (!dev->agp)
 		return -ENODEV;
@@ -104,7 +103,6 @@ int drm_agp_acquire(struct drm_device * dev)
 	dev->agp->acquired = 1;
 	return 0;
 }
-
 EXPORT_SYMBOL(drm_agp_acquire);
 
 /**
@@ -133,7 +131,7 @@ int drm_agp_acquire_ioctl(struct drm_device *dev, void *data,
  *
  * Verifies the AGP device has been acquired and calls \c agp_backend_release.
  */
-int drm_agp_release(struct drm_device * dev)
+int drm_agp_release(struct drm_device *dev)
 {
 	if (!dev->agp || !dev->agp->acquired)
 		return -EINVAL;
@@ -159,7 +157,7 @@ int drm_agp_release_ioctl(struct drm_device *dev, void *data,
  * Verifies the AGP device has been acquired but not enabled, and calls
  * \c agp_enable.
  */
-int drm_agp_enable(struct drm_device * dev, struct drm_agp_mode mode)
+int drm_agp_enable(struct drm_device *dev, struct drm_agp_mode mode)
 {
 	if (!dev->agp || !dev->agp->acquired)
 		return -EINVAL;
@@ -169,7 +167,6 @@ int drm_agp_enable(struct drm_device * dev, struct drm_agp_mode mode)
 	dev->agp->enabled = 1;
 	return 0;
 }
-
 EXPORT_SYMBOL(drm_agp_enable);
 
 int drm_agp_enable_ioctl(struct drm_device *dev, void *data,
@@ -244,7 +241,7 @@ int drm_agp_alloc_ioctl(struct drm_device *dev, void *data,
  *
  * Walks through drm_agp_head::memory until finding a matching handle.
  */
-static struct drm_agp_mem *drm_agp_lookup_entry(struct drm_device * dev,
+static struct drm_agp_mem *drm_agp_lookup_entry(struct drm_device *dev,
 					   unsigned long handle)
 {
 	struct drm_agp_mem *entry;
@@ -421,14 +418,14 @@ struct drm_agp_head *drm_agp_init(struct drm_device *dev)
 }
 
 /** Calls agp_allocate_memory() */
-DRM_AGP_MEM *drm_agp_allocate_memory(struct agp_bridge_data * bridge,
+DRM_AGP_MEM *drm_agp_allocate_memory(struct agp_bridge_data *bridge,
 				     size_t pages, u32 type)
 {
 	return agp_allocate_memory(bridge, pages, type);
 }
 
 /** Calls agp_free_memory() */
-int drm_agp_free_memory(DRM_AGP_MEM * handle)
+int drm_agp_free_memory(DRM_AGP_MEM *handle)
 {
 	if (!handle)
 		return 0;
@@ -437,7 +434,7 @@ int drm_agp_free_memory(DRM_AGP_MEM * handle)
 }
 
 /** Calls agp_bind_memory() */
-int drm_agp_bind_memory(DRM_AGP_MEM * handle, off_t start)
+int drm_agp_bind_memory(DRM_AGP_MEM *handle, off_t start)
 {
 	if (!handle)
 		return -EINVAL;
@@ -445,11 +442,188 @@ int drm_agp_bind_memory(DRM_AGP_MEM * handle, off_t start)
 }
 
 /** Calls agp_unbind_memory() */
-int drm_agp_unbind_memory(DRM_AGP_MEM * handle)
+int drm_agp_unbind_memory(DRM_AGP_MEM *handle)
 {
 	if (!handle)
 		return -EINVAL;
 	return agp_unbind_memory(handle);
 }
 
+
+/*
+ * AGP ttm backend interface.
+ */
+
+#ifndef AGP_USER_TYPES
+#define AGP_USER_TYPES (1 << 16)
+#define AGP_USER_MEMORY (AGP_USER_TYPES)
+#define AGP_USER_CACHED_MEMORY (AGP_USER_TYPES + 1)
+#endif
+#define AGP_REQUIRED_MAJOR 0
+#define AGP_REQUIRED_MINOR 102
+
+static int drm_agp_needs_unbind_cache_adjust(struct drm_ttm_backend *backend)
+{
+	return ((backend->flags & DRM_BE_FLAG_BOUND_CACHED) ? 0 : 1);
+}
+
+
+static int drm_agp_populate(struct drm_ttm_backend *backend,
+			    unsigned long num_pages, struct page **pages,
+			    struct page *dummy_read_page)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	struct page **cur_page, **last_page = pages + num_pages;
+	DRM_AGP_MEM *mem;
+	int dummy_page_count = 0;
+
+	if (drm_alloc_memctl(num_pages * sizeof(void *)))
+		return -1;
+
+	DRM_DEBUG("drm_agp_populate_ttm\n");
+	mem = drm_agp_allocate_memory(agp_be->bridge, num_pages, AGP_USER_MEMORY);
+	if (!mem) {
+		drm_free_memctl(num_pages * sizeof(void *));
+		return -1;
+	}
+
+	DRM_DEBUG("Current page count is %ld\n", (long) mem->page_count);
+	mem->page_count = 0;
+	for (cur_page = pages; cur_page < last_page; ++cur_page) {
+		struct page *page = *cur_page;
+		if (!page) {
+			page = dummy_read_page;
+			++dummy_page_count;
+		}
+		mem->memory[mem->page_count++] = phys_to_gart(page_to_phys(page));
+	}
+	if (dummy_page_count)
+		DRM_DEBUG("Mapped %d dummy pages\n", dummy_page_count);
+	agp_be->mem = mem;
+	return 0;
+}
+
+static int drm_agp_bind_ttm(struct drm_ttm_backend *backend,
+			    struct drm_bo_mem_reg *bo_mem)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	DRM_AGP_MEM *mem = agp_be->mem;
+	int ret;
+	int snooped = (bo_mem->flags & DRM_BO_FLAG_CACHED) && !(bo_mem->flags & DRM_BO_FLAG_CACHED_MAPPED);
+
+	DRM_DEBUG("drm_agp_bind_ttm\n");
+	mem->is_flushed = true;
+	mem->type = AGP_USER_MEMORY;
+	/* CACHED MAPPED implies not snooped memory */
+	if (snooped)
+		mem->type = AGP_USER_CACHED_MEMORY;
+
+	ret = drm_agp_bind_memory(mem, bo_mem->mm_node->start);
+	if (ret)
+		DRM_ERROR("AGP Bind memory failed\n");
+
+	DRM_FLAG_MASKED(backend->flags, (bo_mem->flags & DRM_BO_FLAG_CACHED) ?
+			DRM_BE_FLAG_BOUND_CACHED : 0,
+			DRM_BE_FLAG_BOUND_CACHED);
+	return ret;
+}
+
+static int drm_agp_unbind_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+
+	DRM_DEBUG("drm_agp_unbind_ttm\n");
+	if (agp_be->mem->is_bound)
+		return drm_agp_unbind_memory(agp_be->mem);
+	else
+		return 0;
+}
+
+static void drm_agp_clear_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	DRM_AGP_MEM *mem = agp_be->mem;
+
+	DRM_DEBUG("drm_agp_clear_ttm\n");
+	if (mem) {
+		unsigned long num_pages = mem->page_count;
+		backend->func->unbind(backend);
+		agp_free_memory(mem);
+		drm_free_memctl(num_pages * sizeof(void *));
+	}
+	agp_be->mem = NULL;
+}
+
+static void drm_agp_destroy_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be;
+
+	if (backend) {
+		DRM_DEBUG("drm_agp_destroy_ttm\n");
+		agp_be = container_of(backend, struct drm_agp_ttm_backend, backend);
+		if (agp_be) {
+			if (agp_be->mem)
+				backend->func->clear(backend);
+			drm_ctl_free(agp_be, sizeof(*agp_be), DRM_MEM_TTM);
+		}
+	}
+}
+
+static struct drm_ttm_backend_func agp_ttm_backend = {
+	.needs_ub_cache_adjust = drm_agp_needs_unbind_cache_adjust,
+	.populate = drm_agp_populate,
+	.clear = drm_agp_clear_ttm,
+	.bind = drm_agp_bind_ttm,
+	.unbind = drm_agp_unbind_ttm,
+	.destroy =  drm_agp_destroy_ttm,
+};
+
+struct drm_ttm_backend *drm_agp_init_ttm(struct drm_device *dev)
+{
+	struct drm_agp_ttm_backend *agp_be;
+	struct agp_kern_info *info;
+
+	if (!dev->agp) {
+		DRM_ERROR("AGP is not initialized.\n");
+		return NULL;
+	}
+	info = &dev->agp->agp_info;
+
+	if (info->version.major != AGP_REQUIRED_MAJOR ||
+	    info->version.minor < AGP_REQUIRED_MINOR) {
+		DRM_ERROR("Wrong agpgart version %d.%d\n"
+			  "\tYou need at least version %d.%d.\n",
+			  info->version.major,
+			  info->version.minor,
+			  AGP_REQUIRED_MAJOR,
+			  AGP_REQUIRED_MINOR);
+		return NULL;
+	}
+
+
+	agp_be = drm_ctl_calloc(1, sizeof(*agp_be), DRM_MEM_TTM);
+	if (!agp_be)
+		return NULL;
+
+	agp_be->mem = NULL;
+
+	agp_be->bridge = dev->agp->bridge;
+	agp_be->populated = false;
+	agp_be->backend.func = &agp_ttm_backend;
+	agp_be->backend.dev = dev;
+
+	return &agp_be->backend;
+}
+EXPORT_SYMBOL(drm_agp_init_ttm);
+
+void drm_agp_chipset_flush(struct drm_device *dev)
+{
+	agp_flush_chipset(dev->agp->bridge);
+}
+EXPORT_SYMBOL(drm_agp_chipset_flush);
+
 #endif				/* __OS_HAS_AGP */
diff --git a/drivers/char/drm/drm_auth.c b/drivers/char/drm/drm_auth.c
index a734627..20c8a63 100644
--- a/drivers/char/drm/drm_auth.c
+++ b/drivers/char/drm/drm_auth.c
@@ -45,14 +45,15 @@
  * the one with matching magic number, while holding the drm_device::struct_mutex
  * lock.
  */
-static struct drm_file *drm_find_file(struct drm_device * dev, drm_magic_t magic)
+static struct drm_file *drm_find_file(struct drm_master *master , drm_magic_t magic)
 {
 	struct drm_file *retval = NULL;
 	struct drm_magic_entry *pt;
 	struct drm_hash_item *hash;
+	struct drm_device *dev = master->minor->dev;
 
 	mutex_lock(&dev->struct_mutex);
-	if (!drm_ht_find_item(&dev->magiclist, (unsigned long)magic, &hash)) {
+	if (!drm_ht_find_item(&master->magiclist, (unsigned long)magic, &hash)) {
 		pt = drm_hash_entry(hash, struct drm_magic_entry, hash_item);
 		retval = pt->priv;
 	}
@@ -71,11 +72,11 @@ static struct drm_file *drm_find_file(struct drm_device * dev, drm_magic_t magic
  * associated the magic number hash key in drm_device::magiclist, while holding
  * the drm_device::struct_mutex lock.
  */
-static int drm_add_magic(struct drm_device * dev, struct drm_file * priv,
+static int drm_add_magic(struct drm_master *master, struct drm_file * priv,
 			 drm_magic_t magic)
 {
 	struct drm_magic_entry *entry;
-
+	struct drm_device *dev = master->minor->dev;
 	DRM_DEBUG("%d\n", magic);
 
 	entry = drm_alloc(sizeof(*entry), DRM_MEM_MAGIC);
@@ -83,11 +84,10 @@ static int drm_add_magic(struct drm_device * dev, struct drm_file * priv,
 		return -ENOMEM;
 	memset(entry, 0, sizeof(*entry));
 	entry->priv = priv;
-
 	entry->hash_item.key = (unsigned long)magic;
 	mutex_lock(&dev->struct_mutex);
-	drm_ht_insert_item(&dev->magiclist, &entry->hash_item);
-	list_add_tail(&entry->head, &dev->magicfree);
+	drm_ht_insert_item(&master->magiclist, &entry->hash_item);
+	list_add_tail(&entry->head, &master->magicfree);
 	mutex_unlock(&dev->struct_mutex);
 
 	return 0;
@@ -102,20 +102,21 @@ static int drm_add_magic(struct drm_device * dev, struct drm_file * priv,
  * Searches and unlinks the entry in drm_device::magiclist with the magic
  * number hash key, while holding the drm_device::struct_mutex lock.
  */
-static int drm_remove_magic(struct drm_device * dev, drm_magic_t magic)
+static int drm_remove_magic(struct drm_master *master, drm_magic_t magic)
 {
 	struct drm_magic_entry *pt;
 	struct drm_hash_item *hash;
+	struct drm_device *dev = master->minor->dev;
 
 	DRM_DEBUG("%d\n", magic);
 
 	mutex_lock(&dev->struct_mutex);
-	if (drm_ht_find_item(&dev->magiclist, (unsigned long)magic, &hash)) {
+	if (drm_ht_find_item(&master->magiclist, (unsigned long)magic, &hash)) {
 		mutex_unlock(&dev->struct_mutex);
 		return -EINVAL;
 	}
 	pt = drm_hash_entry(hash, struct drm_magic_entry, hash_item);
-	drm_ht_remove_item(&dev->magiclist, hash);
+	drm_ht_remove_item(&master->magiclist, hash);
 	list_del(&pt->head);
 	mutex_unlock(&dev->struct_mutex);
 
@@ -153,9 +154,9 @@ int drm_getmagic(struct drm_device *dev, void *data, struct drm_file *file_priv)
 				++sequence;	/* reserve 0 */
 			auth->magic = sequence++;
 			spin_unlock(&lock);
-		} while (drm_find_file(dev, auth->magic));
+		} while (drm_find_file(file_priv->master, auth->magic));
 		file_priv->magic = auth->magic;
-		drm_add_magic(dev, file_priv, auth->magic);
+		drm_add_magic(file_priv->master, file_priv, auth->magic);
 	}
 
 	DRM_DEBUG("%u\n", auth->magic);
@@ -181,9 +182,9 @@ int drm_authmagic(struct drm_device *dev, void *data,
 	struct drm_file *file;
 
 	DRM_DEBUG("%u\n", auth->magic);
-	if ((file = drm_find_file(dev, auth->magic))) {
+	if ((file = drm_find_file(file_priv->master, auth->magic))) {
 		file->authenticated = 1;
-		drm_remove_magic(dev, auth->magic);
+		drm_remove_magic(file_priv->master, auth->magic);
 		return 0;
 	}
 	return -EINVAL;
diff --git a/drivers/char/drm/drm_bo.c b/drivers/char/drm/drm_bo.c
new file mode 100644
index 0000000..185aa32
--- /dev/null
+++ b/drivers/char/drm/drm_bo.c
@@ -0,0 +1,2725 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+/*
+ * Locking may look a bit complicated but isn't really:
+ *
+ * The buffer usage atomic_t needs to be protected by dev->struct_mutex
+ * when there is a chance that it can be zero before or after the operation.
+ *
+ * dev->struct_mutex also protects all lists and list heads,
+ * Hash tables and hash heads.
+ *
+ * bo->mutex protects the buffer object itself excluding the usage field.
+ * bo->mutex does also protect the buffer list heads, so to manipulate those,
+ * we need both the bo->mutex and the dev->struct_mutex.
+ *
+ * Locking order is bo->mutex, dev->struct_mutex. Therefore list traversal
+ * is a bit complicated. When dev->struct_mutex is released to grab bo->mutex,
+ * the list traversal will, in general, need to be restarted.
+ *
+ */
+
+static void drm_bo_destroy_locked(struct drm_buffer_object *bo);
+static int drm_bo_setup_vm_locked(struct drm_buffer_object *bo);
+static void drm_bo_takedown_vm_locked(struct drm_buffer_object *bo);
+static void drm_bo_unmap_virtual(struct drm_buffer_object *bo);
+
+static inline uint64_t drm_bo_type_flags(unsigned type)
+{
+	return (1ULL << (24 + type));
+}
+
+/*
+ * bo locked. dev->struct_mutex locked.
+ */
+
+void drm_bo_add_to_pinned_lru(struct drm_buffer_object *bo)
+{
+	struct drm_mem_type_manager *man;
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+	DRM_ASSERT_LOCKED(&bo->mutex);
+
+	man = &bo->dev->bm.man[bo->pinned_mem_type];
+	list_add_tail(&bo->pinned_lru, &man->pinned);
+}
+
+void drm_bo_add_to_lru(struct drm_buffer_object *bo)
+{
+	struct drm_mem_type_manager *man;
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+
+	if (!(bo->mem.proposed_flags & (DRM_BO_FLAG_NO_MOVE | DRM_BO_FLAG_NO_EVICT))
+	    || bo->mem.mem_type != bo->pinned_mem_type) {
+		man = &bo->dev->bm.man[bo->mem.mem_type];
+		list_add_tail(&bo->lru, &man->lru);
+	} else {
+		INIT_LIST_HEAD(&bo->lru);
+	}
+}
+
+static int drm_bo_vm_pre_move(struct drm_buffer_object *bo, int old_is_pci)
+{
+	if (!bo->map_list.map)
+		return 0;
+
+	drm_bo_unmap_virtual(bo);
+	return 0;
+}
+
+static void drm_bo_vm_post_move(struct drm_buffer_object *bo)
+{
+}
+
+/*
+ * Call bo->mutex locked.
+ */
+
+static int drm_bo_add_ttm(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	int ret = 0;
+	uint32_t page_flags = 0;
+
+	DRM_ASSERT_LOCKED(&bo->mutex);
+	bo->ttm = NULL;
+
+	if (bo->mem.proposed_flags & DRM_BO_FLAG_WRITE)
+		page_flags |= DRM_TTM_PAGE_WRITE;
+
+	switch (bo->type) {
+	case drm_bo_type_device:
+	case drm_bo_type_kernel:
+		bo->ttm = drm_ttm_create(dev, bo->num_pages << PAGE_SHIFT,
+					 page_flags, dev->bm.dummy_read_page);
+		if (!bo->ttm)
+			ret = -ENOMEM;
+		break;
+	case drm_bo_type_user:
+		bo->ttm = drm_ttm_create(dev, bo->num_pages << PAGE_SHIFT,
+					 page_flags | DRM_TTM_PAGE_USER,
+					 dev->bm.dummy_read_page);
+		if (!bo->ttm)
+			ret = -ENOMEM;
+
+		ret = drm_ttm_set_user(bo->ttm, current,
+				       bo->buffer_start,
+				       bo->num_pages);
+		if (ret)
+			return ret;
+
+		break;
+	default:
+		DRM_ERROR("Illegal buffer object type\n");
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static int drm_bo_handle_move_mem(struct drm_buffer_object *bo,
+				  struct drm_bo_mem_reg *mem,
+				  int evict, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int old_is_pci = drm_mem_reg_is_pci(dev, &bo->mem);
+	int new_is_pci = drm_mem_reg_is_pci(dev, mem);
+	struct drm_mem_type_manager *old_man = &bm->man[bo->mem.mem_type];
+	struct drm_mem_type_manager *new_man = &bm->man[mem->mem_type];
+	int ret = 0;
+
+	if (old_is_pci || new_is_pci ||
+	    ((mem->flags ^ bo->mem.flags) & DRM_BO_FLAG_CACHED))
+		ret = drm_bo_vm_pre_move(bo, old_is_pci);
+	if (ret)
+		return ret;
+
+	/*
+	 * Create and bind a ttm if required.
+	 */
+
+	if (!(new_man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (bo->ttm == NULL)) {
+		ret = drm_bo_add_ttm(bo);
+		if (ret)
+			goto out_err;
+
+		if (mem->mem_type != DRM_BO_MEM_LOCAL) {
+			ret = drm_ttm_bind(bo->ttm, mem);
+			if (ret)
+				goto out_err;
+		}
+	}
+
+	if ((bo->mem.mem_type == DRM_BO_MEM_LOCAL) && bo->ttm == NULL) {
+
+		struct drm_bo_mem_reg *old_mem = &bo->mem;
+		uint64_t save_flags = old_mem->flags;
+		uint64_t save_proposed_flags = old_mem->proposed_flags;
+
+		*old_mem = *mem;
+		mem->mm_node = NULL;
+		old_mem->proposed_flags = save_proposed_flags;
+		DRM_FLAG_MASKED(save_flags, mem->flags, DRM_BO_MASK_MEMTYPE);
+
+	} else if (!(old_man->flags & _DRM_FLAG_MEMTYPE_FIXED) &&
+		   !(new_man->flags & _DRM_FLAG_MEMTYPE_FIXED)) {
+
+		ret = drm_bo_move_ttm(bo, evict, no_wait, mem);
+
+	} else if (dev->driver->bo_driver->move) {
+		ret = dev->driver->bo_driver->move(bo, evict, no_wait, mem);
+
+	} else {
+
+		ret = drm_bo_move_memcpy(bo, evict, no_wait, mem);
+
+	}
+
+	if (ret)
+		goto out_err;
+
+	if (old_is_pci || new_is_pci)
+		drm_bo_vm_post_move(bo);
+
+	if (bo->priv_flags & _DRM_BO_FLAG_EVICTED) {
+		ret =
+		    dev->driver->bo_driver->invalidate_caches(dev,
+							      bo->mem.flags);
+		if (ret)
+			DRM_ERROR("Can not flush read caches\n");
+	}
+
+	DRM_FLAG_MASKED(bo->priv_flags,
+			(evict) ? _DRM_BO_FLAG_EVICTED : 0,
+			_DRM_BO_FLAG_EVICTED);
+
+	if (bo->mem.mm_node)
+		bo->offset = (bo->mem.mm_node->start << PAGE_SHIFT) +
+			bm->man[bo->mem.mem_type].gpu_offset;
+
+
+	return 0;
+
+out_err:
+	if (old_is_pci || new_is_pci)
+		drm_bo_vm_post_move(bo);
+
+	new_man = &bm->man[bo->mem.mem_type];
+	if ((new_man->flags & _DRM_FLAG_MEMTYPE_FIXED) && bo->ttm) {
+		drm_ttm_unbind(bo->ttm);
+		drm_ttm_destroy(bo->ttm);
+		bo->ttm = NULL;
+	}
+
+	return ret;
+}
+
+/*
+ * Call bo->mutex locked.
+ * Wait until the buffer is idle.
+ */
+
+int drm_bo_wait(struct drm_buffer_object *bo, int lazy, int ignore_signals,
+		int no_wait)
+{
+	int ret;
+
+	DRM_ASSERT_LOCKED(&bo->mutex);
+
+	if (bo->fence) {
+		if (drm_fence_object_signaled(bo->fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		if (no_wait)
+			return -EBUSY;
+
+		ret = drm_fence_object_wait(bo->fence, lazy, ignore_signals,
+					  bo->fence_type);
+		if (ret)
+			return ret;
+
+		drm_fence_usage_deref_unlocked(&bo->fence);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_wait);
+
+static int drm_bo_expire_fence(struct drm_buffer_object *bo, int allow_errors)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	if (bo->fence) {
+		if (bm->nice_mode) {
+			unsigned long _end = jiffies + 3 * DRM_HZ;
+			int ret;
+			do {
+				ret = drm_bo_wait(bo, 0, 1, 0);
+				if (ret && allow_errors)
+					return ret;
+
+			} while (ret && !time_after_eq(jiffies, _end));
+
+			if (bo->fence) {
+				bm->nice_mode = 0;
+				DRM_ERROR("Detected GPU lockup or "
+					  "fence driver was taken down. "
+					  "Evicting buffer.\n");
+			}
+		}
+		if (bo->fence)
+			drm_fence_usage_deref_unlocked(&bo->fence);
+	}
+	return 0;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ * Attempts to remove all private references to a buffer by expiring its
+ * fence object and removing from lru lists and memory managers.
+ */
+
+static void drm_bo_cleanup_refs(struct drm_buffer_object *bo, int remove_all)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	atomic_inc(&bo->usage);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_lock(&bo->mutex);
+
+	DRM_FLAG_MASKED(bo->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+
+	if (bo->fence && drm_fence_object_signaled(bo->fence,
+						   bo->fence_type))
+		drm_fence_usage_deref_unlocked(&bo->fence);
+
+	if (bo->fence && remove_all)
+		(void)drm_bo_expire_fence(bo, 0);
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!atomic_dec_and_test(&bo->usage))
+		goto out;
+
+	if (!bo->fence) {
+		list_del_init(&bo->lru);
+		if (bo->mem.mm_node) {
+			drm_mm_put_block(bo->mem.mm_node);
+			if (bo->pinned_node == bo->mem.mm_node)
+				bo->pinned_node = NULL;
+			bo->mem.mm_node = NULL;
+		}
+		list_del_init(&bo->pinned_lru);
+		if (bo->pinned_node) {
+			drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = NULL;
+		}
+		list_del_init(&bo->ddestroy);
+		mutex_unlock(&bo->mutex);
+		drm_bo_destroy_locked(bo);
+		return;
+	}
+
+	if (list_empty(&bo->ddestroy)) {
+		drm_fence_object_flush(bo->fence, bo->fence_type);
+		list_add_tail(&bo->ddestroy, &bm->ddestroy);
+		schedule_delayed_work(&bm->wq,
+				      ((DRM_HZ / 100) < 1) ? 1 : DRM_HZ / 100);
+	}
+
+out:
+	mutex_unlock(&bo->mutex);
+	return;
+}
+
+/*
+ * Verify that refcount is 0 and that there are no internal references
+ * to the buffer object. Then destroy it.
+ */
+
+static void drm_bo_destroy_locked(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	if (list_empty(&bo->lru) && bo->mem.mm_node == NULL &&
+	    list_empty(&bo->pinned_lru) && bo->pinned_node == NULL &&
+	    list_empty(&bo->ddestroy) && atomic_read(&bo->usage) == 0) {
+		if (bo->fence != NULL) {
+			DRM_ERROR("Fence was non-zero.\n");
+			drm_bo_cleanup_refs(bo, 0);
+			return;
+		}
+
+
+		if (bo->ttm) {
+			drm_ttm_unbind(bo->ttm);
+			drm_ttm_destroy(bo->ttm);
+			bo->ttm = NULL;
+		}
+
+		atomic_dec(&bm->count);
+
+		drm_ctl_free(bo, sizeof(*bo), DRM_MEM_BUFOBJ);
+
+		return;
+	}
+
+	/*
+	 * Some stuff is still trying to reference the buffer object.
+	 * Get rid of those references.
+	 */
+
+	drm_bo_cleanup_refs(bo, 0);
+
+	return;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ */
+
+static void drm_bo_delayed_delete(struct drm_device *dev, int remove_all)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	struct drm_buffer_object *entry, *nentry;
+	struct list_head *list, *next;
+
+	list_for_each_safe(list, next, &bm->ddestroy) {
+		entry = list_entry(list, struct drm_buffer_object, ddestroy);
+
+		nentry = NULL;
+		if (next != &bm->ddestroy) {
+			nentry = list_entry(next, struct drm_buffer_object,
+					    ddestroy);
+			atomic_inc(&nentry->usage);
+		}
+
+		drm_bo_cleanup_refs(entry, remove_all);
+
+		if (nentry)
+			atomic_dec(&nentry->usage);
+	}
+}
+
+static void drm_bo_delayed_workqueue(struct work_struct *work)
+{
+	struct drm_buffer_manager *bm =
+	    container_of(work, struct drm_buffer_manager, wq.work);
+	struct drm_device *dev = container_of(bm, struct drm_device, bm);
+
+	DRM_DEBUG("Delayed delete Worker\n");
+
+	mutex_lock(&dev->struct_mutex);
+	if (!bm->initialized) {
+		mutex_unlock(&dev->struct_mutex);
+		return;
+	}
+	drm_bo_delayed_delete(dev, 0);
+	if (bm->initialized && !list_empty(&bm->ddestroy)) {
+		schedule_delayed_work(&bm->wq,
+				      ((DRM_HZ / 100) < 1) ? 1 : DRM_HZ / 100);
+	}
+	mutex_unlock(&dev->struct_mutex);
+}
+
+void drm_bo_usage_deref_locked(struct drm_buffer_object **bo)
+{
+	struct drm_buffer_object *tmp_bo = *bo;
+	bo = NULL;
+
+	DRM_ASSERT_LOCKED(&tmp_bo->dev->struct_mutex);
+
+	if (atomic_dec_and_test(&tmp_bo->usage))
+		drm_bo_destroy_locked(tmp_bo);
+}
+EXPORT_SYMBOL(drm_bo_usage_deref_locked);
+
+static void drm_bo_base_deref_locked(struct drm_file *file_priv,
+				     struct drm_user_object *uo)
+{
+	struct drm_buffer_object *bo =
+	    drm_user_object_entry(uo, struct drm_buffer_object, base);
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+
+	drm_bo_takedown_vm_locked(bo);
+	drm_bo_usage_deref_locked(&bo);
+}
+
+void drm_bo_usage_deref_unlocked(struct drm_buffer_object **bo)
+{
+	struct drm_buffer_object *tmp_bo = *bo;
+	struct drm_device *dev = tmp_bo->dev;
+
+	*bo = NULL;
+	if (atomic_dec_and_test(&tmp_bo->usage)) {
+		mutex_lock(&dev->struct_mutex);
+		if (atomic_read(&tmp_bo->usage) == 0)
+			drm_bo_destroy_locked(tmp_bo);
+		mutex_unlock(&dev->struct_mutex);
+	}
+}
+EXPORT_SYMBOL(drm_bo_usage_deref_unlocked);
+
+void drm_putback_buffer_objects(struct drm_device *dev)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct list_head *list = &bm->unfenced;
+	struct drm_buffer_object *entry, *next;
+
+	mutex_lock(&dev->struct_mutex);
+	list_for_each_entry_safe(entry, next, list, lru) {
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+
+		mutex_lock(&entry->mutex);
+		BUG_ON(!(entry->priv_flags & _DRM_BO_FLAG_UNFENCED));
+		mutex_lock(&dev->struct_mutex);
+
+		list_del_init(&entry->lru);
+		DRM_FLAG_MASKED(entry->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+		wake_up_all(&entry->event_queue);
+
+		/*
+		 * FIXME: Might want to put back on head of list
+		 * instead of tail here.
+		 */
+
+		drm_bo_add_to_lru(entry);
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_locked(&entry);
+	}
+	mutex_unlock(&dev->struct_mutex);
+}
+EXPORT_SYMBOL(drm_putback_buffer_objects);
+
+
+/*
+ * Note. The caller has to register (if applicable)
+ * and deregister fence object usage.
+ */
+
+int drm_fence_buffer_objects(struct drm_device *dev,
+			     struct list_head *list,
+			     uint32_t fence_flags,
+			     struct drm_fence_object *fence,
+			     struct drm_fence_object **used_fence)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *entry;
+	uint32_t fence_type = 0;
+	uint32_t fence_class = ~0;
+	int count = 0;
+	int ret = 0;
+	struct list_head *l;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!list)
+		list = &bm->unfenced;
+
+	if (fence)
+		fence_class = fence->fence_class;
+
+	list_for_each_entry(entry, list, lru) {
+		BUG_ON(!(entry->priv_flags & _DRM_BO_FLAG_UNFENCED));
+		fence_type |= entry->new_fence_type;
+		if (fence_class == ~0)
+			fence_class = entry->new_fence_class;
+		else if (entry->new_fence_class != fence_class) {
+			DRM_ERROR("Unmatching fence classes on unfenced list: "
+				  "%d and %d.\n",
+				  fence_class,
+				  entry->new_fence_class);
+			ret = -EINVAL;
+			goto out;
+		}
+		count++;
+	}
+
+	if (!count) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (fence) {
+		if ((fence_type & fence->type) != fence_type ||
+		    (fence->fence_class != fence_class)) {
+			DRM_ERROR("Given fence doesn't match buffers "
+				  "on unfenced list.\n");
+			ret = -EINVAL;
+			goto out;
+		}
+	} else {
+		mutex_unlock(&dev->struct_mutex);
+		ret = drm_fence_object_create(dev, fence_class, fence_type,
+					      fence_flags | DRM_FENCE_FLAG_EMIT,
+					      &fence);
+		mutex_lock(&dev->struct_mutex);
+		if (ret)
+			goto out;
+	}
+
+	count = 0;
+	l = list->next;
+	while (l != list) {
+		prefetch(l->next);
+		entry = list_entry(l, struct drm_buffer_object, lru);
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+		mutex_lock(&entry->mutex);
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(l);
+		if (entry->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			count++;
+			if (entry->fence)
+				drm_fence_usage_deref_locked(&entry->fence);
+			entry->fence = drm_fence_reference_locked(fence);
+			entry->fence_class = entry->new_fence_class;
+			entry->fence_type = entry->new_fence_type;
+			DRM_FLAG_MASKED(entry->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+			wake_up_all(&entry->event_queue);
+			drm_bo_add_to_lru(entry);
+		}
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_locked(&entry);
+		l = list->next;
+	}
+	DRM_DEBUG("Fenced %d buffers\n", count);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	*used_fence = fence;
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_buffer_objects);
+
+/*
+ * bo->mutex locked
+ */
+
+static int drm_bo_evict(struct drm_buffer_object *bo, unsigned mem_type,
+			int no_wait)
+{
+	int ret = 0;
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg evict_mem;
+
+	/*
+	 * Someone might have modified the buffer before we took the
+	 * buffer mutex.
+	 */
+
+	if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED)
+		goto out;
+	if (bo->mem.mem_type != mem_type)
+		goto out;
+
+	ret = drm_bo_wait(bo, 0, 0, no_wait);
+
+	if (ret && ret != -EAGAIN) {
+		DRM_ERROR("Failed to expire fence before "
+			  "buffer eviction.\n");
+		goto out;
+	}
+
+	evict_mem = bo->mem;
+	evict_mem.mm_node = NULL;
+
+	evict_mem = bo->mem;
+	evict_mem.proposed_flags = dev->driver->bo_driver->evict_flags(bo);
+	ret = drm_bo_mem_space(bo, &evict_mem, no_wait);
+
+	if (ret) {
+		if (ret != -EAGAIN)
+			DRM_ERROR("Failed to find memory space for "
+				  "buffer 0x%p eviction.\n", bo);
+		goto out;
+	}
+
+	ret = drm_bo_handle_move_mem(bo, &evict_mem, 1, no_wait);
+
+	if (ret) {
+		if (ret != -EAGAIN)
+			DRM_ERROR("Buffer eviction failed\n");
+		goto out;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	if (evict_mem.mm_node) {
+		if (evict_mem.mm_node != bo->pinned_node)
+			drm_mm_put_block(evict_mem.mm_node);
+		evict_mem.mm_node = NULL;
+	}
+	list_del(&bo->lru);
+	drm_bo_add_to_lru(bo);
+	mutex_unlock(&dev->struct_mutex);
+
+	DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_EVICTED,
+			_DRM_BO_FLAG_EVICTED);
+
+out:
+	return ret;
+}
+
+/**
+ * Repeatedly evict memory from the LRU for @mem_type until we create enough
+ * space, or we've evicted everything and there isn't enough space.
+ */
+static int drm_bo_mem_force_space(struct drm_device *dev,
+				  struct drm_bo_mem_reg *mem,
+				  uint32_t mem_type, int no_wait)
+{
+	struct drm_mm_node *node;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *entry;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+	struct list_head *lru;
+	unsigned long num_pages = mem->num_pages;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	do {
+		node = drm_mm_search_free(&man->manager, num_pages,
+					  mem->page_alignment, 1);
+		if (node)
+			break;
+
+		lru = &man->lru;
+		if (lru->next == lru)
+			break;
+
+		entry = list_entry(lru->next, struct drm_buffer_object, lru);
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+		mutex_lock(&entry->mutex);
+		BUG_ON(entry->mem.flags & (DRM_BO_FLAG_NO_MOVE | DRM_BO_FLAG_NO_EVICT));
+
+		ret = drm_bo_evict(entry, mem_type, no_wait);
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_unlocked(&entry);
+		if (ret)
+			return ret;
+		mutex_lock(&dev->struct_mutex);
+	} while (1);
+
+	if (!node) {
+		mutex_unlock(&dev->struct_mutex);
+		return -ENOMEM;
+	}
+
+	node = drm_mm_get_block(node, num_pages, mem->page_alignment);
+	mutex_unlock(&dev->struct_mutex);
+	mem->mm_node = node;
+	mem->mem_type = mem_type;
+	return 0;
+}
+
+static int drm_bo_mt_compatible(struct drm_mem_type_manager *man,
+				int disallow_fixed,
+				uint32_t mem_type,
+				uint64_t mask, uint32_t *res_mask)
+{
+	uint64_t cur_flags = drm_bo_type_flags(mem_type);
+	uint64_t flag_diff;
+
+	if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && disallow_fixed)
+		return 0;
+	if (man->flags & _DRM_FLAG_MEMTYPE_CACHED)
+		cur_flags |= DRM_BO_FLAG_CACHED;
+	if (man->flags & _DRM_FLAG_MEMTYPE_MAPPABLE)
+		cur_flags |= DRM_BO_FLAG_MAPPABLE;
+	if (man->flags & _DRM_FLAG_MEMTYPE_CSELECT)
+		DRM_FLAG_MASKED(cur_flags, mask, DRM_BO_FLAG_CACHED);
+
+	if ((cur_flags & mask & DRM_BO_MASK_MEM) == 0)
+		return 0;
+
+	if (mem_type == DRM_BO_MEM_LOCAL) {
+		*res_mask = cur_flags;
+		return 1;
+	}
+
+	flag_diff = (mask ^ cur_flags);
+	if (flag_diff & DRM_BO_FLAG_CACHED_MAPPED)
+		cur_flags |= DRM_BO_FLAG_CACHED_MAPPED;
+
+	if ((flag_diff & DRM_BO_FLAG_CACHED) &&
+	    (!(mask & DRM_BO_FLAG_CACHED) ||
+	     (mask & DRM_BO_FLAG_FORCE_CACHING)))
+		return 0;
+
+	if ((flag_diff & DRM_BO_FLAG_MAPPABLE) &&
+	    ((mask & DRM_BO_FLAG_MAPPABLE) ||
+	     (mask & DRM_BO_FLAG_FORCE_MAPPABLE)))
+		return 0;
+
+	*res_mask = cur_flags;
+	return 1;
+}
+
+/**
+ * Creates space for memory region @mem according to its type.
+ *
+ * This function first searches for free space in compatible memory types in
+ * the priority order defined by the driver.  If free space isn't found, then
+ * drm_bo_mem_force_space is attempted in priority order to evict and find
+ * space.
+ */
+int drm_bo_mem_space(struct drm_buffer_object *bo,
+		     struct drm_bo_mem_reg *mem, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man;
+
+	uint32_t num_prios = dev->driver->bo_driver->num_mem_type_prio;
+	const uint32_t *prios = dev->driver->bo_driver->mem_type_prio;
+	uint32_t i;
+	uint32_t mem_type = DRM_BO_MEM_LOCAL;
+	uint32_t cur_flags;
+	int type_found = 0;
+	int type_ok = 0;
+	int has_eagain = 0;
+	struct drm_mm_node *node = NULL;
+	int ret;
+
+	mem->mm_node = NULL;
+	for (i = 0; i < num_prios; ++i) {
+		mem_type = prios[i];
+		man = &bm->man[mem_type];
+
+		type_ok = drm_bo_mt_compatible(man,
+					       bo->type == drm_bo_type_user,
+					       mem_type, mem->proposed_flags,
+					       &cur_flags);
+
+		if (!type_ok)
+			continue;
+
+		if (mem_type == DRM_BO_MEM_LOCAL)
+			break;
+
+		if ((mem_type == bo->pinned_mem_type) &&
+		    (bo->pinned_node != NULL)) {
+			node = bo->pinned_node;
+			break;
+		}
+
+		mutex_lock(&dev->struct_mutex);
+		if (man->has_type && man->use_type) {
+			type_found = 1;
+			node = drm_mm_search_free(&man->manager, mem->num_pages,
+						  mem->page_alignment, 1);
+			if (node)
+				node = drm_mm_get_block(node, mem->num_pages,
+							mem->page_alignment);
+		}
+		mutex_unlock(&dev->struct_mutex);
+		if (node)
+			break;
+	}
+
+	if ((type_ok && (mem_type == DRM_BO_MEM_LOCAL)) || node) {
+		mem->mm_node = node;
+		mem->mem_type = mem_type;
+		mem->flags = cur_flags;
+		return 0;
+	}
+
+	if (!type_found)
+		return -EINVAL;
+
+	num_prios = dev->driver->bo_driver->num_mem_busy_prio;
+	prios = dev->driver->bo_driver->mem_busy_prio;
+
+	for (i = 0; i < num_prios; ++i) {
+		mem_type = prios[i];
+		man = &bm->man[mem_type];
+
+		if (!man->has_type)
+			continue;
+
+		if (!drm_bo_mt_compatible(man,
+					  bo->type == drm_bo_type_user,
+					  mem_type,
+					  mem->proposed_flags,
+					  &cur_flags))
+			continue;
+
+		ret = drm_bo_mem_force_space(dev, mem, mem_type, no_wait);
+
+		if (ret == 0 && mem->mm_node) {
+			mem->flags = cur_flags;
+			return 0;
+		}
+
+		if (ret == -EAGAIN)
+			has_eagain = 1;
+	}
+
+	ret = (has_eagain) ? -EAGAIN : -ENOMEM;
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_mem_space);
+
+/*
+ * drm_bo_propose_flags:
+ *
+ * @bo: the buffer object getting new flags
+ *
+ * @new_flags: the new set of proposed flag bits
+ *
+ * @new_mask: the mask of bits changed in new_flags
+ *
+ * Modify the proposed_flag bits in @bo
+ */
+static int drm_bo_modify_proposed_flags(struct drm_buffer_object *bo,
+					uint64_t new_flags, uint64_t new_mask)
+{
+	uint32_t new_access;
+
+	/* Copy unchanging bits from existing proposed_flags */
+	DRM_FLAG_MASKED(new_flags, bo->mem.proposed_flags, ~new_mask);
+
+	if (bo->type == drm_bo_type_user &&
+	    ((new_flags & (DRM_BO_FLAG_CACHED | DRM_BO_FLAG_FORCE_CACHING)) !=
+	     (DRM_BO_FLAG_CACHED | DRM_BO_FLAG_FORCE_CACHING))) {
+		DRM_ERROR("User buffers require cache-coherent memory.\n");
+		return -EINVAL;
+	}
+
+	if (bo->type != drm_bo_type_kernel && (new_mask & DRM_BO_FLAG_NO_EVICT) && !DRM_SUSER(DRM_CURPROC)) {
+		DRM_ERROR("DRM_BO_FLAG_NO_EVICT is only available to priviliged processes.\n");
+		return -EPERM;
+	}
+
+	if ((new_flags & DRM_BO_FLAG_NO_MOVE)) {
+		DRM_ERROR("DRM_BO_FLAG_NO_MOVE is not properly implemented yet.\n");
+		return -EPERM;
+	}
+
+	new_access = new_flags & (DRM_BO_FLAG_EXE | DRM_BO_FLAG_WRITE |
+				  DRM_BO_FLAG_READ);
+
+	if (new_access == 0) {
+		DRM_ERROR("Invalid buffer object rwx properties\n");
+		return -EINVAL;
+	}
+
+	bo->mem.proposed_flags = new_flags;
+	return 0;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ */
+
+struct drm_buffer_object *drm_lookup_buffer_object(struct drm_file *file_priv,
+					      uint32_t handle, int check_owner)
+{
+	struct drm_user_object *uo;
+	struct drm_buffer_object *bo;
+
+	uo = drm_lookup_user_object(file_priv, handle);
+
+	if (!uo || (uo->type != drm_buffer_type)) {
+		DRM_ERROR("Could not find buffer object 0x%08x\n", handle);
+		return NULL;
+	}
+
+	if (check_owner && file_priv != uo->owner) {
+		if (!drm_lookup_ref_object(file_priv, uo, _DRM_REF_USE))
+			return NULL;
+	}
+
+	bo = drm_user_object_entry(uo, struct drm_buffer_object, base);
+	atomic_inc(&bo->usage);
+	return bo;
+}
+EXPORT_SYMBOL(drm_lookup_buffer_object);
+
+/*
+ * Call bo->mutex locked.
+ * Returns 1 if the buffer is currently rendered to or from. 0 otherwise.
+ * Doesn't do any fence flushing as opposed to the drm_bo_busy function.
+ */
+
+static int drm_bo_quick_busy(struct drm_buffer_object *bo)
+{
+	struct drm_fence_object *fence = bo->fence;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (fence) {
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		return 1;
+	}
+	return 0;
+}
+
+/*
+ * Call bo->mutex locked.
+ * Returns 1 if the buffer is currently rendered to or from. 0 otherwise.
+ */
+
+static int drm_bo_busy(struct drm_buffer_object *bo)
+{
+	struct drm_fence_object *fence = bo->fence;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (fence) {
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		drm_fence_object_flush(fence, DRM_FENCE_TYPE_EXE);
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		return 1;
+	}
+	return 0;
+}
+
+static int drm_bo_evict_cached(struct drm_buffer_object *bo)
+{
+	int ret = 0;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (bo->mem.mm_node)
+		ret = drm_bo_evict(bo, DRM_BO_MEM_TT, 1);
+	return ret;
+}
+
+/*
+ * Wait until a buffer is unmapped.
+ */
+
+static int drm_bo_wait_unmapped(struct drm_buffer_object *bo, int no_wait)
+{
+	int ret = 0;
+
+	if ((atomic_read(&bo->mapped) >= 0) && no_wait)
+		return -EBUSY;
+
+	DRM_WAIT_ON(ret, bo->event_queue, 3 * DRM_HZ,
+		    atomic_read(&bo->mapped) == -1);
+
+	if (ret == -EINTR)
+		ret = -EAGAIN;
+
+	return ret;
+}
+
+static int drm_bo_check_unfenced(struct drm_buffer_object *bo)
+{
+	int ret;
+
+	mutex_lock(&bo->mutex);
+	ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+
+/*
+ * Wait until a buffer, scheduled to be fenced moves off the unfenced list.
+ * Until then, we cannot really do anything with it except delete it.
+ */
+
+static int drm_bo_wait_unfenced(struct drm_buffer_object *bo, int no_wait,
+				int eagain_if_wait)
+{
+	int ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+
+	if (ret && no_wait)
+		return -EBUSY;
+	else if (!ret)
+		return 0;
+
+	ret = 0;
+	mutex_unlock(&bo->mutex);
+	DRM_WAIT_ON(ret, bo->event_queue, 3 * DRM_HZ,
+		    !drm_bo_check_unfenced(bo));
+	mutex_lock(&bo->mutex);
+	if (ret == -EINTR)
+		return -EAGAIN;
+	ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (ret) {
+		DRM_ERROR("Timeout waiting for buffer to become fenced\n");
+		return -EBUSY;
+	}
+	if (eagain_if_wait)
+		return -EAGAIN;
+
+	return 0;
+}
+
+/*
+ * Fill in the ioctl reply argument with buffer info.
+ * Bo locked.
+ */
+
+static void drm_bo_fill_rep_arg(struct drm_buffer_object *bo,
+				struct drm_bo_info_rep *rep)
+{
+	if (!rep)
+		return;
+
+	rep->handle = bo->base.hash.key;
+	rep->flags = bo->mem.flags;
+	rep->size = bo->num_pages * PAGE_SIZE;
+	rep->offset = bo->offset;
+
+	/*
+	 * drm_bo_type_device buffers have user-visible
+	 * handles which can be used to share across
+	 * processes. Hand that back to the application
+	 */
+	if (bo->type == drm_bo_type_device)
+		rep->arg_handle = bo->map_list.user_token;
+	else
+		rep->arg_handle = 0;
+
+	rep->proposed_flags = bo->mem.proposed_flags;
+	rep->buffer_start = bo->buffer_start;
+	rep->fence_flags = bo->fence_type;
+	rep->rep_flags = 0;
+	rep->page_alignment = bo->mem.page_alignment;
+
+	if ((bo->priv_flags & _DRM_BO_FLAG_UNFENCED) || drm_bo_quick_busy(bo)) {
+		DRM_FLAG_MASKED(rep->rep_flags, DRM_BO_REP_BUSY,
+				DRM_BO_REP_BUSY);
+	}
+}
+
+/*
+ * Wait for buffer idle and register that we've mapped the buffer.
+ * Mapping is registered as a drm_ref_object with type _DRM_REF_TYPE1,
+ * so that if the client dies, the mapping is automatically
+ * unregistered.
+ */
+
+static int drm_buffer_object_map(struct drm_file *file_priv, uint32_t handle,
+				 uint32_t map_flags, unsigned hint,
+				 struct drm_bo_info_rep *rep)
+{
+	struct drm_buffer_object *bo;
+	struct drm_device *dev = file_priv->minor->dev;
+	int ret = 0;
+	int no_wait = hint & DRM_BO_HINT_DONT_BLOCK;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+	if (ret)
+		goto out;
+
+	/*
+	 * If this returns true, we are currently unmapped.
+	 * We need to do this test, because unmapping can
+	 * be done without the bo->mutex held.
+	 */
+
+	while (1) {
+		if (atomic_inc_and_test(&bo->mapped)) {
+			if (no_wait && drm_bo_busy(bo)) {
+				atomic_dec(&bo->mapped);
+				ret = -EBUSY;
+				goto out;
+			}
+			ret = drm_bo_wait(bo, 0, 0, no_wait);
+			if (ret) {
+				atomic_dec(&bo->mapped);
+				goto out;
+			}
+
+			if (bo->mem.flags & DRM_BO_FLAG_CACHED_MAPPED)
+				drm_bo_evict_cached(bo);
+
+			break;
+		} else if (bo->mem.flags & DRM_BO_FLAG_CACHED_MAPPED) {
+
+			/*
+			 * We are already mapped with different flags.
+			 * need to wait for unmap.
+			 */
+
+			ret = drm_bo_wait_unmapped(bo, no_wait);
+			if (ret)
+				goto out;
+
+			continue;
+		}
+		break;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_ref_object(file_priv, &bo->base, _DRM_REF_TYPE1);
+	mutex_unlock(&dev->struct_mutex);
+	if (ret) {
+		if (atomic_add_negative(-1, &bo->mapped))
+			wake_up_all(&bo->event_queue);
+
+	} else
+		drm_bo_fill_rep_arg(bo, rep);
+out:
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+
+static int drm_buffer_object_unmap(struct drm_file *file_priv, uint32_t handle)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	struct drm_ref_object *ro;
+	int ret = 0;
+
+	mutex_lock(&dev->struct_mutex);
+
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	if (!bo) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ro = drm_lookup_ref_object(file_priv, &bo->base, _DRM_REF_TYPE1);
+	if (!ro) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	drm_remove_ref_object(file_priv, ro);
+	drm_bo_usage_deref_locked(&bo);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/*
+ * Call struct-sem locked.
+ */
+
+static void drm_buffer_user_object_unmap(struct drm_file *file_priv,
+					 struct drm_user_object *uo,
+					 enum drm_ref_type action)
+{
+	struct drm_buffer_object *bo =
+	    drm_user_object_entry(uo, struct drm_buffer_object, base);
+
+	/*
+	 * We DON'T want to take the bo->lock here, because we want to
+	 * hold it when we wait for unmapped buffer.
+	 */
+
+	BUG_ON(action != _DRM_REF_TYPE1);
+
+	if (atomic_add_negative(-1, &bo->mapped))
+		wake_up_all(&bo->event_queue);
+}
+
+/*
+ * bo->mutex locked.
+ * Note that new_mem_flags are NOT transferred to the bo->mem.proposed_flags.
+ */
+
+int drm_bo_move_buffer(struct drm_buffer_object *bo, uint64_t new_mem_flags,
+		       int no_wait, int move_unfenced)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = 0;
+	struct drm_bo_mem_reg mem;
+	/*
+	 * Flush outstanding fences.
+	 */
+
+	drm_bo_busy(bo);
+
+	/*
+	 * Wait for outstanding fences.
+	 */
+
+	ret = drm_bo_wait(bo, 0, 0, no_wait);
+	if (ret)
+		return ret;
+
+	mem.num_pages = bo->num_pages;
+	mem.size = mem.num_pages << PAGE_SHIFT;
+	mem.proposed_flags = new_mem_flags;
+	mem.page_alignment = bo->mem.page_alignment;
+
+	mutex_lock(&bm->evict_mutex);
+	mutex_lock(&dev->struct_mutex);
+	list_del_init(&bo->lru);
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * Determine where to move the buffer.
+	 */
+	ret = drm_bo_mem_space(bo, &mem, no_wait);
+	if (ret)
+		goto out_unlock;
+
+	ret = drm_bo_handle_move_mem(bo, &mem, 0, no_wait);
+
+out_unlock:
+	mutex_lock(&dev->struct_mutex);
+	if (ret || !move_unfenced) {
+		if (mem.mm_node) {
+			if (mem.mm_node != bo->pinned_node)
+				drm_mm_put_block(mem.mm_node);
+			mem.mm_node = NULL;
+		}
+		drm_bo_add_to_lru(bo);
+		if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			wake_up_all(&bo->event_queue);
+			DRM_FLAG_MASKED(bo->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+		}
+	} else {
+		list_add_tail(&bo->lru, &bm->unfenced);
+		DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_UNFENCED,
+				_DRM_BO_FLAG_UNFENCED);
+	}
+	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&bm->evict_mutex);
+	return ret;
+}
+
+static int drm_bo_mem_compat(struct drm_bo_mem_reg *mem)
+{
+	uint32_t flag_diff = (mem->proposed_flags ^ mem->flags);
+
+	if ((mem->proposed_flags & mem->flags & DRM_BO_MASK_MEM) == 0)
+		return 0;
+	if ((flag_diff & DRM_BO_FLAG_CACHED) &&
+	    (/* !(mem->proposed_flags & DRM_BO_FLAG_CACHED) ||*/
+	     (mem->proposed_flags & DRM_BO_FLAG_FORCE_CACHING)))
+		return 0;
+
+	if ((flag_diff & DRM_BO_FLAG_MAPPABLE) &&
+	    ((mem->proposed_flags & DRM_BO_FLAG_MAPPABLE) ||
+	     (mem->proposed_flags & DRM_BO_FLAG_FORCE_MAPPABLE)))
+		return 0;
+	return 1;
+}
+
+/**
+ * drm_buffer_object_validate:
+ *
+ * @bo: the buffer object to modify
+ *
+ * @fence_class: the new fence class covering this buffer
+ *
+ * @move_unfenced: a boolean indicating whether switching the
+ * memory space of this buffer should cause the buffer to
+ * be placed on the unfenced list.
+ *
+ * @no_wait: whether this function should return -EBUSY instead
+ * of waiting.
+ *
+ * Change buffer access parameters. This can involve moving
+ * the buffer to the correct memory type, pinning the buffer
+ * or changing the class/type of fence covering this buffer
+ *
+ * Must be called with bo locked.
+ */
+
+static int drm_buffer_object_validate(struct drm_buffer_object *bo,
+				      uint32_t fence_class,
+				      int move_unfenced, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	uint32_t ftype;
+	int ret;
+
+	DRM_DEBUG("Proposed flags 0x%016llx, Old flags 0x%016llx\n",
+		  (unsigned long long) bo->mem.proposed_flags,
+		  (unsigned long long) bo->mem.flags);
+
+	ret = driver->fence_type(bo, &fence_class, &ftype);
+
+	if (ret) {
+		DRM_ERROR("Driver did not support given buffer permissions\n");
+		return ret;
+	}
+
+	/*
+	 * We're switching command submission mechanism,
+	 * or cannot simply rely on the hardware serializing for us.
+	 *
+	 * Insert a driver-dependant barrier or wait for buffer idle.
+	 */
+
+	if ((fence_class != bo->fence_class) ||
+	    ((ftype ^ bo->fence_type) & bo->fence_type)) {
+
+		ret = -EINVAL;
+		if (driver->command_stream_barrier) {
+			ret = driver->command_stream_barrier(bo,
+							     fence_class,
+							     ftype,
+							     no_wait);
+		}
+		if (ret)
+			ret = drm_bo_wait(bo, 0, 0, no_wait);
+
+		if (ret)
+			return ret;
+
+	}
+
+	bo->new_fence_class = fence_class;
+	bo->new_fence_type = ftype;
+
+	ret = drm_bo_wait_unmapped(bo, no_wait);
+	if (ret) {
+		DRM_ERROR("Timed out waiting for buffer unmap.\n");
+		return ret;
+	}
+
+	/*
+	 * Check whether we need to move buffer.
+	 */
+
+	if (!drm_bo_mem_compat(&bo->mem)) {
+		ret = drm_bo_move_buffer(bo, bo->mem.proposed_flags, no_wait,
+					 move_unfenced);
+		if (ret) {
+			if (ret != -EAGAIN)
+				DRM_ERROR("Failed moving buffer.\n");
+			return ret;
+		}
+	}
+
+	/*
+	 * Pinned buffers.
+	 */
+
+	if (bo->mem.proposed_flags & (DRM_BO_FLAG_NO_EVICT | DRM_BO_FLAG_NO_MOVE)) {
+		bo->pinned_mem_type = bo->mem.mem_type;
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&bo->pinned_lru);
+		drm_bo_add_to_pinned_lru(bo);
+
+		if (bo->pinned_node != bo->mem.mm_node) {
+			if (bo->pinned_node != NULL)
+				drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = bo->mem.mm_node;
+		}
+
+		mutex_unlock(&dev->struct_mutex);
+
+	} else if (bo->pinned_node != NULL) {
+
+		mutex_lock(&dev->struct_mutex);
+
+		if (bo->pinned_node != bo->mem.mm_node)
+			drm_mm_put_block(bo->pinned_node);
+
+		list_del_init(&bo->pinned_lru);
+		bo->pinned_node = NULL;
+		mutex_unlock(&dev->struct_mutex);
+
+	}
+
+	/*
+	 * We might need to add a TTM.
+	 */
+
+	if (bo->mem.mem_type == DRM_BO_MEM_LOCAL && bo->ttm == NULL) {
+		ret = drm_bo_add_ttm(bo);
+		if (ret)
+			return ret;
+	}
+	/*
+	 * Validation has succeeded, move the access and other
+	 * non-mapping-related flag bits from the proposed flags to
+	 * the active flags
+	 */
+
+	DRM_FLAG_MASKED(bo->mem.flags, bo->mem.proposed_flags, ~DRM_BO_MASK_MEMTYPE);
+
+	/*
+	 * Finally, adjust lru to be sure.
+	 */
+
+	mutex_lock(&dev->struct_mutex);
+	list_del(&bo->lru);
+	if (move_unfenced) {
+		list_add_tail(&bo->lru, &bm->unfenced);
+		DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_UNFENCED,
+				_DRM_BO_FLAG_UNFENCED);
+	} else {
+		drm_bo_add_to_lru(bo);
+		if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			wake_up_all(&bo->event_queue);
+			DRM_FLAG_MASKED(bo->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+		}
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
+/**
+ * drm_bo_do_validate:
+ *
+ * @bo:	the buffer object
+ *
+ * @flags: access rights, mapping parameters and cacheability. See
+ * the DRM_BO_FLAG_* values in drm.h
+ *
+ * @mask: Which flag values to change; this allows callers to modify
+ * things without knowing the current state of other flags.
+ *
+ * @hint: changes the proceedure for this operation, see the DRM_BO_HINT_*
+ * values in drm.h.
+ *
+ * @fence_class: a driver-specific way of doing fences. Presumably,
+ * this would be used if the driver had more than one submission and
+ * fencing mechanism. At this point, there isn't any use of this
+ * from the user mode code.
+ *
+ * @rep: To be stuffed with the reply from validation
+ *
+ * 'validate' a buffer object. This changes where the buffer is
+ * located, along with changing access modes.
+ */
+
+int drm_bo_do_validate(struct drm_buffer_object *bo,
+		       uint64_t flags, uint64_t mask, uint32_t hint,
+		       uint32_t fence_class,
+		       struct drm_bo_info_rep *rep)
+{
+	int ret;
+	int no_wait = (hint & DRM_BO_HINT_DONT_BLOCK) != 0;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+
+	if (ret)
+		goto out;
+
+	ret = drm_bo_modify_proposed_flags (bo, flags, mask);
+	if (ret)
+		goto out;
+
+	ret = drm_buffer_object_validate(bo,
+					 fence_class,
+					 !(hint & DRM_BO_HINT_DONT_FENCE),
+					 no_wait);
+out:
+	if (rep)
+		drm_bo_fill_rep_arg(bo, rep);
+
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_do_validate);
+
+/**
+ * drm_bo_handle_validate
+ *
+ * @file_priv: the drm file private, used to get a handle to the user context
+ *
+ * @handle: the buffer object handle
+ *
+ * @flags: access rights, mapping parameters and cacheability. See
+ * the DRM_BO_FLAG_* values in drm.h
+ *
+ * @mask: Which flag values to change; this allows callers to modify
+ * things without knowing the current state of other flags.
+ *
+ * @hint: changes the proceedure for this operation, see the DRM_BO_HINT_*
+ * values in drm.h.
+ *
+ * @fence_class: a driver-specific way of doing fences. Presumably,
+ * this would be used if the driver had more than one submission and
+ * fencing mechanism. At this point, there isn't any use of this
+ * from the user mode code.
+ *
+ * @use_old_fence_class: don't change fence class, pull it from the buffer object
+ *
+ * @rep: To be stuffed with the reply from validation
+ *
+ * @bp_rep: To be stuffed with the buffer object pointer
+ *
+ * Perform drm_bo_do_validate on a buffer referenced by a user-space handle.
+ * Some permissions checking is done on the parameters, otherwise this
+ * is a thin wrapper.
+ */
+
+int drm_bo_handle_validate(struct drm_file *file_priv, uint32_t handle,
+			   uint64_t flags, uint64_t mask,
+			   uint32_t hint,
+			   uint32_t fence_class,
+			   int use_old_fence_class,
+			   struct drm_bo_info_rep *rep,
+			   struct drm_buffer_object **bo_rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	if (use_old_fence_class)
+		fence_class = bo->fence_class;
+
+	/*
+	 * Only allow creator to change shared buffer mask.
+	 */
+
+	if (bo->base.owner != file_priv)
+		mask &= ~(DRM_BO_FLAG_NO_EVICT | DRM_BO_FLAG_NO_MOVE);
+
+
+	ret = drm_bo_do_validate(bo, flags, mask, hint, fence_class, rep);
+
+	if (!ret && bo_rep)
+		*bo_rep = bo;
+	else
+		drm_bo_usage_deref_unlocked(&bo);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_handle_validate);
+
+static int drm_bo_handle_info(struct drm_file *file_priv, uint32_t handle,
+			      struct drm_bo_info_rep *rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	if (!(bo->priv_flags & _DRM_BO_FLAG_UNFENCED))
+		(void)drm_bo_busy(bo);
+	drm_bo_fill_rep_arg(bo, rep);
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return 0;
+}
+
+static int drm_bo_handle_wait(struct drm_file *file_priv, uint32_t handle,
+			      uint32_t hint,
+			      struct drm_bo_info_rep *rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	int no_wait = hint & DRM_BO_HINT_DONT_BLOCK;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+	if (ret)
+		goto out;
+	ret = drm_bo_wait(bo, hint & DRM_BO_HINT_WAIT_LAZY, 0, no_wait);
+	if (ret)
+		goto out;
+
+	drm_bo_fill_rep_arg(bo, rep);
+
+out:
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+
+int drm_buffer_object_create(struct drm_device *dev,
+			     unsigned long size,
+			     enum drm_bo_type type,
+			     uint64_t flags,
+			     uint32_t hint,
+			     uint32_t page_alignment,
+			     unsigned long buffer_start,
+			     struct drm_buffer_object **buf_obj)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *bo;
+	int ret = 0;
+	unsigned long num_pages;
+
+	size += buffer_start & ~PAGE_MASK;
+	num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	if (num_pages == 0) {
+		DRM_ERROR("Illegal buffer object size.\n");
+		return -EINVAL;
+	}
+
+	bo = drm_ctl_calloc(1, sizeof(*bo), DRM_MEM_BUFOBJ);
+
+	if (!bo)
+		return -ENOMEM;
+
+	mutex_init(&bo->mutex);
+	mutex_lock(&bo->mutex);
+
+	atomic_set(&bo->usage, 1);
+	atomic_set(&bo->mapped, -1);
+	DRM_INIT_WAITQUEUE(&bo->event_queue);
+	INIT_LIST_HEAD(&bo->lru);
+	INIT_LIST_HEAD(&bo->pinned_lru);
+	INIT_LIST_HEAD(&bo->ddestroy);
+	bo->dev = dev;
+	bo->type = type;
+	bo->num_pages = num_pages;
+	bo->mem.mem_type = DRM_BO_MEM_LOCAL;
+	bo->mem.num_pages = bo->num_pages;
+	bo->mem.mm_node = NULL;
+	bo->mem.page_alignment = page_alignment;
+	bo->buffer_start = buffer_start & PAGE_MASK;
+	bo->priv_flags = 0;
+	bo->mem.flags = (DRM_BO_FLAG_MEM_LOCAL | DRM_BO_FLAG_CACHED |
+			 DRM_BO_FLAG_MAPPABLE);
+	bo->mem.proposed_flags = 0;
+	atomic_inc(&bm->count);
+	/*
+	 * Use drm_bo_modify_proposed_flags to error-check the proposed flags
+	 */
+	ret = drm_bo_modify_proposed_flags(bo, flags, flags);
+	if (ret)
+		goto out_err;
+
+	/*
+	 * For drm_bo_type_device buffers, allocate
+	 * address space from the device so that applications
+	 * can mmap the buffer from there
+	 */
+	if (bo->type == drm_bo_type_device) {
+		mutex_lock(&dev->struct_mutex);
+		ret = drm_bo_setup_vm_locked(bo);
+		mutex_unlock(&dev->struct_mutex);
+		if (ret)
+			goto out_err;
+	}
+
+	ret = drm_buffer_object_validate(bo, 0, 0, hint & DRM_BO_HINT_DONT_BLOCK);
+	if (ret)
+		goto out_err;
+
+	mutex_unlock(&bo->mutex);
+	*buf_obj = bo;
+	return 0;
+
+out_err:
+	mutex_unlock(&bo->mutex);
+
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+EXPORT_SYMBOL(drm_buffer_object_create);
+
+
+static int drm_bo_add_user_object(struct drm_file *file_priv,
+				  struct drm_buffer_object *bo, int shareable)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(file_priv, &bo->base, shareable);
+	if (ret)
+		goto out;
+
+	bo->base.remove = drm_bo_base_deref_locked;
+	bo->base.type = drm_buffer_type;
+	bo->base.ref_struct_locked = NULL;
+	bo->base.unref = drm_buffer_user_object_unmap;
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+int drm_bo_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_create_arg *arg = data;
+	struct drm_bo_create_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	struct drm_buffer_object *entry;
+	enum drm_bo_type bo_type;
+	int ret = 0;
+
+	DRM_DEBUG("drm_bo_create_ioctl: %dkb, %dkb align\n",
+	    (int)(req->size / 1024), req->page_alignment * 4);
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * If the buffer creation request comes in with a starting address,
+	 * that points at the desired user pages to map. Otherwise, create
+	 * a drm_bo_type_device buffer, which uses pages allocated from the kernel
+	 */
+	bo_type = (req->buffer_start) ? drm_bo_type_user : drm_bo_type_device;
+
+	/*
+	 * User buffers cannot be shared
+	 */
+	if (bo_type == drm_bo_type_user)
+		req->flags &= ~DRM_BO_FLAG_SHAREABLE;
+
+	ret = drm_buffer_object_create(file_priv->minor->dev,
+				       req->size, bo_type, req->flags,
+				       req->hint, req->page_alignment,
+				       req->buffer_start, &entry);
+	if (ret)
+		goto out;
+
+	ret = drm_bo_add_user_object(file_priv, entry,
+				     req->flags & DRM_BO_FLAG_SHAREABLE);
+	if (ret) {
+		drm_bo_usage_deref_unlocked(&entry);
+		goto out;
+	}
+
+	mutex_lock(&entry->mutex);
+	drm_bo_fill_rep_arg(entry, rep);
+	mutex_unlock(&entry->mutex);
+
+out:
+	return ret;
+}
+
+int drm_bo_setstatus_ioctl(struct drm_device *dev,
+			   void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	/*
+	 * validate the buffer. note that 'fence_class' will be unused
+	 * as we pass use_old_fence_class=1 here. Note also that
+	 * the libdrm API doesn't pass fence_class to the kernel,
+	 * so it's a good thing it isn't used here.
+	 */
+	ret = drm_bo_handle_validate(file_priv, req->handle,
+				     req->flags,
+				     req->mask,
+				     req->hint | DRM_BO_HINT_DONT_FENCE,
+				     req->fence_class, 1,
+				     rep, NULL);
+
+	(void) drm_bo_read_unlock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_map_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_buffer_object_map(file_priv, req->handle, req->mask,
+				    req->hint, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_unmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_handle_arg *arg = data;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_buffer_object_unmap(file_priv, arg->handle);
+	return ret;
+}
+
+
+int drm_bo_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_reference_info_arg *arg = data;
+	struct drm_bo_handle_arg *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	struct drm_user_object *uo;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_ref(file_priv, req->handle,
+				  drm_buffer_type, &uo);
+	if (ret)
+		return ret;
+
+	ret = drm_bo_handle_info(file_priv, req->handle, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_handle_arg *arg = data;
+	int ret = 0;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_unref(file_priv, arg->handle, drm_buffer_type);
+	return ret;
+}
+
+int drm_bo_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_reference_info_arg *arg = data;
+	struct drm_bo_handle_arg *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_handle_info(file_priv, req->handle, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_wait_idle_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_handle_wait(file_priv, req->handle,
+				 req->hint, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int drm_bo_leave_list(struct drm_buffer_object *bo,
+			     uint32_t mem_type,
+			     int free_pinned,
+			     int allow_errors)
+{
+	struct drm_device *dev = bo->dev;
+	int ret = 0;
+
+	mutex_lock(&bo->mutex);
+
+	ret = drm_bo_expire_fence(bo, allow_errors);
+	if (ret)
+		goto out;
+
+	if (free_pinned) {
+		DRM_FLAG_MASKED(bo->mem.flags, 0, DRM_BO_FLAG_NO_MOVE);
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&bo->pinned_lru);
+		if (bo->pinned_node == bo->mem.mm_node)
+			bo->pinned_node = NULL;
+		if (bo->pinned_node != NULL) {
+			drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = NULL;
+		}
+		mutex_unlock(&dev->struct_mutex);
+	}
+
+	if (bo->mem.flags & DRM_BO_FLAG_NO_EVICT) {
+		DRM_ERROR("A DRM_BO_NO_EVICT buffer present at "
+			  "cleanup. Removing flag and evicting.\n");
+		bo->mem.flags &= ~DRM_BO_FLAG_NO_EVICT;
+		bo->mem.proposed_flags &= ~DRM_BO_FLAG_NO_EVICT;
+	}
+
+	if (bo->mem.mem_type == mem_type)
+		ret = drm_bo_evict(bo, mem_type, 0);
+
+	if (ret) {
+		if (allow_errors) {
+			goto out;
+		} else {
+			ret = 0;
+			DRM_ERROR("Cleanup eviction failed\n");
+		}
+	}
+
+out:
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+
+
+static struct drm_buffer_object *drm_bo_entry(struct list_head *list,
+					 int pinned_list)
+{
+	if (pinned_list)
+		return list_entry(list, struct drm_buffer_object, pinned_lru);
+	else
+		return list_entry(list, struct drm_buffer_object, lru);
+}
+
+/*
+ * dev->struct_mutex locked.
+ */
+
+static int drm_bo_force_list_clean(struct drm_device *dev,
+				   struct list_head *head,
+				   unsigned mem_type,
+				   int free_pinned,
+				   int allow_errors,
+				   int pinned_list)
+{
+	struct list_head *list, *next, *prev;
+	struct drm_buffer_object *entry, *nentry;
+	int ret;
+	int do_restart;
+
+	/*
+	 * The list traversal is a bit odd here, because an item may
+	 * disappear from the list when we release the struct_mutex or
+	 * when we decrease the usage count. Also we're not guaranteed
+	 * to drain pinned lists, so we can't always restart.
+	 */
+
+restart:
+	nentry = NULL;
+	list_for_each_safe(list, next, head) {
+		prev = list->prev;
+
+		entry = (nentry != NULL) ? nentry: drm_bo_entry(list, pinned_list);
+		atomic_inc(&entry->usage);
+		if (nentry) {
+			atomic_dec(&nentry->usage);
+			nentry = NULL;
+		}
+
+		/*
+		 * Protect the next item from destruction, so we can check
+		 * its list pointers later on.
+		 */
+
+		if (next != head) {
+			nentry = drm_bo_entry(next, pinned_list);
+			atomic_inc(&nentry->usage);
+		}
+		mutex_unlock(&dev->struct_mutex);
+
+		ret = drm_bo_leave_list(entry, mem_type, free_pinned,
+					allow_errors);
+		mutex_lock(&dev->struct_mutex);
+
+		drm_bo_usage_deref_locked(&entry);
+		if (ret)
+			return ret;
+
+		/*
+		 * Has the next item disappeared from the list?
+		 */
+
+		do_restart = ((next->prev != list) && (next->prev != prev));
+
+		if (nentry != NULL && do_restart)
+			drm_bo_usage_deref_locked(&nentry);
+
+		if (do_restart)
+			goto restart;
+	}
+	return 0;
+}
+
+int drm_bo_clean_mm(struct drm_device *dev, unsigned mem_type, int kern_clean)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+	int ret = -EINVAL;
+
+	if (mem_type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory type %d\n", mem_type);
+		return ret;
+	}
+
+	if (!man->has_type) {
+		DRM_ERROR("Trying to take down uninitialized "
+			  "memory manager type %u\n", mem_type);
+		return ret;
+	}
+
+	if ((man->kern_init_type) && (kern_clean == 0)) {
+		DRM_ERROR("Trying to take down kernel initialized "
+			  "memory manager type %u\n", mem_type);
+		return -EPERM;
+	}
+
+	man->use_type = 0;
+	man->has_type = 0;
+
+	ret = 0;
+	if (mem_type > 0) {
+		BUG_ON(!list_empty(&bm->unfenced));
+		drm_bo_force_list_clean(dev, &man->lru, mem_type, 1, 0, 0);
+		drm_bo_force_list_clean(dev, &man->pinned, mem_type, 1, 0, 1);
+
+		if (drm_mm_clean(&man->manager)) {
+			drm_mm_takedown(&man->manager);
+		} else {
+			ret = -EBUSY;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_clean_mm);
+
+/**
+ *Evict all buffers of a particular mem_type, but leave memory manager
+ *regions for NO_MOVE buffers intact. New buffers cannot be added at this
+ *point since we have the hardware lock.
+ */
+
+static int drm_bo_lock_mm(struct drm_device *dev, unsigned mem_type)
+{
+	int ret;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+
+	if (mem_type == 0 || mem_type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory manager memory type %u.\n", mem_type);
+		return -EINVAL;
+	}
+
+	if (!man->has_type) {
+		DRM_ERROR("Memory type %u has not been initialized.\n",
+			  mem_type);
+		return 0;
+	}
+
+	ret = drm_bo_force_list_clean(dev, &man->lru, mem_type, 0, 1, 0);
+	if (ret)
+		return ret;
+	ret = drm_bo_force_list_clean(dev, &man->pinned, mem_type, 0, 1, 1);
+
+	return ret;
+}
+
+int drm_bo_init_mm(struct drm_device *dev, unsigned type,
+		   unsigned long p_offset, unsigned long p_size,
+		   int kern_init)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = -EINVAL;
+	struct drm_mem_type_manager *man;
+
+	if (type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory type %d\n", type);
+		return ret;
+	}
+
+	man = &bm->man[type];
+	if (man->has_type) {
+		DRM_ERROR("Memory manager already initialized for type %d\n",
+			  type);
+		return ret;
+	}
+
+	ret = dev->driver->bo_driver->init_mem_type(dev, type, man);
+	if (ret)
+		return ret;
+
+	ret = 0;
+	if (type != DRM_BO_MEM_LOCAL) {
+		if (!p_size) {
+			DRM_ERROR("Zero size memory manager type %d\n", type);
+			return ret;
+		}
+		ret = drm_mm_init(&man->manager, p_offset, p_size);
+		if (ret)
+			return ret;
+	}
+	man->has_type = 1;
+	man->use_type = 1;
+	man->kern_init_type = kern_init;
+	man->size = p_size;
+
+	INIT_LIST_HEAD(&man->lru);
+	INIT_LIST_HEAD(&man->pinned);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_init_mm);
+
+/*
+ * This function is intended to be called on drm driver unload.
+ * If you decide to call it from lastclose, you must protect the call
+ * from a potentially racing drm_bo_driver_init in firstopen.
+ * (This may happen on X server restart).
+ */
+
+int drm_bo_driver_finish(struct drm_device *dev)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = 0;
+	unsigned i = DRM_BO_MEM_TYPES;
+	struct drm_mem_type_manager *man;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!bm->initialized)
+		goto out;
+	bm->initialized = 0;
+
+	while (i--) {
+		man = &bm->man[i];
+		if (man->has_type) {
+			man->use_type = 0;
+			if ((i != DRM_BO_MEM_LOCAL) && drm_bo_clean_mm(dev, i, 1)) {
+				ret = -EBUSY;
+				DRM_ERROR("DRM memory manager type %d "
+					  "is not clean.\n", i);
+			}
+			man->has_type = 0;
+		}
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!cancel_delayed_work(&bm->wq))
+		flush_scheduled_work();
+
+	mutex_lock(&dev->struct_mutex);
+	drm_bo_delayed_delete(dev, 1);
+	if (list_empty(&bm->ddestroy))
+		DRM_DEBUG("Delayed destroy list was clean\n");
+
+	if (list_empty(&bm->man[0].lru))
+		DRM_DEBUG("Swap list was clean\n");
+
+	if (list_empty(&bm->man[0].pinned))
+		DRM_DEBUG("NO_MOVE list was clean\n");
+
+	if (list_empty(&bm->unfenced))
+		DRM_DEBUG("Unfenced list was clean\n");
+
+	__free_page(bm->dummy_read_page);
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_driver_finish);
+
+/*
+ * This function is intended to be called on drm driver load.
+ * If you decide to call it from firstopen, you must protect the call
+ * from a potentially racing drm_bo_driver_finish in lastclose.
+ * (This may happen on X server restart).
+ */
+
+int drm_bo_driver_init(struct drm_device *dev)
+{
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = -EINVAL;
+
+	bm->dummy_read_page = NULL;
+	drm_bo_init_lock(&bm->bm_lock);
+	mutex_lock(&dev->struct_mutex);
+	if (!driver)
+		goto out_unlock;
+
+	bm->dummy_read_page = alloc_page(__GFP_ZERO | GFP_DMA32);
+	if (!bm->dummy_read_page) {
+		ret = -ENOMEM;
+		goto out_unlock;
+	}
+
+	/*
+	 * Initialize the system memory buffer type.
+	 * Other types need to be driver / IOCTL initialized.
+	 */
+	ret = drm_bo_init_mm(dev, DRM_BO_MEM_LOCAL, 0, 0, 1);
+	if (ret)
+		goto out_unlock;
+
+	INIT_DELAYED_WORK(&bm->wq, drm_bo_delayed_workqueue);
+	bm->initialized = 1;
+	bm->nice_mode = 1;
+	atomic_set(&bm->count, 0);
+	bm->cur_pages = 0;
+	INIT_LIST_HEAD(&bm->unfenced);
+	INIT_LIST_HEAD(&bm->ddestroy);
+out_unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_driver_init);
+
+int drm_mm_init_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_init_arg *arg = data;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_write_lock(&bm->bm_lock, file_priv);
+	if (ret)
+		return ret;
+
+	ret = -EINVAL;
+	if (arg->magic != DRM_BO_INIT_MAGIC) {
+		DRM_ERROR("You are using an old libdrm that is not compatible with\n"
+			  "\tthe kernel DRM module. Please upgrade your libdrm.\n");
+		return -EINVAL;
+	}
+	if (arg->major != DRM_BO_INIT_MAJOR) {
+		DRM_ERROR("libdrm and kernel DRM buffer object interface major\n"
+			  "\tversion don't match. Got %d, expected %d.\n",
+			  arg->major, DRM_BO_INIT_MAJOR);
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	if (!bm->initialized) {
+		DRM_ERROR("DRM memory manager was not initialized.\n");
+		goto out;
+	}
+	if (arg->mem_type == 0) {
+		DRM_ERROR("System memory buffers already initialized.\n");
+		goto out;
+	}
+	ret = drm_bo_init_mm(dev, arg->mem_type,
+			     arg->p_offset, arg->p_size, 0);
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	(void) drm_bo_write_unlock(&bm->bm_lock, file_priv);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_mm_takedown_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_write_lock(&bm->bm_lock, file_priv);
+	if (ret)
+		return ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = -EINVAL;
+	if (!bm->initialized) {
+		DRM_ERROR("DRM memory manager was not initialized\n");
+		goto out;
+	}
+	if (arg->mem_type == 0) {
+		DRM_ERROR("No takedown for System memory buffers.\n");
+		goto out;
+	}
+	ret = 0;
+	if ((ret = drm_bo_clean_mm(dev, arg->mem_type, 0))) {
+		if (ret == -EINVAL)
+			DRM_ERROR("Memory manager type %d not clean. "
+				  "Delaying takedown\n", arg->mem_type);
+		ret = 0;
+	}
+out:
+	mutex_unlock(&dev->struct_mutex);
+	(void) drm_bo_write_unlock(&bm->bm_lock, file_priv);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_mm_lock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_IGNORE_NO_EVICT) {
+		DRM_ERROR("Lock flag DRM_BO_LOCK_IGNORE_NO_EVICT not supported yet.\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_UNLOCK_BM) {
+		ret = drm_bo_write_lock(&dev->bm.bm_lock, file_priv);
+		if (ret)
+			return ret;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_bo_lock_mm(dev, arg->mem_type);
+	mutex_unlock(&dev->struct_mutex);
+	if (ret) {
+		(void) drm_bo_write_unlock(&dev->bm.bm_lock, file_priv);
+		return ret;
+	}
+
+	return 0;
+}
+
+int drm_mm_unlock_ioctl(struct drm_device *dev,
+			void *data,
+			struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_UNLOCK_BM) {
+		ret = drm_bo_write_unlock(&dev->bm.bm_lock, file_priv);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+int drm_mm_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_info_arg *arg = data;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	struct drm_mem_type_manager *man;
+	int ret = 0;
+	int mem_type = arg->mem_type;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	if (mem_type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory type %d\n", arg->mem_type);
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	if (!bm->initialized) {
+		DRM_ERROR("DRM memory manager was not initialized\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+
+	man = &bm->man[arg->mem_type];
+
+	arg->p_size = man->size;
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+     
+	return ret;
+}
+/*
+ * buffer object vm functions.
+ */
+
+int drm_mem_reg_is_pci(struct drm_device *dev, struct drm_bo_mem_reg *mem)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+
+	if (!(man->flags & _DRM_FLAG_MEMTYPE_FIXED)) {
+		if (mem->mem_type == DRM_BO_MEM_LOCAL)
+			return 0;
+
+		if (man->flags & _DRM_FLAG_MEMTYPE_CMA)
+			return 0;
+
+		if (mem->flags & DRM_BO_FLAG_CACHED)
+			return 0;
+	}
+	return 1;
+}
+EXPORT_SYMBOL(drm_mem_reg_is_pci);
+
+/**
+ * \c Get the PCI offset for the buffer object memory.
+ *
+ * \param bo The buffer object.
+ * \param bus_base On return the base of the PCI region
+ * \param bus_offset On return the byte offset into the PCI region
+ * \param bus_size On return the byte size of the buffer object or zero if
+ *     the buffer object memory is not accessible through a PCI region.
+ * \return Failure indication.
+ *
+ * Returns -EINVAL if the buffer object is currently not mappable.
+ * Otherwise returns zero.
+ */
+
+int drm_bo_pci_offset(struct drm_device *dev,
+		      struct drm_bo_mem_reg *mem,
+		      unsigned long *bus_base,
+		      unsigned long *bus_offset, unsigned long *bus_size)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+
+	*bus_size = 0;
+	if (!(man->flags & _DRM_FLAG_MEMTYPE_MAPPABLE))
+		return -EINVAL;
+
+	if (drm_mem_reg_is_pci(dev, mem)) {
+		*bus_offset = mem->mm_node->start << PAGE_SHIFT;
+		*bus_size = mem->num_pages << PAGE_SHIFT;
+		*bus_base = man->io_offset;
+	}
+
+	return 0;
+}
+
+/**
+ * \c Kill all user-space virtual mappings of this buffer object.
+ *
+ * \param bo The buffer object.
+ *
+ * Call bo->mutex locked.
+ */
+
+void drm_bo_unmap_virtual(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	loff_t offset = ((loff_t) bo->map_list.hash.key) << PAGE_SHIFT;
+	loff_t holelen = ((loff_t) bo->mem.num_pages) << PAGE_SHIFT;
+
+	if (!dev->dev_mapping)
+		return;
+
+	unmap_mapping_range(dev->dev_mapping, offset, holelen, 1);
+}
+
+/**
+ * drm_bo_takedown_vm_locked:
+ *
+ * @bo: the buffer object to remove any drm device mapping
+ *
+ * Remove any associated vm mapping on the drm device node that
+ * would have been created for a drm_bo_type_device buffer
+ */
+static void drm_bo_takedown_vm_locked(struct drm_buffer_object *bo)
+{
+	struct drm_map_list *list;
+	drm_local_map_t *map;
+	struct drm_device *dev = bo->dev;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	if (bo->type != drm_bo_type_device)
+		return;
+
+	list = &bo->map_list;
+	if (list->user_token) {
+		drm_ht_remove_item(&dev->map_hash, &list->hash);
+		list->user_token = 0;
+	}
+	if (list->file_offset_node) {
+		drm_mm_put_block(list->file_offset_node);
+		list->file_offset_node = NULL;
+	}
+
+	map = list->map;
+	if (!map)
+		return;
+
+	drm_ctl_free(map, sizeof(*map), DRM_MEM_BUFOBJ);
+	list->map = NULL;
+	list->user_token = 0ULL;
+	drm_bo_usage_deref_locked(&bo);
+}
+
+/**
+ * drm_bo_setup_vm_locked:
+ *
+ * @bo: the buffer to allocate address space for
+ *
+ * Allocate address space in the drm device so that applications
+ * can mmap the buffer and access the contents. This only
+ * applies to drm_bo_type_device objects as others are not
+ * placed in the drm device address space.
+ */
+static int drm_bo_setup_vm_locked(struct drm_buffer_object *bo)
+{
+	struct drm_map_list *list = &bo->map_list;
+	drm_local_map_t *map;
+	struct drm_device *dev = bo->dev;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	list->map = drm_ctl_calloc(1, sizeof(*map), DRM_MEM_BUFOBJ);
+	if (!list->map)
+		return -ENOMEM;
+
+	map = list->map;
+	map->offset = 0;
+	map->type = _DRM_TTM;
+	map->flags = _DRM_REMOVABLE;
+	map->size = bo->mem.num_pages * PAGE_SIZE;
+	atomic_inc(&bo->usage);
+	map->handle = (void *)bo;
+
+	list->file_offset_node = drm_mm_search_free(&dev->offset_manager,
+						    bo->mem.num_pages, 0, 0);
+
+	if (!list->file_offset_node) {
+		drm_bo_takedown_vm_locked(bo);
+		return -ENOMEM;
+	}
+
+	list->file_offset_node = drm_mm_get_block(list->file_offset_node,
+						  bo->mem.num_pages, 0);
+
+	list->hash.key = list->file_offset_node->start;
+	if (drm_ht_insert_item(&dev->map_hash, &list->hash)) {
+		drm_bo_takedown_vm_locked(bo);
+		return -ENOMEM;
+	}
+
+	list->user_token = ((uint64_t) list->hash.key) << PAGE_SHIFT;
+
+	return 0;
+}
+
+int drm_bo_version_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv)
+{
+	struct drm_bo_version_arg *arg = (struct drm_bo_version_arg *)data;
+
+	arg->major = DRM_BO_INIT_MAJOR;
+	arg->minor = DRM_BO_INIT_MINOR;
+	arg->patchlevel = DRM_BO_INIT_PATCH;
+
+	return 0;
+}
diff --git a/drivers/char/drm/drm_bo_lock.c b/drivers/char/drm/drm_bo_lock.c
new file mode 100644
index 0000000..186db12
--- /dev/null
+++ b/drivers/char/drm/drm_bo_lock.c
@@ -0,0 +1,175 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+/*
+ * This file implements a simple replacement for the buffer manager use
+ * of the heavyweight hardware lock.
+ * The lock is a read-write lock. Taking it in read mode is fast, and
+ * intended for in-kernel use only.
+ * Taking it in write mode is slow.
+ *
+ * The write mode is used only when there is a need to block all
+ * user-space processes from allocating a
+ * new memory area.
+ * Typical use in write mode is X server VT switching, and it's allowed
+ * to leave kernel space with the write lock held. If a user-space process
+ * dies while having the write-lock, it will be released during the file
+ * descriptor release.
+ *
+ * The read lock is typically placed at the start of an IOCTL- or
+ * user-space callable function that may end up allocating a memory area.
+ * This includes setstatus, super-ioctls and no_pfn; the latter may move
+ * unmappable regions to mappable. It's a bug to leave kernel space with the
+ * read lock held.
+ *
+ * Both read- and write lock taking is interruptible for low signal-delivery
+ * latency. The locking functions will return -EAGAIN if interrupted by a
+ * signal.
+ *
+ * Locking order: The lock should be taken BEFORE any kernel mutexes
+ * or spinlocks.
+ */
+
+#include "drmP.h"
+
+void drm_bo_init_lock(struct drm_bo_lock *lock)
+{
+	DRM_INIT_WAITQUEUE(&lock->queue);
+	atomic_set(&lock->write_lock_pending, 0);
+	atomic_set(&lock->readers, 0);
+}
+
+void drm_bo_read_unlock(struct drm_bo_lock *lock)
+{
+	if (unlikely(atomic_add_negative(-1, &lock->readers)))
+		BUG();
+	if (atomic_read(&lock->readers) == 0)
+		wake_up_interruptible(&lock->queue);
+}
+EXPORT_SYMBOL(drm_bo_read_unlock);
+
+int drm_bo_read_lock(struct drm_bo_lock *lock)
+{
+	while (unlikely(atomic_read(&lock->write_lock_pending) != 0)) {
+		int ret;
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_read(&lock->write_lock_pending) == 0);
+		if (ret)
+			return -EAGAIN;
+	}
+
+	while (unlikely(!atomic_add_unless(&lock->readers, 1, -1))) {
+		int ret;
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_add_unless(&lock->readers, 1, -1));
+		if (ret)
+			return -EAGAIN;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_read_lock);
+
+static int __drm_bo_write_unlock(struct drm_bo_lock *lock)
+{
+	if (unlikely(atomic_cmpxchg(&lock->readers, -1, 0) != -1))
+		return -EINVAL;
+	if (unlikely(atomic_cmpxchg(&lock->write_lock_pending, 1, 0) != 1))
+		return -EINVAL;
+	wake_up_interruptible(&lock->queue);
+	return 0;
+}
+
+static void drm_bo_write_lock_remove(struct drm_file *file_priv,
+				     struct drm_user_object *item)
+{
+	struct drm_bo_lock *lock = container_of(item, struct drm_bo_lock, base);
+	int ret;
+
+	ret = __drm_bo_write_unlock(lock);
+	BUG_ON(ret);
+}
+
+int drm_bo_write_lock(struct drm_bo_lock *lock, struct drm_file *file_priv)
+{
+	int ret = 0;
+	struct drm_device *dev;
+
+	if (unlikely(atomic_cmpxchg(&lock->write_lock_pending, 0, 1) != 0))
+		return -EINVAL;
+
+	while (unlikely(atomic_cmpxchg(&lock->readers, 0, -1) != 0)) {
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_cmpxchg(&lock->readers, 0, -1) == 0);
+
+		if (ret) {
+			atomic_set(&lock->write_lock_pending, 0);
+			wake_up_interruptible(&lock->queue);
+			return -EAGAIN;
+		}
+	}
+
+	/*
+	 * Add a dummy user-object, the destructor of which will
+	 * make sure the lock is released if the client dies
+	 * while holding it.
+	 */
+
+	dev = file_priv->minor->dev;
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(file_priv, &lock->base, 0);
+	lock->base.remove = &drm_bo_write_lock_remove;
+	lock->base.type = drm_lock_type;
+	if (ret)
+		(void)__drm_bo_write_unlock(lock);
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int drm_bo_write_unlock(struct drm_bo_lock *lock, struct drm_file *file_priv)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_ref_object *ro;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (lock->base.owner != file_priv) {
+		mutex_unlock(&dev->struct_mutex);
+		return -EINVAL;
+	}
+	ro = drm_lookup_ref_object(file_priv, &lock->base, _DRM_REF_USE);
+	BUG_ON(!ro);
+	drm_remove_ref_object(file_priv, ro);
+	lock->base.owner = NULL;
+
+	mutex_unlock(&dev->struct_mutex);
+	return 0;
+}
diff --git a/drivers/char/drm/drm_bo_move.c b/drivers/char/drm/drm_bo_move.c
new file mode 100644
index 0000000..9c6ca95
--- /dev/null
+++ b/drivers/char/drm/drm_bo_move.c
@@ -0,0 +1,610 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+/**
+ * Free the old memory node unless it's a pinned region and we
+ * have not been requested to free also pinned regions.
+ */
+
+static void drm_bo_free_old_node(struct drm_buffer_object *bo)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+
+	if (old_mem->mm_node && (old_mem->mm_node != bo->pinned_node)) {
+		mutex_lock(&bo->dev->struct_mutex);
+		drm_mm_put_block(old_mem->mm_node);
+		old_mem->mm_node = NULL;
+		mutex_unlock(&bo->dev->struct_mutex);
+	}
+	old_mem->mm_node = NULL;
+}
+
+int drm_bo_move_ttm(struct drm_buffer_object *bo,
+		    int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_ttm *ttm = bo->ttm;
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_proposed_flags = old_mem->proposed_flags;
+	int ret;
+
+	if (old_mem->mem_type == DRM_BO_MEM_TT) {
+		if (evict)
+			drm_ttm_evict(ttm);
+		else
+			drm_ttm_unbind(ttm);
+
+		drm_bo_free_old_node(bo);
+		DRM_FLAG_MASKED(old_mem->flags,
+				DRM_BO_FLAG_CACHED | DRM_BO_FLAG_MAPPABLE |
+				DRM_BO_FLAG_MEM_LOCAL, DRM_BO_MASK_MEMTYPE);
+		old_mem->mem_type = DRM_BO_MEM_LOCAL;
+		save_flags = old_mem->flags;
+	}
+	if (new_mem->mem_type != DRM_BO_MEM_LOCAL) {
+		ret = drm_ttm_bind(ttm, new_mem);
+		if (ret)
+			return ret;
+	}
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->proposed_flags = save_proposed_flags;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_move_ttm);
+
+/**
+ * \c Return a kernel virtual address to the buffer object PCI memory.
+ *
+ * \param bo The buffer object.
+ * \return Failure indication.
+ *
+ * Returns -EINVAL if the buffer object is currently not mappable.
+ * Returns -ENOMEM if the ioremap operation failed.
+ * Otherwise returns zero.
+ *
+ * After a successfull call, bo->iomap contains the virtual address, or NULL
+ * if the buffer object content is not accessible through PCI space.
+ * Call bo->mutex locked.
+ */
+
+int drm_mem_reg_ioremap(struct drm_device *dev, struct drm_bo_mem_reg *mem,
+			void **virtual)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+	unsigned long bus_offset;
+	unsigned long bus_size;
+	unsigned long bus_base;
+	int ret;
+	void *addr;
+
+	*virtual = NULL;
+	ret = drm_bo_pci_offset(dev, mem, &bus_base, &bus_offset, &bus_size);
+	if (ret || bus_size == 0)
+		return ret;
+
+	if (!(man->flags & _DRM_FLAG_NEEDS_IOREMAP))
+		addr = (void *)(((u8 *) man->io_addr) + bus_offset);
+	else {
+		addr = ioremap_nocache(bus_base + bus_offset, bus_size);
+		if (!addr)
+			return -ENOMEM;
+	}
+	*virtual = addr;
+	return 0;
+}
+EXPORT_SYMBOL(drm_mem_reg_ioremap);
+
+/**
+ * \c Unmap mapping obtained using drm_bo_ioremap
+ *
+ * \param bo The buffer object.
+ *
+ * Call bo->mutex locked.
+ */
+
+void drm_mem_reg_iounmap(struct drm_device *dev, struct drm_bo_mem_reg *mem,
+			 void *virtual)
+{
+	struct drm_buffer_manager *bm;
+	struct drm_mem_type_manager *man;
+
+	bm = &dev->bm;
+	man = &bm->man[mem->mem_type];
+
+	if (virtual && (man->flags & _DRM_FLAG_NEEDS_IOREMAP))
+		iounmap(virtual);
+}
+EXPORT_SYMBOL(drm_mem_reg_iounmap);
+
+static int drm_copy_io_page(void *dst, void *src, unsigned long page)
+{
+	uint32_t *dstP =
+	    (uint32_t *) ((unsigned long)dst + (page << PAGE_SHIFT));
+	uint32_t *srcP =
+	    (uint32_t *) ((unsigned long)src + (page << PAGE_SHIFT));
+
+	int i;
+	for (i = 0; i < PAGE_SIZE / sizeof(uint32_t); ++i)
+		iowrite32(ioread32(srcP++), dstP++);
+	return 0;
+}
+
+static int drm_copy_io_ttm_page(struct drm_ttm *ttm, void *src,
+				unsigned long page)
+{
+	struct page *d = drm_ttm_get_page(ttm, page);
+	void *dst;
+
+	if (!d)
+		return -ENOMEM;
+
+	src = (void *)((unsigned long)src + (page << PAGE_SHIFT));
+	dst = kmap(d);
+	if (!dst)
+		return -ENOMEM;
+
+	memcpy_fromio(dst, src, PAGE_SIZE);
+	kunmap(d);
+	return 0;
+}
+
+static int drm_copy_ttm_io_page(struct drm_ttm *ttm, void *dst, unsigned long page)
+{
+	struct page *s = drm_ttm_get_page(ttm, page);
+	void *src;
+
+	if (!s)
+		return -ENOMEM;
+
+	dst = (void *)((unsigned long)dst + (page << PAGE_SHIFT));
+	src = kmap(s);
+	if (!src)
+		return -ENOMEM;
+
+	memcpy_toio(dst, src, PAGE_SIZE);
+	kunmap(s);
+	return 0;
+}
+
+int drm_bo_move_memcpy(struct drm_buffer_object *bo,
+		       int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_mem_type_manager *man = &dev->bm.man[new_mem->mem_type];
+	struct drm_ttm *ttm = bo->ttm;
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	struct drm_bo_mem_reg old_copy = *old_mem;
+	void *old_iomap;
+	void *new_iomap;
+	int ret;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_proposed_flags = old_mem->proposed_flags;
+	unsigned long i;
+	unsigned long page;
+	unsigned long add = 0;
+	int dir;
+
+	ret = drm_mem_reg_ioremap(dev, old_mem, &old_iomap);
+	if (ret)
+		return ret;
+	ret = drm_mem_reg_ioremap(dev, new_mem, &new_iomap);
+	if (ret)
+		goto out;
+
+	if (old_iomap == NULL && new_iomap == NULL)
+		goto out2;
+	if (old_iomap == NULL && ttm == NULL)
+		goto out2;
+
+	add = 0;
+	dir = 1;
+
+	if ((old_mem->mem_type == new_mem->mem_type) &&
+	    (new_mem->mm_node->start <
+	     old_mem->mm_node->start + old_mem->mm_node->size)) {
+		dir = -1;
+		add = new_mem->num_pages - 1;
+	}
+
+	for (i = 0; i < new_mem->num_pages; ++i) {
+		page = i * dir + add;
+		if (old_iomap == NULL)
+			ret = drm_copy_ttm_io_page(ttm, new_iomap, page);
+		else if (new_iomap == NULL)
+			ret = drm_copy_io_ttm_page(ttm, old_iomap, page);
+		else
+			ret = drm_copy_io_page(new_iomap, old_iomap, page);
+		if (ret)
+			goto out1;
+	}
+	mb();
+out2:
+	drm_bo_free_old_node(bo);
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->proposed_flags = save_proposed_flags;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+
+	if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (ttm != NULL)) {
+		drm_ttm_unbind(ttm);
+		drm_ttm_destroy(ttm);
+		bo->ttm = NULL;
+	}
+
+out1:
+	drm_mem_reg_iounmap(dev, new_mem, new_iomap);
+out:
+	drm_mem_reg_iounmap(dev, &old_copy, old_iomap);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_move_memcpy);
+
+/*
+ * Transfer a buffer object's memory and LRU status to a newly
+ * created object. User-space references remains with the old
+ * object. Call bo->mutex locked.
+ */
+
+int drm_buffer_object_transfer(struct drm_buffer_object *bo,
+			       struct drm_buffer_object **new_obj)
+{
+	struct drm_buffer_object *fbo;
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	fbo = drm_ctl_calloc(1, sizeof(*fbo), DRM_MEM_BUFOBJ);
+	if (!fbo)
+		return -ENOMEM;
+
+	*fbo = *bo;
+	mutex_init(&fbo->mutex);
+	mutex_lock(&fbo->mutex);
+	mutex_lock(&dev->struct_mutex);
+
+	DRM_INIT_WAITQUEUE(&bo->event_queue);
+	INIT_LIST_HEAD(&fbo->ddestroy);
+	INIT_LIST_HEAD(&fbo->lru);
+	INIT_LIST_HEAD(&fbo->pinned_lru);
+
+	fbo->fence = drm_fence_reference_locked(bo->fence);
+	fbo->pinned_node = NULL;
+	fbo->mem.mm_node->private = (void *)fbo;
+	atomic_set(&fbo->usage, 1);
+	atomic_inc(&bm->count);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&fbo->mutex);
+
+	*new_obj = fbo;
+	return 0;
+}
+
+/*
+ * Since move is underway, we need to block signals in this function.
+ * We cannot restart until it has finished.
+ */
+
+int drm_bo_move_accel_cleanup(struct drm_buffer_object *bo,
+			      int evict, int no_wait, uint32_t fence_class,
+			      uint32_t fence_type, uint32_t fence_flags,
+			      struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_mem_type_manager *man = &dev->bm.man[new_mem->mem_type];
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	int ret;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_proposed_flags = old_mem->proposed_flags;
+	struct drm_buffer_object *old_obj;
+
+	if (bo->fence)
+		drm_fence_usage_deref_unlocked(&bo->fence);
+	ret = drm_fence_object_create(dev, fence_class, fence_type,
+				      fence_flags | DRM_FENCE_FLAG_EMIT,
+				      &bo->fence);
+	bo->fence_type = fence_type;
+	if (ret)
+		return ret;
+
+	if (evict || ((bo->mem.mm_node == bo->pinned_node) &&
+		      bo->mem.mm_node != NULL)) {
+		ret = drm_bo_wait(bo, 0, 1, 0);
+		if (ret)
+			return ret;
+
+		drm_bo_free_old_node(bo);
+
+		if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (bo->ttm != NULL)) {
+			drm_ttm_unbind(bo->ttm);
+			drm_ttm_destroy(bo->ttm);
+			bo->ttm = NULL;
+		}
+	} else {
+
+		/* This should help pipeline ordinary buffer moves.
+		 *
+		 * Hang old buffer memory on a new buffer object,
+		 * and leave it to be released when the GPU
+		 * operation has completed.
+		 */
+
+		ret = drm_buffer_object_transfer(bo, &old_obj);
+
+		if (ret)
+			return ret;
+
+		if (!(man->flags & _DRM_FLAG_MEMTYPE_FIXED))
+			old_obj->ttm = NULL;
+		else
+			bo->ttm = NULL;
+
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&old_obj->lru);
+		DRM_FLAG_MASKED(bo->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+		drm_bo_add_to_lru(old_obj);
+
+		drm_bo_usage_deref_locked(&old_obj);
+		mutex_unlock(&dev->struct_mutex);
+
+	}
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->proposed_flags = save_proposed_flags;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_move_accel_cleanup);
+
+int drm_bo_same_page(unsigned long offset,
+		     unsigned long offset2)
+{
+	return (offset & PAGE_MASK) == (offset2 & PAGE_MASK);
+}
+EXPORT_SYMBOL(drm_bo_same_page);
+
+unsigned long drm_bo_offset_end(unsigned long offset,
+				unsigned long end)
+{
+	offset = (offset + PAGE_SIZE) & PAGE_MASK;
+	return (end < offset) ? end : offset;
+}
+EXPORT_SYMBOL(drm_bo_offset_end);
+
+static pgprot_t drm_kernel_io_prot(uint32_t map_type)
+{
+	pgprot_t tmp = PAGE_KERNEL;
+
+#if defined(__i386__) || defined(__x86_64__)
+#ifdef USE_PAT_WC
+#warning using pat
+	if (drm_use_pat() && map_type == _DRM_TTM) {
+		pgprot_val(tmp) |= _PAGE_PAT;
+		return tmp;
+	}
+#endif
+	if (boot_cpu_data.x86 > 3 && map_type != _DRM_AGP) {
+		pgprot_val(tmp) |= _PAGE_PCD;
+		pgprot_val(tmp) &= ~_PAGE_PWT;
+	}
+#elif defined(__powerpc__)
+	pgprot_val(tmp) |= _PAGE_NO_CACHE;
+	if (map_type == _DRM_REGISTERS)
+		pgprot_val(tmp) |= _PAGE_GUARDED;
+#endif
+#if defined(__ia64__)
+	if (map_type == _DRM_TTM)
+		tmp = pgprot_writecombine(tmp);
+	else
+		tmp = pgprot_noncached(tmp);
+#endif
+	return tmp;
+}
+
+static int drm_bo_ioremap(struct drm_buffer_object *bo, unsigned long bus_base,
+			  unsigned long bus_offset, unsigned long bus_size,
+			  struct drm_bo_kmap_obj *map)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg *mem = &bo->mem;
+	struct drm_mem_type_manager *man = &dev->bm.man[mem->mem_type];
+
+	if (!(man->flags & _DRM_FLAG_NEEDS_IOREMAP)) {
+		map->bo_kmap_type = bo_map_premapped;
+		map->virtual = (void *)(((u8 *) man->io_addr) + bus_offset);
+	} else {
+		map->bo_kmap_type = bo_map_iomap;
+		map->virtual = ioremap_nocache(bus_base + bus_offset, bus_size);
+	}
+	return (!map->virtual) ? -ENOMEM : 0;
+}
+
+static int drm_bo_kmap_ttm(struct drm_buffer_object *bo,
+			   unsigned long start_page, unsigned long num_pages,
+			   struct drm_bo_kmap_obj *map)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg *mem = &bo->mem;
+	struct drm_mem_type_manager *man = &dev->bm.man[mem->mem_type];
+	pgprot_t prot;
+	struct drm_ttm *ttm = bo->ttm;
+	struct page *d;
+	int i;
+
+	BUG_ON(!ttm);
+
+	if (num_pages == 1 && (mem->flags & DRM_BO_FLAG_CACHED)) {
+
+		/*
+		 * We're mapping a single page, and the desired
+		 * page protection is consistent with the bo.
+		 */
+
+		map->bo_kmap_type = bo_map_kmap;
+		map->page = drm_ttm_get_page(ttm, start_page);
+		map->virtual = kmap(map->page);
+	} else {
+		/*
+		 * Populate the part we're mapping;
+		 */
+
+		for (i = start_page; i < start_page + num_pages; ++i) {
+			d = drm_ttm_get_page(ttm, i);
+			if (!d)
+				return -ENOMEM;
+		}
+
+		/*
+		 * We need to use vmap to get the desired page protection
+		 * or to make the buffer object look contigous.
+		 */
+
+		prot = (mem->flags & DRM_BO_FLAG_CACHED) ?
+			PAGE_KERNEL :
+			drm_kernel_io_prot(man->drm_bus_maptype);
+		map->bo_kmap_type = bo_map_vmap;
+		map->virtual = vmap(ttm->pages + start_page,
+				    num_pages, 0, prot);
+	}
+	return (!map->virtual) ? -ENOMEM : 0;
+}
+
+/*
+ * This function is to be used for kernel mapping of buffer objects.
+ * It chooses the appropriate mapping method depending on the memory type
+ * and caching policy the buffer currently has.
+ * Mapping multiple pages or buffers that live in io memory is a bit slow and
+ * consumes vmalloc space. Be restrictive with such mappings.
+ * Mapping single pages usually returns the logical kernel address,
+ * (which is fast)
+ * BUG may use slower temporary mappings for high memory pages or
+ * uncached / write-combined pages.
+ *
+ * The function fills in a drm_bo_kmap_obj which can be used to return the
+ * kernel virtual address of the buffer.
+ *
+ * Code servicing a non-priviliged user request is only allowed to map one
+ * page at a time. We might need to implement a better scheme to stop such
+ * processes from consuming all vmalloc space.
+ */
+
+int drm_bo_kmap(struct drm_buffer_object *bo, unsigned long start_page,
+		unsigned long num_pages, struct drm_bo_kmap_obj *map)
+{
+	int ret;
+	unsigned long bus_base;
+	unsigned long bus_offset;
+	unsigned long bus_size;
+
+	map->virtual = NULL;
+
+	if (num_pages > bo->num_pages)
+		return -EINVAL;
+	if (start_page > bo->num_pages)
+		return -EINVAL;
+	ret = drm_bo_pci_offset(bo->dev, &bo->mem, &bus_base,
+				&bus_offset, &bus_size);
+
+	if (ret)
+		return ret;
+
+	if (bus_size == 0) {
+		return drm_bo_kmap_ttm(bo, start_page, num_pages, map);
+	} else {
+		bus_offset += start_page << PAGE_SHIFT;
+		bus_size = num_pages << PAGE_SHIFT;
+		return drm_bo_ioremap(bo, bus_base, bus_offset, bus_size, map);
+	}
+}
+EXPORT_SYMBOL(drm_bo_kmap);
+
+void drm_bo_kunmap(struct drm_bo_kmap_obj *map)
+{
+	if (!map->virtual)
+		return;
+
+	switch (map->bo_kmap_type) {
+	case bo_map_iomap:
+		iounmap(map->virtual);
+		break;
+	case bo_map_vmap:
+		vunmap(map->virtual);
+		break;
+	case bo_map_kmap:
+		kunmap(map->page);
+		break;
+	case bo_map_premapped:
+		break;
+	default:
+		BUG();
+	}
+	map->virtual = NULL;
+	map->page = NULL;
+}
+EXPORT_SYMBOL(drm_bo_kunmap);
+
+int drm_bo_pfn_prot(struct drm_buffer_object *bo,
+		    unsigned long dst_offset,
+		    unsigned long *pfn,
+		    pgprot_t *prot)
+{
+	struct drm_bo_mem_reg *mem = &bo->mem;
+	struct drm_device *dev = bo->dev;
+	unsigned long bus_offset;
+	unsigned long bus_size;
+	unsigned long bus_base;
+	struct drm_mem_type_manager *man = &dev->bm.man[mem->mem_type];
+	int ret;
+
+	ret = drm_bo_pci_offset(dev, mem, &bus_base, &bus_offset,
+				&bus_size);
+	if (ret)
+		return -EINVAL;
+
+	if (bus_size != 0)
+		*pfn = (bus_base + bus_offset + dst_offset) >> PAGE_SHIFT;
+	else if (!bo->ttm)
+		return -EINVAL;
+	else
+		*pfn = page_to_pfn(drm_ttm_get_page(bo->ttm, dst_offset >> PAGE_SHIFT));
+
+	*prot = (mem->flags & DRM_BO_FLAG_CACHED) ?
+		PAGE_KERNEL : drm_kernel_io_prot(man->drm_bus_maptype);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_pfn_prot);
+
diff --git a/drivers/char/drm/drm_bufs.c b/drivers/char/drm/drm_bufs.c
index bde64b8..b8e1dd6 100644
--- a/drivers/char/drm/drm_bufs.c
+++ b/drivers/char/drm/drm_bufs.c
@@ -46,23 +46,33 @@ unsigned long drm_get_resource_len(struct drm_device *dev, unsigned int resource
 {
 	return pci_resource_len(dev->pdev, resource);
 }
-
 EXPORT_SYMBOL(drm_get_resource_len);
 
-static struct drm_map_list *drm_find_matching_map(struct drm_device *dev,
-					     drm_local_map_t *map)
+static __inline__ bool map_has_master(struct drm_device *dev,
+				      struct drm_map_list *entry)
+{
+	if (entry->master && entry->master == dev->primary->master)
+		return true;
+	if (!entry->master && entry->map->flags & _DRM_DRIVER)
+		return true;
+	return false;
+}
+
+struct drm_map_list *drm_find_matching_map(struct drm_device *dev,
+					   drm_local_map_t *map)
 {
 	struct drm_map_list *entry;
 	list_for_each_entry(entry, &dev->maplist, head) {
-		if (entry->map && map->type == entry->map->type &&
-		    ((entry->map->offset == map->offset) ||
-		     (map->type == _DRM_SHM && map->flags==_DRM_CONTAINS_LOCK))) {
+		if (entry->map && map_has_master(dev, entry) && (map->type == entry->map->type) &&
+		    ((entry->map->offset == map->offset) || 
+		     ((map->type == _DRM_SHM) && (map->flags&_DRM_CONTAINS_LOCK)))) {
 			return entry;
 		}
 	}
 
 	return NULL;
 }
+EXPORT_SYMBOL(drm_find_matching_map);
 
 static int drm_map_handle(struct drm_device *dev, struct drm_hash_item *hash,
 			  unsigned long user_token, int hashed_handle)
@@ -101,10 +111,10 @@ static int drm_map_handle(struct drm_device *dev, struct drm_hash_item *hash,
  * type.  Adds the map to the map list drm_device::maplist. Adds MTRR's where
  * applicable and if supported by the kernel.
  */
-static int drm_addmap_core(struct drm_device * dev, unsigned int offset,
+static int drm_addmap_core(struct drm_device *dev, unsigned int offset,
 			   unsigned int size, enum drm_map_type type,
 			   enum drm_map_flags flags,
-			   struct drm_map_list ** maplist)
+			   struct drm_map_list **maplist)
 {
 	struct drm_map *map;
 	struct drm_map_list *list;
@@ -189,7 +199,7 @@ static int drm_addmap_core(struct drm_device * dev, unsigned int offset,
 	case _DRM_SHM:
 		list = drm_find_matching_map(dev, map);
 		if (list != NULL) {
-			if(list->map->size != map->size) {
+			if (list->map->size != map->size) {
 				DRM_DEBUG("Matching maps of type %d with "
 					  "mismatched sizes, (%ld vs %ld)\n",
 					  map->type, map->size, list->map->size);
@@ -210,12 +220,12 @@ static int drm_addmap_core(struct drm_device * dev, unsigned int offset,
 		map->offset = (unsigned long)map->handle;
 		if (map->flags & _DRM_CONTAINS_LOCK) {
 			/* Prevent a 2nd X Server from creating a 2nd lock */
-			if (dev->lock.hw_lock != NULL) {
+			if (dev->primary->master->lock.hw_lock != NULL) {
 				vfree(map->handle);
 				drm_free(map, sizeof(*map), DRM_MEM_MAPS);
 				return -EBUSY;
 			}
-			dev->sigdata.lock = dev->lock.hw_lock = map->handle;	/* Pointer to lock */
+			dev->sigdata.lock = dev->primary->master->lock.hw_lock = map->handle;	/* Pointer to lock */
 		}
 		break;
 	case _DRM_AGP: {
@@ -319,13 +329,14 @@ static int drm_addmap_core(struct drm_device * dev, unsigned int offset,
 	list->user_token = list->hash.key << PAGE_SHIFT;
 	mutex_unlock(&dev->struct_mutex);
 
+	list->master = dev->primary->master;
 	*maplist = list;
 	return 0;
 	}
 
-int drm_addmap(struct drm_device * dev, unsigned int offset,
+int drm_addmap(struct drm_device *dev, unsigned int offset,
 	       unsigned int size, enum drm_map_type type,
-	       enum drm_map_flags flags, drm_local_map_t ** map_ptr)
+	       enum drm_map_flags flags, drm_local_map_t **map_ptr)
 {
 	struct drm_map_list *list;
 	int rc;
@@ -335,7 +346,6 @@ int drm_addmap(struct drm_device * dev, unsigned int offset,
 		*map_ptr = list->map;
 	return rc;
 }
-
 EXPORT_SYMBOL(drm_addmap);
 
 int drm_addmap_ioctl(struct drm_device *dev, void *data,
@@ -409,6 +419,9 @@ int drm_rmmap_locked(struct drm_device *dev, drm_local_map_t *map)
 		break;
 	case _DRM_SHM:
 		vfree(map->handle);
+		dev->sigdata.lock = dev->primary->master->lock.hw_lock = NULL;   /* SHM removed */
+		dev->primary->master->lock.file_priv = NULL;
+		wake_up_interruptible(&dev->primary->master->lock.lock_queue);
 		break;
 	case _DRM_AGP:
 	case _DRM_SCATTER_GATHER:
@@ -419,6 +432,8 @@ int drm_rmmap_locked(struct drm_device *dev, drm_local_map_t *map)
 		dmah.size = map->size;
 		__drm_pci_free(dev, &dmah);
 		break;
+	case _DRM_TTM:
+		BUG_ON(1);
 	}
 	drm_free(map, sizeof(*map), DRM_MEM_MAPS);
 
@@ -493,16 +508,15 @@ int drm_rmmap_ioctl(struct drm_device *dev, void *data,
  *
  * Frees any pages and buffers associated with the given entry.
  */
-static void drm_cleanup_buf_error(struct drm_device * dev,
-				  struct drm_buf_entry * entry)
+static void drm_cleanup_buf_error(struct drm_device *dev,
+				  struct drm_buf_entry *entry)
 {
 	int i;
 
 	if (entry->seg_count) {
 		for (i = 0; i < entry->seg_count; i++) {
-			if (entry->seglist[i]) {
+			if (entry->seglist[i])
 				drm_pci_free(dev, entry->seglist[i]);
-			}
 		}
 		drm_free(entry->seglist,
 			 entry->seg_count *
@@ -539,7 +553,7 @@ static void drm_cleanup_buf_error(struct drm_device * dev,
  * reallocates the buffer list of the same size order to accommodate the new
  * buffers.
  */
-int drm_addbufs_agp(struct drm_device * dev, struct drm_buf_desc * request)
+int drm_addbufs_agp(struct drm_device *dev, struct drm_buf_desc *request)
 {
 	struct drm_device_dma *dma = dev->dma;
 	struct drm_buf_entry *entry;
@@ -684,9 +698,8 @@ int drm_addbufs_agp(struct drm_device * dev, struct drm_buf_desc * request)
 	}
 	dma->buflist = temp_buflist;
 
-	for (i = 0; i < entry->buf_count; i++) {
+	for (i = 0; i < entry->buf_count; i++)
 		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
-	}
 
 	dma->buf_count += entry->buf_count;
 	dma->seg_count += entry->seg_count;
@@ -709,7 +722,7 @@ int drm_addbufs_agp(struct drm_device * dev, struct drm_buf_desc * request)
 EXPORT_SYMBOL(drm_addbufs_agp);
 #endif				/* __OS_HAS_AGP */
 
-int drm_addbufs_pci(struct drm_device * dev, struct drm_buf_desc * request)
+int drm_addbufs_pci(struct drm_device *dev, struct drm_buf_desc *request)
 {
 	struct drm_device_dma *dma = dev->dma;
 	int count;
@@ -902,9 +915,8 @@ int drm_addbufs_pci(struct drm_device * dev, struct drm_buf_desc * request)
 	}
 	dma->buflist = temp_buflist;
 
-	for (i = 0; i < entry->buf_count; i++) {
+	for (i = 0; i < entry->buf_count; i++)
 		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
-	}
 
 	/* No allocations failed, so now we can replace the orginal pagelist
 	 * with the new one.
@@ -1097,7 +1109,7 @@ static int drm_addbufs_sg(struct drm_device * dev, struct drm_buf_desc * request
 	return 0;
 }
 
-static int drm_addbufs_fb(struct drm_device * dev, struct drm_buf_desc * request)
+static int drm_addbufs_fb(struct drm_device *dev, struct drm_buf_desc *request)
 {
 	struct drm_device_dma *dma = dev->dma;
 	struct drm_buf_entry *entry;
@@ -1234,9 +1246,8 @@ static int drm_addbufs_fb(struct drm_device * dev, struct drm_buf_desc * request
 	}
 	dma->buflist = temp_buflist;
 
-	for (i = 0; i < entry->buf_count; i++) {
+	for (i = 0; i < entry->buf_count; i++)
 		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
-	}
 
 	dma->buf_count += entry->buf_count;
 	dma->seg_count += entry->seg_count;
@@ -1487,7 +1498,7 @@ int drm_freebufs(struct drm_device *dev, void *data,
  * drm_mmap_dma().
  */
 int drm_mapbufs(struct drm_device *dev, void *data,
-	        struct drm_file *file_priv)
+		struct drm_file *file_priv)
 {
 	struct drm_device_dma *dma = dev->dma;
 	int retcode = 0;
@@ -1570,7 +1581,7 @@ int drm_mapbufs(struct drm_device *dev, void *data,
 			}
 		}
 	}
-      done:
+done:
 	request->count = dma->buf_count;
 	DRM_DEBUG("%d buffers, retcode = %d\n", request->count, retcode);
 
diff --git a/drivers/char/drm/drm_context.c b/drivers/char/drm/drm_context.c
index d505f69..bfd0adc 100644
--- a/drivers/char/drm/drm_context.c
+++ b/drivers/char/drm/drm_context.c
@@ -195,11 +195,11 @@ int drm_setsareactx(struct drm_device *dev, void *data,
 		    && r_list->user_token == (unsigned long) request->handle)
 			goto found;
 	}
-      bad:
+bad:
 	mutex_unlock(&dev->struct_mutex);
 	return -EINVAL;
 
-      found:
+found:
 	map = r_list->map;
 	if (!map)
 		goto bad;
@@ -256,12 +256,13 @@ static int drm_context_switch(struct drm_device * dev, int old, int new)
  * hardware lock is held, clears the drm_device::context_flag and wakes up
  * drm_device::context_wait.
  */
-static int drm_context_switch_complete(struct drm_device * dev, int new)
+static int drm_context_switch_complete(struct drm_device *dev, 
+				       struct drm_file *file_priv, int new)
 {
 	dev->last_context = new;	/* PRE/POST: This is the _only_ writer. */
 	dev->last_switch = jiffies;
 
-	if (!_DRM_LOCK_IS_HELD(dev->lock.hw_lock->lock)) {
+	if (!_DRM_LOCK_IS_HELD(file_priv->master->lock.hw_lock->lock)) {
 		DRM_ERROR("Lock isn't held after context switch\n");
 	}
 
@@ -420,7 +421,7 @@ int drm_newctx(struct drm_device *dev, void *data,
 	struct drm_ctx *ctx = data;
 
 	DRM_DEBUG("%d\n", ctx->handle);
-	drm_context_switch_complete(dev, ctx->handle);
+	drm_context_switch_complete(dev, file_priv, ctx->handle);
 
 	return 0;
 }
diff --git a/drivers/char/drm/drm_crtc.c b/drivers/char/drm/drm_crtc.c
new file mode 100644
index 0000000..2d8a49b
--- /dev/null
+++ b/drivers/char/drm/drm_crtc.c
@@ -0,0 +1,2451 @@
+/*
+ * Copyright (c) 2006-2007 Intel Corporation
+ * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>
+ *
+ * DRM core CRTC related functions
+ *
+ * Permission to use, copy, modify, distribute, and sell this software and its
+ * documentation for any purpose is hereby granted without fee, provided that
+ * the above copyright notice appear in all copies and that both that copyright
+ * notice and this permission notice appear in supporting documentation, and
+ * that the name of the copyright holders not be used in advertising or
+ * publicity pertaining to distribution of the software without specific,
+ * written prior permission.  The copyright holders make no representations
+ * about the suitability of this software for any purpose.  It is provided "as
+ * is" without express or implied warranty.
+ *
+ * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
+ * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO
+ * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
+ * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,
+ * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER
+ * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE
+ * OF THIS SOFTWARE.
+ *
+ * Authors:
+ *      Keith Packard
+ *	Eric Anholt <eric@anholt.net>
+ *      Dave Airlie <airlied@linux.ie>
+ *      Jesse Barnes <jesse.barnes@intel.com>
+ */
+#include <linux/list.h>
+#include "drm.h"
+#include "drmP.h"
+#include "drm_crtc.h"
+
+struct drm_prop_enum_list {
+	int type;
+	char *name;
+};
+
+/*
+ * Global properties
+ */
+static struct drm_prop_enum_list drm_dpms_enum_list[] =
+{ { DPMSModeOn, "On" },
+  { DPMSModeStandby, "Standby" },
+  { DPMSModeSuspend, "Suspend" },
+  { DPMSModeOff, "Off" }
+};
+static struct drm_prop_enum_list drm_conn_enum_list[] = 
+{ { ConnectorVGA, "VGA" },
+  { ConnectorDVII, "DVI-I" },
+  { ConnectorDVID, "DVI-D" },
+  { ConnectorDVIA, "DVI-A" },
+  { ConnectorComposite, "Composite" },
+  { ConnectorSVIDEO, "SVIDEO" },
+  { ConnectorLVDS, "LVDS" },
+  { ConnectorComponent, "Component" },
+  { Connector9PinDIN, "9-pin DIN" },
+  { ConnectorDisplayPort, "DisplayPort" },
+  { ConnectorHDMIA, "HDMI Type A" },
+  { ConnectorHDMIB, "HDMI Type B" },
+};
+static struct drm_prop_enum_list drm_output_enum_list[] =
+{ { DRM_MODE_OUTPUT_NONE, "None" },
+  { DRM_MODE_OUTPUT_DAC, "DAC" },
+  { DRM_MODE_OUTPUT_TMDS, "TMDS" },
+  { DRM_MODE_OUTPUT_LVDS, "LVDS" },
+  { DRM_MODE_OUTPUT_TVDAC, "TV" },
+};
+
+char *drm_get_output_name(struct drm_output *output)
+{
+	static char buf[32];
+
+	snprintf(buf, 32, "%s-%d", drm_output_enum_list[output->output_type].name,
+		 output->output_type_id);
+	return buf;
+}
+
+/**
+ * drm_idr_get - allocate a new identifier
+ * @dev: DRM device
+ * @ptr: object pointer, used to generate unique ID
+ *
+ * LOCKING:
+ * Caller must hold DRM mode_config lock.
+ *
+ * Create a unique identifier based on @ptr in @dev's identifier space.  Used
+ * for tracking modes, CRTCs and outputs.
+ *
+ * RETURNS:
+ * New unique (relative to other objects in @dev) integer identifier for the
+ * object.
+ */
+int drm_idr_get(struct drm_device *dev, void *ptr)
+{
+	int new_id = 0;
+	int ret;
+again:
+	if (idr_pre_get(&dev->mode_config.crtc_idr, GFP_KERNEL) == 0) {
+		DRM_ERROR("Ran out memory getting a mode number\n");
+		return 0;
+	}
+
+	ret = idr_get_new_above(&dev->mode_config.crtc_idr, ptr, 1, &new_id);
+	if (ret == -EAGAIN)
+		goto again;	
+
+	return new_id;
+}
+
+/**
+ * drm_idr_put - free an identifer
+ * @dev: DRM device
+ * @id: ID to free
+ *
+ * LOCKING:
+ * Caller must hold DRM mode_config lock.
+ *
+ * Free @id from @dev's unique identifier pool.
+ */
+void drm_idr_put(struct drm_device *dev, int id)
+{
+	idr_remove(&dev->mode_config.crtc_idr, id);
+}
+
+/**
+ * drm_crtc_from_fb - find the CRTC structure associated with an fb
+ * @dev: DRM device
+ * @fb: framebuffer in question
+ *
+ * LOCKING:
+ * Caller must hold mode_config lock.
+ *
+ * Find CRTC in the mode_config structure that matches @fb.
+ *
+ * RETURNS:
+ * Pointer to the CRTC or NULL if it wasn't found.
+ */
+struct drm_crtc *drm_crtc_from_fb(struct drm_device *dev,
+				  struct drm_framebuffer *fb)
+{
+	struct drm_crtc *crtc;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (crtc->fb == fb)
+			return crtc;
+	}
+	return NULL;
+}
+
+/**
+ * drm_framebuffer_create - create a new framebuffer object
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Creates a new framebuffer objects and adds it to @dev's DRM mode_config.
+ *
+ * RETURNS:
+ * Pointer to new framebuffer or NULL on error.
+ */
+struct drm_framebuffer *drm_framebuffer_create(struct drm_device *dev)
+{
+	struct drm_framebuffer *fb;
+
+	fb = kzalloc(sizeof(struct drm_framebuffer), GFP_KERNEL);
+	if (!fb)
+		return NULL;
+	
+	fb->id = drm_idr_get(dev, fb);
+	fb->dev = dev;
+	dev->mode_config.num_fb++;
+	list_add(&fb->head, &dev->mode_config.fb_list);
+
+	return fb;
+}
+EXPORT_SYMBOL(drm_framebuffer_create);
+
+/**
+ * drm_framebuffer_destroy - remove a framebuffer object
+ * @fb: framebuffer to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Scans all the CRTCs in @dev's mode_config.  If they're using @fb, removes
+ * it, setting it to NULL.
+ */
+void drm_framebuffer_destroy(struct drm_framebuffer *fb)
+{
+	struct drm_device *dev = fb->dev;
+	struct drm_crtc *crtc;
+
+	/* remove from any CRTC */
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (crtc->fb == fb)
+			crtc->fb = NULL;
+	}
+
+	drm_idr_put(dev, fb->id);
+	list_del(&fb->head);
+	dev->mode_config.num_fb--;
+
+	kfree(fb);
+}
+EXPORT_SYMBOL(drm_framebuffer_destroy);
+
+/**
+ * drm_crtc_create - create a new CRTC object
+ * @dev: DRM device
+ * @funcs: callbacks for the new CRTC
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Creates a new CRTC object and adds it to @dev's mode_config structure.
+ *
+ * RETURNS:
+ * Pointer to new CRTC object or NULL on error.
+ */
+struct drm_crtc *drm_crtc_create(struct drm_device *dev,
+				 const struct drm_crtc_funcs *funcs)
+{
+	struct drm_crtc *crtc;
+
+	crtc = kzalloc(sizeof(struct drm_crtc), GFP_KERNEL);
+	if (!crtc)
+		return NULL;
+
+	crtc->dev = dev;
+	crtc->funcs = funcs;
+
+	crtc->id = drm_idr_get(dev, crtc);
+
+	list_add_tail(&crtc->head, &dev->mode_config.crtc_list);
+	dev->mode_config.num_crtc++;
+
+	return crtc;
+}
+EXPORT_SYMBOL(drm_crtc_create);
+
+/**
+ * drm_crtc_destroy - remove a CRTC object
+ * @crtc: CRTC to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Cleanup @crtc.  Calls @crtc's cleanup function, then removes @crtc from
+ * its associated DRM device's mode_config.  Frees it afterwards.
+ */
+void drm_crtc_destroy(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+
+	if (crtc->funcs->cleanup)
+		(*crtc->funcs->cleanup)(crtc);
+
+	drm_idr_put(dev, crtc->id);
+	list_del(&crtc->head);
+	dev->mode_config.num_crtc--;
+	kfree(crtc);
+}
+EXPORT_SYMBOL(drm_crtc_destroy);
+
+/**
+ * drm_crtc_in_use - check if a given CRTC is in a mode_config
+ * @crtc: CRTC to check
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Walk @crtc's DRM device's mode_config and see if it's in use.
+ *
+ * RETURNS:
+ * True if @crtc is part of the mode_config, false otherwise.
+ */
+bool drm_crtc_in_use(struct drm_crtc *crtc)
+{
+	struct drm_output *output;
+	struct drm_device *dev = crtc->dev;
+	/* FIXME: Locking around list access? */
+	list_for_each_entry(output, &dev->mode_config.output_list, head)
+		if (output->crtc == crtc)
+			return true;
+	return false;
+}
+EXPORT_SYMBOL(drm_crtc_in_use);
+
+/*
+ * Detailed mode info for a standard 640x480@60Hz monitor
+ */
+static struct drm_display_mode std_mode[] = {
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DEFAULT, 25200, 640, 656,
+		   752, 800, 0, 480, 490, 492, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@60Hz */
+};
+
+/**
+ * drm_crtc_probe_output_modes - get complete set of display modes
+ * @dev: DRM device
+ * @maxX: max width for modes
+ * @maxY: max height for modes
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Based on @dev's mode_config layout, scan all the outputs and try to detect
+ * modes on them.  Modes will first be added to the output's probed_modes
+ * list, then culled (based on validity and the @maxX, @maxY parameters) and
+ * put into the normal modes list.
+ *
+ * Intended to be used either at bootup time or when major configuration
+ * changes have occurred.
+ *
+ * FIXME: take into account monitor limits
+ */
+void drm_crtc_probe_single_output_modes(struct drm_output *output, int maxX, int maxY)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_display_mode *mode, *t;
+	int ret;
+	//if (maxX == 0 || maxY == 0) 
+	// TODO
+
+	/* set all modes to the unverified state */
+	list_for_each_entry_safe(mode, t, &output->modes, head)
+		mode->status = MODE_UNVERIFIED;
+		
+	output->status = (*output->funcs->detect)(output);
+	
+	if (output->status == output_status_disconnected) {
+		DRM_DEBUG("%s is disconnected\n", drm_get_output_name(output));
+		/* TODO set EDID to NULL */
+		return;
+	}
+	
+	ret = (*output->funcs->get_modes)(output);
+	
+	if (ret) {
+		drm_mode_output_list_update(output);
+	}
+	
+	if (maxX && maxY)
+		drm_mode_validate_size(dev, &output->modes, maxX,
+				       maxY, 0);
+	list_for_each_entry_safe(mode, t, &output->modes, head) {
+		if (mode->status == MODE_OK)
+			mode->status = (*output->funcs->mode_valid)(output,mode);
+	}
+	
+	
+	drm_mode_prune_invalid(dev, &output->modes, true);
+	
+	if (list_empty(&output->modes)) {
+		struct drm_display_mode *stdmode;
+		
+		DRM_DEBUG("No valid modes on %s\n", drm_get_output_name(output));
+		
+		/* Should we do this here ???
+		 * When no valid EDID modes are available we end up
+		 * here and bailed in the past, now we add a standard
+		 * 640x480@60Hz mode and carry on.
+		 */
+		stdmode = drm_mode_duplicate(dev, &std_mode[0]);
+		drm_mode_probed_add(output, stdmode);
+		drm_mode_list_concat(&output->probed_modes,
+				     &output->modes);
+		
+		DRM_DEBUG("Adding standard 640x480 @ 60Hz to %s\n",
+			  drm_get_output_name(output));
+	}
+	
+	drm_mode_sort(&output->modes);
+	
+	DRM_DEBUG("Probed modes for %s\n", drm_get_output_name(output));
+	list_for_each_entry_safe(mode, t, &output->modes, head) {
+		mode->vrefresh = drm_mode_vrefresh(mode);
+		
+		drm_mode_set_crtcinfo(mode, CRTC_INTERLACE_HALVE_V);
+		drm_mode_debug_printmodeline(dev, mode);
+	}
+}
+
+void drm_crtc_probe_output_modes(struct drm_device *dev, int maxX, int maxY)
+{
+	struct drm_output *output;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		drm_crtc_probe_single_output_modes(output, maxX, maxY);
+	}
+}
+EXPORT_SYMBOL(drm_crtc_probe_output_modes);
+
+/**
+ * drm_crtc_set_mode - set a mode
+ * @crtc: CRTC to program
+ * @mode: mode to use
+ * @x: width of mode
+ * @y: height of mode
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Try to set @mode on @crtc.  Give @crtc and its associated outputs a chance
+ * to fixup or reject the mode prior to trying to set it.
+ *
+ * RETURNS:
+ * True if the mode was set successfully, or false otherwise.
+ */
+bool drm_crtc_set_mode(struct drm_crtc *crtc, struct drm_display_mode *mode,
+		       int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_display_mode *adjusted_mode, saved_mode;
+	int saved_x, saved_y;
+	bool didLock = false;
+	struct drm_output *output;
+
+	adjusted_mode = drm_mode_duplicate(dev, mode);
+
+	crtc->enabled = drm_crtc_in_use(crtc);
+
+	if (!crtc->enabled) {
+		return true;
+	}
+
+	didLock = crtc->funcs->lock(crtc);
+
+	saved_mode = crtc->mode;
+	saved_x = crtc->x;
+	saved_y = crtc->y;
+	
+	/* Update crtc values up front so the driver can rely on them for mode
+	 * setting.
+	 */
+	crtc->mode = *mode;
+	crtc->x = x;
+	crtc->y = y;
+
+	if (drm_mode_equal(&saved_mode, &crtc->mode)) {
+		if (saved_x != crtc->x || saved_y != crtc->y) {
+			crtc->funcs->mode_set_base(crtc, crtc->x, crtc->y);
+			goto done;
+		}
+	}
+
+	/* Pass our mode to the outputs and the CRTC to give them a chance to
+	 * adjust it according to limitations or output properties, and also
+	 * a chance to reject the mode entirely.
+	 */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		
+		if (output->crtc != crtc)
+			continue;
+		
+		if (!output->funcs->mode_fixup(output, mode, adjusted_mode)) {
+			goto done;
+		}
+	}
+	
+	if (!crtc->funcs->mode_fixup(crtc, mode, adjusted_mode)) {
+		goto done;
+	}
+
+	/* Prepare the outputs and CRTCs before setting the mode. */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+		
+		/* Disable the output as the first thing we do. */
+		output->funcs->prepare(output);
+	}
+	
+	crtc->funcs->prepare(crtc);
+	
+	/* Set up the DPLL and any output state that needs to adjust or depend
+	 * on the DPLL.
+	 */
+	crtc->funcs->mode_set(crtc, mode, adjusted_mode, x, y);
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+		
+		DRM_INFO("%s: set mode %s %x\n", drm_get_output_name(output), mode->name, mode->mode_id);
+
+		output->funcs->mode_set(output, mode, adjusted_mode);
+	}
+	
+	/* Now, enable the clocks, plane, pipe, and outputs that we set up. */
+	crtc->funcs->commit(crtc);
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+		
+		output->funcs->commit(output);
+
+#if 0 // TODO def RANDR_12_INTERFACE
+		if (output->randr_output)
+			RRPostPendingProperties (output->randr_output);
+#endif
+	}
+	
+	/* XXX free adjustedmode */
+	drm_mode_destroy(dev, adjusted_mode);
+	/* TODO */
+//	if (scrn->pScreen)
+//		drm_crtc_set_screen_sub_pixel_order(dev);
+
+done:
+	if (didLock)
+		crtc->funcs->unlock (crtc);
+	
+	return true;
+}
+EXPORT_SYMBOL(drm_crtc_set_mode);
+
+/**
+ * drm_disable_unused_functions - disable unused objects
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * If an output or CRTC isn't part of @dev's mode_config, it can be disabled
+ * by calling its dpms function, which should power it off.
+ */
+void drm_disable_unused_functions(struct drm_device *dev)
+{
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (!output->crtc)
+			(*output->funcs->dpms)(output, DPMSModeOff);
+	}
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (!crtc->enabled)
+			crtc->funcs->dpms(crtc, DPMSModeOff);
+	}
+}
+EXPORT_SYMBOL(drm_disable_unused_functions);
+
+/**
+ * drm_mode_probed_add - add a mode to the specified output's probed mode list
+ * @output: output the new mode
+ * @mode: mode data
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ * 
+ * Add @mode to @output's mode list for later use.
+ */
+void drm_mode_probed_add(struct drm_output *output,
+			 struct drm_display_mode *mode)
+{
+	list_add(&mode->head, &output->probed_modes);
+}
+EXPORT_SYMBOL(drm_mode_probed_add);
+
+/**
+ * drm_mode_remove - remove and free a mode
+ * @output: output list to modify
+ * @mode: mode to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ * 
+ * Remove @mode from @output's mode list, then free it.
+ */
+void drm_mode_remove(struct drm_output *output, struct drm_display_mode *mode)
+{
+	list_del(&mode->head);
+	kfree(mode);
+}
+EXPORT_SYMBOL(drm_mode_remove);
+
+/**
+ * drm_output_create - create a new output
+ * @dev: DRM device
+ * @funcs: callbacks for this output
+ * @name: user visible name of the output
+ *
+ * LOCKING:
+ * Caller must hold @dev's mode_config lock.
+ *
+ * Creates a new drm_output structure and adds it to @dev's mode_config
+ * structure.
+ *
+ * RETURNS:
+ * Pointer to the new output or NULL on error.
+ */
+struct drm_output *drm_output_create(struct drm_device *dev,
+				     const struct drm_output_funcs *funcs,
+				     int output_type)
+{
+	struct drm_output *output = NULL;
+
+	output = kzalloc(sizeof(struct drm_output), GFP_KERNEL);
+	if (!output)
+		return NULL;
+		
+	output->dev = dev;
+	output->funcs = funcs;
+	output->id = drm_idr_get(dev, output);
+	output->output_type = output_type;
+	output->output_type_id = 1; /* TODO */
+	output->subpixel_order = SubPixelUnknown;
+	INIT_LIST_HEAD(&output->user_modes);
+	INIT_LIST_HEAD(&output->probed_modes);
+	INIT_LIST_HEAD(&output->modes);
+	/* randr_output? */
+	/* output_set_monitor(output)? */
+	/* check for output_ignored(output)? */
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_add_tail(&output->head, &dev->mode_config.output_list);
+	dev->mode_config.num_output++;
+
+	drm_output_attach_property(output, dev->mode_config.edid_property, 0);
+
+	drm_output_attach_property(output, dev->mode_config.dpms_property, 0);
+
+	mutex_unlock(&dev->mode_config.mutex);
+
+	return output;
+
+}
+EXPORT_SYMBOL(drm_output_create);
+
+/**
+ * drm_output_destroy - remove an output
+ * @output: output to remove
+ *
+ * LOCKING:
+ * Caller must hold @dev's mode_config lock.
+ *
+ * Call @output's cleanup function, then remove the output from the DRM
+ * mode_config after freeing @output's modes.
+ */
+void drm_output_destroy(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_display_mode *mode, *t;
+
+	if (*output->funcs->cleanup)
+		(*output->funcs->cleanup)(output);
+
+	list_for_each_entry_safe(mode, t, &output->probed_modes, head)
+		drm_mode_remove(output, mode);
+
+	list_for_each_entry_safe(mode, t, &output->modes, head)
+		drm_mode_remove(output, mode);
+
+	list_for_each_entry_safe(mode, t, &output->user_modes, head)
+		drm_mode_remove(output, mode);
+
+	mutex_lock(&dev->mode_config.mutex);
+	drm_idr_put(dev, output->id);
+	list_del(&output->head);
+	mutex_unlock(&dev->mode_config.mutex);
+	kfree(output);
+}
+EXPORT_SYMBOL(drm_output_destroy);
+
+
+/**
+ * drm_mode_create - create a new display mode
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * None.
+ *
+ * Create a new drm_display_mode, give it an ID, and return it.
+ *
+ * RETURNS:
+ * Pointer to new mode on success, NULL on error.
+ */
+struct drm_display_mode *drm_mode_create(struct drm_device *dev)
+{
+	struct drm_display_mode *nmode;
+
+	nmode = kzalloc(sizeof(struct drm_display_mode), GFP_KERNEL);
+	if (!nmode)
+		return NULL;
+
+	nmode->mode_id = drm_idr_get(dev, nmode);
+	return nmode;
+}
+EXPORT_SYMBOL(drm_mode_create);
+
+/**
+ * drm_mode_destroy - remove a mode
+ * @dev: DRM device
+ * @mode: mode to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Free @mode's unique identifier, then free it.
+ */
+void drm_mode_destroy(struct drm_device *dev, struct drm_display_mode *mode)
+{
+	drm_idr_put(dev, mode->mode_id);
+
+	kfree(mode);
+}
+EXPORT_SYMBOL(drm_mode_destroy);
+
+static int drm_mode_create_standard_output_properties(struct drm_device *dev)
+{
+	int i;
+
+	/*
+	 * Standard properties (apply to all outputs)
+	 */
+	dev->mode_config.edid_property =
+		drm_property_create(dev, DRM_MODE_PROP_BLOB | DRM_MODE_PROP_IMMUTABLE,
+				    "EDID", 0);
+
+	dev->mode_config.dpms_property =
+		drm_property_create(dev, DRM_MODE_PROP_ENUM, "DPMS", 4);
+
+	for (i = 0; i < ARRAY_SIZE(drm_dpms_enum_list); i++)
+		drm_property_add_enum(dev->mode_config.dpms_property, i, drm_dpms_enum_list[i].type, drm_dpms_enum_list[i].name);
+
+	dev->mode_config.connector_type_property =
+		drm_property_create(dev, DRM_MODE_PROP_ENUM | DRM_MODE_PROP_IMMUTABLE,
+				    "Connector Type", 10);
+	for (i = 0; i < ARRAY_SIZE(drm_conn_enum_list); i++)
+		drm_property_add_enum(dev->mode_config.connector_type_property, i, drm_conn_enum_list[i].type, drm_conn_enum_list[i].name);
+
+	dev->mode_config.connector_num_property =
+		drm_property_create(dev, DRM_MODE_PROP_RANGE | DRM_MODE_PROP_IMMUTABLE,
+				    "Connector ID", 2);
+	dev->mode_config.connector_num_property->values[0] = 0;
+	dev->mode_config.connector_num_property->values[1] = 20;
+
+	/*
+	 * TV specific properties
+	 */
+	dev->mode_config.tv_left_margin_property =
+		drm_property_create(dev, DRM_MODE_PROP_RANGE |
+				    DRM_MODE_PROP_IMMUTABLE,
+				    "left margin", 2);
+	dev->mode_config.tv_left_margin_property->values[0] = 0;
+	dev->mode_config.tv_left_margin_property->values[1] = 100;
+
+	dev->mode_config.tv_right_margin_property =
+		drm_property_create(dev, DRM_MODE_PROP_RANGE |
+				    DRM_MODE_PROP_IMMUTABLE,
+				    "right margin", 2);
+	dev->mode_config.tv_right_margin_property->values[0] = 0;
+	dev->mode_config.tv_right_margin_property->values[1] = 100;
+
+	dev->mode_config.tv_top_margin_property =
+		drm_property_create(dev, DRM_MODE_PROP_RANGE |
+				    DRM_MODE_PROP_IMMUTABLE,
+				    "top margin", 2);
+	dev->mode_config.tv_top_margin_property->values[0] = 0;
+	dev->mode_config.tv_top_margin_property->values[1] = 100;
+
+	dev->mode_config.tv_bottom_margin_property =
+		drm_property_create(dev, DRM_MODE_PROP_RANGE |
+				    DRM_MODE_PROP_IMMUTABLE,
+				    "bottom margin", 2);
+	dev->mode_config.tv_bottom_margin_property->values[0] = 0;
+	dev->mode_config.tv_bottom_margin_property->values[1] = 100;
+
+
+	return 0;
+}
+
+/**
+ * drm_mode_config_init - initialize DRM mode_configuration structure
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * None, should happen single threaded at init time.
+ *
+ * Initialize @dev's mode_config structure, used for tracking the graphics
+ * configuration of @dev.
+ */
+void drm_mode_config_init(struct drm_device *dev)
+{
+	mutex_init(&dev->mode_config.mutex);
+	INIT_LIST_HEAD(&dev->mode_config.fb_list);
+	INIT_LIST_HEAD(&dev->mode_config.crtc_list);
+	INIT_LIST_HEAD(&dev->mode_config.output_list);
+	INIT_LIST_HEAD(&dev->mode_config.property_list);
+	INIT_LIST_HEAD(&dev->mode_config.property_blob_list);
+	idr_init(&dev->mode_config.crtc_idr);
+
+	drm_mode_create_standard_output_properties(dev);
+
+	/* Just to be sure */
+	dev->mode_config.num_fb = 0;
+	dev->mode_config.num_output = 0;
+	dev->mode_config.num_crtc = 0;
+	dev->mode_config.hotplug_counter = 0;
+}
+EXPORT_SYMBOL(drm_mode_config_init);
+
+/**
+ * drm_get_buffer_object - find the buffer object for a given handle
+ * @dev: DRM device
+ * @bo: pointer to caller's buffer_object pointer
+ * @handle: handle to lookup
+ *
+ * LOCKING:
+ * Must take @dev's struct_mutex to protect buffer object lookup.
+ *
+ * Given @handle, lookup the buffer object in @dev and put it in the caller's
+ * @bo pointer.
+ *
+ * RETURNS:
+ * Zero on success, -EINVAL if the handle couldn't be found.
+ */
+static int drm_get_buffer_object(struct drm_device *dev, struct drm_buffer_object **bo, unsigned long handle)
+{
+	struct drm_user_object *uo;
+	struct drm_hash_item *hash;
+	int ret;
+
+	*bo = NULL;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_ht_find_item(&dev->object_hash, handle, &hash);
+	if (ret) {
+		DRM_ERROR("Couldn't find handle.\n");
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	uo = drm_hash_entry(hash, struct drm_user_object, hash);
+	if (uo->type != drm_buffer_type) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	
+	*bo = drm_user_object_entry(uo, struct drm_buffer_object, base);
+	ret = 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/**
+ * drm_pick_crtcs - pick crtcs for output devices
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ */
+static void drm_pick_crtcs (struct drm_device *dev)
+{
+	int c, o, assigned;
+	struct drm_output *output, *output_equal;
+	struct drm_crtc   *crtc;
+	struct drm_display_mode *des_mode = NULL, *modes, *modes_equal;
+	int found;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+       		output->crtc = NULL;
+    
+    		/* Don't hook up outputs that are disconnected ??
+		 *
+		 * This is debateable. Do we want fixed /dev/fbX or
+		 * dynamic on hotplug (need mode code for that though) ?
+		 *
+		 * If we don't hook up outputs now, then we only create
+		 * /dev/fbX for the output that's enabled, that's good as
+		 * the users console will be on that output.
+		 *
+		 * If we do hook up outputs that are disconnected now, then
+		 * the user may end up having to muck about with the fbcon
+		 * map flags to assign his console to the enabled output. Ugh.
+		 */
+    		if (output->status != output_status_connected)
+			continue;
+
+		if (list_empty(&output->modes))
+			continue;
+
+		des_mode = NULL;
+		found = 0;
+		list_for_each_entry(des_mode, &output->modes, head) {
+			if (des_mode->type & DRM_MODE_TYPE_PREFERRED) {
+				found = 1;
+				break;
+			}
+		}
+
+		/* No preferred mode, let's just select the first available */
+		if (!found) {
+			des_mode = NULL;
+			list_for_each_entry(des_mode, &output->modes, head) {
+				break;
+			}
+		}
+
+		c = -1;
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+			assigned = 0;
+
+			c++;
+			if ((output->possible_crtcs & (1 << c)) == 0)
+		    		continue;
+	
+			list_for_each_entry(output_equal, &dev->mode_config.output_list, head) {
+				if (output->id == output_equal->id)
+					continue;
+
+				/* Find out if crtc has been assigned before */
+				if (output_equal->crtc == crtc)
+					assigned = 1;
+			}
+
+#if 1 /* continue for now */
+			if (assigned)
+				continue;
+#endif
+
+			o = -1;
+			list_for_each_entry(output_equal, &dev->mode_config.output_list, head) {
+				o++;
+				if (output->id == output_equal->id)
+					continue;
+
+				list_for_each_entry(modes, &output->modes, head) {
+					list_for_each_entry(modes_equal, &output_equal->modes, head) {
+						if (drm_mode_equal (modes, modes_equal)) {
+							if ((output->possible_clones & output_equal->possible_clones) && (output_equal->crtc == crtc)) {
+								printk("Cloning %s (0x%lx) to %s (0x%lx)\n",drm_get_output_name(output),output->possible_clones,drm_get_output_name(output_equal),output_equal->possible_clones);
+								assigned = 0;
+								goto clone;
+							}
+						}
+					}
+				}
+			}
+
+clone:
+			/* crtc has been assigned skip it */
+			if (assigned)
+				continue;
+
+			/* Found a CRTC to attach to, do it ! */
+			output->crtc = crtc;
+			output->crtc->desired_mode = des_mode;
+			output->initial_x = 0;
+			output->initial_y = 0;
+			DRM_DEBUG("Desired mode for CRTC %d is 0x%x:%s\n",c,des_mode->mode_id, des_mode->name);
+			break;
+    		}
+	}
+}
+EXPORT_SYMBOL(drm_pick_crtcs);
+
+/**
+ * drm_initial_config - setup a sane initial output configuration
+ * @dev: DRM device
+ * @can_grow: this configuration is growable
+ *
+ * LOCKING:
+ * Called at init time, must take mode config lock.
+ *
+ * Scan the CRTCs and outputs and try to put together an initial setup.
+ * At the moment, this is a cloned configuration across all heads with
+ * a new framebuffer object as the backing store.
+ *
+ * RETURNS:
+ * Zero if everything went ok, nonzero otherwise.
+ */
+bool drm_initial_config(struct drm_device *dev, bool can_grow)
+{
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+	int ret = false;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	drm_crtc_probe_output_modes(dev, 2048, 2048);
+
+	drm_pick_crtcs(dev);
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+
+		/* can't setup the crtc if there's no assigned mode */
+		if (!crtc->desired_mode)
+			continue;
+
+		/* Now setup the fbdev for attached crtcs */
+		dev->driver->fb_probe(dev, crtc);
+	}
+
+	/* This is a little screwy, as we've already walked the outputs 
+	 * above, but it's a little bit of magic too. There's the potential
+	 * for things not to get setup above if an existing device gets
+	 * re-assigned thus confusing the hardware. By walking the outputs
+	 * this fixes up their crtc's.
+	 */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		/* can't setup the output if there's no assigned mode */
+		if (!output->crtc || !output->crtc->desired_mode)
+			continue;
+
+		/* and needs an attached fb */
+		if (output->crtc->fb)
+			drm_crtc_set_mode(output->crtc, output->crtc->desired_mode, 0, 0);
+	}
+
+	drm_disable_unused_functions(dev);
+
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_initial_config);
+
+/**
+ * drm_mode_config_cleanup - free up DRM mode_config info
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Free up all the outputs and CRTCs associated with this DRM device, then
+ * free up the framebuffers and associated buffer objects.
+ *
+ * FIXME: cleanup any dangling user buffer objects too
+ */
+void drm_mode_config_cleanup(struct drm_device *dev)
+{
+	struct drm_output *output, *ot;
+	struct drm_crtc *crtc, *ct;
+	struct drm_framebuffer *fb, *fbt;
+	struct drm_property *property, *pt;
+
+	list_for_each_entry_safe(output, ot, &dev->mode_config.output_list, head) {
+		drm_output_destroy(output);
+	}
+
+	list_for_each_entry_safe(property, pt, &dev->mode_config.property_list, head) {
+		drm_property_destroy(dev, property);
+	}
+
+	list_for_each_entry_safe(fb, fbt, &dev->mode_config.fb_list, head) {
+		/* there should only be bo of kernel type left */
+		if (fb->bo->type != drm_bo_type_kernel)
+			drm_framebuffer_destroy(fb);
+		else
+			dev->driver->fb_remove(dev, drm_crtc_from_fb(dev, fb));
+	}
+
+	list_for_each_entry_safe(crtc, ct, &dev->mode_config.crtc_list, head) {
+		drm_crtc_destroy(crtc);
+	}
+
+}
+EXPORT_SYMBOL(drm_mode_config_cleanup);
+
+/**
+ * drm_crtc_set_config - set a new config from userspace
+ * @crtc: CRTC to setup
+ * @crtc_info: user provided configuration
+ * @new_mode: new mode to set
+ * @output_set: set of outputs for the new config
+ * @fb: new framebuffer
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Setup a new configuration, provided by the user in @crtc_info, and enable
+ * it.
+ *
+ * RETURNS:
+ * Zero. (FIXME)
+ */
+int drm_crtc_set_config(struct drm_crtc *crtc, struct drm_mode_crtc *crtc_info, struct drm_display_mode *new_mode, struct drm_output **output_set, struct drm_framebuffer *fb)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_crtc **save_crtcs, *new_crtc;
+	bool save_enabled = crtc->enabled;
+	bool changed = false;
+	bool flip_or_move = false;
+	struct drm_output *output;
+	int count = 0, ro;
+
+	/* this is meant to be num_output not num_crtc */
+	save_crtcs = kzalloc(dev->mode_config.num_output * sizeof(struct drm_crtc *), GFP_KERNEL);
+	if (!save_crtcs)
+		return -ENOMEM;
+
+	/* We should be able to check here if the fb has the same properties
+	 * and then just flip_or_move it */
+	if (crtc->fb != fb)
+		flip_or_move = true;
+
+	if (crtc_info->x != crtc->x || crtc_info->y != crtc->y)
+		flip_or_move = true;
+
+	if (new_mode && !drm_mode_equal(new_mode, &crtc->mode))
+		changed = true;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		save_crtcs[count++] = output->crtc;
+
+		if (output->crtc == crtc)
+			new_crtc = NULL;
+		else
+			new_crtc = output->crtc;
+
+		for (ro = 0; ro < crtc_info->count_outputs; ro++) {
+			if (output_set[ro] == output)
+				new_crtc = crtc;
+		}
+		if (new_crtc != output->crtc) {
+			changed = true;
+			output->crtc = new_crtc;
+		}
+	}
+
+	/* mode_set_base is not a required function */
+	if (flip_or_move && !crtc->funcs->mode_set_base)
+		changed = true;
+
+	if (changed) {
+		crtc->fb = fb;
+		crtc->enabled = (new_mode != NULL);
+		if (new_mode != NULL) {
+			DRM_DEBUG("attempting to set mode from userspace\n");
+			drm_mode_debug_printmodeline(dev, &crtc->mode);
+			drm_mode_debug_printmodeline(dev, new_mode);
+			if (!drm_crtc_set_mode(crtc, new_mode, crtc_info->x,
+					       crtc_info->y)) {
+				crtc->enabled = save_enabled;
+				count = 0;
+				list_for_each_entry(output, &dev->mode_config.output_list, head)
+					output->crtc = save_crtcs[count++];
+				kfree(save_crtcs);
+				return -EINVAL;
+			}
+			crtc->desired_x = crtc_info->x;
+			crtc->desired_y = crtc_info->y;
+			crtc->desired_mode = new_mode;
+		}
+		drm_disable_unused_functions(dev);
+	} else if (flip_or_move) {
+		crtc->fb = fb;
+		crtc->funcs->mode_set_base(crtc, crtc_info->x, crtc_info->y);
+	}
+
+	kfree(save_crtcs);
+	return 0;
+}
+
+/**
+ * drm_hotplug_stage_two
+ * @dev DRM device
+ * @output hotpluged output
+ *
+ * LOCKING.
+ * Caller must hold mode config lock, function might grap struct lock.
+ *
+ * Stage two of a hotplug.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_hotplug_stage_two(struct drm_device *dev, struct drm_output *output,
+			  bool connected)
+{
+	int has_config = 0;
+
+	/* We might want to do something more here */
+	if (!connected) {
+		DRM_DEBUG("not connected\n");
+		dev->mode_config.hotplug_counter++;
+		return 0;
+	}
+
+	if (output->crtc && output->crtc->desired_mode) {
+		DRM_DEBUG("drm thinks that the output already has a config\n");
+		has_config = 1;
+	}
+
+	drm_crtc_probe_output_modes(dev, 2048, 2048);
+
+	if (!has_config)
+		drm_pick_crtcs(dev);
+
+	if (!output->crtc || !output->crtc->desired_mode) {
+		DRM_DEBUG("could not find a desired mode or crtc for output\n");
+		goto out_err;
+	}
+
+	/* We should realy check if there is a fb using this crtc */
+	if (!has_config)
+		dev->driver->fb_probe(dev, output->crtc);
+	else {
+		dev->driver->fb_resize(dev, output->crtc);
+
+		if (!drm_crtc_set_mode(output->crtc, output->crtc->desired_mode, 0, 0))
+			DRM_ERROR("failed to set mode after hotplug\n");
+	}
+
+	drm_disable_unused_functions(dev);
+
+	dev->mode_config.hotplug_counter++;
+	return 0;
+
+out_err:
+	dev->mode_config.hotplug_counter++;
+	return 1;
+}
+EXPORT_SYMBOL(drm_hotplug_stage_two);
+
+int drm_mode_hotplug_ioctl(struct drm_device *dev,
+			   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_hotplug *arg = data;
+
+	arg->counter = dev->mode_config.hotplug_counter;
+
+	return 0;
+}
+
+/**
+ * drm_crtc_convert_to_umode - convert a drm_display_mode into a modeinfo
+ * @out: drm_mode_modeinfo struct to return to the user
+ * @in: drm_display_mode to use
+ *
+ * LOCKING:
+ * None.
+ *
+ * Convert a drm_display_mode into a drm_mode_modeinfo structure to return to
+ * the user.
+ */
+void drm_crtc_convert_to_umode(struct drm_mode_modeinfo *out, struct drm_display_mode *in)
+{
+	out->clock = in->clock;
+	out->hdisplay = in->hdisplay;
+	out->hsync_start = in->hsync_start;
+	out->hsync_end = in->hsync_end;
+	out->htotal = in->htotal;
+	out->hskew = in->hskew;
+	out->vdisplay = in->vdisplay;
+	out->vsync_start = in->vsync_start;
+	out->vsync_end = in->vsync_end;
+	out->vtotal = in->vtotal;
+	out->vscan = in->vscan;
+	out->vrefresh = in->vrefresh;
+	out->flags = in->flags;
+	out->type = in->type;
+	strncpy(out->name, in->name, DRM_DISPLAY_MODE_LEN);
+	out->name[DRM_DISPLAY_MODE_LEN-1] = 0;
+}
+
+/**
+ * drm_crtc_convert_to_umode - convert a modeinfo into a drm_display_mode
+ * @out: drm_display_mode to return to the user
+ * @in: drm_mode_modeinfo to use
+ *
+ * LOCKING:
+ * None.
+ *
+ * Convert a drmo_mode_modeinfo into a drm_display_mode structure to return to
+ * the caller.
+ */
+void drm_crtc_convert_umode(struct drm_display_mode *out, struct drm_mode_modeinfo *in)
+{
+	out->clock = in->clock;
+	out->hdisplay = in->hdisplay;
+	out->hsync_start = in->hsync_start;
+	out->hsync_end = in->hsync_end;
+	out->htotal = in->htotal;
+	out->hskew = in->hskew;
+	out->vdisplay = in->vdisplay;
+	out->vsync_start = in->vsync_start;
+	out->vsync_end = in->vsync_end;
+	out->vtotal = in->vtotal;
+	out->vscan = in->vscan;
+	out->vrefresh = in->vrefresh;
+	out->flags = in->flags;
+	out->type = in->type;
+	strncpy(out->name, in->name, DRM_DISPLAY_MODE_LEN);
+	out->name[DRM_DISPLAY_MODE_LEN-1] = 0;
+}
+	
+/**
+ * drm_mode_getresources - get graphics configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Construct a set of configuration description structures and return
+ * them to the user, including CRTC, output and framebuffer configuration.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getresources(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_card_res *card_res = data;
+	struct list_head *lh;
+	struct drm_framebuffer *fb;
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+	int ret = 0;
+	int output_count = 0;
+	int crtc_count = 0;
+	int fb_count = 0;
+	int copied = 0;
+	uint32_t __user *fb_id;
+	uint32_t __user *crtc_id;
+	uint32_t __user *output_id;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	list_for_each(lh, &dev->mode_config.fb_list)
+		fb_count++;
+
+	list_for_each(lh, &dev->mode_config.crtc_list)
+		crtc_count++;
+
+	list_for_each(lh, &dev->mode_config.output_list)
+		output_count++;
+
+	card_res->max_height = dev->mode_config.max_height;
+	card_res->min_height = dev->mode_config.min_height;
+	card_res->max_width = dev->mode_config.max_width;
+	card_res->min_width = dev->mode_config.min_width;
+
+	/* handle this in 4 parts */
+	/* FBs */
+	if (card_res->count_fbs >= fb_count) {
+		copied = 0;
+		fb_id = (uint32_t *)(unsigned long)card_res->fb_id_ptr;
+		list_for_each_entry(fb, &dev->mode_config.fb_list, head) {
+			if (put_user(fb->id, fb_id + copied)) {
+				ret = -EFAULT;
+				goto out;
+			}
+			copied++;
+		}
+	}
+	card_res->count_fbs = fb_count;
+
+	/* CRTCs */
+	if (card_res->count_crtcs >= crtc_count) {
+		copied = 0;
+		crtc_id = (uint32_t *)(unsigned long)card_res->crtc_id_ptr;
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head){
+			DRM_DEBUG("CRTC ID is %d\n", crtc->id);
+			if (put_user(crtc->id, crtc_id + copied)) {
+				ret = -EFAULT;
+				goto out;
+			}
+			copied++;
+		}
+	}
+	card_res->count_crtcs = crtc_count;
+
+
+	/* Outputs */
+	if (card_res->count_outputs >= output_count) {
+		copied = 0;
+		output_id = (uint32_t *)(unsigned long)card_res->output_id_ptr;
+		list_for_each_entry(output, &dev->mode_config.output_list,
+				    head) {
+ 			DRM_DEBUG("OUTPUT ID is %d\n", output->id);
+			if (put_user(output->id, output_id + copied)) {
+				ret = -EFAULT;
+				goto out;
+			}
+			copied++;
+		}
+	}
+	card_res->count_outputs = output_count;
+	
+	DRM_DEBUG("Counted %d %d\n", card_res->count_crtcs,
+		  card_res->count_outputs);
+
+out:	
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getcrtc - get CRTC configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Construct a CRTC configuration structure to return to the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getcrtc(struct drm_device *dev,
+		     void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_crtc *crtc_resp = data;
+	struct drm_crtc *crtc;
+	struct drm_output *output;
+	int ocount;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	crtc = idr_find(&dev->mode_config.crtc_idr, crtc_resp->crtc_id);
+	if (!crtc || (crtc->id != crtc_resp->crtc_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	crtc_resp->x = crtc->x;
+	crtc_resp->y = crtc->y;
+
+	if (crtc->fb)
+		crtc_resp->fb_id = crtc->fb->id;
+	else
+		crtc_resp->fb_id = 0;
+
+	crtc_resp->outputs = 0;
+	if (crtc->enabled) {
+
+		drm_crtc_convert_to_umode(&crtc_resp->mode, &crtc->mode);
+		crtc_resp->mode_valid = 1;
+		ocount = 0;
+		list_for_each_entry(output, &dev->mode_config.output_list, head) {
+			if (output->crtc == crtc)
+				crtc_resp->outputs |= 1 << (ocount++);
+		}
+		
+	} else {
+		crtc_resp->mode_valid = 0;
+	}
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getoutput - get output configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Construct a output configuration structure to return to the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getoutput(struct drm_device *dev,
+		       void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_get_output *out_resp = data;
+	struct drm_output *output;
+	struct drm_display_mode *mode;
+	int mode_count = 0;
+	int props_count = 0;
+	int ret = 0;
+	int copied = 0;
+	int i;
+	struct drm_mode_modeinfo u_mode;
+	struct drm_mode_modeinfo __user *mode_ptr;
+	uint32_t __user *prop_ptr;
+	uint64_t __user *prop_values;
+
+	memset(&u_mode, 0, sizeof(struct drm_mode_modeinfo));
+
+	DRM_DEBUG("output id %d:\n", out_resp->output);
+
+	mutex_lock(&dev->mode_config.mutex);
+	output= idr_find(&dev->mode_config.crtc_idr, out_resp->output);
+	if (!output || (output->id != out_resp->output)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	list_for_each_entry(mode, &output->modes, head)
+		mode_count++;
+	
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] != 0) {
+			props_count++;
+		}
+	}
+
+	if (out_resp->count_modes == 0) {
+		drm_crtc_probe_single_output_modes(output, dev->mode_config.max_width, dev->mode_config.max_height);
+	}
+
+	out_resp->output_type = output->output_type;
+	out_resp->output_type_id = output->output_type_id;
+	out_resp->mm_width = output->mm_width;
+	out_resp->mm_height = output->mm_height;
+	out_resp->subpixel = output->subpixel_order;
+	out_resp->connection = output->status;
+	if (output->crtc)
+		out_resp->crtc = output->crtc->id;
+	else
+		out_resp->crtc = 0;
+
+	out_resp->crtcs = output->possible_crtcs;
+	out_resp->clones = output->possible_clones;
+
+	if ((out_resp->count_modes >= mode_count) && mode_count) {
+		copied = 0;
+		mode_ptr = (struct drm_mode_modeinfo *)(unsigned long)out_resp->modes_ptr;
+		list_for_each_entry(mode, &output->modes, head) {
+			drm_crtc_convert_to_umode(&u_mode, mode);
+			if (copy_to_user(mode_ptr + copied,
+					 &u_mode, sizeof(u_mode))) {
+				ret = -EFAULT;
+				goto out;
+			}
+			copied++;
+			
+		}
+	}
+	out_resp->count_modes = mode_count;
+
+	if ((out_resp->count_props >= props_count) && props_count) {
+		copied = 0;
+		prop_ptr = (uint32_t *)(unsigned long)(out_resp->props_ptr);
+		prop_values = (uint64_t *)(unsigned long)(out_resp->prop_values_ptr);
+		for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+			if (output->property_ids[i] != 0) {
+				if (put_user(output->property_ids[i], prop_ptr + copied)) {
+					ret = -EFAULT;
+					goto out;
+				}
+
+				if (put_user(output->property_values[i], prop_values + copied)) {
+					ret = -EFAULT;
+					goto out;
+				}
+				copied++;
+			}
+		}
+	}
+	out_resp->count_props = props_count;
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_setcrtc - set CRTC configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Build a new CRTC configuration based on user request.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_setcrtc(struct drm_device *dev,
+		     void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_crtc *crtc_req = data;
+	struct drm_crtc *crtc, *crtcfb;
+	struct drm_output **output_set = NULL, *output;
+	struct drm_framebuffer *fb = NULL;
+	struct drm_display_mode *mode = NULL;
+	uint32_t __user *set_outputs_ptr;
+	int ret = 0;
+	int i;
+
+	mutex_lock(&dev->mode_config.mutex);
+	crtc = idr_find(&dev->mode_config.crtc_idr, crtc_req->crtc_id);
+	if (!crtc || (crtc->id != crtc_req->crtc_id)) {
+		DRM_DEBUG("Unknown CRTC ID %d\n", crtc_req->crtc_id);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->mode_valid) {
+		/* If we have a mode we need a framebuffer. */
+		/* If we pass -1, set the mode with the currently bound fb */
+		if (crtc_req->fb_id == -1) {
+			list_for_each_entry(crtcfb, &dev->mode_config.crtc_list, head) {
+				if (crtcfb == crtc) {
+					DRM_DEBUG("Using current fb for setmode\n");
+					fb = crtc->fb;		
+				}
+			}
+		} else {
+			fb = idr_find(&dev->mode_config.crtc_idr, crtc_req->fb_id);
+			if (!fb || (fb->id != crtc_req->fb_id)) {
+				DRM_DEBUG("Unknown FB ID%d\n", crtc_req->fb_id);
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+
+		mode = drm_mode_create(dev);
+		drm_crtc_convert_umode(mode, &crtc_req->mode);
+		drm_mode_set_crtcinfo(mode, CRTC_INTERLACE_HALVE_V);
+	}
+
+	if (crtc_req->count_outputs == 0 && mode) {
+		DRM_DEBUG("Count outputs is 0 but mode set\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->count_outputs > 0 && !mode && !fb) {
+		DRM_DEBUG("Count outputs is %d but no mode or fb set\n", crtc_req->count_outputs);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->count_outputs > 0) {
+		u32 out_id;
+		/* Maybe we should check that count_outputs is a sensible value. */
+		output_set = kmalloc(crtc_req->count_outputs *
+				     sizeof(struct drm_output *), GFP_KERNEL);
+		if (!output_set) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		for (i = 0; i < crtc_req->count_outputs; i++) {
+			set_outputs_ptr = (uint32_t *)(unsigned long)crtc_req->set_outputs_ptr;
+			if (get_user(out_id, &set_outputs_ptr[i])) {
+				ret = -EFAULT;
+				goto out;
+			}
+
+			output = idr_find(&dev->mode_config.crtc_idr, out_id);
+			if (!output || (out_id != output->id)) {
+				DRM_DEBUG("Output id %d unknown\n", out_id);
+				ret = -EINVAL;
+				goto out;
+			}
+
+			output_set[i] = output;
+		}
+	}
+
+	ret = drm_crtc_set_config(crtc, crtc_req, mode, output_set, fb);
+
+out:
+	kfree(output_set);
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+int drm_mode_cursor_ioctl(struct drm_device *dev,
+			void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_cursor *req = data;
+	struct drm_crtc *crtc;
+	struct drm_buffer_object *bo = NULL; /* must be set */
+	int ret = 0;
+
+	DRM_DEBUG("\n");
+
+	if (!req->flags) {
+		DRM_ERROR("no operation set\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->mode_config.mutex);
+	crtc = idr_find(&dev->mode_config.crtc_idr, req->crtc);
+	if (!crtc || (crtc->id != req->crtc)) {
+		DRM_DEBUG("Unknown CRTC ID %d\n", req->crtc);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (req->flags & DRM_MODE_CURSOR_BO) {
+		/* Turn of the cursor if handle is 0 */
+		if (req->handle)
+			ret = drm_get_buffer_object(dev, &bo, req->handle);
+
+		if (ret) {
+			DRM_ERROR("invalid buffer id\n");
+			ret = -EINVAL;
+			goto out;
+		}
+
+		if (crtc->funcs->cursor_set) {
+			ret = crtc->funcs->cursor_set(crtc, bo, req->width, req->height);
+		} else {
+			DRM_ERROR("crtc does not support cursor\n");
+			ret = -EFAULT;
+			goto out;
+		}
+	}
+
+	if (req->flags & DRM_MODE_CURSOR_MOVE) {
+		if (crtc->funcs->cursor_move) {
+			ret = crtc->funcs->cursor_move(crtc, req->x, req->y);
+		} else {
+			DRM_ERROR("crtc does not support cursor\n");
+			ret = -EFAULT;
+			goto out;
+		}
+	}
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_addfb - add an FB to the graphics configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Add a new FB to the specified CRTC, given a user request.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_addfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd *r = data;
+	struct drm_mode_config *config = &dev->mode_config;
+	struct drm_framebuffer *fb;
+	struct drm_buffer_object *bo;
+	int ret = 0;
+
+	if ((config->min_width > r->width) || (r->width > config->max_width)) {
+		DRM_ERROR("mode new framebuffer width not within limits\n");
+		return -EINVAL;
+	}
+	if ((config->min_height > r->height) || (r->height > config->max_height)) {
+		DRM_ERROR("mode new framebuffer height not within limits\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->mode_config.mutex);
+	/* TODO check limits are okay */
+	ret = drm_get_buffer_object(dev, &bo, r->handle);
+	if (ret || !bo) {
+		DRM_ERROR("BO handle not valid\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* TODO check buffer is sufficently large */
+	/* TODO setup destructor callback */
+
+	fb = drm_framebuffer_create(dev);
+	if (!fb) {
+		DRM_ERROR("could not create framebuffer\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	fb->width = r->width;
+	fb->height = r->height;
+	fb->pitch = r->pitch;
+	fb->bits_per_pixel = r->bpp;
+	fb->depth = r->depth;
+	fb->bo = bo;
+
+	r->buffer_id = fb->id;
+
+	list_add(&fb->filp_head, &file_priv->fbs);
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_rmfb - remove an FB from the configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Remove the FB specified by the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_rmfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_framebuffer *fb = 0;
+	struct drm_framebuffer *fbl = 0;
+	uint32_t *id = data;
+	int ret = 0;
+	int found = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	fb = idr_find(&dev->mode_config.crtc_idr, *id);
+	/* TODO check that we realy get a framebuffer back. */
+	if (!fb || (*id != fb->id)) {
+		DRM_ERROR("mode invalid framebuffer id\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	list_for_each_entry(fbl, &file_priv->fbs, filp_head)
+		if (fb == fbl)
+			found = 1;
+
+	if (!found) {
+		DRM_ERROR("tried to remove a fb that we didn't own\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* TODO release all crtc connected to the framebuffer */
+	/* TODO unhock the destructor from the buffer object */
+
+	if (fb->bo->type == drm_bo_type_kernel)
+		DRM_ERROR("the bo type should not be of kernel type\n");
+
+	list_del(&fb->filp_head);
+	drm_framebuffer_destroy(fb);
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getfb - get FB info
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Lookup the FB given its ID and return info about it.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd *r = data;
+	struct drm_framebuffer *fb;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	fb = idr_find(&dev->mode_config.crtc_idr, r->buffer_id);
+	if (!fb || (r->buffer_id != fb->id)) {
+		DRM_ERROR("invalid framebuffer id\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	r->height = fb->height;
+	r->width = fb->width;
+	r->depth = fb->depth;
+	r->bpp = fb->bits_per_pixel;
+	r->handle = fb->bo->base.hash.key;
+	r->pitch = fb->pitch;
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_fb_release - remove and free the FBs on this file
+ * @filp: file * from the ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Destroy all the FBs associated with @filp.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+void drm_fb_release(struct file *filp)
+{
+	struct drm_file *priv = filp->private_data;
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_framebuffer *fb, *tfb;
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_for_each_entry_safe(fb, tfb, &priv->fbs, filp_head) {
+		list_del(&fb->filp_head);
+		if (fb->bo->type == drm_bo_type_kernel)
+			DRM_ERROR("the bo type should not be of kernel_type, the kernel will probably explode, why Dave\n");
+
+		drm_framebuffer_destroy(fb);
+	}
+	mutex_unlock(&dev->mode_config.mutex);
+}
+
+/*
+ *
+ */
+
+static int drm_mode_attachmode(struct drm_device *dev,
+			       struct drm_output *output,
+			       struct drm_display_mode *mode)
+{
+	int ret = 0;
+
+	list_add_tail(&mode->head, &output->user_modes);
+	return ret;
+}
+
+int drm_mode_attachmode_crtc(struct drm_device *dev, struct drm_crtc *crtc,
+			     struct drm_display_mode *mode)
+{
+	struct drm_output *output;
+	int ret = 0;
+	struct drm_display_mode *dup_mode;
+	int need_dup = 0;
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (output->crtc == crtc) {
+			if (need_dup)
+				dup_mode = drm_mode_duplicate(dev, mode);
+			else
+				dup_mode = mode;
+			ret = drm_mode_attachmode(dev, output, dup_mode); 
+			if (ret)
+				return ret;
+			need_dup = 1;
+		}
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_mode_attachmode_crtc);
+
+static int drm_mode_detachmode(struct drm_device *dev,
+			       struct drm_output *output,
+			       struct drm_display_mode *mode)
+{
+	int found = 0;
+	int ret = 0;
+	struct drm_display_mode *match_mode, *t;
+
+	list_for_each_entry_safe(match_mode, t, &output->user_modes, head) {
+		if (drm_mode_equal(match_mode, mode)) {
+			list_del(&match_mode->head);
+			drm_mode_destroy(dev, match_mode);
+			found = 1;
+			break;
+		}
+	}
+
+	if (!found)
+		ret = -EINVAL;
+
+	return ret;
+}
+
+int drm_mode_detachmode_crtc(struct drm_device *dev, struct drm_display_mode *mode)
+{
+	struct drm_output *output;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		drm_mode_detachmode(dev, output, mode);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_mode_detachmode_crtc);
+
+/**
+ * drm_fb_attachmode - Attach a user mode to an output
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * This attaches a user specified mode to an output.
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_attachmode_ioctl(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_mode_cmd *mode_cmd = data;
+	struct drm_output *output;
+	struct drm_display_mode *mode;
+	struct drm_mode_modeinfo *umode = &mode_cmd->mode;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	output = idr_find(&dev->mode_config.crtc_idr, mode_cmd->output_id);
+	if (!output || (output->id != mode_cmd->output_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	mode = drm_mode_create(dev);
+	if (!mode) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	
+	drm_crtc_convert_umode(mode, umode);
+
+	ret = drm_mode_attachmode(dev, output, mode);
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+
+/**
+ * drm_fb_detachmode - Detach a user specified mode from an output
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_detachmode_ioctl(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_mode_cmd *mode_cmd = data;
+	struct drm_output *output;
+	struct drm_display_mode mode;
+	struct drm_mode_modeinfo *umode = &mode_cmd->mode;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	output = idr_find(&dev->mode_config.crtc_idr, mode_cmd->output_id);
+	if (!output || (output->id != mode_cmd->output_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+	
+	drm_crtc_convert_umode(&mode, umode);
+	ret = drm_mode_detachmode(dev, output, &mode);
+out:	       
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+struct drm_property *drm_property_create(struct drm_device *dev, int flags,
+					 const char *name, int num_values)
+{
+	struct drm_property *property = NULL;
+
+	property = kzalloc(sizeof(struct drm_property), GFP_KERNEL);
+	if (!property)
+		return NULL;
+
+	if (num_values) {
+		property->values = kzalloc(sizeof(uint64_t)*num_values, GFP_KERNEL);
+		if (!property->values)
+			goto fail;
+	}
+
+	property->id = drm_idr_get(dev, property);
+	property->flags = flags;
+	property->num_values = num_values;
+	INIT_LIST_HEAD(&property->enum_blob_list);
+
+	if (name)
+		strncpy(property->name, name, DRM_PROP_NAME_LEN);
+
+	list_add_tail(&property->head, &dev->mode_config.property_list);
+	return property;
+fail:
+	kfree(property);
+	return NULL;
+}
+EXPORT_SYMBOL(drm_property_create);
+
+int drm_property_add_enum(struct drm_property *property, int index,
+			  uint64_t value, const char *name)
+{
+	struct drm_property_enum *prop_enum;
+
+	if (!(property->flags & DRM_MODE_PROP_ENUM))
+		return -EINVAL;
+
+	if (!list_empty(&property->enum_blob_list)) {
+		list_for_each_entry(prop_enum, &property->enum_blob_list, head) {
+			if (prop_enum->value == value) {
+				strncpy(prop_enum->name, name, DRM_PROP_NAME_LEN); 
+				prop_enum->name[DRM_PROP_NAME_LEN-1] = '\0';
+				return 0;
+			}
+		}
+	}
+
+	prop_enum = kzalloc(sizeof(struct drm_property_enum), GFP_KERNEL);
+	if (!prop_enum)
+		return -ENOMEM;
+
+	strncpy(prop_enum->name, name, DRM_PROP_NAME_LEN); 
+	prop_enum->name[DRM_PROP_NAME_LEN-1] = '\0';
+	prop_enum->value = value;
+
+	property->values[index] = value;
+	list_add_tail(&prop_enum->head, &property->enum_blob_list);
+	return 0;
+}
+EXPORT_SYMBOL(drm_property_add_enum);
+
+void drm_property_destroy(struct drm_device *dev, struct drm_property *property)
+{
+	struct drm_property_enum *prop_enum, *pt;
+
+	list_for_each_entry_safe(prop_enum, pt, &property->enum_blob_list, head) {
+		list_del(&prop_enum->head);
+		kfree(prop_enum);
+	}
+
+	if (property->num_values)
+		kfree(property->values);
+	drm_idr_put(dev, property->id);
+	list_del(&property->head);
+	kfree(property);	
+}
+EXPORT_SYMBOL(drm_property_destroy);
+
+int drm_output_attach_property(struct drm_output *output,
+			       struct drm_property *property, uint64_t init_val)
+{
+	int i;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] == 0) {
+			output->property_ids[i] = property->id;
+			output->property_values[i] = init_val;
+			break;
+		}
+	}
+
+	if (i == DRM_OUTPUT_MAX_PROPERTY)
+		return -EINVAL;
+	return 0;
+}
+EXPORT_SYMBOL(drm_output_attach_property);
+
+int drm_output_property_set_value(struct drm_output *output,
+				  struct drm_property *property, uint64_t value)
+{
+	int i;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] == property->id) {
+			output->property_values[i] = value;
+			break;
+		}
+	}
+
+	if (i == DRM_OUTPUT_MAX_PROPERTY)
+		return -EINVAL;
+	return 0;
+}
+EXPORT_SYMBOL(drm_output_property_set_value);
+
+int drm_mode_getproperty_ioctl(struct drm_device *dev,
+			       void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_get_property *out_resp = data;
+	struct drm_property *property;
+	int enum_count = 0;
+	int blob_count = 0;
+	int value_count = 0;
+	int ret = 0, i;
+	int copied;
+	struct drm_property_enum *prop_enum;
+	struct drm_mode_property_enum __user *enum_ptr;
+	struct drm_property_blob *prop_blob;
+	uint32_t *blob_id_ptr;
+	uint64_t __user *values_ptr;
+	uint32_t __user *blob_length_ptr;
+
+	mutex_lock(&dev->mode_config.mutex);
+	property = idr_find(&dev->mode_config.crtc_idr, out_resp->prop_id);
+	if (!property || (property->id != out_resp->prop_id)) {
+		ret = -EINVAL;
+		goto done;
+	}
+
+	if (property->flags & DRM_MODE_PROP_ENUM) {
+		list_for_each_entry(prop_enum, &property->enum_blob_list, head)
+			enum_count++;
+	} else if (property->flags & DRM_MODE_PROP_BLOB) {
+		list_for_each_entry(prop_blob, &property->enum_blob_list, head)
+			blob_count++;
+	}
+
+	value_count = property->num_values;
+
+	strncpy(out_resp->name, property->name, DRM_PROP_NAME_LEN);
+	out_resp->name[DRM_PROP_NAME_LEN-1] = 0;
+	out_resp->flags = property->flags;
+
+	if ((out_resp->count_values >= value_count) && value_count) {
+		values_ptr = (uint64_t *)(unsigned long)out_resp->values_ptr;
+		for (i = 0; i < value_count; i++) {
+			if (copy_to_user(values_ptr + i, &property->values[i], sizeof(uint64_t))) {
+				ret = -EFAULT;
+				goto done;
+			}
+		}
+	}
+	out_resp->count_values = value_count;
+
+	if (property->flags & DRM_MODE_PROP_ENUM) {
+
+		if ((out_resp->count_enum_blobs >= enum_count) && enum_count) {
+			copied = 0;
+			enum_ptr = (struct drm_mode_property_enum *)(unsigned long)out_resp->enum_blob_ptr;
+			list_for_each_entry(prop_enum, &property->enum_blob_list, head) {
+				
+				if (copy_to_user(&enum_ptr[copied].value, &prop_enum->value, sizeof(uint64_t))) {
+					ret = -EFAULT;
+					goto done;
+				}
+				
+				if (copy_to_user(&enum_ptr[copied].name,
+						 &prop_enum->name, DRM_PROP_NAME_LEN)) {
+					ret = -EFAULT;
+					goto done;
+				}
+				copied++;
+			}
+		}
+		out_resp->count_enum_blobs = enum_count;
+	}
+
+	if (property->flags & DRM_MODE_PROP_BLOB) {
+		if ((out_resp->count_enum_blobs >= blob_count) && blob_count) {
+			copied = 0;
+			blob_id_ptr = (uint32_t *)(unsigned long)out_resp->enum_blob_ptr;
+			blob_length_ptr = (uint32_t *)(unsigned long)out_resp->values_ptr;
+			
+			list_for_each_entry(prop_blob, &property->enum_blob_list, head) {
+				if (put_user(prop_blob->id, blob_id_ptr + copied)) {
+					ret = -EFAULT;
+					goto done;
+				}
+				
+				if (put_user(prop_blob->length, blob_length_ptr + copied)) {
+					ret = -EFAULT;
+					goto done;
+				}
+				
+				copied++;
+			}
+		}
+		out_resp->count_enum_blobs = enum_count;
+	}
+done:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+static struct drm_property_blob *drm_property_create_blob(struct drm_device *dev, int length,
+							  void *data)
+{
+	struct drm_property_blob *blob;
+
+	if (!length || !data)
+		return NULL;
+
+	blob = kzalloc(sizeof(struct drm_property_blob)+length, GFP_KERNEL);
+	if (!blob)
+		return NULL;
+
+	blob->data = (void *)((char *)blob + sizeof(struct drm_property_blob));
+	blob->length = length;
+
+	memcpy(blob->data, data, length);
+
+	blob->id = drm_idr_get(dev, blob);
+	
+	list_add_tail(&blob->head, &dev->mode_config.property_blob_list);
+	return blob;
+}
+
+static void drm_property_destroy_blob(struct drm_device *dev,
+			       struct drm_property_blob *blob)
+{
+	drm_idr_put(dev, blob->id);
+	list_del(&blob->head);
+	kfree(blob);
+}
+
+int drm_mode_getblob_ioctl(struct drm_device *dev,
+			   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_get_blob *out_resp = data;
+	struct drm_property_blob *blob;
+	int ret = 0;
+	void *blob_ptr;
+
+	mutex_lock(&dev->mode_config.mutex);
+	
+	blob = idr_find(&dev->mode_config.crtc_idr, out_resp->blob_id);
+	if (!blob || (blob->id != out_resp->blob_id)) {
+		ret = -EINVAL;
+		goto done;
+	}
+
+	if (out_resp->length == blob->length) {
+		blob_ptr = (void *)(unsigned long)out_resp->data;
+		if (copy_to_user(blob_ptr, blob->data, blob->length)){
+			ret = -EFAULT;
+			goto done;
+		}
+	}
+	out_resp->length = blob->length;
+
+done:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+int drm_mode_output_update_edid_property(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	int ret = 0;
+	if (output->edid_blob_ptr)
+		drm_property_destroy_blob(dev, output->edid_blob_ptr);
+
+	output->edid_blob_ptr = drm_property_create_blob(output->dev, 128, edid);
+	
+	ret = drm_output_property_set_value(output, dev->mode_config.edid_property, output->edid_blob_ptr->id);
+	return ret;
+}
+EXPORT_SYMBOL(drm_mode_output_update_edid_property);
+
+int drm_mode_output_property_set_ioctl(struct drm_device *dev,
+				       void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_output_set_property *out_resp = data;
+	struct drm_property *property;
+	struct drm_output *output;
+	int ret = -EINVAL;
+	int i;
+
+	mutex_lock(&dev->mode_config.mutex);
+	output = idr_find(&dev->mode_config.crtc_idr, out_resp->output_id);
+	if (!output || (output->id != out_resp->output_id)) {
+		goto out;
+	}
+
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] == out_resp->prop_id)
+			break;
+	}
+
+	if (i == DRM_OUTPUT_MAX_PROPERTY) {
+		goto out;
+	}
+	
+	property = idr_find(&dev->mode_config.crtc_idr, out_resp->prop_id);
+	if (!property || (property->id != out_resp->prop_id)) {
+		goto out;
+	}
+
+	if (property->flags & DRM_MODE_PROP_IMMUTABLE)
+		goto out;
+
+	if (property->flags & DRM_MODE_PROP_RANGE) {
+		if (out_resp->value < property->values[0])
+			goto out;
+
+		if (out_resp->value > property->values[1])
+			goto out;
+	} else {
+		int found = 0;
+		for (i = 0; i < property->num_values; i++) {
+			if (property->values[i] == out_resp->value) {
+				found = 1;
+				break;
+			}
+		}
+		if (!found) {
+			goto out;
+		}
+	}
+
+	if (output->funcs->set_property)
+		ret = output->funcs->set_property(output, property, out_resp->value);
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
diff --git a/drivers/char/drm/drm_crtc.h b/drivers/char/drm/drm_crtc.h
new file mode 100644
index 0000000..bbeab60
--- /dev/null
+++ b/drivers/char/drm/drm_crtc.h
@@ -0,0 +1,666 @@
+/*
+ * Copyright  2006 Keith Packard
+ * Copyright  2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+#ifndef __DRM_CRTC_H__
+#define __DRM_CRTC_H__
+
+#include <linux/i2c.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/idr.h>
+
+#include <linux/fb.h>
+
+struct drm_device;
+
+/*
+ * Note on terminology:  here, for brevity and convenience, we refer to output
+ * control chips as 'CRTCs'.  They can control any type of output, VGA, LVDS,
+ * DVI, etc.  And 'screen' refers to the whole of the visible display, which
+ * may span multiple monitors (and therefore multiple CRTC and output
+ * structures).
+ */
+
+enum drm_mode_status {
+    MODE_OK	= 0,	/* Mode OK */
+    MODE_HSYNC,		/* hsync out of range */
+    MODE_VSYNC,		/* vsync out of range */
+    MODE_H_ILLEGAL,	/* mode has illegal horizontal timings */
+    MODE_V_ILLEGAL,	/* mode has illegal horizontal timings */
+    MODE_BAD_WIDTH,	/* requires an unsupported linepitch */
+    MODE_NOMODE,	/* no mode with a maching name */
+    MODE_NO_INTERLACE,	/* interlaced mode not supported */
+    MODE_NO_DBLESCAN,	/* doublescan mode not supported */
+    MODE_NO_VSCAN,	/* multiscan mode not supported */
+    MODE_MEM,		/* insufficient video memory */
+    MODE_VIRTUAL_X,	/* mode width too large for specified virtual size */
+    MODE_VIRTUAL_Y,	/* mode height too large for specified virtual size */
+    MODE_MEM_VIRT,	/* insufficient video memory given virtual size */
+    MODE_NOCLOCK,	/* no fixed clock available */
+    MODE_CLOCK_HIGH,	/* clock required is too high */
+    MODE_CLOCK_LOW,	/* clock required is too low */
+    MODE_CLOCK_RANGE,	/* clock/mode isn't in a ClockRange */
+    MODE_BAD_HVALUE,	/* horizontal timing was out of range */
+    MODE_BAD_VVALUE,	/* vertical timing was out of range */
+    MODE_BAD_VSCAN,	/* VScan value out of range */
+    MODE_HSYNC_NARROW,	/* horizontal sync too narrow */
+    MODE_HSYNC_WIDE,	/* horizontal sync too wide */
+    MODE_HBLANK_NARROW,	/* horizontal blanking too narrow */
+    MODE_HBLANK_WIDE,	/* horizontal blanking too wide */
+    MODE_VSYNC_NARROW,	/* vertical sync too narrow */
+    MODE_VSYNC_WIDE,	/* vertical sync too wide */
+    MODE_VBLANK_NARROW,	/* vertical blanking too narrow */
+    MODE_VBLANK_WIDE,	/* vertical blanking too wide */
+    MODE_PANEL,         /* exceeds panel dimensions */
+    MODE_INTERLACE_WIDTH, /* width too large for interlaced mode */
+    MODE_ONE_WIDTH,     /* only one width is supported */
+    MODE_ONE_HEIGHT,    /* only one height is supported */
+    MODE_ONE_SIZE,      /* only one resolution is supported */
+    MODE_NO_REDUCED,    /* monitor doesn't accept reduced blanking */
+    MODE_UNVERIFIED = -3, /* mode needs to reverified */
+    MODE_BAD = -2,	/* unspecified reason */
+    MODE_ERROR	= -1	/* error condition */
+};
+
+#define DRM_MODE_TYPE_CLOCK_CRTC_C (DRM_MODE_TYPE_CLOCK_C | \
+				    DRM_MODE_TYPE_CRTC_C)
+
+#define DRM_MODE(nm, t, c, hd, hss, hse, ht, hsk, vd, vss, vse, vt, vs, f) \
+	.name = nm, .status = 0, .type = (t), .clock = (c), \
+	.hdisplay = (hd), .hsync_start = (hss), .hsync_end = (hse), \
+	.htotal = (ht), .hskew = (hsk), .vdisplay = (vd), \
+	.vsync_start = (vss), .vsync_end = (vse), .vtotal = (vt), \
+	.vscan = (vs), .flags = (f), .vrefresh = 0
+
+struct drm_display_mode {
+	/* Header */
+	struct list_head head;
+	char name[DRM_DISPLAY_MODE_LEN];
+	int mode_id;
+	int output_count;
+	enum drm_mode_status status;
+	int type;
+
+	/* Proposed mode values */
+	int clock;
+	int hdisplay;
+	int hsync_start;
+	int hsync_end;
+	int htotal;
+	int hskew;
+	int vdisplay;
+	int vsync_start;
+	int vsync_end;
+	int vtotal;
+	int vscan;
+	unsigned int flags;
+
+	/* Actual mode we give to hw */
+	int clock_index;
+	int synth_clock;
+	int crtc_hdisplay;
+	int crtc_hblank_start;
+	int crtc_hblank_end;
+	int crtc_hsync_start;
+	int crtc_hsync_end;
+	int crtc_htotal;
+	int crtc_hskew;
+	int crtc_vdisplay;
+	int crtc_vblank_start;
+	int crtc_vblank_end;
+	int crtc_vsync_start;
+	int crtc_vsync_end;
+	int crtc_vtotal;
+	int crtc_hadjusted;
+	int crtc_vadjusted;
+
+	/* Driver private mode info */
+	int private_size;
+	int *private;
+	int private_flags;
+
+	int vrefresh;
+	float hsync;
+};
+
+/* Video mode flags */
+#define V_PHSYNC	(1<<0)
+#define V_NHSYNC	(1<<1)
+#define V_PVSYNC	(1<<2)
+#define V_NVSYNC	(1<<3)
+#define V_INTERLACE	(1<<4)
+#define V_DBLSCAN	(1<<5)
+#define V_CSYNC		(1<<6)
+#define V_PCSYNC	(1<<7)
+#define V_NCSYNC	(1<<8)
+#define V_HSKEW		(1<<9) /* hskew provided */
+#define V_BCAST		(1<<10)
+#define V_PIXMUX	(1<<11)
+#define V_DBLCLK	(1<<12)
+#define V_CLKDIV2	(1<<13)
+
+#define CRTC_INTERLACE_HALVE_V 0x1 /* halve V values for interlacing */
+
+#define DPMSModeOn 0
+#define DPMSModeStandby 1
+#define DPMSModeSuspend 2
+#define DPMSModeOff 3
+
+#define ConnectorUnknown 0
+#define ConnectorVGA 1
+#define ConnectorDVII 2
+#define ConnectorDVID 3
+#define ConnectorDVIA 4
+#define ConnectorComposite 5
+#define ConnectorSVIDEO 6
+#define ConnectorLVDS 7
+#define ConnectorComponent 8
+#define Connector9PinDIN 9
+#define ConnectorDisplayPort 10
+#define ConnectorHDMIA 11
+#define ConnectorHDMIB 12
+
+enum drm_output_status {
+	output_status_connected = 1,
+	output_status_disconnected = 2,
+	output_status_unknown = 3,
+};
+
+enum subpixel_order {
+	SubPixelUnknown = 0,
+	SubPixelHorizontalRGB,
+	SubPixelHorizontalBGR,
+	SubPixelVerticalRGB,
+	SubPixelVerticalBGR,
+	SubPixelNone,
+};
+
+/*
+ * Describes a given display (e.g. CRT or flat panel) and its limitations.
+ */
+struct drm_display_info {
+	char name[DRM_DISPLAY_INFO_LEN];
+	/* Input info */
+	bool serration_vsync;
+	bool sync_on_green;
+	bool composite_sync;
+	bool separate_syncs;
+	bool blank_to_black;
+	unsigned char video_level;
+	bool digital;
+	/* Physical size */
+        unsigned int width_mm;
+	unsigned int height_mm;
+
+	/* Display parameters */
+	unsigned char gamma; /* FIXME: storage format */
+	bool gtf_supported;
+	bool standard_color;
+	enum {
+		monochrome,
+		rgb,
+		other,
+		unknown,
+	} display_type;
+	bool active_off_supported;
+	bool suspend_supported;
+	bool standby_supported;
+
+	/* Color info FIXME: storage format */
+	unsigned short redx, redy;
+	unsigned short greenx, greeny;
+	unsigned short bluex, bluey;
+	unsigned short whitex, whitey;
+
+	/* Clock limits FIXME: storage format */
+	unsigned int min_vfreq, max_vfreq;
+	unsigned int min_hfreq, max_hfreq;
+	unsigned int pixel_clock;
+
+	/* White point indices FIXME: storage format */
+	unsigned int wpx1, wpy1;
+	unsigned int wpgamma1;
+	unsigned int wpx2, wpy2;
+	unsigned int wpgamma2;
+
+	/* Preferred mode (if any) */
+	struct drm_display_mode *preferred_mode;
+	char *raw_edid; /* if any */
+};
+
+struct drm_framebuffer {
+	struct drm_device *dev;
+	struct list_head head;
+	int id; /* idr assigned */
+	unsigned int pitch;
+	unsigned int width;
+	unsigned int height;
+	/* depth can be 15 or 16 */
+	unsigned int depth;
+	int bits_per_pixel;
+	int flags;
+	struct drm_buffer_object *bo;
+	void *fbdev;
+	u32 pseudo_palette[17];
+	struct drm_bo_kmap_obj kmap;
+	struct list_head filp_head;
+};
+
+struct drm_property_blob {
+	struct list_head head;
+	unsigned int length;
+	unsigned int id;
+	void *data;
+};
+
+struct drm_property_enum {
+	uint64_t value;
+	struct list_head head;
+	char name[DRM_PROP_NAME_LEN];
+};
+
+struct drm_property {
+	struct list_head head;
+	int id; /* idr assigned */
+	uint32_t flags;
+	char name[DRM_PROP_NAME_LEN];
+	uint32_t num_values;
+	uint64_t *values;
+
+	struct list_head enum_blob_list;
+};
+
+struct drm_crtc;
+struct drm_output;
+
+/**
+ * drm_crtc_funcs - control CRTCs for a given device
+ * @dpms: control display power levels
+ * @save: save CRTC state
+ * @resore: restore CRTC state
+ * @lock: lock the CRTC
+ * @unlock: unlock the CRTC
+ * @shadow_allocate: allocate shadow pixmap
+ * @shadow_create: create shadow pixmap for rotation support
+ * @shadow_destroy: free shadow pixmap
+ * @mode_fixup: fixup proposed mode
+ * @mode_set: set the desired mode on the CRTC
+ * @gamma_set: specify color ramp for CRTC
+ * @cleanup: cleanup driver private state prior to close
+ *
+ * The drm_crtc_funcs structure is the central CRTC management structure
+ * in the DRM.  Each CRTC controls one or more outputs (note that the name
+ * CRTC is simply historical, a CRTC may control LVDS, VGA, DVI, TV out, etc.
+ * outputs, not just CRTs).
+ *
+ * Each driver is responsible for filling out this structure at startup time,
+ * in addition to providing other modesetting features, like i2c and DDC
+ * bus accessors.
+ */
+struct drm_crtc_funcs {
+	/*
+	 * Control power levels on the CRTC.  If the mode passed in is
+	 * unsupported, the provider must use the next lowest power level.
+	 */
+	void (*dpms)(struct drm_crtc *crtc, int mode);
+
+	/* JJJ:  Are these needed? */
+	/* Save CRTC state */
+	void (*save)(struct drm_crtc *crtc); /* suspend? */
+	/* Restore CRTC state */
+	void (*restore)(struct drm_crtc *crtc); /* resume? */
+	bool (*lock)(struct drm_crtc *crtc);
+	void (*unlock)(struct drm_crtc *crtc);
+
+	void (*prepare)(struct drm_crtc *crtc);
+	void (*commit)(struct drm_crtc *crtc);
+
+	/* Provider can fixup or change mode timings before modeset occurs */
+	bool (*mode_fixup)(struct drm_crtc *crtc,
+			   struct drm_display_mode *mode,
+			   struct drm_display_mode *adjusted_mode);
+	/* Actually set the mode */
+	void (*mode_set)(struct drm_crtc *crtc, struct drm_display_mode *mode,
+			 struct drm_display_mode *adjusted_mode, int x, int y);
+
+	/* Move the crtc on the current fb to the given position *optional* */
+	void (*mode_set_base)(struct drm_crtc *crtc, int x, int y);
+
+	/* cursor controls */
+	int (*cursor_set)(struct drm_crtc *crtc, struct drm_buffer_object *bo,
+			  uint32_t width, uint32_t height);
+	int (*cursor_move)(struct drm_crtc *crtc, int x, int y);
+
+	/* Set gamma on the CRTC */
+	void (*gamma_set)(struct drm_crtc *crtc, u16 r, u16 g, u16 b,
+			  int regno);
+	/* Driver cleanup routine */
+	void (*cleanup)(struct drm_crtc *crtc);
+};
+
+/**
+ * drm_crtc - central CRTC control structure
+ * @enabled: is this CRTC enabled?
+ * @x: x position on screen
+ * @y: y position on screen
+ * @desired_mode: new desired mode
+ * @desired_x: desired x for desired_mode
+ * @desired_y: desired y for desired_mode
+ * @funcs: CRTC control functions
+ * @driver_private: arbitrary driver data
+ *
+ * Each CRTC may have one or more outputs associated with it.  This structure
+ * allows the CRTC to be controlled.
+ */
+struct drm_crtc {
+	struct drm_device *dev;
+	struct list_head head;
+
+	int id; /* idr assigned */
+
+	/* framebuffer the output is currently bound to */
+	struct drm_framebuffer *fb;
+
+	bool enabled;
+
+	/* JJJ: are these needed? */
+	bool cursor_in_range;
+	bool cursor_shown;
+
+	struct drm_display_mode mode;
+
+	int x, y;
+	struct drm_display_mode *desired_mode;
+	int desired_x, desired_y;
+	const struct drm_crtc_funcs *funcs;
+	void *driver_private;
+
+	/* RRCrtcPtr randr_crtc? */
+};
+
+extern struct drm_crtc *drm_crtc_create(struct drm_device *dev,
+					const struct drm_crtc_funcs *funcs);
+
+/**
+ * drm_output_funcs - control outputs on a given device
+ * @init: setup this output
+ * @dpms: set power state (see drm_crtc_funcs above)
+ * @save: save output state
+ * @restore: restore output state
+ * @mode_valid: is this mode valid on the given output?
+ * @mode_fixup: try to fixup proposed mode for this output
+ * @mode_set: set this mode
+ * @detect: is this output active?
+ * @get_modes: get mode list for this output
+ * @set_property: property for this output may need update
+ * @cleanup: output is going away, cleanup
+ *
+ * Each CRTC may have one or more outputs attached to it.  The functions
+ * below allow the core DRM code to control outputs, enumerate available modes,
+ * etc.
+ */
+struct drm_output_funcs {
+	void (*init)(struct drm_output *output);
+	void (*dpms)(struct drm_output *output, int mode);
+	void (*save)(struct drm_output *output);
+	void (*restore)(struct drm_output *output);
+	int (*mode_valid)(struct drm_output *output,
+			  struct drm_display_mode *mode);
+	bool (*mode_fixup)(struct drm_output *output,
+			   struct drm_display_mode *mode,
+			   struct drm_display_mode *adjusted_mode);
+	void (*prepare)(struct drm_output *output);
+	void (*commit)(struct drm_output *output);
+	void (*mode_set)(struct drm_output *output,
+			 struct drm_display_mode *mode,
+			 struct drm_display_mode *adjusted_mode);
+	enum drm_output_status (*detect)(struct drm_output *output);
+	int (*get_modes)(struct drm_output *output);
+	/* JJJ: type checking for properties via property value type */
+	bool (*set_property)(struct drm_output *output, struct drm_property *property,
+			     uint64_t val);
+	void (*cleanup)(struct drm_output *output);
+};
+
+#define DRM_OUTPUT_MAX_UMODES 16
+#define DRM_OUTPUT_MAX_PROPERTY 16
+#define DRM_OUTPUT_LEN 32
+/**
+ * drm_output - central DRM output control structure
+ * @crtc: CRTC this output is currently connected to, NULL if none
+ * @possible_crtcs: bitmap of CRTCS this output could be attached to
+ * @possible_clones: bitmap of possible outputs this output could clone
+ * @interlace_allowed: can this output handle interlaced modes?
+ * @doublescan_allowed: can this output handle doublescan?
+ * @available_modes: modes available on this output (from get_modes() + user)
+ * @initial_x: initial x position for this output
+ * @initial_y: initial y position for this output
+ * @status: output connected?
+ * @subpixel_order: for this output
+ * @mm_width: displayable width of output in mm
+ * @mm_height: displayable height of output in mm
+ * @funcs: output control functions
+ * @driver_private: private driver data
+ *
+ * Each output may be connected to one or more CRTCs, or may be clonable by
+ * another output if they can share a CRTC.  Each output also has a specific
+ * position in the broader display (referred to as a 'screen' though it could
+ * span multiple monitors).
+ */
+struct drm_output {
+	struct drm_device *dev;
+	struct list_head head;
+	struct drm_crtc *crtc;
+	int id; /* idr assigned */
+
+	int output_type;
+	int output_type_id;
+	unsigned long possible_crtcs;
+	unsigned long possible_clones;
+	bool interlace_allowed;
+	bool doublescan_allowed;
+	struct list_head modes; /* list of modes on this output */
+
+	/*
+	  OptionInfoPtr options;
+	  XF86ConfMonitorPtr conf_monitor;
+	 */
+	int initial_x, initial_y;
+	enum drm_output_status status;
+
+	/* these are modes added by probing with DDC or the BIOS */
+	struct list_head probed_modes;
+	
+	/* xf86MonPtr MonInfo; */
+	enum subpixel_order subpixel_order;
+	int mm_width, mm_height;
+	struct drm_display_info *monitor_info; /* if any */
+  	const struct drm_output_funcs *funcs;
+	void *driver_private;
+
+	struct list_head user_modes;
+	struct drm_property_blob *edid_blob_ptr;
+	u32 property_ids[DRM_OUTPUT_MAX_PROPERTY];
+	uint64_t property_values[DRM_OUTPUT_MAX_PROPERTY];
+};
+
+/**
+ * struct drm_mode_set
+ *
+ * Represents a single crtc the outputs that it drives with what mode
+ * and from which framebuffer it scans out from.
+ */
+struct drm_mode_set
+{
+	struct drm_framebuffer *fb;
+	struct drm_crtc *crtc;
+
+	struct drm_output **outputs;
+	size_t num_outputs;
+};
+
+/**
+ * struct drm_mode_config_funcs - configure CRTCs for a given screen layout
+ * @resize: adjust CRTCs as necessary for the proposed layout
+ *
+ * Currently only a resize hook is available.  DRM will call back into the
+ * driver with a new screen width and height.  If the driver can't support
+ * the proposed size, it can return false.  Otherwise it should adjust
+ * the CRTC<->output mappings as needed and update its view of the screen.
+ */
+struct drm_mode_config_funcs {
+	bool (*resize)(struct drm_device *dev, int width, int height);
+};
+
+/**
+ * drm_mode_config - Mode configuration control structure
+ *
+ */
+struct drm_mode_config {
+	struct mutex mutex; /* protects configuration and IDR */
+	struct idr crtc_idr; /* use this idr for all IDs, fb, crtc, output, modes - just makes life easier */
+	/* this is limited to one for now */
+	int num_fb;
+	struct list_head fb_list;
+	int num_output;
+	struct list_head output_list;
+
+	/* int compat_output? */
+	int num_crtc;
+	struct list_head crtc_list;
+
+	struct list_head property_list;
+
+	int min_width, min_height;
+	int max_width, max_height;
+	/* DamagePtr rotationDamage? */
+	/* DGA stuff? */
+	struct drm_mode_config_funcs *funcs;
+	unsigned long fb_base;
+
+	/* pointers to standard properties */
+	struct list_head property_blob_list;
+	struct drm_property *edid_property;
+	struct drm_property *dpms_property;
+	struct drm_property *connector_type_property;
+	struct drm_property *connector_num_property;
+
+	/* TV properties */
+	struct drm_property *tv_mode_property;
+	struct drm_property *tv_left_margin_property;
+	struct drm_property *tv_right_margin_property;
+	struct drm_property *tv_top_margin_property;
+	struct drm_property *tv_bottom_margin_property;
+
+	/* hotplug */
+	uint32_t hotplug_counter;
+};
+
+struct drm_output *drm_output_create(struct drm_device *dev,
+				     const struct drm_output_funcs *funcs,
+				     int type);
+
+extern char *drm_get_output_name(struct drm_output *output);
+extern void drm_output_destroy(struct drm_output *output);
+extern void drm_fb_release(struct file *filp);
+
+extern struct edid *drm_get_edid(struct drm_output *output,
+				 struct i2c_adapter *adapter);
+extern int drm_add_edid_modes(struct drm_output *output, struct edid *edid);
+extern void drm_mode_probed_add(struct drm_output *output, struct drm_display_mode *mode);
+extern void drm_mode_remove(struct drm_output *output, struct drm_display_mode *mode);
+extern struct drm_display_mode *drm_mode_duplicate(struct drm_device *dev,
+						   struct drm_display_mode *mode);
+extern void drm_mode_debug_printmodeline(struct drm_device *dev,
+					 struct drm_display_mode *mode);
+extern void drm_mode_config_init(struct drm_device *dev);
+extern void drm_mode_config_cleanup(struct drm_device *dev);
+extern void drm_mode_set_name(struct drm_display_mode *mode);
+extern bool drm_mode_equal(struct drm_display_mode *mode1, struct drm_display_mode *mode2);
+extern void drm_disable_unused_functions(struct drm_device *dev);
+
+/* for us by fb module */
+extern int drm_mode_attachmode_crtc(struct drm_device *dev,
+				    struct drm_crtc *crtc,
+				    struct drm_display_mode *mode);
+extern int drm_mode_detachmode_crtc(struct drm_device *dev, struct drm_display_mode *mode);
+
+extern struct drm_display_mode *drm_mode_create(struct drm_device *dev);
+extern void drm_mode_destroy(struct drm_device *dev, struct drm_display_mode *mode);
+extern void drm_mode_list_concat(struct list_head *head,
+				 struct list_head *new);
+extern void drm_mode_validate_size(struct drm_device *dev,
+				   struct list_head *mode_list,
+				   int maxX, int maxY, int maxPitch);
+extern void drm_mode_prune_invalid(struct drm_device *dev,
+				   struct list_head *mode_list, bool verbose);
+extern void drm_mode_sort(struct list_head *mode_list);
+extern int drm_mode_vrefresh(struct drm_display_mode *mode);
+extern void drm_mode_set_crtcinfo(struct drm_display_mode *p,
+				  int adjust_flags);
+extern void drm_mode_output_list_update(struct drm_output *output);
+extern int drm_mode_output_update_edid_property(struct drm_output *output,
+						struct edid *edid);
+extern int drm_output_property_set_value(struct drm_output *output,
+					 struct drm_property *property,
+					 uint64_t value);
+extern struct drm_display_mode *drm_crtc_mode_create(struct drm_device *dev);
+extern bool drm_initial_config(struct drm_device *dev, bool cangrow);
+extern void drm_framebuffer_set_object(struct drm_device *dev,
+				       unsigned long handle);
+extern struct drm_framebuffer *drm_framebuffer_create(struct drm_device *dev);
+extern void drm_framebuffer_destroy(struct drm_framebuffer *fb);
+extern int drmfb_probe(struct drm_device *dev, struct drm_crtc *crtc);
+extern int drmfb_remove(struct drm_device *dev, struct drm_framebuffer *fb);
+extern bool drm_crtc_set_mode(struct drm_crtc *crtc, struct drm_display_mode *mode,
+		       int x, int y);
+extern int drm_hotplug_stage_two(struct drm_device *dev, struct drm_output *output, bool connected);
+
+extern int drm_output_attach_property(struct drm_output *output,
+				      struct drm_property *property, uint64_t init_val);
+extern struct drm_property *drm_property_create(struct drm_device *dev, int flags,
+						const char *name, int num_values);
+extern void drm_property_destroy(struct drm_device *dev, struct drm_property *property);
+extern int drm_property_add_enum(struct drm_property *property, int index, 
+				 uint64_t value, const char *name);
+
+/* IOCTLs */
+extern int drm_mode_getresources(struct drm_device *dev,
+				 void *data, struct drm_file *file_priv);
+
+extern int drm_mode_getcrtc(struct drm_device *dev,
+			    void *data, struct drm_file *file_priv);
+extern int drm_mode_getoutput(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv);
+extern int drm_mode_setcrtc(struct drm_device *dev,
+			    void *data, struct drm_file *file_priv);
+extern int drm_mode_cursor_ioctl(struct drm_device *dev,
+				void *data, struct drm_file *file_priv);
+extern int drm_mode_addfb(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv);
+extern int drm_mode_rmfb(struct drm_device *dev,
+			 void *data, struct drm_file *file_priv);
+extern int drm_mode_getfb(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv);
+extern int drm_mode_addmode_ioctl(struct drm_device *dev,
+				  void *data, struct drm_file *file_priv);
+extern int drm_mode_rmmode_ioctl(struct drm_device *dev,
+				 void *data, struct drm_file *file_priv);
+extern int drm_mode_attachmode_ioctl(struct drm_device *dev,
+				     void *data, struct drm_file *file_priv);
+extern int drm_mode_detachmode_ioctl(struct drm_device *dev,
+				     void *data, struct drm_file *file_priv);
+
+extern int drm_mode_getproperty_ioctl(struct drm_device *dev,
+				      void *data, struct drm_file *file_priv);
+extern int drm_mode_getblob_ioctl(struct drm_device *dev,
+				  void *data, struct drm_file *file_priv);
+extern int drm_mode_output_property_set_ioctl(struct drm_device *dev,
+					      void *data, struct drm_file *file_priv);
+extern int drm_mode_hotplug_ioctl(struct drm_device *dev,
+				  void *data, struct drm_file *file_priv);
+
+#endif /* __DRM_CRTC_H__ */
+
diff --git a/drivers/char/drm/drm_drv.c b/drivers/char/drm/drm_drv.c
index 5641387..be84bd6 100644
--- a/drivers/char/drm/drm_drv.c
+++ b/drivers/char/drm/drm_drv.c
@@ -117,6 +117,52 @@ static struct drm_ioctl_desc drm_ioctls[] = {
 	DRM_IOCTL_DEF(DRM_IOCTL_WAIT_VBLANK, drm_wait_vblank, 0),
 
 	DRM_IOCTL_DEF(DRM_IOCTL_UPDATE_DRAW, drm_update_drawable_info, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETRESOURCES, drm_mode_getresources, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCRTC, drm_mode_getcrtc, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETOUTPUT, drm_mode_getoutput, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETCRTC, drm_mode_setcrtc, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CURSOR, drm_mode_cursor_ioctl, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ADDFB, drm_mode_addfb, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_RMFB, drm_mode_rmfb, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETFB, drm_mode_getfb, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETPROPERTY, drm_mode_output_property_set_ioctl, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPBLOB, drm_mode_getblob_ioctl, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATTACHMODE, drm_mode_attachmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DETACHMODE, drm_mode_detachmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY|DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPERTY, drm_mode_getproperty_ioctl, DRM_MASTER | DRM_ROOT_ONLY | DRM_CONTROL_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_HOTPLUG, drm_mode_hotplug_ioctl, DRM_CONTROL_ALLOW),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_INIT, drm_mm_init_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_TAKEDOWN, drm_mm_takedown_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_LOCK, drm_mm_lock_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_UNLOCK, drm_mm_unlock_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_CREATE, drm_fence_create_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_REFERENCE, drm_fence_reference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_UNREFERENCE, drm_fence_unreference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_SIGNALED, drm_fence_signaled_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_FLUSH, drm_fence_flush_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_WAIT, drm_fence_wait_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_EMIT, drm_fence_emit_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_BUFFERS, drm_fence_buffers_ioctl, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_CREATE, drm_bo_create_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_MAP, drm_bo_map_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_UNMAP, drm_bo_unmap_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_REFERENCE, drm_bo_reference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_UNREFERENCE, drm_bo_unreference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_SETSTATUS, drm_bo_setstatus_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_INFO, drm_bo_info_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_WAIT_IDLE, drm_bo_wait_idle_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_VERSION, drm_bo_version_ioctl, 0),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_INFO, drm_mm_info_ioctl, 0),
 };
 
 #define DRM_CORE_IOCTL_COUNT	ARRAY_SIZE( drm_ioctls )
@@ -132,23 +178,19 @@ static struct drm_ioctl_desc drm_ioctls[] = {
  */
 int drm_lastclose(struct drm_device * dev)
 {
-	struct drm_magic_entry *pt, *next;
 	struct drm_map_list *r_list, *list_t;
 	struct drm_vma_entry *vma, *vma_temp;
 	int i;
 
 	DRM_DEBUG("\n");
 
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		drm_bo_driver_finish(dev);
+
 	if (dev->driver->lastclose)
 		dev->driver->lastclose(dev);
 	DRM_DEBUG("driver lastclose completed\n");
 
-	if (dev->unique) {
-		drm_free(dev->unique, strlen(dev->unique) + 1, DRM_MEM_DRIVER);
-		dev->unique = NULL;
-		dev->unique_len = 0;
-	}
-
 	if (dev->irq_enabled)
 		drm_irq_uninstall(dev);
 
@@ -158,14 +200,9 @@ int drm_lastclose(struct drm_device * dev)
 	drm_drawable_free_all(dev);
 	del_timer(&dev->timer);
 
-	/* Clear pid list */
-	if (dev->magicfree.next) {
-		list_for_each_entry_safe(pt, next, &dev->magicfree, head) {
-			list_del(&pt->head);
-			drm_ht_remove_item(&dev->magiclist, &pt->hash_item);
-			drm_free(pt, sizeof(*pt), DRM_MEM_MAGIC);
-		}
-		drm_ht_remove(&dev->magiclist);
+	if (dev->primary->master) {
+		drm_put_master(dev->primary->master);
+		dev->primary->master = NULL;
 	}
 
 	/* Clear AGP information */
@@ -196,15 +233,17 @@ int drm_lastclose(struct drm_device * dev)
 	/* Clear vma list (only built for debugging) */
 	list_for_each_entry_safe(vma, vma_temp, &dev->vmalist, head) {
 		list_del(&vma->head);
-		drm_free(vma, sizeof(*vma), DRM_MEM_VMAS);
+		drm_ctl_free(vma, sizeof(*vma), DRM_MEM_VMAS);
 	}
 
+#if 0
 	list_for_each_entry_safe(r_list, list_t, &dev->maplist, head) {
 		if (!(r_list->map->flags & _DRM_DRIVER)) {
 			drm_rmmap_locked(dev, r_list->map);
 			r_list = NULL;
 		}
 	}
+#endif
 
 	if (drm_core_check_feature(dev, DRIVER_DMA_QUEUE) && dev->queuelist) {
 		for (i = 0; i < dev->queue_count; i++) {
@@ -225,11 +264,7 @@ int drm_lastclose(struct drm_device * dev)
 	if (drm_core_check_feature(dev, DRIVER_HAVE_DMA))
 		drm_dma_takedown(dev);
 
-	if (dev->lock.hw_lock) {
-		dev->sigdata.lock = dev->lock.hw_lock = NULL;	/* SHM removed */
-		dev->lock.file_priv = NULL;
-		wake_up_interruptible(&dev->lock.lock_queue);
-	}
+	dev->dev_mapping = NULL;
 	mutex_unlock(&dev->struct_mutex);
 
 	DRM_DEBUG("lastclose completed\n");
@@ -265,6 +300,10 @@ int drm_init(struct drm_driver *driver)
 		while ((pdev =
 			pci_get_subsys(pid->vendor, pid->device, pid->subvendor,
 				       pid->subdevice, pdev)) != NULL) {
+
+			if ((pid->class != 0)
+			    && ((pdev->class & pid->class_mask) != pid->class))
+				continue;
 			/* stealth mode requires a manual probe */
 			pci_dev_get(pdev);
 			drm_get_dev(pdev, pid, driver);
@@ -292,6 +331,7 @@ static void drm_cleanup(struct drm_device * dev)
 	}
 
 	drm_lastclose(dev);
+	drm_fence_manager_takedown(dev);
 
 	if (drm_core_has_MTRR(dev) && drm_core_has_AGP(dev) &&
 	    dev->agp && dev->agp->agp_mtrr >= 0) {
@@ -302,18 +342,25 @@ static void drm_cleanup(struct drm_device * dev)
 		DRM_DEBUG("mtrr_del=%d\n", retval);
 	}
 
+	if (dev->driver->unload)
+		dev->driver->unload(dev);
+        
+	drm_ht_remove(&dev->map_hash);
 	if (drm_core_has_AGP(dev) && dev->agp) {
 		drm_free(dev->agp, sizeof(*dev->agp), DRM_MEM_AGPLISTS);
 		dev->agp = NULL;
 	}
 
-	if (dev->driver->unload)
-		dev->driver->unload(dev);
-
 	drm_ht_remove(&dev->map_hash);
+	drm_mm_takedown(&dev->offset_manager);
+	drm_ht_remove(&dev->object_hash);
 	drm_ctxbitmap_cleanup(dev);
 
 	drm_put_minor(&dev->primary);
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		drm_put_minor(&dev->control);
+
 	if (drm_put_dev(dev))
 		DRM_ERROR("Cannot unload module\n");
 }
@@ -328,8 +375,13 @@ int drm_minors_cleanup(int id, void *ptr, void *data)
 	if (minor->dev->driver != driver)
 		return 0;
 
-	if (minor->type != DRM_MINOR_LEGACY)
-		return 0;
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+		if (minor->type != DRM_MINOR_CONTROL)
+			return 0;
+	} else {
+		if (minor->type != DRM_MINOR_LEGACY)
+			return 0;
+	}
 
 	if (dev)
 		pci_dev_put(dev->pdev);
@@ -342,7 +394,6 @@ void drm_exit(struct drm_driver *driver)
 	DRM_DEBUG("\n");
 
 	idr_for_each(&drm_minors_idr, &drm_minors_cleanup, driver);
-
 	DRM_INFO("Module unloaded\n");
 }
 
@@ -356,9 +407,35 @@ static const struct file_operations drm_stub_fops = {
 
 static int __init drm_core_init(void)
 {
-	int ret = -ENOMEM;
+	int ret;
+	struct sysinfo si;
+	unsigned long avail_memctl_mem;
+	unsigned long max_memctl_mem;
 
 	idr_init(&drm_minors_idr);
+	si_meminfo(&si);
+
+	/*
+	 * AGP only allows low / DMA32 memory ATM.
+	 */
+
+	avail_memctl_mem = si.totalram - si.totalhigh;
+
+	/*
+	 * Avoid overflows
+	 */
+
+	max_memctl_mem = 1UL << (32 - PAGE_SHIFT);
+	max_memctl_mem = (max_memctl_mem / si.mem_unit) * PAGE_SIZE;
+
+	if (avail_memctl_mem >= max_memctl_mem)
+		avail_memctl_mem = max_memctl_mem;
+
+	drm_init_memctl(avail_memctl_mem/2, avail_memctl_mem*3/4, si.mem_unit);
+
+	idr_init(&drm_minors_idr);
+
+	ret = -ENOMEM;
 
 	if (register_chrdev(DRM_MAJOR, "drm", &drm_stub_fops))
 		goto err_p1;
@@ -514,15 +591,13 @@ int drm_ioctl(struct inode *inode, struct file *filp,
 		}
 	}
 
-      err_i1:
-	if (kdata)
-		kfree(kdata);
+err_i1:
+	kfree(kdata);
 	atomic_dec(&dev->ioctl_count);
 	if (retcode)
 		DRM_DEBUG("ret = %x\n", retcode);
 	return retcode;
 }
-
 EXPORT_SYMBOL(drm_ioctl);
 
 drm_local_map_t *drm_getsarea(struct drm_device *dev)
diff --git a/drivers/char/drm/drm_edid.c b/drivers/char/drm/drm_edid.c
new file mode 100644
index 0000000..338a5f5
--- /dev/null
+++ b/drivers/char/drm/drm_edid.c
@@ -0,0 +1,491 @@
+/*
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ *
+ * DDC probing routines (drm_ddc_read & drm_do_probe_ddc_edid) originally from
+ * FB layer.
+ *   Copyright (C) 2006 Dennis Munsie <dmunsie@cecropia.com>
+ */
+#include <linux/i2c.h>
+#include <linux/i2c-algo-bit.h>
+#include "drmP.h"
+#include "drm_edid.h"
+
+/* Valid EDID header has these bytes */
+static u8 edid_header[] = { 0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00 };
+
+/**
+ * edid_valid - sanity check EDID data
+ * @edid: EDID data
+ *
+ * Sanity check the EDID block by looking at the header, the version number
+ * and the checksum.  Return 0 if the EDID doesn't check out, or 1 if it's
+ * valid.
+ */
+static bool edid_valid(struct edid *edid)
+{
+	int i;
+	u8 csum = 0;
+	u8 *raw_edid = (u8 *)edid;
+
+	if (memcmp(edid->header, edid_header, sizeof(edid_header)))
+		goto bad;
+	if (edid->version != 1)
+		goto bad;
+	if (edid->revision <= 0 || edid->revision > 3)
+		goto bad;
+
+	for (i = 0; i < EDID_LENGTH; i++)
+		csum += raw_edid[i];
+	if (csum)
+		goto bad;
+
+	return 1;
+
+bad:
+	return 0;
+}
+
+/**
+ * drm_mode_std - convert standard mode info (width, height, refresh) into mode
+ * @t: standard timing params
+ *
+ * Take the standard timing params (in this case width, aspect, and refresh)
+ * and convert them into a real mode using CVT.
+ *
+ * Punts for now, but should eventually use the FB layer's CVT based mode
+ * generation code.
+ */
+struct drm_display_mode *drm_mode_std(struct drm_device *dev,
+				      struct std_timing *t)
+{
+//	struct fb_videomode mode;
+
+//	fb_find_mode_cvt(&mode, 0, 0);
+	/* JJJ:  convert to drm_display_mode */
+	struct drm_display_mode *mode;
+	int hsize = t->hsize * 8 + 248, vsize;
+
+	mode = drm_mode_create(dev);
+	if (!mode)
+		return NULL;
+
+	if (t->aspect_ratio == 0)
+		vsize = (hsize * 10) / 16;
+	else if (t->aspect_ratio == 1)
+		vsize = (hsize * 3) / 4;
+	else if (t->aspect_ratio == 2)
+		vsize = (hsize * 4) / 5;
+	else
+		vsize = (hsize * 9) / 16;
+
+	drm_mode_set_name(mode);
+
+	return mode;
+}
+
+/**
+ * drm_mode_detailed - create a new mode from an EDID detailed timing section
+ * @timing: EDID detailed timing info
+ * @preferred: is this a preferred mode?
+ *
+ * An EDID detailed timing block contains enough info for us to create and
+ * return a new struct drm_display_mode.  The @preferred flag will be set
+ * if this is the display's preferred timing, and we'll use it to indicate
+ * to the other layers that this mode is desired.
+ */
+struct drm_display_mode *drm_mode_detailed(struct drm_device *dev,
+					   struct detailed_timing *timing)
+{
+	struct drm_display_mode *mode;
+	struct detailed_pixel_timing *pt = &timing->data.pixel_data;
+
+	if (pt->stereo) {
+		printk(KERN_WARNING "stereo mode not supported\n");
+		return NULL;
+	}
+	if (!pt->separate_sync) {
+		printk(KERN_WARNING "integrated sync not supported\n");
+		return NULL;
+	}
+
+	mode = drm_mode_create(dev);
+	if (!mode)
+		return NULL;
+
+	mode->type = DRM_MODE_TYPE_DRIVER;
+	mode->clock = timing->pixel_clock * 10;
+
+	mode->hdisplay = (pt->hactive_hi << 8) | pt->hactive_lo;
+	mode->hsync_start = mode->hdisplay + ((pt->hsync_offset_hi << 8) |
+					      pt->hsync_offset_lo);
+	mode->hsync_end = mode->hsync_start +
+		((pt->hsync_pulse_width_hi << 8) |
+		 pt->hsync_pulse_width_lo);
+	mode->htotal = mode->hdisplay + ((pt->hblank_hi << 8) | pt->hblank_lo);
+
+	mode->vdisplay = (pt->vactive_hi << 8) | pt->vactive_lo;
+	mode->vsync_start = mode->vdisplay + ((pt->vsync_offset_hi << 8) |
+					      pt->vsync_offset_lo);
+	mode->vsync_end = mode->vsync_start +
+		((pt->vsync_pulse_width_hi << 8) |
+		 pt->vsync_pulse_width_lo);
+	mode->vtotal = mode->vdisplay + ((pt->vblank_hi << 8) | pt->vblank_lo);
+
+	drm_mode_set_name(mode);
+
+	if (pt->interlaced)
+		mode->flags |= V_INTERLACE;
+
+	mode->flags |= pt->hsync_positive ? V_PHSYNC : V_NHSYNC;
+	mode->flags |= pt->vsync_positive ? V_PVSYNC : V_NVSYNC;
+
+	return mode;
+}
+
+/*
+ * Detailed mode info for the EDID "established modes" data to use.
+ */
+static struct drm_display_mode edid_est_modes[] = {
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 40000, 800, 840,
+		   968, 1056, 0, 600, 601, 605, 628, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@60Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 36000, 800, 824,
+		   896, 1024, 0, 600, 601, 603,  625, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@56Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 31500, 640, 656,
+		   720, 840, 0, 480, 481, 484, 500, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@75Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 31500, 640, 664,
+		   704,  832, 0, 480, 489, 491, 520, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@72Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 30240, 640, 704,
+		   768,  864, 0, 480, 483, 486, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@67Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 25200, 640, 656,
+		   752, 800, 0, 480, 490, 492, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@60Hz */
+	{ DRM_MODE("720x400", DRM_MODE_TYPE_DRIVER, 35500, 720, 738,
+		   846, 900, 0, 400, 421, 423,  449, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 720x400@88Hz */
+	{ DRM_MODE("720x400", DRM_MODE_TYPE_DRIVER, 28320, 720, 738,
+		   846,  900, 0, 400, 412, 414, 449, 0,
+		   V_NHSYNC | V_PVSYNC) }, /* 720x400@70Hz */
+	{ DRM_MODE("1280x1024", DRM_MODE_TYPE_DRIVER, 135000, 1280, 1296,
+		   1440, 1688, 0, 1024, 1025, 1028, 1066, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1280x1024@75Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 78800, 1024, 1040,
+		   1136, 1312, 0,  768, 769, 772, 800, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1024x768@75Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 75000, 1024, 1048,
+		   1184, 1328, 0,  768, 771, 777, 806, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 1024x768@70Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 65000, 1024, 1048,
+		   1184, 1344, 0,  768, 771, 777, 806, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 1024x768@60Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER,44900, 1024, 1032,
+		   1208, 1264, 0, 768, 768, 776, 817, 0,
+		   V_PHSYNC | V_PVSYNC | V_INTERLACE) }, /* 1024x768@43Hz */
+	{ DRM_MODE("832x624", DRM_MODE_TYPE_DRIVER, 57284, 832, 864,
+		   928, 1152, 0, 624, 625, 628, 667, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 832x624@75Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 49500, 800, 816,
+		   896, 1056, 0, 600, 601, 604,  625, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@75Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 50000, 800, 856,
+		   976, 1040, 0, 600, 637, 643, 666, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@72Hz */
+	{ DRM_MODE("1152x864", DRM_MODE_TYPE_DRIVER, 108000, 1152, 1216,
+		   1344, 1600, 0,  864, 865, 868, 900, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1152x864@75Hz */
+};
+
+#define EDID_EST_TIMINGS 16
+#define EDID_STD_TIMINGS 8
+#define EDID_DETAILED_TIMINGS 4
+
+/**
+ * add_established_modes - get est. modes from EDID and add them
+ * @edid: EDID block to scan
+ *
+ * Each EDID block contains a bitmap of the supported "established modes" list
+ * (defined above).  Tease them out and add them to the global modes list.
+ */
+static int add_established_modes(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	unsigned long est_bits = edid->established_timings.t1 |
+		(edid->established_timings.t2 << 8) |
+		((edid->established_timings.mfg_rsvd & 0x80) << 9);
+	int i, modes = 0;
+
+	for (i = 0; i <= EDID_EST_TIMINGS; i++)
+		if (est_bits & (1<<i)) {
+			struct drm_display_mode *newmode;
+			newmode = drm_mode_duplicate(dev, &edid_est_modes[i]);
+			drm_mode_probed_add(output, newmode);
+			modes++;
+		}
+
+	return modes;
+}
+
+/**
+ * add_standard_modes - get std. modes from EDID and add them
+ * @edid: EDID block to scan
+ *
+ * Standard modes can be calculated using the CVT standard.  Grab them from
+ * @edid, calculate them, and add them to the list.
+ */
+static int add_standard_modes(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	int i, modes = 0;
+
+	for (i = 0; i < EDID_STD_TIMINGS; i++) {
+		struct std_timing *t = &edid->standard_timings[i];
+		struct drm_display_mode *newmode;
+
+		/* If std timings bytes are 1, 1 it's empty */
+		if (t->hsize == 1 && (t->aspect_ratio | t->vfreq) == 1)
+			continue;
+
+		newmode = drm_mode_std(dev, &edid->standard_timings[i]);
+		if (newmode) {
+			drm_mode_probed_add(output, newmode);
+			modes++;
+		}
+	}
+
+	return modes;
+}
+
+/**
+ * add_detailed_modes - get detailed mode info from EDID data
+ * @edid: EDID block to scan
+ *
+ * Some of the detailed timing sections may contain mode information.  Grab
+ * it and add it to the list.
+ */
+static int add_detailed_info(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	int i, j, modes = 0;
+
+	for (i = 0; i < EDID_DETAILED_TIMINGS; i++) {
+		struct detailed_timing *timing = &edid->detailed_timings[i];
+		struct detailed_non_pixel *data = &timing->data.other_data;
+		struct drm_display_mode *newmode;
+
+		/* EDID up to and including 1.2 may put monitor info here */
+		if (edid->version == 1 && edid->revision < 3)
+			continue;
+
+		/* Detailed mode timing */
+		if (timing->pixel_clock) {
+			newmode = drm_mode_detailed(dev, timing);
+			if (newmode) {
+				/* First detailed mode is preferred */
+				if (i == 0 && edid->preferred_timing)
+					newmode->type |= DRM_MODE_TYPE_PREFERRED;
+				drm_mode_probed_add(output, newmode);
+				     
+				modes++;
+			}
+			continue;
+		}
+
+		/* Other timing or info */
+		switch (data->type) {
+		case EDID_DETAIL_MONITOR_SERIAL:
+			break;
+		case EDID_DETAIL_MONITOR_STRING:
+			break;
+		case EDID_DETAIL_MONITOR_RANGE:
+			/* Get monitor range data */
+			break;
+		case EDID_DETAIL_MONITOR_NAME:
+			break;
+		case EDID_DETAIL_MONITOR_CPDATA:
+			break;
+		case EDID_DETAIL_STD_MODES:
+			/* Five modes per detailed section */
+			for (j = 0; j < 5; i++) {
+				struct std_timing *std;
+				struct drm_display_mode *newmode;
+
+				std = &data->data.timings[j];
+				newmode = drm_mode_std(dev, std);
+				if (newmode) {
+					drm_mode_probed_add(output, newmode);
+					modes++;
+				}
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	return modes;
+}
+
+#define DDC_ADDR 0x50
+
+static unsigned char *drm_do_probe_ddc_edid(struct i2c_adapter *adapter)
+{
+	unsigned char start = 0x0;
+	unsigned char *buf = kmalloc(EDID_LENGTH, GFP_KERNEL);
+	struct i2c_msg msgs[] = {
+		{
+			.addr	= DDC_ADDR,
+			.flags	= 0,
+			.len	= 1,
+			.buf	= &start,
+		}, {
+			.addr	= DDC_ADDR,
+			.flags	= I2C_M_RD,
+			.len	= EDID_LENGTH,
+			.buf	= buf,
+		}
+	};
+
+	if (!buf) {
+		dev_warn(&adapter->dev, "unable to allocate memory for EDID "
+			 "block.\n");
+		return NULL;
+	}
+
+	if (i2c_transfer(adapter, msgs, 2) == 2)
+		return buf;
+
+	dev_info(&adapter->dev, "unable to read EDID block.\n");
+	kfree(buf);
+	return NULL;
+}
+
+static unsigned char *drm_ddc_read(struct i2c_adapter *adapter)
+{
+	struct i2c_algo_bit_data *algo_data = adapter->algo_data;
+	unsigned char *edid = NULL;
+	int i, j;
+
+	/*
+	 * Startup the bus:
+	 *   Set clock line high (but give it time to come up)
+	 *   Then set clock & data low
+	 */
+	algo_data->setscl(algo_data->data, 1);
+	udelay(550); /* startup delay */
+	algo_data->setscl(algo_data->data, 0);
+	algo_data->setsda(algo_data->data, 0);
+
+	for (i = 0; i < 3; i++) {
+		/* For some old monitors we need the
+		 * following process to initialize/stop DDC
+		 */
+		algo_data->setsda(algo_data->data, 0);
+		msleep(13);
+
+		algo_data->setscl(algo_data->data, 1);
+		for (j = 0; j < 5; j++) {
+			msleep(10);
+			if (algo_data->getscl(algo_data->data))
+				break;
+		}
+		if (j == 5)
+			continue;
+
+		algo_data->setsda(algo_data->data, 0);
+		msleep(15);
+		algo_data->setscl(algo_data->data, 0);
+		msleep(15);
+		algo_data->setsda(algo_data->data, 1);
+		msleep(15);
+
+		/* Do the real work */
+		edid = drm_do_probe_ddc_edid(adapter);
+		algo_data->setsda(algo_data->data, 0);
+		algo_data->setscl(algo_data->data, 0);
+		msleep(15);
+
+		algo_data->setscl(algo_data->data, 1);
+		for (j = 0; j < 10; j++) {
+			msleep(10);
+			if (algo_data->getscl(algo_data->data))
+				break;
+		}
+
+		algo_data->setsda(algo_data->data, 1);
+		msleep(15);
+		algo_data->setscl(algo_data->data, 0);
+		if (edid)
+			break;
+	}
+	/* Release the DDC lines when done or the Apple Cinema HD display
+	 * will switch off
+	 */
+	algo_data->setsda(algo_data->data, 0);
+	algo_data->setscl(algo_data->data, 0);
+	algo_data->setscl(algo_data->data, 1);
+
+	return edid;
+}
+
+/**
+ * drm_get_edid - get EDID data, if available
+ * @output: output we're probing
+ * @adapter: i2c adapter to use for DDC
+ *
+ * Poke the given output's i2c channel to grab EDID data if possible.
+ * 
+ * Return edid data or NULL if we couldn't find any.
+ */
+struct edid *drm_get_edid(struct drm_output *output,
+			  struct i2c_adapter *adapter)
+{
+	struct edid *edid;
+
+	edid = (struct edid *)drm_ddc_read(adapter);
+	if (!edid) {
+		dev_warn(&output->dev->pdev->dev, "%s: no EDID data\n",
+			 drm_get_output_name(output));
+		return NULL;
+	}
+	if (!edid_valid(edid)) {
+		dev_warn(&output->dev->pdev->dev, "%s: EDID invalid.\n",
+			 drm_get_output_name(output));
+		kfree(edid);
+		return NULL;
+	}
+	return edid;
+}
+EXPORT_SYMBOL(drm_get_edid);
+
+/**
+ * drm_add_edid_modes - add modes from EDID data, if available
+ * @output: output we're probing
+ * @edid: edid data
+ *
+ * Add the specified modes to the output's mode list.
+ *
+ * Return number of modes added or 0 if we couldn't find any.
+ */
+int drm_add_edid_modes(struct drm_output *output, struct edid *edid)
+{
+	int num_modes = 0;
+
+	if (edid == NULL) {
+		return 0;
+	}
+	if (!edid_valid(edid)) {
+		dev_warn(&output->dev->pdev->dev, "%s: EDID invalid.\n",
+			 drm_get_output_name(output));
+		return 0;
+	}
+	num_modes += add_established_modes(output, edid);
+	num_modes += add_standard_modes(output, edid);
+	num_modes += add_detailed_info(output, edid);
+	return num_modes;
+}
+EXPORT_SYMBOL(drm_add_edid_modes);
diff --git a/drivers/char/drm/drm_edid.h b/drivers/char/drm/drm_edid.h
new file mode 100644
index 0000000..0d2eeaa
--- /dev/null
+++ b/drivers/char/drm/drm_edid.h
@@ -0,0 +1,176 @@
+#ifndef __DRM_EDID_H__
+#define __DRM_EDID_H__
+
+#include <linux/types.h>
+
+#define EDID_LENGTH 128
+#define DDC_ADDR 0x50
+
+#ifdef BIG_ENDIAN
+#error "EDID structure is little endian, need big endian versions"
+#endif
+
+struct est_timings {
+	u8 t1;
+	u8 t2;
+	u8 mfg_rsvd;
+} __attribute__((packed));
+
+struct std_timing {
+	u8 hsize; /* need to multiply by 8 then add 248 */
+	u8 vfreq:6; /* need to add 60 */
+	u8 aspect_ratio:2; /* 00=16:10, 01=4:3, 10=5:4, 11=16:9 */
+} __attribute__((packed));
+
+/* If detailed data is pixel timing */
+struct detailed_pixel_timing {
+	u8 hactive_lo;
+	u8 hblank_lo;
+	u8 hblank_hi:4;
+	u8 hactive_hi:4;
+	u8 vactive_lo;
+	u8 vblank_lo;
+	u8 vblank_hi:4;
+	u8 vactive_hi:4;
+	u8 hsync_offset_lo;
+	u8 hsync_pulse_width_lo;
+	u8 vsync_pulse_width_lo:4;
+	u8 vsync_offset_lo:4;
+	u8 hsync_pulse_width_hi:2;
+	u8 hsync_offset_hi:2;
+	u8 vsync_pulse_width_hi:2;
+	u8 vsync_offset_hi:2;
+	u8 width_mm_lo;
+	u8 height_mm_lo;
+	u8 height_mm_hi:4;
+	u8 width_mm_hi:4;
+	u8 hborder;
+	u8 vborder;
+	u8 unknown0:1;
+	u8 vsync_positive:1;
+	u8 hsync_positive:1;
+	u8 separate_sync:2;
+	u8 stereo:1;
+	u8 unknown6:1;
+	u8 interlaced:1;
+} __attribute__((packed));
+
+/* If it's not pixel timing, it'll be one of the below */
+struct detailed_data_string {
+	u8 str[13];
+} __attribute__((packed));
+
+struct detailed_data_monitor_range {
+	u8 min_vfreq;
+	u8 max_vfreq;
+	u8 min_hfreq_khz;
+	u8 max_hfreq_khz;
+	u8 pixel_clock_mhz; /* need to multiply by 10 */
+	u16 sec_gtf_toggle; /* A000=use above, 20=use below */ /* FIXME: byte order */
+	u8 hfreq_start_khz; /* need to multiply by 2 */
+	u8 c; /* need to divide by 2 */
+	u16 m; /* FIXME: byte order */
+	u8 k;
+	u8 j; /* need to divide by 2 */
+} __attribute__((packed));
+
+struct detailed_data_wpindex {
+	u8 white_y_lo:2;
+	u8 white_x_lo:2;
+	u8 pad:4;
+	u8 white_x_hi;
+	u8 white_y_hi;
+	u8 gamma; /* need to divide by 100 then add 1 */
+} __attribute__((packed));
+
+struct detailed_data_color_point {
+	u8 windex1;
+	u8 wpindex1[3];
+	u8 windex2;
+	u8 wpindex2[3];
+} __attribute__((packed));
+
+struct detailed_non_pixel {
+	u8 pad1;
+	u8 type; /* ff=serial, fe=string, fd=monitor range, fc=monitor name
+		    fb=color point data, fa=standard timing data,
+		    f9=undefined, f8=mfg. reserved */
+	u8 pad2;
+	union {
+		struct detailed_data_string str;
+		struct detailed_data_monitor_range range;
+		struct detailed_data_wpindex color;
+		struct std_timing timings[5];
+	} data;
+} __attribute__((packed));
+
+#define EDID_DETAIL_STD_MODES 0xfa
+#define EDID_DETAIL_MONITOR_CPDATA 0xfb
+#define EDID_DETAIL_MONITOR_NAME 0xfc
+#define EDID_DETAIL_MONITOR_RANGE 0xfd
+#define EDID_DETAIL_MONITOR_STRING 0xfe
+#define EDID_DETAIL_MONITOR_SERIAL 0xff
+
+struct detailed_timing {
+	u16 pixel_clock; /* need to multiply by 10 KHz */ /* FIXME: byte order */
+	union {
+		struct detailed_pixel_timing pixel_data;
+		struct detailed_non_pixel other_data;
+	} data;
+} __attribute__((packed));
+
+struct edid {
+	u8 header[8];
+	/* Vendor & product info */
+	u16 mfg_id; /* FIXME: byte order */
+	u16 prod_code; /* FIXME: byte order */
+	u32 serial; /* FIXME: byte order */
+	u8 mfg_week;
+	u8 mfg_year;
+	/* EDID version */
+	u8 version;
+	u8 revision;
+	/* Display info: */
+	/*   input definition */
+	u8 serration_vsync:1;
+	u8 sync_on_green:1;
+	u8 composite_sync:1;
+	u8 separate_syncs:1;
+	u8 blank_to_black:1;
+	u8 video_level:2;
+	u8 digital:1; /* bits below must be zero if set */
+	u8 width_cm;
+	u8 height_cm;
+	u8 gamma;
+	/*   feature support */
+	u8 default_gtf:1;
+	u8 preferred_timing:1;
+	u8 standard_color:1;
+	u8 display_type:2; /* 00=mono, 01=rgb, 10=non-rgb, 11=unknown */
+	u8 pm_active_off:1;
+	u8 pm_suspend:1;
+	u8 pm_standby:1;
+	/* Color characteristics */
+	u8 red_green_lo;
+	u8 black_white_lo;
+	u8 red_x;
+	u8 red_y;
+	u8 green_x;
+	u8 green_y;
+	u8 blue_x;
+	u8 blue_y;
+	u8 white_x;
+	u8 white_y;
+	/* Est. timings and mfg rsvd timings*/
+	struct est_timings established_timings;
+	/* Standard timings 1-8*/
+	struct std_timing standard_timings[8];
+	/* Detailing timings 1-4 */
+	struct detailed_timing detailed_timings[4];
+	/* Number of 128 byte ext. blocks */
+	u8 extensions;
+	/* Checksum */
+	u8 checksum;
+} __attribute__((packed));
+
+#endif /* __DRM_EDID_H__ */
diff --git a/drivers/char/drm/drm_fence.c b/drivers/char/drm/drm_fence.c
new file mode 100644
index 0000000..4ed4865
--- /dev/null
+++ b/drivers/char/drm/drm_fence.c
@@ -0,0 +1,824 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+
+/*
+ * Convenience function to be called by fence::wait methods that
+ * need polling.
+ */
+
+int drm_fence_wait_polling(struct drm_fence_object *fence, int lazy,
+			   int interruptible, uint32_t mask,
+			   unsigned long end_jiffies)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	uint32_t count = 0;
+	int ret;
+
+	DECLARE_WAITQUEUE(entry, current);
+	add_wait_queue(&fc->fence_queue, &entry);
+
+	ret = 0;
+
+	for (;;) {
+		__set_current_state((interruptible) ?
+				    TASK_INTERRUPTIBLE :
+				    TASK_UNINTERRUPTIBLE);
+		if (drm_fence_object_signaled(fence, mask))
+			break;
+		if (time_after_eq(jiffies, end_jiffies)) {
+			ret = -EBUSY;
+			break;
+		}
+		if (lazy)
+			schedule_timeout(1);
+		else if ((++count & 0x0F) == 0) {
+			__set_current_state(TASK_RUNNING);
+			schedule();
+			__set_current_state((interruptible) ?
+					    TASK_INTERRUPTIBLE :
+					    TASK_UNINTERRUPTIBLE);
+		}
+		if (interruptible && signal_pending(current)) {
+			ret = -EAGAIN;
+			break;
+		}
+	}
+	__set_current_state(TASK_RUNNING);
+	remove_wait_queue(&fc->fence_queue, &entry);
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_wait_polling);
+
+/*
+ * Typically called by the IRQ handler.
+ */
+
+void drm_fence_handler(struct drm_device *dev, uint32_t fence_class,
+		       uint32_t sequence, uint32_t type, uint32_t error)
+{
+	int wake = 0;
+	uint32_t diff;
+	uint32_t relevant_type;
+	uint32_t new_type;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct list_head *head;
+	struct drm_fence_object *fence, *next;
+	int found = 0;
+
+	if (list_empty(&fc->ring))
+		return;
+
+	list_for_each_entry(fence, &fc->ring, ring) {
+		diff = (sequence - fence->sequence) & driver->sequence_mask;
+		if (diff > driver->wrap_diff) {
+			found = 1;
+			break;
+		}
+	}
+
+	fc->waiting_types &= ~type;
+	head = (found) ? &fence->ring : &fc->ring;
+
+	list_for_each_entry_safe_reverse(fence, next, head, ring) {
+		if (&fence->ring == &fc->ring)
+			break;
+
+		if (error) {
+			fence->error = error;
+			fence->signaled_types = fence->type;
+			list_del_init(&fence->ring);
+			wake = 1;
+			break;
+		}
+
+		if (type & DRM_FENCE_TYPE_EXE)
+			type |= fence->native_types;
+
+		relevant_type = type & fence->type;
+		new_type = (fence->signaled_types | relevant_type) ^
+			fence->signaled_types;
+
+		if (new_type) {
+			fence->signaled_types |= new_type;
+			DRM_DEBUG("Fence 0x%08lx signaled 0x%08x\n",
+				  fence->base.hash.key, fence->signaled_types);
+
+			if (driver->needed_flush)
+				fc->pending_flush |= driver->needed_flush(fence);
+
+			if (new_type & fence->waiting_types)
+				wake = 1;
+		}
+
+		fc->waiting_types |= fence->waiting_types & ~fence->signaled_types;
+
+		if (!(fence->type & ~fence->signaled_types)) {
+			DRM_DEBUG("Fence completely signaled 0x%08lx\n",
+				  fence->base.hash.key);
+			list_del_init(&fence->ring);
+		}
+	}
+
+	/*
+	 * Reinstate lost waiting types.
+	 */
+
+	if ((fc->waiting_types & type) != type) {
+		head = head->prev;
+		list_for_each_entry(fence, head, ring) {
+			if (&fence->ring == &fc->ring)
+				break;
+			diff = (fc->highest_waiting_sequence - fence->sequence) &
+				driver->sequence_mask;
+			if (diff > driver->wrap_diff)
+				break;
+
+			fc->waiting_types |= fence->waiting_types & ~fence->signaled_types;
+		}
+	}
+
+	if (wake)
+		wake_up_all(&fc->fence_queue);
+}
+EXPORT_SYMBOL(drm_fence_handler);
+
+static void drm_fence_unring(struct drm_device *dev, struct list_head *ring)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	unsigned long flags;
+
+	write_lock_irqsave(&fm->lock, flags);
+	list_del_init(ring);
+	write_unlock_irqrestore(&fm->lock, flags);
+}
+
+void drm_fence_usage_deref_locked(struct drm_fence_object **fence)
+{
+	struct drm_fence_object *tmp_fence = *fence;
+	struct drm_device *dev = tmp_fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	*fence = NULL;
+	if (atomic_dec_and_test(&tmp_fence->usage)) {
+		drm_fence_unring(dev, &tmp_fence->ring);
+		DRM_DEBUG("Destroyed a fence object 0x%08lx\n",
+			  tmp_fence->base.hash.key);
+		atomic_dec(&fm->count);
+		BUG_ON(!list_empty(&tmp_fence->base.list));
+		drm_ctl_free(tmp_fence, sizeof(*tmp_fence), DRM_MEM_FENCE);
+	}
+}
+EXPORT_SYMBOL(drm_fence_usage_deref_locked);
+
+void drm_fence_usage_deref_unlocked(struct drm_fence_object **fence)
+{
+	struct drm_fence_object *tmp_fence = *fence;
+	struct drm_device *dev = tmp_fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	*fence = NULL;
+	if (atomic_dec_and_test(&tmp_fence->usage)) {
+		mutex_lock(&dev->struct_mutex);
+		if (atomic_read(&tmp_fence->usage) == 0) {
+			drm_fence_unring(dev, &tmp_fence->ring);
+			atomic_dec(&fm->count);
+			BUG_ON(!list_empty(&tmp_fence->base.list));
+			drm_ctl_free(tmp_fence, sizeof(*tmp_fence), DRM_MEM_FENCE);
+		}
+		mutex_unlock(&dev->struct_mutex);
+	}
+}
+EXPORT_SYMBOL(drm_fence_usage_deref_unlocked);
+
+struct drm_fence_object
+*drm_fence_reference_locked(struct drm_fence_object *src)
+{
+	DRM_ASSERT_LOCKED(&src->dev->struct_mutex);
+
+	atomic_inc(&src->usage);
+	return src;
+}
+
+void drm_fence_reference_unlocked(struct drm_fence_object **dst,
+				  struct drm_fence_object *src)
+{
+	mutex_lock(&src->dev->struct_mutex);
+	*dst = src;
+	atomic_inc(&src->usage);
+	mutex_unlock(&src->dev->struct_mutex);
+}
+EXPORT_SYMBOL(drm_fence_reference_unlocked);
+
+static void drm_fence_object_destroy(struct drm_file *priv,
+				     struct drm_user_object *base)
+{
+	struct drm_fence_object *fence =
+	    drm_user_object_entry(base, struct drm_fence_object, base);
+
+	drm_fence_usage_deref_locked(&fence);
+}
+
+int drm_fence_object_signaled(struct drm_fence_object *fence, uint32_t mask)
+{
+	unsigned long flags;
+	int signaled;
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+
+	mask &= fence->type;
+	read_lock_irqsave(&fm->lock, flags);
+	signaled = (mask & fence->signaled_types) == mask;
+	read_unlock_irqrestore(&fm->lock, flags);
+	if (!signaled && driver->poll) {
+		write_lock_irqsave(&fm->lock, flags);
+		driver->poll(dev, fence->fence_class, mask);
+		signaled = (mask & fence->signaled_types) == mask;
+		write_unlock_irqrestore(&fm->lock, flags);
+	}
+	return signaled;
+}
+EXPORT_SYMBOL(drm_fence_object_signaled);
+
+
+int drm_fence_object_flush(struct drm_fence_object *fence,
+			   uint32_t type)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	unsigned long irq_flags;
+	uint32_t saved_pending_flush;
+	uint32_t diff;
+	int call_flush;
+
+	if (type & ~fence->type) {
+		DRM_ERROR("Flush trying to extend fence type, "
+			  "0x%x, 0x%x\n", type, fence->type);
+		return -EINVAL;
+	}
+
+	write_lock_irqsave(&fm->lock, irq_flags);
+	fence->waiting_types |= type;
+	fc->waiting_types |= fence->waiting_types;
+	diff = (fence->sequence - fc->highest_waiting_sequence) &
+		driver->sequence_mask;
+
+	if (diff < driver->wrap_diff)
+		fc->highest_waiting_sequence = fence->sequence;
+
+	/*
+	 * fence->waiting_types has changed. Determine whether
+	 * we need to initiate some kind of flush as a result of this.
+	 */
+
+	saved_pending_flush = fc->pending_flush;
+	if (driver->needed_flush)
+		fc->pending_flush |= driver->needed_flush(fence);
+
+	if (driver->poll)
+		driver->poll(dev, fence->fence_class, fence->waiting_types);
+
+	call_flush = fc->pending_flush;
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+
+	if (call_flush && driver->flush)
+		driver->flush(dev, fence->fence_class);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_flush);
+
+/*
+ * Make sure old fence objects are signaled before their fence sequences are
+ * wrapped around and reused.
+ */
+
+void drm_fence_flush_old(struct drm_device *dev, uint32_t fence_class,
+			 uint32_t sequence)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+	struct drm_fence_object *fence;
+	unsigned long irq_flags;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	int call_flush;
+
+	uint32_t diff;
+
+	write_lock_irqsave(&fm->lock, irq_flags);
+
+	list_for_each_entry_reverse(fence, &fc->ring, ring) {
+		diff = (sequence - fence->sequence) & driver->sequence_mask;
+		if (diff <= driver->flush_diff)
+			break;
+
+		fence->waiting_types = fence->type;
+		fc->waiting_types |= fence->type;
+
+		if (driver->needed_flush)
+			fc->pending_flush |= driver->needed_flush(fence);
+	}
+
+	if (driver->poll)
+		driver->poll(dev, fence_class, fc->waiting_types);
+
+	call_flush = fc->pending_flush;
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+
+	if (call_flush && driver->flush)
+		driver->flush(dev, fence->fence_class);
+
+	/*
+	 * FIXME: Shold we implement a wait here for really old fences?
+	 */
+
+}
+EXPORT_SYMBOL(drm_fence_flush_old);
+
+int drm_fence_object_wait(struct drm_fence_object *fence,
+			  int lazy, int ignore_signals, uint32_t mask)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	int ret = 0;
+	unsigned long _end = 3 * DRM_HZ;
+
+	if (mask & ~fence->type) {
+		DRM_ERROR("Wait trying to extend fence type"
+			  " 0x%08x 0x%08x\n", mask, fence->type);
+		BUG();
+		return -EINVAL;
+	}
+
+	if (driver->wait)
+		return driver->wait(fence, lazy, !ignore_signals, mask);
+
+
+	drm_fence_object_flush(fence, mask);
+	if (driver->has_irq(dev, fence->fence_class, mask)) {
+		if (!ignore_signals)
+			ret = wait_event_interruptible_timeout
+				(fc->fence_queue,
+				 drm_fence_object_signaled(fence, mask),
+				 3 * DRM_HZ);
+		else
+			ret = wait_event_timeout
+				(fc->fence_queue,
+				 drm_fence_object_signaled(fence, mask),
+				 3 * DRM_HZ);
+
+		if (unlikely(ret == -ERESTARTSYS))
+			return -EAGAIN;
+
+		if (unlikely(ret == 0))
+			return -EBUSY;
+
+		return 0;
+	}
+
+	return drm_fence_wait_polling(fence, lazy, !ignore_signals, mask,
+				      _end);
+}
+EXPORT_SYMBOL(drm_fence_object_wait);
+
+
+
+int drm_fence_object_emit(struct drm_fence_object *fence, uint32_t fence_flags,
+			  uint32_t fence_class, uint32_t type)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	unsigned long flags;
+	uint32_t sequence;
+	uint32_t native_types;
+	int ret;
+
+	drm_fence_unring(dev, &fence->ring);
+	ret = driver->emit(dev, fence_class, fence_flags, &sequence,
+			   &native_types);
+	if (ret)
+		return ret;
+
+	write_lock_irqsave(&fm->lock, flags);
+	fence->fence_class = fence_class;
+	fence->type = type;
+	fence->waiting_types = 0;
+	fence->signaled_types = 0;
+	fence->sequence = sequence;
+	fence->native_types = native_types;
+	if (list_empty(&fc->ring))
+		fc->highest_waiting_sequence = sequence - 1;
+	list_add_tail(&fence->ring, &fc->ring);
+	fc->latest_queued_sequence = sequence;
+	write_unlock_irqrestore(&fm->lock, flags);
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_emit);
+
+static int drm_fence_object_init(struct drm_device *dev, uint32_t fence_class,
+				 uint32_t type,
+				 uint32_t fence_flags,
+				 struct drm_fence_object *fence)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	mutex_lock(&dev->struct_mutex);
+	atomic_set(&fence->usage, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	write_lock_irqsave(&fm->lock, flags);
+	INIT_LIST_HEAD(&fence->ring);
+
+	/*
+	 *  Avoid hitting BUG() for kernel-only fence objects.
+	 */
+
+	INIT_LIST_HEAD(&fence->base.list);
+	fence->fence_class = fence_class;
+	fence->type = type;
+	fence->signaled_types = 0;
+	fence->waiting_types = 0;
+	fence->sequence = 0;
+	fence->dev = dev;
+	write_unlock_irqrestore(&fm->lock, flags);
+	if (fence_flags & DRM_FENCE_FLAG_EMIT) {
+		ret = drm_fence_object_emit(fence, fence_flags,
+					    fence->fence_class, type);
+	}
+	return ret;
+}
+
+int drm_fence_add_user_object(struct drm_file *priv,
+			      struct drm_fence_object *fence, int shareable)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(priv, &fence->base, shareable);
+	if (ret)
+		goto out;
+	atomic_inc(&fence->usage);
+	fence->base.type = drm_fence_type;
+	fence->base.remove = &drm_fence_object_destroy;
+	DRM_DEBUG("Fence 0x%08lx created\n", fence->base.hash.key);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_add_user_object);
+
+int drm_fence_object_create(struct drm_device *dev, uint32_t fence_class,
+			    uint32_t type, unsigned flags,
+			    struct drm_fence_object **c_fence)
+{
+	struct drm_fence_object *fence;
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	fence = drm_ctl_calloc(1, sizeof(*fence), DRM_MEM_FENCE);
+	if (!fence)
+		return -ENOMEM;
+	ret = drm_fence_object_init(dev, fence_class, type, flags, fence);
+	if (ret) {
+		drm_fence_usage_deref_unlocked(&fence);
+		return ret;
+	}
+	*c_fence = fence;
+	atomic_inc(&fm->count);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_create);
+
+void drm_fence_manager_init(struct drm_device *dev)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fence_class;
+	struct drm_fence_driver *fed = dev->driver->fence_driver;
+	int i;
+	unsigned long flags;
+
+	rwlock_init(&fm->lock);
+	write_lock_irqsave(&fm->lock, flags);
+	fm->initialized = 0;
+	if (!fed)
+	    goto out_unlock;
+
+	fm->initialized = 1;
+	fm->num_classes = fed->num_classes;
+	BUG_ON(fm->num_classes > _DRM_FENCE_CLASSES);
+
+	for (i = 0; i < fm->num_classes; ++i) {
+		fence_class = &fm->fence_class[i];
+		memset(fence_class, 0, sizeof(*fence_class));
+		INIT_LIST_HEAD(&fence_class->ring);
+		DRM_INIT_WAITQUEUE(&fence_class->fence_queue);
+	}
+
+	atomic_set(&fm->count, 0);
+ out_unlock:
+	write_unlock_irqrestore(&fm->lock, flags);
+}
+
+void drm_fence_fill_arg(struct drm_fence_object *fence,
+			struct drm_fence_arg *arg)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	unsigned long irq_flags;
+
+	read_lock_irqsave(&fm->lock, irq_flags);
+	arg->handle = fence->base.hash.key;
+	arg->fence_class = fence->fence_class;
+	arg->type = fence->type;
+	arg->signaled = fence->signaled_types;
+	arg->error = fence->error;
+	arg->sequence = fence->sequence;
+	read_unlock_irqrestore(&fm->lock, irq_flags);
+}
+EXPORT_SYMBOL(drm_fence_fill_arg);
+
+void drm_fence_manager_takedown(struct drm_device *dev)
+{
+}
+
+struct drm_fence_object *drm_lookup_fence_object(struct drm_file *priv,
+						 uint32_t handle)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_fence_object *fence;
+
+	mutex_lock(&dev->struct_mutex);
+	uo = drm_lookup_user_object(priv, handle);
+	if (!uo || (uo->type != drm_fence_type)) {
+		mutex_unlock(&dev->struct_mutex);
+		return NULL;
+	}
+	fence = drm_fence_reference_locked(drm_user_object_entry(uo, struct drm_fence_object, base));
+	mutex_unlock(&dev->struct_mutex);
+	return fence;
+}
+
+int drm_fence_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	if (arg->flags & DRM_FENCE_FLAG_EMIT)
+		LOCK_TEST_WITH_RETURN(dev, file_priv);
+	ret = drm_fence_object_create(dev, arg->fence_class,
+				      arg->type, arg->flags, &fence);
+	if (ret)
+		return ret;
+	ret = drm_fence_add_user_object(file_priv, fence,
+					arg->flags &
+					DRM_FENCE_FLAG_SHAREABLE);
+	if (ret) {
+		drm_fence_usage_deref_unlocked(&fence);
+		return ret;
+	}
+
+	/*
+	 * usage > 0. No need to lock dev->struct_mutex;
+	 */
+
+	arg->handle = fence->base.hash.key;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	struct drm_user_object *uo;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_ref(file_priv, arg->handle, drm_fence_type, &uo);
+	if (ret)
+		return ret;
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	return drm_user_object_unref(file_priv, arg->handle, drm_fence_type);
+}
+
+int drm_fence_signaled_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_flush_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_flush(fence, arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_wait(fence,
+				    arg->flags & DRM_FENCE_FLAG_WAIT_LAZY,
+				    0, arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_emit_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	LOCK_TEST_WITH_RETURN(dev, file_priv);
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_emit(fence, arg->flags, arg->fence_class,
+				    arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_buffers_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized\n");
+		return -EINVAL;
+	}
+	LOCK_TEST_WITH_RETURN(dev, file_priv);
+	ret = drm_fence_buffer_objects(dev, NULL, arg->flags,
+				       NULL, &fence);
+	if (ret)
+		return ret;
+
+	if (!(arg->flags & DRM_FENCE_FLAG_NO_USER)) {
+		ret = drm_fence_add_user_object(file_priv, fence,
+						arg->flags &
+						DRM_FENCE_FLAG_SHAREABLE);
+		if (ret)
+			return ret;
+	}
+
+	arg->handle = fence->base.hash.key;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
diff --git a/drivers/char/drm/drm_fops.c b/drivers/char/drm/drm_fops.c
index d2e6da8..5dae9fb 100644
--- a/drivers/char/drm/drm_fops.c
+++ b/drivers/char/drm/drm_fops.c
@@ -43,10 +43,8 @@ static int drm_open_helper(struct inode *inode, struct file *filp,
 
 static int drm_setup(struct drm_device * dev)
 {
-	drm_local_map_t *map;
 	int i;
 	int ret;
-	u32 sareapage;
 
 	if (dev->driver->firstopen) {
 		ret = dev->driver->firstopen(dev);
@@ -54,14 +52,6 @@ static int drm_setup(struct drm_device * dev)
 			return ret;
 	}
 
-	dev->magicfree.next = NULL;
-
-	/* prebuild the SAREA */
-	sareapage = max_t(unsigned, SAREA_MAX, PAGE_SIZE);
-	i = drm_addmap(dev, 0, sareapage, _DRM_SHM, _DRM_CONTAINS_LOCK, &map);
-	if (i != 0)
-		return i;
-
 	atomic_set(&dev->ioctl_count, 0);
 	atomic_set(&dev->vma_count, 0);
 	dev->buf_use = 0;
@@ -76,16 +66,12 @@ static int drm_setup(struct drm_device * dev)
 	for (i = 0; i < ARRAY_SIZE(dev->counts); i++)
 		atomic_set(&dev->counts[i], 0);
 
-	drm_ht_create(&dev->magiclist, DRM_MAGIC_HASH_ORDER);
-	INIT_LIST_HEAD(&dev->magicfree);
-
 	dev->sigdata.lock = NULL;
-	init_waitqueue_head(&dev->lock.lock_queue);
+
 	dev->queue_count = 0;
 	dev->queue_reserved = 0;
 	dev->queue_slots = 0;
 	dev->queuelist = NULL;
-	dev->irq_enabled = 0;
 	dev->context_flag = 0;
 	dev->interrupt_flag = 0;
 	dev->dma_flag = 0;
@@ -146,11 +132,19 @@ int drm_open(struct inode *inode, struct file *filp)
 		spin_lock(&dev->count_lock);
 		if (!dev->open_count++) {
 			spin_unlock(&dev->count_lock);
-			return drm_setup(dev);
+			retcode = drm_setup(dev);
+			goto out;
 		}
 		spin_unlock(&dev->count_lock);
 	}
 
+out:
+	mutex_lock(&dev->struct_mutex);
+	BUG_ON((dev->dev_mapping != NULL) &&
+	       (dev->dev_mapping != inode->i_mapping));
+	if (dev->dev_mapping == NULL)
+		dev->dev_mapping = inode->i_mapping;
+	mutex_unlock(&dev->struct_mutex);
 	return retcode;
 }
 EXPORT_SYMBOL(drm_open);
@@ -226,6 +220,7 @@ static int drm_open_helper(struct inode *inode, struct file *filp,
 	int minor_id = iminor(inode);
 	struct drm_file *priv;
 	int ret;
+	int i, j;
 
 	if (filp->f_flags & O_EXCL)
 		return -EBUSY;	/* No exclusive opens */
@@ -250,6 +245,21 @@ static int drm_open_helper(struct inode *inode, struct file *filp,
 	priv->lock_count = 0;
 
 	INIT_LIST_HEAD(&priv->lhead);
+	INIT_LIST_HEAD(&priv->refd_objects);
+	INIT_LIST_HEAD(&priv->fbs);
+
+	for (i = 0; i < _DRM_NO_REF_TYPES; ++i) {
+		ret = drm_ht_create(&priv->refd_object_hash[i],
+				    DRM_FILE_HASH_ORDER);
+		if (ret)
+			break;
+	}
+
+	if (ret) {
+		for (j = 0; j < i; ++j)
+			drm_ht_remove(&priv->refd_object_hash[j]);
+		goto out_free;
+	}
 
 	if (dev->driver->open) {
 		ret = dev->driver->open(dev, priv);
@@ -257,10 +267,35 @@ static int drm_open_helper(struct inode *inode, struct file *filp,
 			goto out_free;
 	}
 
+
+	/* if there is no current master make this fd it */
 	mutex_lock(&dev->struct_mutex);
-	if (list_empty(&dev->filelist))
-		priv->master = 1;
+	if (!priv->minor->master) {
+		priv->minor->master = drm_get_master(priv->minor);
+		if (!priv->minor->master) {
+			ret = -ENOMEM;
+			goto out_free;
+		}
+
+		priv->is_master = 1;
+		priv->master = priv->minor->master;
+
+		mutex_unlock(&dev->struct_mutex);
+		if (dev->driver->master_create) {
+			ret = dev->driver->master_create(dev, priv->master);
+			if (ret) {
+				drm_put_master(priv->minor->master);
+				priv->minor->master = priv->master = NULL;
+				mutex_unlock(&dev->struct_mutex);
+				goto out_free;
+			}
+		}
+	} else {
+		priv->master = priv->minor->master;
+		mutex_unlock(&dev->struct_mutex);
+	}
 
+	mutex_lock(&dev->struct_mutex);
 	list_add(&priv->lhead, &dev->filelist);
 	mutex_unlock(&dev->struct_mutex);
 
@@ -284,7 +319,7 @@ static int drm_open_helper(struct inode *inode, struct file *filp,
 #endif
 
 	return 0;
-      out_free:
+out_free:
 	drm_free(priv, sizeof(*priv), DRM_MEM_FILES);
 	filp->private_data = NULL;
 	return ret;
@@ -306,6 +341,32 @@ int drm_fasync(int fd, struct file *filp, int on)
 }
 EXPORT_SYMBOL(drm_fasync);
 
+static void drm_object_release(struct file *filp)
+{
+	struct drm_file *priv = filp->private_data;
+	struct list_head *head;
+	struct drm_ref_object *ref_object;
+	int i;
+
+	/*
+	 * Free leftover ref objects created by me. Note that we cannot use
+	 * list_for_each() here, as the struct_mutex may be temporarily
+	 * released by the remove_() functions, and thus the lists may be
+	 * altered.
+	 * Also, a drm_remove_ref_object() will not remove it
+	 * from the list unless its refcount is 1.
+	 */
+	head = &priv->refd_objects;
+	while (head->next != head) {
+		ref_object = list_entry(head->next, struct drm_ref_object, list);
+		drm_remove_ref_object(priv, ref_object);
+		head = &priv->refd_objects;
+	}
+
+	for (i = 0; i < _DRM_NO_REF_TYPES; ++i)
+		drm_ht_remove(&priv->refd_object_hash[i]);
+}
+
 /**
  * Release file.
  *
@@ -340,59 +401,62 @@ int drm_release(struct inode *inode, struct file *filp)
 		  (long)old_encode_dev(file_priv->minor->device),
 		  dev->open_count);
 
-	if (dev->driver->reclaim_buffers_locked && dev->lock.hw_lock) {
-		if (drm_i_have_hw_lock(dev, file_priv)) {
-			dev->driver->reclaim_buffers_locked(dev, file_priv);
-		} else {
-			unsigned long endtime = jiffies + 3 * DRM_HZ;
-			int locked = 0;
-
-			drm_idlelock_take(&dev->lock);
-
-			/*
-			 * Wait for a while.
-			 */
-
-			do{
-				spin_lock_bh(&dev->lock.spinlock);
-				locked = dev->lock.idle_has_lock;
-				spin_unlock_bh(&dev->lock.spinlock);
-				if (locked)
-					break;
-				schedule();
-			} while (!time_after_eq(jiffies, endtime));
-
-			if (!locked) {
-				DRM_ERROR("reclaim_buffers_locked() deadlock. Please rework this\n"
-					  "\tdriver to use reclaim_buffers_idlelocked() instead.\n"
-					  "\tI will go on reclaiming the buffers anyway.\n");
+	/* if the master has gone away we can't do anything with the lock */
+	if (file_priv->minor->master) {
+		if (dev->driver->reclaim_buffers_locked && file_priv->master->lock.hw_lock) {
+			if (drm_i_have_hw_lock(dev, file_priv)) {
+				dev->driver->reclaim_buffers_locked(dev, file_priv);
+			} else {
+				unsigned long _end = jiffies + 3*DRM_HZ;
+				int locked = 0;
+
+				drm_idlelock_take(&file_priv->master->lock);
+
+				/*
+				 * Wait for a while.
+				 */
+
+				do{
+					spin_lock_bh(&file_priv->master->lock.spinlock);
+					locked = file_priv->master->lock.idle_has_lock;
+					spin_unlock_bh(&file_priv->master->lock.spinlock);
+					if (locked)
+						break;
+					schedule();
+				} while (!time_after_eq(jiffies, _end));
+
+				if (!locked) {
+					DRM_ERROR("reclaim_buffers_locked() deadlock. Please rework this\n"
+						  "\tdriver to use reclaim_buffers_idlelocked() instead.\n"
+						  "\tI will go on reclaiming the buffers anyway.\n");
+				}
+
+				dev->driver->reclaim_buffers_locked(dev, file_priv);
+				drm_idlelock_release(&file_priv->master->lock);
 			}
-
-			dev->driver->reclaim_buffers_locked(dev, file_priv);
-			drm_idlelock_release(&dev->lock);
 		}
-	}
 
-	if (dev->driver->reclaim_buffers_idlelocked && dev->lock.hw_lock) {
+		if (dev->driver->reclaim_buffers_idlelocked && file_priv->master->lock.hw_lock) {
 
-		drm_idlelock_take(&dev->lock);
-		dev->driver->reclaim_buffers_idlelocked(dev, file_priv);
-		drm_idlelock_release(&dev->lock);
+			drm_idlelock_take(&file_priv->master->lock);
+			dev->driver->reclaim_buffers_idlelocked(dev, file_priv);
+			drm_idlelock_release(&file_priv->master->lock);
 
-	}
+		}
 
-	if (drm_i_have_hw_lock(dev, file_priv)) {
-		DRM_DEBUG("File %p released, freeing lock for context %d\n",
-			  filp, _DRM_LOCKING_CONTEXT(dev->lock.hw_lock->lock));
+		if (drm_i_have_hw_lock(dev, file_priv)) {
+			DRM_DEBUG("File %p released, freeing lock for context %d\n",
+				  filp, _DRM_LOCKING_CONTEXT(file_priv->master->lock.hw_lock->lock));
 
-		drm_lock_free(&dev->lock,
-			      _DRM_LOCKING_CONTEXT(dev->lock.hw_lock->lock));
-	}
+			drm_lock_free(&file_priv->master->lock,
+				      _DRM_LOCKING_CONTEXT(file_priv->master->lock.hw_lock->lock));
+		}
 
 
-	if (drm_core_check_feature(dev, DRIVER_HAVE_DMA) &&
-	    !dev->driver->reclaim_buffers_locked) {
-		dev->driver->reclaim_buffers(dev, file_priv);
+		if (drm_core_check_feature(dev, DRIVER_HAVE_DMA) &&
+		    !dev->driver->reclaim_buffers_locked) {
+			dev->driver->reclaim_buffers(dev, file_priv);
+		}
 	}
 
 	drm_fasync(-1, filp, 0);
@@ -418,7 +482,18 @@ int drm_release(struct inode *inode, struct file *filp)
 	}
 	mutex_unlock(&dev->ctxlist_mutex);
 
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		drm_fb_release(filp);
+
+	file_priv->master = NULL;
+
+	if (file_priv->is_master) {
+	       drm_put_master(file_priv->minor->master);
+	       file_priv->minor->master = NULL;
+	}
+
 	mutex_lock(&dev->struct_mutex);
+	drm_object_release(filp);
 	if (file_priv->remove_auth_on_close == 1) {
 		struct drm_file *temp;
 
@@ -439,9 +514,9 @@ int drm_release(struct inode *inode, struct file *filp)
 	atomic_inc(&dev->counts[_DRM_STAT_CLOSES]);
 	spin_lock(&dev->count_lock);
 	if (!--dev->open_count) {
-		if (atomic_read(&dev->ioctl_count) || dev->blocked) {
-			DRM_ERROR("Device busy: %d %d\n",
-				  atomic_read(&dev->ioctl_count), dev->blocked);
+		if (atomic_read(&dev->ioctl_count)) {
+			DRM_ERROR("Device busy: %d\n",
+				  atomic_read(&dev->ioctl_count));
 			spin_unlock(&dev->count_lock);
 			unlock_kernel();
 			return -EBUSY;
diff --git a/drivers/char/drm/drm_hashtab.c b/drivers/char/drm/drm_hashtab.c
index 3316067..72a783e 100644
--- a/drivers/char/drm/drm_hashtab.c
+++ b/drivers/char/drm/drm_hashtab.c
@@ -57,7 +57,7 @@ int drm_ht_create(struct drm_open_hash *ht, unsigned int order)
 		DRM_ERROR("Out of memory for hash table\n");
 		return -ENOMEM;
 	}
-	for (i=0; i< ht->size; ++i) {
+	for (i = 0; i < ht->size; ++i) {
 		INIT_HLIST_HEAD(&ht->table[i]);
 	}
 	return 0;
@@ -147,7 +147,7 @@ int drm_ht_just_insert_please(struct drm_open_hash *ht, struct drm_hash_item *it
 		ret = drm_ht_insert_item(ht, item);
 		if (ret)
 			unshifted_key = (unshifted_key + 1) & mask;
-	} while(ret && (unshifted_key != first));
+	} while (ret && (unshifted_key != first));
 
 	if (ret) {
 		DRM_ERROR("Available key bit space exhausted\n");
diff --git a/drivers/char/drm/drm_ioctl.c b/drivers/char/drm/drm_ioctl.c
index 16829fb..3b69b74 100644
--- a/drivers/char/drm/drm_ioctl.c
+++ b/drivers/char/drm/drm_ioctl.c
@@ -53,12 +53,13 @@ int drm_getunique(struct drm_device *dev, void *data,
 		  struct drm_file *file_priv)
 {
 	struct drm_unique *u = data;
+	struct drm_master *master = file_priv->master;
 
-	if (u->unique_len >= dev->unique_len) {
-		if (copy_to_user(u->unique, dev->unique, dev->unique_len))
+	if (u->unique_len >= master->unique_len) {
+		if (copy_to_user(u->unique, master->unique, master->unique_len))
 			return -EFAULT;
 	}
-	u->unique_len = dev->unique_len;
+	u->unique_len = master->unique_len;
 
 	return 0;
 }
@@ -81,36 +82,37 @@ int drm_setunique(struct drm_device *dev, void *data,
 		  struct drm_file *file_priv)
 {
 	struct drm_unique *u = data;
+	struct drm_master *master = file_priv->master;
 	int domain, bus, slot, func, ret;
 
-	if (dev->unique_len || dev->unique)
+	if (master->unique_len || master->unique)
 		return -EBUSY;
 
 	if (!u->unique_len || u->unique_len > 1024)
 		return -EINVAL;
 
-	dev->unique_len = u->unique_len;
-	dev->unique = drm_alloc(u->unique_len + 1, DRM_MEM_DRIVER);
-	if (!dev->unique)
+	master->unique_len = u->unique_len;
+	master->unique = drm_alloc(u->unique_len + 1, DRM_MEM_DRIVER);
+	if (!master->unique)
 		return -ENOMEM;
-	if (copy_from_user(dev->unique, u->unique, dev->unique_len))
+	if (copy_from_user(master->unique, u->unique, master->unique_len))
 		return -EFAULT;
 
-	dev->unique[dev->unique_len] = '\0';
+	master->unique[master->unique_len] = '\0';
 
 	dev->devname =
 	    drm_alloc(strlen(dev->driver->pci_driver.name) +
-		      strlen(dev->unique) + 2, DRM_MEM_DRIVER);
+		      strlen(master->unique) + 2, DRM_MEM_DRIVER);
 	if (!dev->devname)
 		return -ENOMEM;
 
 	sprintf(dev->devname, "%s@%s", dev->driver->pci_driver.name,
-		dev->unique);
+		master->unique);
 
 	/* Return error if the busid submitted doesn't match the device's actual
 	 * busid.
 	 */
-	ret = sscanf(dev->unique, "PCI:%d:%d:%d", &bus, &slot, &func);
+	ret = sscanf(master->unique, "PCI:%d:%d:%d", &bus, &slot, &func);
 	if (ret != 3)
 		return -EINVAL;
 	domain = bus >> 8;
@@ -125,34 +127,35 @@ int drm_setunique(struct drm_device *dev, void *data,
 	return 0;
 }
 
-static int drm_set_busid(struct drm_device * dev)
+static int drm_set_busid(struct drm_device *dev, struct drm_file *file_priv)
 {
+	struct drm_master *master = file_priv->master;
 	int len;
 
-	if (dev->unique != NULL)
-		return 0;
+	if (master->unique != NULL)
+		return -EBUSY;
 
-	dev->unique_len = 40;
-	dev->unique = drm_alloc(dev->unique_len + 1, DRM_MEM_DRIVER);
-	if (dev->unique == NULL)
+	master->unique_len = 40;
+	master->unique = drm_alloc(master->unique_len + 1, DRM_MEM_DRIVER);
+	if (master->unique == NULL)
 		return -ENOMEM;
 
-	len = snprintf(dev->unique, dev->unique_len, "pci:%04x:%02x:%02x.%d",
-		       drm_get_pci_domain(dev), dev->pdev->bus->number,
+	len = snprintf(master->unique, master->unique_len, "pci:%04x:%02x:%02x.%d",
+		       drm_get_pci_domain(dev),
+		       dev->pdev->bus->number,
 		       PCI_SLOT(dev->pdev->devfn),
 		       PCI_FUNC(dev->pdev->devfn));
-
-	if (len > dev->unique_len)
-		DRM_ERROR("Unique buffer overflowed\n");
+	if (len > master->unique_len)
+		DRM_ERROR("buffer overflow");
 
 	dev->devname =
-	    drm_alloc(strlen(dev->driver->pci_driver.name) + dev->unique_len +
+	    drm_alloc(strlen(dev->driver->pci_driver.name) + master->unique_len +
 		      2, DRM_MEM_DRIVER);
 	if (dev->devname == NULL)
 		return -ENOMEM;
 
 	sprintf(dev->devname, "%s@%s", dev->driver->pci_driver.name,
-		dev->unique);
+		master->unique);
 
 	return 0;
 }
@@ -276,7 +279,7 @@ int drm_getstats(struct drm_device *dev, void *data,
 	for (i = 0; i < dev->counters; i++) {
 		if (dev->types[i] == _DRM_STAT_LOCK)
 			stats->data[i].value =
-			    (dev->lock.hw_lock ? dev->lock.hw_lock->lock : 0);
+			    (file_priv->master->lock.hw_lock ? file_priv->master->lock.hw_lock->lock : 0);
 		else
 			stats->data[i].value = atomic_read(&dev->counts[i]);
 		stats->data[i].type = dev->types[i];
@@ -305,6 +308,9 @@ int drm_setversion(struct drm_device *dev, void *data, struct drm_file *file_pri
 	struct drm_set_version *sv = data;
 	int if_version, retcode = 0;
 
+	if (!file_priv->is_master)
+		return -EPERM;
+
 	if (sv->drm_di_major != -1) {
 		if (sv->drm_di_major != DRM_IF_MAJOR ||
 		    sv->drm_di_minor < 0 || sv->drm_di_minor > DRM_IF_MINOR) {
@@ -318,7 +324,7 @@ int drm_setversion(struct drm_device *dev, void *data, struct drm_file *file_pri
 			/*
 			 * Version 1.1 includes tying of DRM to specific device
 			 */
-			drm_set_busid(dev);
+			drm_set_busid(dev, file_priv);
 		}
 	}
 
diff --git a/drivers/char/drm/drm_irq.c b/drivers/char/drm/drm_irq.c
index 089c015..01bed65 100644
--- a/drivers/char/drm/drm_irq.c
+++ b/drivers/char/drm/drm_irq.c
@@ -81,7 +81,7 @@ int drm_irq_by_busid(struct drm_device *dev, void *data,
  * \c drm_driver_irq_preinstall() and \c drm_driver_irq_postinstall() functions
  * before and after the installation.
  */
-static int drm_irq_install(struct drm_device * dev)
+int drm_irq_install(struct drm_device * dev)
 {
 	int ret;
 	unsigned long sh_flags = 0;
@@ -141,6 +141,7 @@ static int drm_irq_install(struct drm_device * dev)
 
 	return 0;
 }
+EXPORT_SYMBOL(drm_irq_install);
 
 /**
  * Uninstall the IRQ handler.
@@ -200,6 +201,8 @@ int drm_control(struct drm_device *dev, void *data,
 	case DRM_INST_HANDLER:
 		if (!drm_core_check_feature(dev, DRIVER_HAVE_IRQ))
 			return 0;
+		if (drm_core_check_feature(dev, DRIVER_MODESET))
+			return 0;
 		if (dev->if_version < DRM_IF_VERSION(1, 2) &&
 		    ctl->irq != dev->irq)
 			return -EINVAL;
@@ -207,6 +210,8 @@ int drm_control(struct drm_device *dev, void *data,
 	case DRM_UNINST_HANDLER:
 		if (!drm_core_check_feature(dev, DRIVER_HAVE_IRQ))
 			return 0;
+		if (drm_core_check_feature(dev, DRIVER_MODESET))
+			return 0;
 		return drm_irq_uninstall(dev);
 	default:
 		return -EINVAL;
@@ -340,7 +345,7 @@ int drm_wait_vblank(struct drm_device *dev, void *data, struct drm_file *file_pr
 		vblwait->reply.tval_usec = now.tv_usec;
 	}
 
-      done:
+done:
 	return ret;
 }
 
@@ -404,18 +409,18 @@ static void drm_locked_tasklet_func(unsigned long data)
 	spin_lock_irqsave(&dev->tasklet_lock, irqflags);
 
 	if (!dev->locked_tasklet_func ||
-	    !drm_lock_take(&dev->lock,
+	    !drm_lock_take(&dev->primary->master->lock,
 			   DRM_KERNEL_CONTEXT)) {
 		spin_unlock_irqrestore(&dev->tasklet_lock, irqflags);
 		return;
 	}
 
-	dev->lock.lock_time = jiffies;
+	dev->primary->master->lock.lock_time = jiffies;
 	atomic_inc(&dev->counts[_DRM_STAT_LOCKS]);
 
 	dev->locked_tasklet_func(dev);
 
-	drm_lock_free(&dev->lock,
+	drm_lock_free(&dev->primary->master->lock,
 		      DRM_KERNEL_CONTEXT);
 
 	dev->locked_tasklet_func = NULL;
diff --git a/drivers/char/drm/drm_lock.c b/drivers/char/drm/drm_lock.c
index 0998723..b9629eb 100644
--- a/drivers/char/drm/drm_lock.c
+++ b/drivers/char/drm/drm_lock.c
@@ -52,6 +52,7 @@ int drm_lock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 {
 	DECLARE_WAITQUEUE(entry, current);
 	struct drm_lock *lock = data;
+	struct drm_master *master = file_priv->master;
 	int ret = 0;
 
 	++file_priv->lock_count;
@@ -63,27 +64,27 @@ int drm_lock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 	}
 
 	DRM_DEBUG("%d (pid %d) requests lock (0x%08x), flags = 0x%08x\n",
-		  lock->context, task_pid_nr(current),
-		  dev->lock.hw_lock->lock, lock->flags);
+		  lock->context, current->pid,
+		  master->lock.hw_lock->lock, lock->flags);
 
 	if (drm_core_check_feature(dev, DRIVER_DMA_QUEUE))
 		if (lock->context < 0)
 			return -EINVAL;
 
-	add_wait_queue(&dev->lock.lock_queue, &entry);
-	spin_lock_bh(&dev->lock.spinlock);
-	dev->lock.user_waiters++;
-	spin_unlock_bh(&dev->lock.spinlock);
+	add_wait_queue(&master->lock.lock_queue, &entry);
+	spin_lock_bh(&master->lock.spinlock);
+	master->lock.user_waiters++;
+	spin_unlock_bh(&master->lock.spinlock);
 	for (;;) {
 		__set_current_state(TASK_INTERRUPTIBLE);
-		if (!dev->lock.hw_lock) {
+		if (!master->lock.hw_lock) {
 			/* Device has been unregistered */
 			ret = -EINTR;
 			break;
 		}
-		if (drm_lock_take(&dev->lock, lock->context)) {
-			dev->lock.file_priv = file_priv;
-			dev->lock.lock_time = jiffies;
+		if (drm_lock_take(&master->lock, lock->context)) {
+			master->lock.file_priv = file_priv;
+			master->lock.lock_time = jiffies;
 			atomic_inc(&dev->counts[_DRM_STAT_LOCKS]);
 			break;	/* Got lock */
 		}
@@ -95,11 +96,12 @@ int drm_lock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 			break;
 		}
 	}
-	spin_lock_bh(&dev->lock.spinlock);
-	dev->lock.user_waiters--;
-	spin_unlock_bh(&dev->lock.spinlock);
+	spin_lock_bh(&master->lock.spinlock);
+	master->lock.user_waiters--;
+	spin_unlock_bh(&master->lock.spinlock);
+
 	__set_current_state(TASK_RUNNING);
-	remove_wait_queue(&dev->lock.lock_queue, &entry);
+	remove_wait_queue(&master->lock.lock_queue, &entry);
 
 	DRM_DEBUG("%d %s\n", lock->context,
 		  ret ? "interrupted" : "has lock");
@@ -111,7 +113,7 @@ int drm_lock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 	sigaddset(&dev->sigmask, SIGTTIN);
 	sigaddset(&dev->sigmask, SIGTTOU);
 	dev->sigdata.context = lock->context;
-	dev->sigdata.lock = dev->lock.hw_lock;
+	dev->sigdata.lock = master->lock.hw_lock;
 	block_all_signals(drm_notifier, &dev->sigdata, &dev->sigmask);
 
 	if (dev->driver->dma_ready && (lock->flags & _DRM_LOCK_READY))
@@ -149,6 +151,7 @@ int drm_lock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 int drm_unlock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 {
 	struct drm_lock *lock = data;
+	struct drm_master *master = file_priv->master;
 	unsigned long irqflags;
 
 	if (lock->context == DRM_KERNEL_CONTEXT) {
@@ -175,7 +178,7 @@ int drm_unlock(struct drm_device *dev, void *data, struct drm_file *file_priv)
 	if (dev->driver->kernel_context_switch_unlock)
 		dev->driver->kernel_context_switch_unlock(dev);
 	else {
-		if (drm_lock_free(&dev->lock,lock->context)) {
+		if (drm_lock_free(&master->lock,lock->context)) {
 			/* FIXME: Should really bail out here. */
 		}
 	}
@@ -383,9 +386,10 @@ EXPORT_SYMBOL(drm_idlelock_release);
 
 int drm_i_have_hw_lock(struct drm_device *dev, struct drm_file *file_priv)
 {
-	return (file_priv->lock_count && dev->lock.hw_lock &&
-		_DRM_LOCK_IS_HELD(dev->lock.hw_lock->lock) &&
-		dev->lock.file_priv == file_priv);
+	struct drm_master *master = file_priv->master;
+	return (file_priv->lock_count && master->lock.hw_lock &&
+		_DRM_LOCK_IS_HELD(master->lock.hw_lock->lock) &&
+		master->lock.file_priv == file_priv);
 }
 
 EXPORT_SYMBOL(drm_i_have_hw_lock);
diff --git a/drivers/char/drm/drm_memory.c b/drivers/char/drm/drm_memory.c
index 845081b..9a01a91 100644
--- a/drivers/char/drm/drm_memory.c
+++ b/drivers/char/drm/drm_memory.c
@@ -36,6 +36,75 @@
 #include <linux/highmem.h>
 #include "drmP.h"
 
+static struct {
+	spinlock_t lock;
+	uint64_t cur_used;
+	uint64_t low_threshold;
+	uint64_t high_threshold;
+} drm_memctl = {
+	.lock = __SPIN_LOCK_UNLOCKED(drm_memctl.lock)
+};
+
+static inline size_t drm_size_align(size_t size)
+{
+	size_t tmpSize = 4;
+	if (size > PAGE_SIZE)
+		return PAGE_ALIGN(size);
+
+	while (tmpSize < size)
+		tmpSize <<= 1;
+
+	return (size_t) tmpSize;
+}
+
+int drm_alloc_memctl(size_t size)
+{
+	int ret;
+	unsigned long a_size = drm_size_align(size);
+
+	spin_lock(&drm_memctl.lock);
+	ret = ((drm_memctl.cur_used + a_size) > drm_memctl.high_threshold) ?
+		-ENOMEM : 0;
+	if (!ret)
+		drm_memctl.cur_used += a_size;
+	spin_unlock(&drm_memctl.lock);
+	return ret;
+}
+EXPORT_SYMBOL(drm_alloc_memctl);
+
+void drm_free_memctl(size_t size)
+{
+	unsigned long a_size = drm_size_align(size);
+
+	spin_lock(&drm_memctl.lock);
+	drm_memctl.cur_used -= a_size;
+	spin_unlock(&drm_memctl.lock);
+}
+EXPORT_SYMBOL(drm_free_memctl);
+
+void drm_query_memctl(uint64_t *cur_used,
+		      uint64_t *low_threshold,
+		      uint64_t *high_threshold)
+{
+	spin_lock(&drm_memctl.lock);
+	*cur_used = drm_memctl.cur_used;
+	*low_threshold = drm_memctl.low_threshold;
+	*high_threshold = drm_memctl.high_threshold;
+	spin_unlock(&drm_memctl.lock);
+}
+EXPORT_SYMBOL(drm_query_memctl);
+
+void drm_init_memctl(size_t p_low_threshold,
+		     size_t p_high_threshold,
+		     size_t unit_size)
+{
+	spin_lock(&drm_memctl.lock);
+	drm_memctl.cur_used = 0;
+	drm_memctl.low_threshold = p_low_threshold * unit_size;
+	drm_memctl.high_threshold = p_high_threshold * unit_size;
+	spin_unlock(&drm_memctl.lock);
+}
+
 #ifdef DEBUG_MEMORY
 #include "drm_memory_debug.h"
 #else
diff --git a/drivers/char/drm/drm_mm.c b/drivers/char/drm/drm_mm.c
index dcff9e9..7d8b670 100644
--- a/drivers/char/drm/drm_mm.c
+++ b/drivers/char/drm/drm_mm.c
@@ -82,7 +82,7 @@ static int drm_mm_create_tail_node(struct drm_mm *mm,
 	struct drm_mm_node *child;
 
 	child = (struct drm_mm_node *)
-		drm_alloc(sizeof(*child), DRM_MEM_MM);
+		drm_ctl_alloc(sizeof(*child), DRM_MEM_MM);
 	if (!child)
 		return -ENOMEM;
 
@@ -118,7 +118,7 @@ static struct drm_mm_node *drm_mm_split_at_start(struct drm_mm_node *parent,
 	struct drm_mm_node *child;
 
 	child = (struct drm_mm_node *)
-		drm_alloc(sizeof(*child), DRM_MEM_MM);
+		drm_ctl_alloc(sizeof(*child), DRM_MEM_MM);
 	if (!child)
 		return NULL;
 
@@ -200,8 +200,8 @@ void drm_mm_put_block(struct drm_mm_node * cur)
 				prev_node->size += next_node->size;
 				list_del(&next_node->ml_entry);
 				list_del(&next_node->fl_entry);
-				drm_free(next_node, sizeof(*next_node),
-					 DRM_MEM_MM);
+				drm_ctl_free(next_node, sizeof(*next_node),
+					     DRM_MEM_MM);
 			} else {
 				next_node->size += cur->size;
 				next_node->start = cur->start;
@@ -214,9 +214,10 @@ void drm_mm_put_block(struct drm_mm_node * cur)
 		list_add(&cur->fl_entry, &mm->fl_entry);
 	} else {
 		list_del(&cur->ml_entry);
-		drm_free(cur, sizeof(*cur), DRM_MEM_MM);
+		drm_ctl_free(cur, sizeof(*cur), DRM_MEM_MM);
 	}
 }
+EXPORT_SYMBOL(drm_mm_put_block);
 
 struct drm_mm_node *drm_mm_search_free(const struct drm_mm * mm,
 				  unsigned long size,
@@ -291,5 +292,5 @@ void drm_mm_takedown(struct drm_mm * mm)
 	list_del(&entry->fl_entry);
 	list_del(&entry->ml_entry);
 
-	drm_free(entry, sizeof(*entry), DRM_MEM_MM);
+	drm_ctl_free(entry, sizeof(*entry), DRM_MEM_MM);
 }
diff --git a/drivers/char/drm/drm_modes.c b/drivers/char/drm/drm_modes.c
new file mode 100644
index 0000000..8009cae
--- /dev/null
+++ b/drivers/char/drm/drm_modes.c
@@ -0,0 +1,560 @@
+/*
+ * Copyright  1997-2003 by The XFree86 Project, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Except as contained in this notice, the name of the copyright holder(s)
+ * and author(s) shall not be used in advertising or otherwise to promote
+ * the sale, use or other dealings in this Software without prior written
+ * authorization from the copyright holder(s) and author(s).
+ */
+/*
+ * Copyright  2007 Dave Airlie
+ */
+
+#include <linux/list.h>
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+
+/**
+ * drm_mode_debug_printmodeline - debug print a mode
+ * @dev: DRM device
+ * @mode: mode to print
+ *
+ * LOCKING:
+ * None.
+ *
+ * Describe @mode using DRM_DEBUG.
+ */
+void drm_mode_debug_printmodeline(struct drm_device *dev,
+				  struct drm_display_mode *mode)
+{
+	DRM_DEBUG("Modeline %d:\"%s\" %d %d %d %d %d %d %d %d %d %d 0x%x 0x%x\n",
+		  mode->mode_id, mode->name, mode->vrefresh, mode->clock,
+		  mode->hdisplay, mode->hsync_start,
+		  mode->hsync_end, mode->htotal,
+		  mode->vdisplay, mode->vsync_start,
+		  mode->vsync_end, mode->vtotal, mode->type, mode->flags);
+}
+EXPORT_SYMBOL(drm_mode_debug_printmodeline);
+
+/**
+ * drm_mode_set_name - set the name on a mode
+ * @mode: name will be set in this mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Set the name of @mode to a standard format.
+ */
+void drm_mode_set_name(struct drm_display_mode *mode)
+{
+	snprintf(mode->name, DRM_DISPLAY_MODE_LEN, "%dx%d", mode->hdisplay,
+		 mode->vdisplay);
+}
+EXPORT_SYMBOL(drm_mode_set_name);
+
+/**
+ * drm_mode_list_concat - move modes from one list to another
+ * @head: source list
+ * @new: dst list
+ *
+ * LOCKING:
+ * Caller must ensure both lists are locked.
+ *
+ * Move all the modes from @head to @new.
+ */
+void drm_mode_list_concat(struct list_head *head, struct list_head *new)
+{
+
+	struct list_head *entry, *tmp;
+
+	list_for_each_safe(entry, tmp, head) {
+		list_move_tail(entry, new);
+	}
+}
+
+/**
+ * drm_mode_width - get the width of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's width (hdisplay) value.
+ *
+ * FIXME: is this needed?
+ *
+ * RETURNS:
+ * @mode->hdisplay
+ */
+int drm_mode_width(struct drm_display_mode *mode)
+{
+	return mode->hdisplay;
+
+}
+EXPORT_SYMBOL(drm_mode_width);
+
+/**
+ * drm_mode_height - get the height of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's height (vdisplay) value.
+ *
+ * FIXME: is this needed?
+ *
+ * RETURNS:
+ * @mode->vdisplay
+ */
+int drm_mode_height(struct drm_display_mode *mode)
+{
+	return mode->vdisplay;
+}
+EXPORT_SYMBOL(drm_mode_height);
+
+/**
+ * drm_mode_vrefresh - get the vrefresh of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's vrefresh rate or calculate it if necessary.
+ *
+ * FIXME: why is this needed?  shouldn't vrefresh be set already?
+ *
+ * RETURNS:
+ * Vertical refresh rate of @mode x 1000. For precision reasons.
+ */
+int drm_mode_vrefresh(struct drm_display_mode *mode)
+{
+	int refresh = 0;
+	unsigned int calc_val;
+
+	if (mode->vrefresh > 0)
+		refresh = mode->vrefresh;
+	else if (mode->htotal > 0 && mode->vtotal > 0) {
+		/* work out vrefresh the value will be x1000 */
+		calc_val = (mode->clock * 1000);
+
+		calc_val /= mode->htotal;
+		calc_val *= 1000;
+		calc_val /= mode->vtotal;
+
+		refresh = calc_val;
+		if (mode->flags & V_INTERLACE)
+			refresh *= 2;
+		if (mode->flags & V_DBLSCAN)
+			refresh /= 2;
+		if (mode->vscan > 1)
+			refresh /= mode->vscan;
+	}
+	return refresh;
+}
+EXPORT_SYMBOL(drm_mode_vrefresh);
+	
+/**
+ * drm_mode_set_crtcinfo - set CRTC modesetting parameters
+ * @p: mode
+ * @adjust_flags: unused? (FIXME)
+ *
+ * LOCKING:
+ * None.
+ *
+ * Setup the CRTC modesetting parameters for @p, adjusting if necessary.
+ */
+void drm_mode_set_crtcinfo(struct drm_display_mode *p, int adjust_flags)
+{
+	if ((p == NULL) || ((p->type & DRM_MODE_TYPE_CRTC_C) == DRM_MODE_TYPE_BUILTIN))
+		return;
+
+	p->crtc_hdisplay = p->hdisplay;
+	p->crtc_hsync_start = p->hsync_start;
+	p->crtc_hsync_end = p->hsync_end;
+	p->crtc_htotal = p->htotal;
+	p->crtc_hskew = p->hskew;
+	p->crtc_vdisplay = p->vdisplay;
+	p->crtc_vsync_start = p->vsync_start;
+	p->crtc_vsync_end = p->vsync_end;
+	p->crtc_vtotal = p->vtotal;
+
+	if (p->flags & V_INTERLACE) {
+		if (adjust_flags & CRTC_INTERLACE_HALVE_V) {
+			p->crtc_vdisplay /= 2;
+			p->crtc_vsync_start /= 2;
+			p->crtc_vsync_end /= 2;
+			p->crtc_vtotal /= 2;
+		}
+
+		p->crtc_vtotal |= 1;
+	}
+
+	if (p->flags & V_DBLSCAN) {
+		p->crtc_vdisplay *= 2;
+		p->crtc_vsync_start *= 2;
+		p->crtc_vsync_end *= 2;
+		p->crtc_vtotal *= 2;
+	}
+
+	if (p->vscan > 1) {
+		p->crtc_vdisplay *= p->vscan;
+		p->crtc_vsync_start *= p->vscan;
+		p->crtc_vsync_end *= p->vscan;
+		p->crtc_vtotal *= p->vscan;
+	}
+
+	p->crtc_vblank_start = min(p->crtc_vsync_start, p->crtc_vdisplay);
+	p->crtc_vblank_end = max(p->crtc_vsync_end, p->crtc_vtotal);
+	p->crtc_hblank_start = min(p->crtc_hsync_start, p->crtc_hdisplay);
+	p->crtc_hblank_end = max(p->crtc_hsync_end, p->crtc_htotal);
+
+	p->crtc_hadjusted = false;
+	p->crtc_vadjusted = false;
+}
+EXPORT_SYMBOL(drm_mode_set_crtcinfo);
+
+
+/**
+ * drm_mode_duplicate - allocate and duplicate an existing mode
+ * @m: mode to duplicate
+ *
+ * LOCKING:
+ * None.
+ *
+ * Just allocate a new mode, copy the existing mode into it, and return
+ * a pointer to it.  Used to create new instances of established modes.
+ */
+struct drm_display_mode *drm_mode_duplicate(struct drm_device *dev,
+					    struct drm_display_mode *mode)
+{
+	struct drm_display_mode *nmode;
+	int new_id;
+
+	nmode = drm_mode_create(dev);
+	if (!nmode)
+		return NULL;
+
+	new_id = nmode->mode_id;
+	*nmode = *mode;
+	nmode->mode_id = new_id;
+	INIT_LIST_HEAD(&nmode->head);
+	return nmode;
+}
+EXPORT_SYMBOL(drm_mode_duplicate);
+
+/**
+ * drm_mode_equal - test modes for equality
+ * @mode1: first mode
+ * @mode2: second mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Check to see if @mode1 and @mode2 are equivalent.
+ *
+ * RETURNS:
+ * True if the modes are equal, false otherwise.
+ */
+bool drm_mode_equal(struct drm_display_mode *mode1, struct drm_display_mode *mode2)
+{
+	if (/*mode1->clock == mode2->clock &&*/
+	    mode1->hdisplay == mode2->hdisplay &&
+	    mode1->hsync_start == mode2->hsync_start &&
+	    mode1->hsync_end == mode2->hsync_end &&
+	    mode1->htotal == mode2->htotal &&
+	    mode1->hskew == mode2->hskew &&
+	    mode1->vdisplay == mode2->vdisplay &&
+	    mode1->vsync_start == mode2->vsync_start &&
+	    mode1->vsync_end == mode2->vsync_end &&
+	    mode1->vtotal == mode2->vtotal &&
+	    mode1->vscan == mode2->vscan &&
+	    mode1->flags == mode2->flags)
+		return true;
+	
+	return false;
+}
+EXPORT_SYMBOL(drm_mode_equal);
+
+/**
+ * drm_mode_validate_size - make sure modes adhere to size constraints
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @maxX: maximum width
+ * @maxY: maximum height
+ * @maxPitch: max pitch
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * The DRM device (@dev) has size and pitch limits.  Here we validate the
+ * modes we probed for @dev against those limits and set their status as
+ * necessary.
+ */
+void drm_mode_validate_size(struct drm_device *dev,
+			    struct list_head *mode_list,
+			    int maxX, int maxY, int maxPitch)
+{
+	struct drm_display_mode *mode;
+
+	list_for_each_entry(mode, mode_list, head) {
+		if (maxPitch > 0 && mode->hdisplay > maxPitch)
+			mode->status = MODE_BAD_WIDTH;
+		
+		if (maxX > 0 && mode->hdisplay > maxX)
+			mode->status = MODE_VIRTUAL_X;
+
+		if (maxY > 0 && mode->vdisplay > maxY)
+			mode->status = MODE_VIRTUAL_Y;
+	}
+}
+EXPORT_SYMBOL(drm_mode_validate_size);
+
+/**
+ * drm_mode_validate_clocks - validate modes against clock limits
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @min: minimum clock rate array
+ * @max: maximum clock rate array
+ * @n_ranges: number of clock ranges (size of arrays)
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Some code may need to check a mode list against the clock limits of the
+ * device in question.  This function walks the mode list, testing to make
+ * sure each mode falls within a given range (defined by @min and @max
+ * arrays) and sets @mode->status as needed.
+ */
+void drm_mode_validate_clocks(struct drm_device *dev,
+			      struct list_head *mode_list,
+			      int *min, int *max, int n_ranges)
+{
+	struct drm_display_mode *mode;
+	int i;
+
+	list_for_each_entry(mode, mode_list, head) {
+		bool good = false;
+		for (i = 0; i < n_ranges; i++) {
+			if (mode->clock >= min[i] && mode->clock <= max[i]) {
+				good = true;
+				break;
+			}
+		}
+		if (!good)
+			mode->status = MODE_CLOCK_RANGE;
+	}
+}
+EXPORT_SYMBOL(drm_mode_validate_clocks);
+
+/**
+ * drm_mode_prune_invalid - remove invalid modes from mode list
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @verbose: be verbose about it
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Once mode list generation is complete, a caller can use this routine to
+ * remove invalid modes from a mode list.  If any of the modes have a
+ * status other than %MODE_OK, they are removed from @mode_list and freed.
+ */
+void drm_mode_prune_invalid(struct drm_device *dev,
+			    struct list_head *mode_list, bool verbose)
+{
+	struct drm_display_mode *mode, *t;
+
+	list_for_each_entry_safe(mode, t, mode_list, head) {
+		if (mode->status != MODE_OK) {
+			list_del(&mode->head);
+			if (verbose) {
+				drm_mode_debug_printmodeline(dev, mode);
+				DRM_DEBUG("Not using %s mode %d\n", mode->name, mode->status);
+			}
+			kfree(mode);
+		}
+	}
+}
+
+/**
+ * drm_mode_compare - compare modes for favorability
+ * @lh_a: list_head for first mode
+ * @lh_b: list_head for second mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Compare two modes, given by @lh_a and @lh_b, returning a value indicating
+ * which is better.
+ *
+ * RETURNS:
+ * Negative if @lh_a is better than @lh_b, zero if they're equivalent, or
+ * positive if @lh_b is better than @lh_a.
+ */
+static int drm_mode_compare(struct list_head *lh_a, struct list_head *lh_b)
+{
+	struct drm_display_mode *a = list_entry(lh_a, struct drm_display_mode, head);
+	struct drm_display_mode *b = list_entry(lh_b, struct drm_display_mode, head);
+	int diff;
+
+	diff = ((b->type & DRM_MODE_TYPE_PREFERRED) != 0) -
+		((a->type & DRM_MODE_TYPE_PREFERRED) != 0);
+	if (diff)
+		return diff;
+	diff = b->hdisplay * b->vdisplay - a->hdisplay * a->vdisplay;
+	if (diff)
+		return diff;
+	diff = b->clock - a->clock;
+	return diff;
+}
+
+/* FIXME: what we don't have a list sort function? */
+/* list sort from Mark J Roberts (mjr@znex.org) */
+void list_sort(struct list_head *head, int (*cmp)(struct list_head *a, struct list_head *b))
+{
+	struct list_head *p, *q, *e, *list, *tail, *oldhead;
+	int insize, nmerges, psize, qsize, i;
+	
+	list = head->next;
+	list_del(head);
+	insize = 1;
+	for (;;) {
+		p = oldhead = list;
+		list = tail = NULL;
+		nmerges = 0;
+		
+		while (p) {
+			nmerges++;
+			q = p;
+			psize = 0;
+			for (i = 0; i < insize; i++) {
+				psize++;
+				q = q->next == oldhead ? NULL : q->next;
+				if (!q)
+					break;
+			}
+			
+			qsize = insize;
+			while (psize > 0 || (qsize > 0 && q)) {
+				if (!psize) {
+					e = q;
+					q = q->next;
+					qsize--;
+					if (q == oldhead)
+						q = NULL;
+				} else if (!qsize || !q) {
+					e = p;
+					p = p->next;
+					psize--;
+					if (p == oldhead)
+						p = NULL;
+				} else if (cmp(p, q) <= 0) {
+					e = p;
+					p = p->next;
+					psize--;
+					if (p == oldhead)
+						p = NULL;
+				} else {
+					e = q;
+					q = q->next;
+					qsize--;
+					if (q == oldhead)
+						q = NULL;
+				}
+				if (tail)
+					tail->next = e;
+				else
+					list = e;
+				e->prev = tail;
+				tail = e;
+			}
+			p = q;
+		}
+		
+		tail->next = list;
+		list->prev = tail;
+		
+		if (nmerges <= 1)
+			break;
+		
+		insize *= 2;
+	}
+	
+	head->next = list;
+	head->prev = list->prev;
+	list->prev->next = head;
+	list->prev = head;
+}
+
+/**
+ * drm_mode_sort - sort mode list
+ * @mode_list: list to sort
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Sort @mode_list by favorability, putting good modes first.
+ */
+void drm_mode_sort(struct list_head *mode_list)
+{
+	list_sort(mode_list, drm_mode_compare);
+}
+
+
+/**
+ * drm_mode_output_list_update - update the mode list for the output
+ * @output: the output to update
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * This moves the modes from the @output probed_modes list
+ * to the actual mode list. It compares the probed mode against the current
+ * list and only adds different modes. All modes unverified after this point
+ * will be removed by the prune invalid modes.
+ */
+void drm_mode_output_list_update(struct drm_output *output)
+{
+	struct drm_display_mode *mode;
+	struct drm_display_mode *pmode, *pt;
+	int found_it;
+	list_for_each_entry_safe(pmode, pt, &output->probed_modes,
+				 head) {
+		found_it = 0;
+		/* go through current modes checking for the new probed mode */
+		list_for_each_entry(mode, &output->modes, head) {
+			if (drm_mode_equal(pmode, mode)) {
+				found_it = 1;
+				/* if equal delete the probed mode */
+				mode->status = pmode->status;
+				list_del(&pmode->head);
+				kfree(pmode);
+				break;
+			}
+		}
+
+		if (!found_it) {
+			list_move_tail(&pmode->head, &output->modes);
+		}
+	}
+}
diff --git a/drivers/char/drm/drm_object.c b/drivers/char/drm/drm_object.c
new file mode 100644
index 0000000..1acf527
--- /dev/null
+++ b/drivers/char/drm/drm_object.c
@@ -0,0 +1,293 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+int drm_add_user_object(struct drm_file *priv, struct drm_user_object *item,
+			int shareable)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	/* The refcount will be bumped to 1 when we add the ref object below. */
+	atomic_set(&item->refcount, 0);
+	item->shareable = shareable;
+	item->owner = priv;
+
+	ret = drm_ht_just_insert_please(&dev->object_hash, &item->hash,
+					(unsigned long)item, 31, 0, 0);
+	if (ret)
+		return ret;
+
+	ret = drm_add_ref_object(priv, item, _DRM_REF_USE);
+	if (ret)
+		ret = drm_ht_remove_item(&dev->object_hash, &item->hash);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_add_user_object);
+
+struct drm_user_object *drm_lookup_user_object(struct drm_file *priv, uint32_t key)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_hash_item *hash;
+	int ret;
+	struct drm_user_object *item;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	ret = drm_ht_find_item(&dev->object_hash, key, &hash);
+	if (ret)
+		return NULL;
+
+	item = drm_hash_entry(hash, struct drm_user_object, hash);
+
+	if (priv != item->owner) {
+		struct drm_open_hash *ht = &priv->refd_object_hash[_DRM_REF_USE];
+		ret = drm_ht_find_item(ht, (unsigned long)item, &hash);
+		if (ret) {
+			DRM_ERROR("Object not registered for usage\n");
+			return NULL;
+		}
+	}
+	return item;
+}
+EXPORT_SYMBOL(drm_lookup_user_object);
+
+static void drm_deref_user_object(struct drm_file *priv, struct drm_user_object *item)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	if (atomic_dec_and_test(&item->refcount)) {
+		ret = drm_ht_remove_item(&dev->object_hash, &item->hash);
+		BUG_ON(ret);
+		item->remove(priv, item);
+	}
+}
+
+static int drm_object_ref_action(struct drm_file *priv, struct drm_user_object *ro,
+				 enum drm_ref_type action)
+{
+	int ret = 0;
+
+	switch (action) {
+	case _DRM_REF_USE:
+		atomic_inc(&ro->refcount);
+		break;
+	default:
+		if (!ro->ref_struct_locked) {
+			break;
+		} else {
+			ro->ref_struct_locked(priv, ro, action);
+		}
+	}
+	return ret;
+}
+
+int drm_add_ref_object(struct drm_file *priv, struct drm_user_object *referenced_object,
+		       enum drm_ref_type ref_action)
+{
+	int ret = 0;
+	struct drm_ref_object *item;
+	struct drm_open_hash *ht = &priv->refd_object_hash[ref_action];
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	if (!referenced_object->shareable && priv != referenced_object->owner) {
+		DRM_ERROR("Not allowed to reference this object\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * If this is not a usage reference, Check that usage has been registered
+	 * first. Otherwise strange things may happen on destruction.
+	 */
+
+	if ((ref_action != _DRM_REF_USE) && priv != referenced_object->owner) {
+		item =
+		    drm_lookup_ref_object(priv, referenced_object,
+					  _DRM_REF_USE);
+		if (!item) {
+			DRM_ERROR
+			    ("Object not registered for usage by this client\n");
+			return -EINVAL;
+		}
+	}
+
+	if (NULL !=
+	    (item =
+	     drm_lookup_ref_object(priv, referenced_object, ref_action))) {
+		atomic_inc(&item->refcount);
+		return drm_object_ref_action(priv, referenced_object,
+					     ref_action);
+	}
+
+	item = drm_ctl_calloc(1, sizeof(*item), DRM_MEM_OBJECTS);
+	if (item == NULL) {
+		DRM_ERROR("Could not allocate reference object\n");
+		return -ENOMEM;
+	}
+
+	atomic_set(&item->refcount, 1);
+	item->hash.key = (unsigned long)referenced_object;
+	ret = drm_ht_insert_item(ht, &item->hash);
+	item->unref_action = ref_action;
+
+	if (ret)
+		goto out;
+
+	list_add(&item->list, &priv->refd_objects);
+	ret = drm_object_ref_action(priv, referenced_object, ref_action);
+out:
+	return ret;
+}
+
+struct drm_ref_object *drm_lookup_ref_object(struct drm_file *priv,
+					struct drm_user_object *referenced_object,
+					enum drm_ref_type ref_action)
+{
+	struct drm_hash_item *hash;
+	int ret;
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	ret = drm_ht_find_item(&priv->refd_object_hash[ref_action],
+			       (unsigned long)referenced_object, &hash);
+	if (ret)
+		return NULL;
+
+	return drm_hash_entry(hash, struct drm_ref_object, hash);
+}
+EXPORT_SYMBOL(drm_lookup_ref_object);
+
+static void drm_remove_other_references(struct drm_file *priv,
+					struct drm_user_object *ro)
+{
+	int i;
+	struct drm_open_hash *ht;
+	struct drm_hash_item *hash;
+	struct drm_ref_object *item;
+
+	for (i = _DRM_REF_USE + 1; i < _DRM_NO_REF_TYPES; ++i) {
+		ht = &priv->refd_object_hash[i];
+		while (!drm_ht_find_item(ht, (unsigned long)ro, &hash)) {
+			item = drm_hash_entry(hash, struct drm_ref_object, hash);
+			drm_remove_ref_object(priv, item);
+		}
+	}
+}
+
+void drm_remove_ref_object(struct drm_file *priv, struct drm_ref_object *item)
+{
+	int ret;
+	struct drm_user_object *user_object = (struct drm_user_object *) item->hash.key;
+	struct drm_open_hash *ht = &priv->refd_object_hash[item->unref_action];
+	enum drm_ref_type unref_action;
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	unref_action = item->unref_action;
+	if (atomic_dec_and_test(&item->refcount)) {
+		ret = drm_ht_remove_item(ht, &item->hash);
+		BUG_ON(ret);
+		list_del_init(&item->list);
+		if (unref_action == _DRM_REF_USE)
+			drm_remove_other_references(priv, user_object);
+		drm_ctl_free(item, sizeof(*item), DRM_MEM_OBJECTS);
+	}
+
+	switch (unref_action) {
+	case _DRM_REF_USE:
+		drm_deref_user_object(priv, user_object);
+		break;
+	default:
+		BUG_ON(!user_object->unref);
+		user_object->unref(priv, user_object, unref_action);
+		break;
+	}
+
+}
+
+int drm_user_object_ref(struct drm_file *priv, uint32_t user_token,
+			enum drm_object_type type, struct drm_user_object **object)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_hash_item *hash;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_ht_find_item(&dev->object_hash, user_token, &hash);
+	if (ret) {
+		DRM_ERROR("Could not find user object to reference.\n");
+		goto out_err;
+	}
+	uo = drm_hash_entry(hash, struct drm_user_object, hash);
+	if (uo->type != type) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	ret = drm_add_ref_object(priv, uo, _DRM_REF_USE);
+	if (ret)
+		goto out_err;
+	mutex_unlock(&dev->struct_mutex);
+	*object = uo;
+	return 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+int drm_user_object_unref(struct drm_file *priv, uint32_t user_token,
+			  enum drm_object_type type)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_ref_object *ro;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	uo = drm_lookup_user_object(priv, user_token);
+	if (!uo || (uo->type != type)) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	ro = drm_lookup_ref_object(priv, uo, _DRM_REF_USE);
+	if (!ro) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	drm_remove_ref_object(priv, ro);
+	mutex_unlock(&dev->struct_mutex);
+	return 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
diff --git a/drivers/char/drm/drm_objects.h b/drivers/char/drm/drm_objects.h
new file mode 100644
index 0000000..45f6dbb
--- /dev/null
+++ b/drivers/char/drm/drm_objects.h
@@ -0,0 +1,768 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#ifndef _DRM_OBJECTS_H
+#define _DRM_OBJECTS_H
+
+struct drm_device;
+struct drm_bo_mem_reg;
+
+/***************************************************
+ * User space objects. (drm_object.c)
+ */
+
+#define drm_user_object_entry(_ptr, _type, _member) container_of(_ptr, _type, _member)
+
+enum drm_object_type {
+	drm_fence_type,
+	drm_buffer_type,
+	drm_lock_type,
+	    /*
+	     * Add other user space object types here.
+	     */
+	drm_driver_type0 = 256,
+	drm_driver_type1,
+	drm_driver_type2,
+	drm_driver_type3,
+	drm_driver_type4
+};
+
+/*
+ * A user object is a structure that helps the drm give out user handles
+ * to kernel internal objects and to keep track of these objects so that
+ * they can be destroyed, for example when the user space process exits.
+ * Designed to be accessible using a user space 32-bit handle.
+ */
+
+struct drm_user_object {
+	struct drm_hash_item hash;
+	struct list_head list;
+	enum drm_object_type type;
+	atomic_t refcount;
+	int shareable;
+	struct drm_file *owner;
+	void (*ref_struct_locked) (struct drm_file *priv,
+				   struct drm_user_object *obj,
+				   enum drm_ref_type ref_action);
+	void (*unref) (struct drm_file *priv, struct drm_user_object *obj,
+		       enum drm_ref_type unref_action);
+	void (*remove) (struct drm_file *priv, struct drm_user_object *obj);
+};
+
+/*
+ * A ref object is a structure which is used to
+ * keep track of references to user objects and to keep track of these
+ * references so that they can be destroyed for example when the user space
+ * process exits. Designed to be accessible using a pointer to the _user_ object.
+ */
+
+struct drm_ref_object {
+	struct drm_hash_item hash;
+	struct list_head list;
+	atomic_t refcount;
+	enum drm_ref_type unref_action;
+};
+
+/**
+ * Must be called with the struct_mutex held.
+ */
+
+extern int drm_add_user_object(struct drm_file *priv, struct drm_user_object *item,
+			       int shareable);
+/**
+ * Must be called with the struct_mutex held.
+ */
+
+extern struct drm_user_object *drm_lookup_user_object(struct drm_file *priv,
+						 uint32_t key);
+
+/*
+ * Must be called with the struct_mutex held. May temporarily release it.
+ */
+
+extern int drm_add_ref_object(struct drm_file *priv,
+			      struct drm_user_object *referenced_object,
+			      enum drm_ref_type ref_action);
+
+/*
+ * Must be called with the struct_mutex held.
+ */
+
+struct drm_ref_object *drm_lookup_ref_object(struct drm_file *priv,
+					struct drm_user_object *referenced_object,
+					enum drm_ref_type ref_action);
+/*
+ * Must be called with the struct_mutex held.
+ * If "item" has been obtained by a call to drm_lookup_ref_object. You may not
+ * release the struct_mutex before calling drm_remove_ref_object.
+ * This function may temporarily release the struct_mutex.
+ */
+
+extern void drm_remove_ref_object(struct drm_file *priv, struct drm_ref_object *item);
+extern int drm_user_object_ref(struct drm_file *priv, uint32_t user_token,
+			       enum drm_object_type type,
+			       struct drm_user_object **object);
+extern int drm_user_object_unref(struct drm_file *priv, uint32_t user_token,
+				 enum drm_object_type type);
+
+/***************************************************
+ * Fence objects. (drm_fence.c)
+ */
+
+struct drm_fence_object {
+	struct drm_user_object base;
+	struct drm_device *dev;
+	atomic_t usage;
+
+	/*
+	 * The below three fields are protected by the fence manager spinlock.
+	 */
+
+	struct list_head ring;
+	int fence_class;
+	uint32_t native_types;
+	uint32_t type;
+	uint32_t signaled_types;
+	uint32_t sequence;
+	uint32_t waiting_types;
+	uint32_t error;
+};
+
+#define _DRM_FENCE_CLASSES 8
+#define _DRM_FENCE_TYPE_EXE 0x00
+
+struct drm_fence_class_manager {
+	struct list_head ring;
+	uint32_t pending_flush;
+	uint32_t waiting_types;
+	wait_queue_head_t fence_queue;
+	uint32_t highest_waiting_sequence;
+	uint32_t latest_queued_sequence;
+};
+
+struct drm_fence_manager {
+	int initialized;
+	rwlock_t lock;
+	struct drm_fence_class_manager fence_class[_DRM_FENCE_CLASSES];
+	uint32_t num_classes;
+	atomic_t count;
+};
+
+struct drm_fence_driver {
+	unsigned long *waiting_jiffies;
+	uint32_t num_classes;
+	uint32_t wrap_diff;
+	uint32_t flush_diff;
+	uint32_t sequence_mask;
+
+	/*
+	 * Driver implemented functions:
+	 * has_irq() : 1 if the hardware can update the indicated type_flags using an
+	 * irq handler. 0 if polling is required.
+	 *
+	 * emit() : Emit a sequence number to the command stream.
+	 * Return the sequence number.
+	 *
+	 * flush() : Make sure the flags indicated in fc->pending_flush will eventually
+	 * signal for fc->highest_received_sequence and all preceding sequences.
+	 * Acknowledge by clearing the flags fc->pending_flush.
+	 *
+	 * poll() : Call drm_fence_handler with any new information.
+	 *
+	 * needed_flush() : Given the current state of the fence->type flags and previusly
+	 * executed or queued flushes, return the type_flags that need flushing.
+	 *
+	 * wait(): Wait for the "mask" flags to signal on a given fence, performing
+	 * whatever's necessary to make this happen.
+	 */
+
+	int (*has_irq) (struct drm_device *dev, uint32_t fence_class,
+			uint32_t flags);
+	int (*emit) (struct drm_device *dev, uint32_t fence_class,
+		     uint32_t flags, uint32_t *breadcrumb,
+		     uint32_t *native_type);
+	void (*flush) (struct drm_device *dev, uint32_t fence_class);
+	void (*poll) (struct drm_device *dev, uint32_t fence_class,
+		uint32_t types);
+	uint32_t (*needed_flush) (struct drm_fence_object *fence);
+	int (*wait) (struct drm_fence_object *fence, int lazy,
+		     int interruptible, uint32_t mask);
+};
+
+extern int drm_fence_wait_polling(struct drm_fence_object *fence, int lazy,
+				  int interruptible, uint32_t mask,
+				  unsigned long end_jiffies);
+extern void drm_fence_handler(struct drm_device *dev, uint32_t fence_class,
+			      uint32_t sequence, uint32_t type,
+			      uint32_t error);
+extern void drm_fence_manager_init(struct drm_device *dev);
+extern void drm_fence_manager_takedown(struct drm_device *dev);
+extern void drm_fence_flush_old(struct drm_device *dev, uint32_t fence_class,
+				uint32_t sequence);
+extern int drm_fence_object_flush(struct drm_fence_object *fence,
+				  uint32_t type);
+extern int drm_fence_object_signaled(struct drm_fence_object *fence,
+				     uint32_t type);
+extern void drm_fence_usage_deref_locked(struct drm_fence_object **fence);
+extern void drm_fence_usage_deref_unlocked(struct drm_fence_object **fence);
+extern struct drm_fence_object *drm_fence_reference_locked(struct drm_fence_object *src);
+extern void drm_fence_reference_unlocked(struct drm_fence_object **dst,
+					 struct drm_fence_object *src);
+extern int drm_fence_object_wait(struct drm_fence_object *fence,
+				 int lazy, int ignore_signals, uint32_t mask);
+extern int drm_fence_object_create(struct drm_device *dev, uint32_t type,
+				   uint32_t fence_flags, uint32_t fence_class,
+				   struct drm_fence_object **c_fence);
+extern int drm_fence_object_emit(struct drm_fence_object *fence,
+				 uint32_t fence_flags, uint32_t class,
+				 uint32_t type);
+extern void drm_fence_fill_arg(struct drm_fence_object *fence,
+			       struct drm_fence_arg *arg);
+
+extern int drm_fence_add_user_object(struct drm_file *priv,
+				     struct drm_fence_object *fence,
+				     int shareable);
+
+extern int drm_fence_create_ioctl(struct drm_device *dev, void *data,
+				  struct drm_file *file_priv);
+extern int drm_fence_destroy_ioctl(struct drm_device *dev, void *data,
+				   struct drm_file *file_priv);
+extern int drm_fence_reference_ioctl(struct drm_device *dev, void *data,
+				     struct drm_file *file_priv);
+extern int drm_fence_unreference_ioctl(struct drm_device *dev, void *data,
+				       struct drm_file *file_priv);
+extern int drm_fence_signaled_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file_priv);
+extern int drm_fence_flush_ioctl(struct drm_device *dev, void *data,
+				 struct drm_file *file_priv);
+extern int drm_fence_wait_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int drm_fence_emit_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int drm_fence_buffers_ioctl(struct drm_device *dev, void *data,
+				   struct drm_file *file_priv);
+
+/**************************************************
+ *TTMs
+ */
+
+/*
+ * The ttm backend GTT interface. (In our case AGP).
+ * Any similar type of device (PCIE?)
+ * needs only to implement these functions to be usable with the TTM interface.
+ * The AGP backend implementation lives in drm_agpsupport.c
+ * basically maps these calls to available functions in agpgart.
+ * Each drm device driver gets an
+ * additional function pointer that creates these types,
+ * so that the device can choose the correct aperture.
+ * (Multiple AGP apertures, etc.)
+ * Most device drivers will let this point to the standard AGP implementation.
+ */
+
+#define DRM_BE_FLAG_NEEDS_FREE     0x00000001
+#define DRM_BE_FLAG_BOUND_CACHED   0x00000002
+
+struct drm_ttm_backend;
+struct drm_ttm_backend_func {
+	int (*needs_ub_cache_adjust) (struct drm_ttm_backend *backend);
+	int (*populate) (struct drm_ttm_backend *backend,
+			 unsigned long num_pages, struct page **pages,
+			 struct page *dummy_read_page);
+	void (*clear) (struct drm_ttm_backend *backend);
+	int (*bind) (struct drm_ttm_backend *backend,
+		     struct drm_bo_mem_reg *bo_mem);
+	int (*unbind) (struct drm_ttm_backend *backend);
+	void (*destroy) (struct drm_ttm_backend *backend);
+};
+
+
+struct drm_ttm_backend {
+	struct drm_device *dev;
+	uint32_t flags;
+	struct drm_ttm_backend_func *func;
+};
+
+struct drm_ttm {
+	struct page *dummy_read_page;
+	struct page **pages;
+	uint32_t page_flags;
+	unsigned long num_pages;
+	atomic_t vma_count;
+	struct drm_device *dev;
+	int destroy;
+	uint32_t mapping_offset;
+	struct drm_ttm_backend *be;
+	enum {
+		ttm_bound,
+		ttm_evicted,
+		ttm_unbound,
+		ttm_unpopulated,
+	} state;
+
+};
+
+extern struct drm_ttm *drm_ttm_create(struct drm_device *dev, unsigned long size,
+				      uint32_t page_flags,
+				      struct page *dummy_read_page);
+extern int drm_ttm_bind(struct drm_ttm *ttm, struct drm_bo_mem_reg *bo_mem);
+extern void drm_ttm_unbind(struct drm_ttm *ttm);
+extern void drm_ttm_evict(struct drm_ttm *ttm);
+extern void drm_ttm_fixup_caching(struct drm_ttm *ttm);
+extern struct page *drm_ttm_get_page(struct drm_ttm *ttm, int index);
+extern void drm_ttm_cache_flush(void);
+extern int drm_ttm_populate(struct drm_ttm *ttm);
+extern int drm_ttm_set_user(struct drm_ttm *ttm,
+			    struct task_struct *tsk,
+			    unsigned long start,
+			    unsigned long num_pages);
+
+/*
+ * Destroy a ttm. The user normally calls drmRmMap or a similar IOCTL to do
+ * this which calls this function iff there are no vmas referencing it anymore.
+ * Otherwise it is called when the last vma exits.
+ */
+
+extern int drm_ttm_destroy(struct drm_ttm *ttm);
+
+#define DRM_FLAG_MASKED(_old, _new, _mask) {\
+(_old) ^= (((_old) ^ (_new)) & (_mask)); \
+}
+
+#define DRM_TTM_MASK_FLAGS ((1 << PAGE_SHIFT) - 1)
+#define DRM_TTM_MASK_PFN (0xFFFFFFFFU - DRM_TTM_MASK_FLAGS)
+
+/*
+ * Page flags.
+ */
+
+/*
+ * This ttm should not be cached by the CPU
+ */
+#define DRM_TTM_PAGE_UNCACHED   (1 << 0)
+/*
+ * This flat is not used at this time; I don't know what the
+ * intent was
+ */
+#define DRM_TTM_PAGE_USED       (1 << 1)
+/*
+ * This flat is not used at this time; I don't know what the
+ * intent was
+ */
+#define DRM_TTM_PAGE_BOUND      (1 << 2)
+/*
+ * This flat is not used at this time; I don't know what the
+ * intent was
+ */
+#define DRM_TTM_PAGE_PRESENT    (1 << 3)
+/*
+ * The array of page pointers was allocated with vmalloc
+ * instead of drm_calloc.
+ */
+#define DRM_TTM_PAGE_VMALLOC    (1 << 4)
+/*
+ * This ttm is mapped from user space
+ */
+#define DRM_TTM_PAGE_USER       (1 << 5)
+/*
+ * This ttm will be written to by the GPU
+ */
+#define DRM_TTM_PAGE_WRITE	(1 << 6)
+/*
+ * This ttm was mapped to the GPU, and so the contents may have
+ * been modified
+ */
+#define DRM_TTM_PAGE_USER_DIRTY (1 << 7)
+/*
+ * This flag is not used at this time; I don't know what the
+ * intent was.
+ */
+#define DRM_TTM_PAGE_USER_DMA   (1 << 8)
+
+/***************************************************
+ * Buffer objects. (drm_bo.c, drm_bo_move.c)
+ */
+
+struct drm_bo_mem_reg {
+	struct drm_mm_node *mm_node;
+	unsigned long size;
+	unsigned long num_pages;
+	uint32_t page_alignment;
+	uint32_t mem_type;
+	/*
+	 * Current buffer status flags, indicating
+	 * where the buffer is located and which
+	 * access modes are in effect
+	 */
+	uint64_t flags;
+	/**
+	 * These are the flags proposed for
+	 * a validate operation. If the
+	 * validate succeeds, they'll get moved
+	 * into the flags field
+	 */
+	uint64_t proposed_flags;
+
+	uint32_t desired_tile_stride;
+	uint32_t hw_tile_stride;
+};
+
+enum drm_bo_type {
+	/*
+	 * drm_bo_type_device are 'normal' drm allocations,
+	 * pages are allocated from within the kernel automatically
+	 * and the objects can be mmap'd from the drm device. Each
+	 * drm_bo_type_device object has a unique name which can be
+	 * used by other processes to share access to the underlying
+	 * buffer.
+	 */
+	drm_bo_type_device,
+	/*
+	 * drm_bo_type_user are buffers of pages that already exist
+	 * in the process address space. They are more limited than
+	 * drm_bo_type_device buffers in that they must always
+	 * remain cached (as we assume the user pages are mapped cached),
+	 * and they are not sharable to other processes through DRM
+	 * (although, regular shared memory should still work fine).
+	 */
+	drm_bo_type_user,
+	/*
+	 * drm_bo_type_kernel are buffers that exist solely for use
+	 * within the kernel. The pages cannot be mapped into the
+	 * process. One obvious use would be for the ring
+	 * buffer where user access would not (ideally) be required.
+	 */
+	drm_bo_type_kernel,
+};
+
+struct drm_buffer_object {
+	struct drm_device *dev;
+	struct drm_user_object base;
+
+	/*
+	 * If there is a possibility that the usage variable is zero,
+	 * then dev->struct_mutext should be locked before incrementing it.
+	 */
+
+	atomic_t usage;
+	unsigned long buffer_start;
+	enum drm_bo_type type;
+	unsigned long offset;
+	atomic_t mapped;
+	struct drm_bo_mem_reg mem;
+
+	struct list_head lru;
+	struct list_head ddestroy;
+
+	uint32_t fence_type;
+	uint32_t fence_class;
+	uint32_t new_fence_type;
+	uint32_t new_fence_class;
+	struct drm_fence_object *fence;
+	uint32_t priv_flags;
+	wait_queue_head_t event_queue;
+	struct mutex mutex;
+	unsigned long num_pages;
+
+	/* For pinned buffers */
+	struct drm_mm_node *pinned_node;
+	uint32_t pinned_mem_type;
+	struct list_head pinned_lru;
+
+	/* For vm */
+	struct drm_ttm *ttm;
+	struct drm_map_list map_list;
+	uint32_t memory_type;
+	unsigned long bus_offset;
+	uint32_t vm_flags;
+	void *iomap;
+
+
+};
+
+#define _DRM_BO_FLAG_UNFENCED 0x00000001
+#define _DRM_BO_FLAG_EVICTED  0x00000002
+
+struct drm_mem_type_manager {
+	int has_type;
+	int use_type;
+	int kern_init_type;
+	struct drm_mm manager;
+	struct list_head lru;
+	struct list_head pinned;
+	uint32_t flags;
+	uint32_t drm_bus_maptype;
+	unsigned long gpu_offset;
+	unsigned long io_offset;
+	unsigned long io_size;
+	void *io_addr;
+	uint64_t size; /* size of managed area for reporting to userspace */
+};
+
+struct drm_bo_lock {
+	struct drm_user_object base;
+	wait_queue_head_t queue;
+	atomic_t write_lock_pending;
+	atomic_t readers;
+};
+
+#define _DRM_FLAG_MEMTYPE_FIXED     0x00000001	/* Fixed (on-card) PCI memory */
+#define _DRM_FLAG_MEMTYPE_MAPPABLE  0x00000002	/* Memory mappable */
+#define _DRM_FLAG_MEMTYPE_CACHED    0x00000004	/* Cached binding */
+#define _DRM_FLAG_NEEDS_IOREMAP     0x00000008	/* Fixed memory needs ioremap
+						   before kernel access. */
+#define _DRM_FLAG_MEMTYPE_CMA       0x00000010	/* Can't map aperture */
+#define _DRM_FLAG_MEMTYPE_CSELECT   0x00000020	/* Select caching */
+
+struct drm_buffer_manager {
+	struct drm_bo_lock bm_lock;
+	struct mutex evict_mutex;
+	int nice_mode;
+	int initialized;
+	struct drm_file *last_to_validate;
+	struct drm_mem_type_manager man[DRM_BO_MEM_TYPES];
+	struct list_head unfenced;
+	struct list_head ddestroy;
+	struct delayed_work wq;
+	uint32_t fence_type;
+	unsigned long cur_pages;
+	atomic_t count;
+	struct page *dummy_read_page;
+};
+
+struct drm_bo_driver {
+	const uint32_t *mem_type_prio;
+	const uint32_t *mem_busy_prio;
+	uint32_t num_mem_type_prio;
+	uint32_t num_mem_busy_prio;
+	struct drm_ttm_backend *(*create_ttm_backend_entry)
+	 (struct drm_device *dev);
+	int (*fence_type) (struct drm_buffer_object *bo, uint32_t *fclass,
+			   uint32_t *type);
+	int (*invalidate_caches) (struct drm_device *dev, uint64_t flags);
+	int (*init_mem_type) (struct drm_device *dev, uint32_t type,
+			      struct drm_mem_type_manager *man);
+	/*
+	 * evict_flags:
+	 *
+	 * @bo: the buffer object to be evicted
+	 *
+	 * Return the bo flags for a buffer which is not mapped to the hardware.
+	 * These will be placed in proposed_flags so that when the move is
+	 * finished, they'll end up in bo->mem.flags
+	 */
+	uint64_t(*evict_flags) (struct drm_buffer_object *bo);
+	/*
+	 * move:
+	 *
+	 * @bo: the buffer to move
+	 *
+	 * @evict: whether this motion is evicting the buffer from
+	 * the graphics address space
+	 *
+	 * @no_wait: whether this should give up and return -EBUSY
+	 * if this move would require sleeping
+	 *
+	 * @new_mem: the new memory region receiving the buffer
+	 *
+	 * Move a buffer between two memory regions.
+	 */
+	int (*move) (struct drm_buffer_object *bo,
+		     int evict, int no_wait, struct drm_bo_mem_reg *new_mem);
+	/*
+	 * ttm_cache_flush
+	 */
+	void (*ttm_cache_flush)(struct drm_ttm *ttm);
+
+	/*
+	 * command_stream_barrier
+	 *
+	 * @dev: The drm device.
+	 *
+	 * @bo: The buffer object to validate.
+	 *
+	 * @new_fence_class: The new fence class for the buffer object.
+	 *
+	 * @new_fence_type: The new fence type for the buffer object.
+	 *
+	 * @no_wait: whether this should give up and return -EBUSY
+	 * if this operation would require sleeping
+	 *
+	 * Insert a command stream barrier that makes sure that the
+	 * buffer is idle once the commands associated with the
+	 * current validation are starting to execute. If an error
+	 * condition is returned, or the function pointer is NULL,
+	 * the drm core will force buffer idle
+	 * during validation.
+	 */
+
+	int (*command_stream_barrier) (struct drm_buffer_object *bo,
+				       uint32_t new_fence_class,
+				       uint32_t new_fence_type,
+				       int no_wait);
+};
+
+/*
+ * buffer objects (drm_bo.c)
+ */
+
+extern int drm_bo_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_destroy_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_map_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_unmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_wait_idle_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_setstatus_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_init_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_takedown_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_lock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_unlock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_version_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_driver_finish(struct drm_device *dev);
+extern int drm_bo_driver_init(struct drm_device *dev);
+extern int drm_bo_pci_offset(struct drm_device *dev,
+			     struct drm_bo_mem_reg *mem,
+			     unsigned long *bus_base,
+			     unsigned long *bus_offset,
+			     unsigned long *bus_size);
+extern int drm_mem_reg_is_pci(struct drm_device *dev, struct drm_bo_mem_reg *mem);
+
+extern void drm_bo_usage_deref_locked(struct drm_buffer_object **bo);
+extern void drm_bo_usage_deref_unlocked(struct drm_buffer_object **bo);
+extern void drm_putback_buffer_objects(struct drm_device *dev);
+extern int drm_fence_buffer_objects(struct drm_device *dev,
+				    struct list_head *list,
+				    uint32_t fence_flags,
+				    struct drm_fence_object *fence,
+				    struct drm_fence_object **used_fence);
+extern void drm_bo_add_to_lru(struct drm_buffer_object *bo);
+extern int drm_buffer_object_create(struct drm_device *dev, unsigned long size,
+				    enum drm_bo_type type, uint64_t flags,
+				    uint32_t hint, uint32_t page_alignment,
+				    unsigned long buffer_start,
+				    struct drm_buffer_object **bo);
+extern int drm_bo_wait(struct drm_buffer_object *bo, int lazy, int ignore_signals,
+		       int no_wait);
+extern int drm_bo_mem_space(struct drm_buffer_object *bo,
+			    struct drm_bo_mem_reg *mem, int no_wait);
+extern int drm_bo_move_buffer(struct drm_buffer_object *bo,
+			      uint64_t new_mem_flags,
+			      int no_wait, int move_unfenced);
+extern int drm_bo_clean_mm(struct drm_device *dev, unsigned mem_type, int kern_clean);
+extern int drm_bo_init_mm(struct drm_device *dev, unsigned type,
+			  unsigned long p_offset, unsigned long p_size,
+			  int kern_init);
+extern int drm_bo_handle_validate(struct drm_file *file_priv, uint32_t handle,
+				  uint64_t flags, uint64_t mask, uint32_t hint,
+				  uint32_t fence_class, int use_old_fence_class,
+				  struct drm_bo_info_rep *rep,
+				  struct drm_buffer_object **bo_rep);
+extern struct drm_buffer_object *drm_lookup_buffer_object(struct drm_file *file_priv,
+							  uint32_t handle,
+							  int check_owner);
+extern int drm_bo_do_validate(struct drm_buffer_object *bo,
+			      uint64_t flags, uint64_t mask, uint32_t hint,
+			      uint32_t fence_class,
+			      struct drm_bo_info_rep *rep);
+
+/*
+ * Buffer object memory move- and map helpers.
+ * drm_bo_move.c
+ */
+
+extern int drm_bo_move_ttm(struct drm_buffer_object *bo,
+			   int evict, int no_wait,
+			   struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_move_memcpy(struct drm_buffer_object *bo,
+			      int evict,
+			      int no_wait, struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_move_accel_cleanup(struct drm_buffer_object *bo,
+				     int evict, int no_wait,
+				     uint32_t fence_class, uint32_t fence_type,
+				     uint32_t fence_flags,
+				     struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_same_page(unsigned long offset, unsigned long offset2);
+extern unsigned long drm_bo_offset_end(unsigned long offset,
+				       unsigned long end);
+
+struct drm_bo_kmap_obj {
+	void *virtual;
+	struct page *page;
+	enum {
+		bo_map_iomap,
+		bo_map_vmap,
+		bo_map_kmap,
+		bo_map_premapped,
+	} bo_kmap_type;
+};
+
+static inline void *drm_bmo_virtual(struct drm_bo_kmap_obj *map, int *is_iomem)
+{
+	*is_iomem = (map->bo_kmap_type == bo_map_iomap ||
+		     map->bo_kmap_type == bo_map_premapped);
+	return map->virtual;
+}
+extern void drm_bo_kunmap(struct drm_bo_kmap_obj *map);
+extern int drm_bo_kmap(struct drm_buffer_object *bo, unsigned long start_page,
+		       unsigned long num_pages, struct drm_bo_kmap_obj *map);
+extern int drm_bo_pfn_prot(struct drm_buffer_object *bo,
+			   unsigned long dst_offset,
+			   unsigned long *pfn,
+			   pgprot_t *prot);
+
+extern int drm_mem_reg_ioremap(struct drm_device *dev, struct drm_bo_mem_reg * mem,
+			       void **virtual);
+extern void drm_mem_reg_iounmap(struct drm_device *dev, struct drm_bo_mem_reg * mem,
+				void *virtual);
+/*
+ * drm_bo_lock.c
+ * Simple replacement for the hardware lock on buffer manager init and clean.
+ */
+
+
+extern void drm_bo_init_lock(struct drm_bo_lock *lock);
+extern void drm_bo_read_unlock(struct drm_bo_lock *lock);
+extern int drm_bo_read_lock(struct drm_bo_lock *lock);
+extern int drm_bo_write_lock(struct drm_bo_lock *lock,
+			     struct drm_file *file_priv);
+
+extern int drm_bo_write_unlock(struct drm_bo_lock *lock,
+			       struct drm_file *file_priv);
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define DRM_ASSERT_LOCKED(_mutex)					\
+	BUG_ON(!mutex_is_locked(_mutex) ||				\
+	       ((_mutex)->owner != current_thread_info()))
+#else
+#define DRM_ASSERT_LOCKED(_mutex)
+#endif
+#endif
diff --git a/drivers/char/drm/drm_pciids.h b/drivers/char/drm/drm_pciids.h
index 135bd19..8c19f4c 100644
--- a/drivers/char/drm/drm_pciids.h
+++ b/drivers/char/drm/drm_pciids.h
@@ -413,3 +413,265 @@
 	{0x8086, 0x2e12, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0}, \
 	{0x8086, 0x2e22, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0}, \
 	{0, 0, 0}
+
+#define nouveau_PCI_IDS \
+	{0x10de, 0x0008, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x10de, 0x0009, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x10de, 0x0010, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x10de, 0x0020, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x0028, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x0029, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x002f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x0040, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0041, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0042, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0043, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0044, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0045, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0046, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0047, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0048, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0049, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x004d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x004e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0090, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0091, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0092, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0093, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0095, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0098, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0099, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x009d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00a0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x10de, 0x00c0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00c1, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00c2, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00c3, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00c8, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00c9, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00cc, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00cd, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00ce, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f1, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f2, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f3, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f4, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f5, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f6, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f8, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00f9, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x00fa, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x00fb, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x00fc, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x00fd, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x00fe, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x00ff, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0100, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_10}, \
+	{0x10de, 0x0101, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_10}, \
+	{0x10de, 0x0103, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_10}, \
+	{0x10de, 0x0110, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_11}, \
+	{0x10de, 0x0111, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_11}, \
+	{0x10de, 0x0112, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_11}, \
+	{0x10de, 0x0113, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_11}, \
+	{0x10de, 0x0140, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0141, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0142, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0143, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0144, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0145, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0146, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0147, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0148, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0149, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x014a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x014c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x014d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x014e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x014f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0150, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_15}, \
+	{0x10de, 0x0151, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_15}, \
+	{0x10de, 0x0152, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_15}, \
+	{0x10de, 0x0153, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_15}, \
+	{0x10de, 0x0160, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0161, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0162, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0163, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0164, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0165, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0166, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0167, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0168, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0169, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0170, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0171, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0172, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0173, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0174, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0175, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0176, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0177, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0178, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0179, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x017a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x017b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x017c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x017d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0181, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0182, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0183, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0185, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0186, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0187, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0188, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x018a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x018b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x018c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x018d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17}, \
+	{0x10de, 0x0191, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0193, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0194, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x019d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x019e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x01a0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_11|NV_NFORCE}, \
+	{0x10de, 0x01d1, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01d3, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01d6, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01d7, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01d8, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01d9, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01da, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01db, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01dc, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01dd, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01de, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01df, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x01f0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_17|NV_NFORCE2}, \
+	{0x10de, 0x0200, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_20}, \
+	{0x10de, 0x0201, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_20}, \
+	{0x10de, 0x0202, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_20}, \
+	{0x10de, 0x0203, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_20}, \
+	{0x10de, 0x0211, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0212, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0215, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0218, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0221, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0222, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0240, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0241, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0242, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0244, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0247, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0250, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0251, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0252, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0253, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0258, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0259, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x025b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0280, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0281, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0282, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0286, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0288, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0289, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x028c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_25}, \
+	{0x10de, 0x0290, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0291, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0292, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0298, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0299, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x029f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x02a0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_20}, \
+	{0x10de, 0x02e1, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0300, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0301, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0302, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0308, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0309, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0311, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0312, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0313, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0314, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0316, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0317, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x031a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x031b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x031d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x031e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x031f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0320, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0321, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0322, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0323, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0324, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0325, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0326, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0327, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0328, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0329, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x032a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x032b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x032c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x032d, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x032f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_34}, \
+	{0x10de, 0x0330, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0331, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0332, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0333, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0334, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0338, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x033f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0341, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0342, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0343, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0344, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0345, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0347, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0348, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0349, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x034b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x034c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x034e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x034f, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_30}, \
+	{0x10de, 0x0391, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0392, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0393, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0394, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0395, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0397, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0398, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x0399, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x039a, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x039b, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x039c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x039e, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_40}, \
+	{0x10de, 0x03d0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x03d1, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x03d2, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x03d5, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_44}, \
+	{0x10de, 0x0400, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0402, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0421, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0422, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x10de, 0x0423, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_50}, \
+	{0x12d2, 0x0008, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x12d2, 0x0009, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x12d2, 0x0018, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x12d2, 0x0019, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_03}, \
+	{0x12d2, 0x0020, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x12d2, 0x0028, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x12d2, 0x0029, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x12d2, 0x002c, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0x12d2, 0x00a0, PCI_ANY_ID, PCI_ANY_ID, 0, 0, NV_04}, \
+	{0, 0, 0}
diff --git a/drivers/char/drm/drm_proc.c b/drivers/char/drm/drm_proc.c
index 93b1e04..2995198 100644
--- a/drivers/char/drm/drm_proc.c
+++ b/drivers/char/drm/drm_proc.c
@@ -49,6 +49,8 @@ static int drm_queues_info(char *buf, char **start, off_t offset,
 			   int request, int *eof, void *data);
 static int drm_bufs_info(char *buf, char **start, off_t offset,
 			 int request, int *eof, void *data);
+static int drm_objects_info(char *buf, char **start, off_t offset,
+			 int request, int *eof, void *data);
 #if DRM_DEBUG_CODE
 static int drm_vma_info(char *buf, char **start, off_t offset,
 			int request, int *eof, void *data);
@@ -67,6 +69,7 @@ static struct drm_proc_list {
 	{"clients", drm_clients_info},
 	{"queues", drm_queues_info},
 	{"bufs", drm_bufs_info},
+	{"objects", drm_objects_info},
 #if DRM_DEBUG_CODE
 	{"vma", drm_vma_info},
 #endif
@@ -117,7 +120,6 @@ int drm_proc_init(struct drm_minor *minor, int minor_id,
 		ent->read_proc = drm_proc_list[i].f;
 		ent->data = minor;
 	}
-
 	return 0;
 }
 
@@ -164,6 +166,7 @@ static int drm_name_info(char *buf, char **start, off_t offset, int request,
 			 int *eof, void *data)
 {
 	struct drm_minor *minor = (struct drm_minor *) data;
+	struct drm_master *master = minor->master;
 	struct drm_device *dev = minor->dev;
 	int len = 0;
 
@@ -175,10 +178,10 @@ static int drm_name_info(char *buf, char **start, off_t offset, int request,
 	*start = &buf[offset];
 	*eof = 0;
 
-	if (dev->unique) {
+	if (master->unique) {
 		DRM_PROC_PRINT("%s %s %s\n",
 			       dev->driver->pci_driver.name,
-			       pci_name(dev->pdev), dev->unique);
+			       pci_name(dev->pdev), master->unique);
 	} else {
 		DRM_PROC_PRINT("%s %s\n", dev->driver->pci_driver.name,
 			       pci_name(dev->pdev));
@@ -213,8 +216,8 @@ static int drm__vm_info(char *buf, char **start, off_t offset, int request,
 	struct drm_map_list *r_list;
 
 	/* Hardcoded from _DRM_FRAME_BUFFER,
-	   _DRM_REGISTERS, _DRM_SHM, _DRM_AGP, and
-	   _DRM_SCATTER_GATHER and _DRM_CONSISTENT */
+	   _DRM_REGISTERS, _DRM_SHM, _DRM_AGP,
+	   _DRM_SCATTER_GATHER, and _DRM_CONSISTENT. */
 	const char *types[] = { "FB", "REG", "SHM", "AGP", "SG", "PCI" };
 	const char *type;
 	int i;
@@ -243,6 +246,7 @@ static int drm__vm_info(char *buf, char **start, off_t offset, int request,
 			       map->offset,
 			       map->size, type, map->flags,
 			       (unsigned long) r_list->user_token);
+
 		if (map->mtrr < 0) {
 			DRM_PROC_PRINT("none\n");
 		} else {
@@ -423,6 +427,95 @@ static int drm_bufs_info(char *buf, char **start, off_t offset, int request,
 }
 
 /**
+ * Called when "/proc/dri/.../objects" is read.
+ *
+ * \param buf output buffer.
+ * \param start start of output data.
+ * \param offset requested start offset.
+ * \param request requested number of bytes.
+ * \param eof whether there is no more data to return.
+ * \param data private data.
+ * \return number of written bytes.
+ */
+static int drm__objects_info(char *buf, char **start, off_t offset, int request,
+			  int *eof, void *data)
+{
+	struct drm_minor *minor = (struct drm_minor *) data; 
+	struct drm_device *dev = minor->dev;
+	int len = 0;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_fence_manager *fm = &dev->fm;
+	uint64_t used_mem;
+	uint64_t low_mem;
+	uint64_t high_mem;
+
+
+	if (offset > DRM_PROC_LIMIT) {
+		*eof = 1;
+		return 0;
+	}
+
+	*start = &buf[offset];
+	*eof = 0;
+
+	DRM_PROC_PRINT("Object accounting:\n\n");
+	if (fm->initialized) {
+		DRM_PROC_PRINT("Number of active fence objects: %d.\n",
+			       atomic_read(&fm->count));
+	} else {
+		DRM_PROC_PRINT("Fence objects are not supported by this driver\n");
+	}
+
+	if (bm->initialized) {
+		DRM_PROC_PRINT("Number of active buffer objects: %d.\n\n",
+			       atomic_read(&bm->count));
+	}
+	DRM_PROC_PRINT("Memory accounting:\n\n");
+	if (bm->initialized) {
+		DRM_PROC_PRINT("Number of locked GATT pages: %lu.\n", bm->cur_pages);
+	} else {
+		DRM_PROC_PRINT("Buffer objects are not supported by this driver.\n");
+	}
+
+	drm_query_memctl(&used_mem, &low_mem, &high_mem);
+
+	if (used_mem > 16*PAGE_SIZE) {
+		DRM_PROC_PRINT("Used object memory is %lu pages.\n",
+			       (unsigned long) (used_mem >> PAGE_SHIFT));
+	} else {
+		DRM_PROC_PRINT("Used object memory is %lu bytes.\n",
+			       (unsigned long) used_mem);
+	}
+	DRM_PROC_PRINT("Soft object memory usage threshold is %lu pages.\n",
+		       (unsigned long) (low_mem >> PAGE_SHIFT));
+	DRM_PROC_PRINT("Hard object memory usage threshold is %lu pages.\n",
+		       (unsigned long) (high_mem >> PAGE_SHIFT));
+
+	DRM_PROC_PRINT("\n");
+
+	if (len > request + offset)
+		return request;
+	*eof = 1;
+	return len - offset;
+}
+
+/**
+ * Simply calls _objects_info() while holding the drm_device::struct_mutex lock.
+ */
+static int drm_objects_info(char *buf, char **start, off_t offset, int request,
+			 int *eof, void *data)
+{
+	struct drm_minor *minor = (struct drm_minor *) data; 
+	struct drm_device *dev = minor->dev;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm__objects_info(buf, start, offset, request, eof, data);
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/**
  * Called when "/proc/dri/.../clients" is read.
  *
  * \param buf output buffer.
diff --git a/drivers/char/drm/drm_scatter.c b/drivers/char/drm/drm_scatter.c
index b2b0f3d..3b31040 100644
--- a/drivers/char/drm/drm_scatter.c
+++ b/drivers/char/drm/drm_scatter.c
@@ -188,7 +188,7 @@ int drm_sg_alloc(struct drm_device *dev, struct drm_scatter_gather * request)
 
 	return 0;
 
-      failed:
+failed:
 	drm_sg_cleanup(entry);
 	return -ENOMEM;
 }
diff --git a/drivers/char/drm/drm_stub.c b/drivers/char/drm/drm_stub.c
index c2f584f..9b6ae82 100644
--- a/drivers/char/drm/drm_stub.c
+++ b/drivers/char/drm/drm_stub.c
@@ -36,7 +36,7 @@
 #include "drmP.h"
 #include "drm_core.h"
 
-unsigned int drm_debug = 0;	/* 1 to enable debug output */
+unsigned int drm_debug = 0;		/* 1 to enable debug output */
 EXPORT_SYMBOL(drm_debug);
 
 MODULE_AUTHOR(CORE_AUTHOR);
@@ -57,6 +57,14 @@ static int drm_minor_get_id(struct drm_device *dev, int type)
 	int ret;
 	int base = 0, limit = 63;
 
+	if (type == DRM_MINOR_CONTROL) {
+		base += 64;
+		limit = base + 127;
+	} else if (type == DRM_MINOR_RENDER) {
+		base += 128;
+		limit = base + 255;
+	}	
+
 again:
 	if (idr_pre_get(&drm_minors_idr, GFP_KERNEL) == 0) {
 		DRM_ERROR("Out of memory expanding drawable idr\n");
@@ -79,24 +87,88 @@ again:
 	return new_id;
 }
 
+struct drm_master *drm_get_master(struct drm_minor *minor)
+{
+	struct drm_master *master;
+
+	master = drm_calloc(1, sizeof(*master), DRM_MEM_DRIVER);
+	if (!master)
+		return NULL;
+
+//	INIT_LIST_HEAD(&master->filelist);
+	spin_lock_init(&master->lock.spinlock);
+	init_waitqueue_head(&master->lock.lock_queue);
+	drm_ht_create(&master->magiclist, DRM_MAGIC_HASH_ORDER);
+	INIT_LIST_HEAD(&master->magicfree);
+	master->minor = minor;
+
+	list_add_tail(&master->head, &minor->master_list);
+
+	return master;
+}
+
+void drm_put_master(struct drm_master *master)
+{
+	struct drm_magic_entry *pt, *next;
+	struct drm_device *dev = master->minor->dev;
+	struct drm_map_list *r_list, *list_t;
+
+	list_del(&master->head);
+
+	if (dev->driver->master_destroy)
+		dev->driver->master_destroy(dev, master);
+
+	if (master->unique) {
+		drm_free(master->unique, strlen(master->unique) + 1, DRM_MEM_DRIVER);
+		master->unique = NULL;
+		master->unique_len = 0;
+	}
+
+	list_for_each_entry_safe(pt, next, &master->magicfree, head) {
+		list_del(&pt->head);
+		drm_ht_remove_item(&master->magiclist, &pt->hash_item);
+		drm_free(pt, sizeof(*pt), DRM_MEM_MAGIC);
+	}
+
+	drm_ht_remove(&master->magiclist);
+
+	if (master->lock.hw_lock) {
+		if (dev->sigdata.lock == master->lock.hw_lock)
+			dev->sigdata.lock = NULL;
+		master->lock.hw_lock = NULL;	/* SHM removed */
+		master->lock.file_priv = NULL;
+		wake_up_interruptible(&master->lock.lock_queue);
+	}
+
+	list_for_each_entry_safe(r_list, list_t, &dev->maplist, head) {
+		if (!(r_list->map->flags & _DRM_DRIVER) && (r_list->master == master)) {
+			drm_rmmap_locked(dev, r_list->map);
+			r_list = NULL;
+		}
+	}
+
+	drm_free(master, sizeof(*master), DRM_MEM_DRIVER);
+}
+
 static int drm_fill_in_dev(struct drm_device * dev, struct pci_dev *pdev,
 			   const struct pci_device_id *ent,
 			   struct drm_driver *driver)
 {
 	int retcode;
 
-	INIT_LIST_HEAD(&dev->filelist);
 	INIT_LIST_HEAD(&dev->ctxlist);
 	INIT_LIST_HEAD(&dev->vmalist);
 	INIT_LIST_HEAD(&dev->maplist);
+	INIT_LIST_HEAD(&dev->filelist);
 
 	spin_lock_init(&dev->count_lock);
 	spin_lock_init(&dev->drw_lock);
 	spin_lock_init(&dev->tasklet_lock);
-	spin_lock_init(&dev->lock.spinlock);
+//	spin_lock_init(&dev->lock.spinlock);
 	init_timer(&dev->timer);
 	mutex_init(&dev->struct_mutex);
 	mutex_init(&dev->ctxlist_mutex);
+	mutex_init(&dev->bm.evict_mutex);
 
 	idr_init(&dev->drw_idr);
 
@@ -108,8 +180,21 @@ static int drm_fill_in_dev(struct drm_device * dev, struct pci_dev *pdev,
 	dev->hose = pdev->sysdata;
 #endif
 	dev->irq = pdev->irq;
+	dev->irq_enabled = 0;
+
+	if (drm_ht_create(&dev->map_hash, DRM_MAP_HASH_ORDER)) {
+		return -ENOMEM;
+	}
+
+	if (drm_mm_init(&dev->offset_manager, DRM_FILE_PAGE_OFFSET_START,
+			DRM_FILE_PAGE_OFFSET_SIZE)) {
+		drm_ht_remove(&dev->map_hash);
+		return -ENOMEM;
+	}
 
-	if (drm_ht_create(&dev->map_hash, 12)) {
+	if (drm_ht_create(&dev->object_hash, DRM_OBJECT_HASH_ORDER)) {
+		drm_ht_remove(&dev->map_hash);
+		drm_mm_takedown(&dev->offset_manager);
 		return -ENOMEM;
 	}
 
@@ -142,19 +227,16 @@ static int drm_fill_in_dev(struct drm_device * dev, struct pci_dev *pdev,
 		}
 	}
 
-	if (dev->driver->load)
-		if ((retcode = dev->driver->load(dev, ent->driver_data)))
-			goto error_out_unreg;
-
 	retcode = drm_ctxbitmap_init(dev);
 	if (retcode) {
 		DRM_ERROR("Cannot allocate memory for context bitmap.\n");
 		goto error_out_unreg;
 	}
 
+	drm_fence_manager_init(dev);
 	return 0;
 
-      error_out_unreg:
+error_out_unreg:
 	drm_lastclose(dev);
 	return retcode;
 }
@@ -193,9 +275,10 @@ static int drm_get_minor(struct drm_device *dev, struct drm_minor **minor, int t
 	new_minor->device = MKDEV(DRM_MAJOR, minor_id);
 	new_minor->dev = dev;
 	new_minor->index = minor_id;
+	INIT_LIST_HEAD(&new_minor->master_list);
 
 	idr_replace(&drm_minors_idr, new_minor, minor_id);
-
+	
 	if (type == DRM_MINOR_LEGACY) {
 		ret = drm_proc_init(new_minor, minor_id, drm_proc_root);
 		if (ret) {
@@ -253,28 +336,45 @@ int drm_get_dev(struct pci_dev *pdev, const struct pci_device_id *ent,
 
 	ret = pci_enable_device(pdev);
 	if (ret)
-		goto err_g1;
-
+		goto err_g2;
 	pci_set_master(pdev);
+
 	if ((ret = drm_fill_in_dev(dev, pdev, ent, driver))) {
-		printk(KERN_ERR "DRM: Fill_in_dev failed.\n");
-		goto err_g2;
+		printk(KERN_ERR "DRM: fill_in_dev failed\n");
+		goto err_g3;
 	}
+
+	/* only add the control node on a modesetting platform */
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		if ((ret = drm_get_minor(dev, &dev->control, DRM_MINOR_CONTROL)))
+			goto err_g3;
+
 	if ((ret = drm_get_minor(dev, &dev->primary, DRM_MINOR_LEGACY)))
-		goto err_g2;
+		goto err_g4;
+
+	if (dev->driver->load)
+		if ((ret = dev->driver->load(dev, ent->driver_data)))
+			goto err_g5;
 
 	DRM_INFO("Initialized %s %d.%d.%d %s on minor %d\n",
 		 driver->name, driver->major, driver->minor, driver->patchlevel,
 		 driver->date, dev->primary->index);
 
 	return 0;
-
+err_g5:
+	drm_put_minor(&dev->primary);
+err_g4:
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		drm_put_minor(&dev->control);
+err_g3:
 err_g2:
-	pci_disable_device(pdev);
 err_g1:
 	drm_free(dev, sizeof(*dev), DRM_MEM_STUB);
+	printk(KERN_ERR "DRM: drm_get_dev failed.\n");
 	return ret;
 }
+EXPORT_SYMBOL(drm_get_dev);
+
 
 /**
  * Put a device minor number.
@@ -290,11 +390,6 @@ int drm_put_dev(struct drm_device * dev)
 {
 	DRM_DEBUG("release primary %s\n", dev->driver->pci_driver.name);
 
-	if (dev->unique) {
-		drm_free(dev->unique, strlen(dev->unique) + 1, DRM_MEM_DRIVER);
-		dev->unique = NULL;
-		dev->unique_len = 0;
-	}
 	if (dev->devname) {
 		drm_free(dev->devname, strlen(dev->devname) + 1,
 			 DRM_MEM_DRIVER);
diff --git a/drivers/char/drm/drm_sysfs.c b/drivers/char/drm/drm_sysfs.c
index af211a0..6f430f7 100644
--- a/drivers/char/drm/drm_sysfs.c
+++ b/drivers/char/drm/drm_sysfs.c
@@ -34,8 +34,11 @@ static int drm_sysfs_suspend(struct device *dev, pm_message_t state)
 	struct drm_minor *drm_minor = to_drm_minor(dev);
 	struct drm_device *drm_dev = drm_minor->dev;
 
-	if (drm_dev->driver->suspend)
-		return drm_dev->driver->suspend(drm_dev, state);
+	printk(KERN_ERR "%s\n", __FUNCTION__);
+
+	if (drm_minor->type == DRM_MINOR_CONTROL)
+		if (drm_dev->driver->suspend)
+			return drm_dev->driver->suspend(drm_dev, state);
 
 	return 0;
 }
@@ -52,8 +55,9 @@ static int drm_sysfs_resume(struct device *dev)
 	struct drm_minor *drm_minor = to_drm_minor(dev);
 	struct drm_device *drm_dev = drm_minor->dev;
 
-	if (drm_dev->driver->resume)
-		return drm_dev->driver->resume(drm_dev);
+	if (drm_minor->type == DRM_MINOR_CONTROL)
+		if (drm_dev->driver->resume)
+			return drm_dev->driver->resume(drm_dev);
 
 	return 0;
 }
@@ -163,8 +167,14 @@ int drm_sysfs_device_add(struct drm_minor *minor)
 	minor->kdev.class = drm_class;
 	minor->kdev.release = drm_sysfs_device_release;
 	minor->kdev.devt = minor->device;
-	minor_str = "card%d";
 
+	if (minor->type == DRM_MINOR_CONTROL)
+		minor_str = "controlD%d";
+	else if (minor->type == DRM_MINOR_RENDER)
+		minor_str = "renderD%d";
+	else
+		minor_str = "card%d";
+	
 	snprintf(minor->kdev.bus_id, BUS_ID_SIZE, minor_str, minor->index);
 
 	err = device_register(&minor->kdev);
diff --git a/drivers/char/drm/drm_ttm.c b/drivers/char/drm/drm_ttm.c
new file mode 100644
index 0000000..ab4e8e5
--- /dev/null
+++ b/drivers/char/drm/drm_ttm.c
@@ -0,0 +1,464 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+static void drm_ttm_ipi_handler(void *null)
+{
+	flush_agp_cache();
+}
+
+void drm_ttm_cache_flush(void)
+{
+	if (on_each_cpu(drm_ttm_ipi_handler, NULL, 1, 1) != 0)
+		DRM_ERROR("Timed out waiting for drm cache flush.\n");
+}
+EXPORT_SYMBOL(drm_ttm_cache_flush);
+
+/*
+ * Use kmalloc if possible. Otherwise fall back to vmalloc.
+ */
+
+static void drm_ttm_alloc_pages(struct drm_ttm *ttm)
+{
+	unsigned long size = ttm->num_pages * sizeof(*ttm->pages);
+	ttm->pages = NULL;
+
+	if (drm_alloc_memctl(size))
+		return;
+
+	if (size <= PAGE_SIZE)
+		ttm->pages = drm_calloc(1, size, DRM_MEM_TTM);
+
+	if (!ttm->pages) {
+		ttm->pages = vmalloc_user(size);
+		if (ttm->pages)
+			ttm->page_flags |= DRM_TTM_PAGE_VMALLOC;
+	}
+	if (!ttm->pages)
+		drm_free_memctl(size);
+}
+
+static void drm_ttm_free_pages(struct drm_ttm *ttm)
+{
+	unsigned long size = ttm->num_pages * sizeof(*ttm->pages);
+
+	if (ttm->page_flags & DRM_TTM_PAGE_VMALLOC) {
+		vfree(ttm->pages);
+		ttm->page_flags &= ~DRM_TTM_PAGE_VMALLOC;
+	} else {
+		drm_free(ttm->pages, size, DRM_MEM_TTM);
+	}
+	drm_free_memctl(size);
+	ttm->pages = NULL;
+}
+
+static struct page *drm_ttm_alloc_page(void)
+{
+	struct page *page;
+
+	if (drm_alloc_memctl(PAGE_SIZE))
+		return NULL;
+
+	page = alloc_page(GFP_KERNEL | __GFP_ZERO | GFP_DMA32);
+	if (!page) {
+		drm_free_memctl(PAGE_SIZE);
+		return NULL;
+	}
+	return page;
+}
+
+/*
+ * Change caching policy for the linear kernel map
+ * for range of pages in a ttm.
+ */
+
+static int drm_ttm_set_caching(struct drm_ttm *ttm, int noncached)
+{
+	int i;
+	struct page **cur_page;
+
+	if ((ttm->page_flags & DRM_TTM_PAGE_UNCACHED) == noncached)
+		return 0;
+
+	if (noncached)
+		drm_ttm_cache_flush();
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		cur_page = ttm->pages + i;
+		if (*cur_page) {
+			if (!PageHighMem(*cur_page)) {
+				if (noncached) {
+					map_page_into_agp(*cur_page);
+				} else {
+					unmap_page_from_agp(*cur_page);
+				}
+			}
+		}
+	}
+
+	DRM_FLAG_MASKED(ttm->page_flags, noncached, DRM_TTM_PAGE_UNCACHED);
+
+	return 0;
+}
+
+
+static void drm_ttm_free_user_pages(struct drm_ttm *ttm)
+{
+	int write;
+	int dirty;
+	struct page *page;
+	int i;
+
+	BUG_ON(!(ttm->page_flags & DRM_TTM_PAGE_USER));
+	write = ((ttm->page_flags & DRM_TTM_PAGE_WRITE) != 0);
+	dirty = ((ttm->page_flags & DRM_TTM_PAGE_USER_DIRTY) != 0);
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		page = ttm->pages[i];
+		if (page == NULL)
+			continue;
+
+		if (page == ttm->dummy_read_page) {
+			BUG_ON(write);
+			continue;
+		}
+
+		if (write && dirty && !PageReserved(page))
+			set_page_dirty_lock(page);
+
+		ttm->pages[i] = NULL;
+		put_page(page);
+	}
+}
+
+static void drm_ttm_free_alloced_pages(struct drm_ttm *ttm)
+{
+	int i;
+	struct drm_buffer_manager *bm = &ttm->dev->bm;
+	struct page **cur_page;
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		cur_page = ttm->pages + i;
+		if (*cur_page) {
+			if (page_count(*cur_page) != 1)
+				DRM_ERROR("Erroneous page count. Leaking pages.\n");
+			if (page_mapped(*cur_page))
+				DRM_ERROR("Erroneous map count. Leaking page mappings.\n");
+			__free_page(*cur_page);
+			drm_free_memctl(PAGE_SIZE);
+			--bm->cur_pages;
+		}
+	}
+}
+
+/*
+ * Free all resources associated with a ttm.
+ */
+
+int drm_ttm_destroy(struct drm_ttm *ttm)
+{
+	struct drm_ttm_backend *be;
+
+	if (!ttm)
+		return 0;
+
+	be = ttm->be;
+	if (be) {
+		be->func->destroy(be);
+		ttm->be = NULL;
+	}
+
+	if (ttm->pages) {
+		if (ttm->page_flags & DRM_TTM_PAGE_UNCACHED)
+			drm_ttm_set_caching(ttm, 0);
+
+		if (ttm->page_flags & DRM_TTM_PAGE_USER)
+			drm_ttm_free_user_pages(ttm);
+		else
+			drm_ttm_free_alloced_pages(ttm);
+
+		drm_ttm_free_pages(ttm);
+	}
+
+	drm_ctl_free(ttm, sizeof(*ttm), DRM_MEM_TTM);
+	return 0;
+}
+
+struct page *drm_ttm_get_page(struct drm_ttm *ttm, int index)
+{
+	struct page *p;
+	struct drm_buffer_manager *bm = &ttm->dev->bm;
+
+	p = ttm->pages[index];
+	if (!p) {
+		p = drm_ttm_alloc_page();
+		if (!p)
+			return NULL;
+		ttm->pages[index] = p;
+		++bm->cur_pages;
+	}
+	return p;
+}
+EXPORT_SYMBOL(drm_ttm_get_page);
+
+/**
+ * drm_ttm_set_user:
+ *
+ * @ttm: the ttm to map pages to. This must always be
+ * a freshly created ttm.
+ *
+ * @tsk: a pointer to the address space from which to map
+ * pages.
+ *
+ * @write: a boolean indicating that write access is desired
+ *
+ * start: the starting address
+ *
+ * Map a range of user addresses to a new ttm object. This
+ * provides access to user memory from the graphics device.
+ */
+int drm_ttm_set_user(struct drm_ttm *ttm,
+		     struct task_struct *tsk,
+		     unsigned long start,
+		     unsigned long num_pages)
+{
+	struct mm_struct *mm = tsk->mm;
+	int ret;
+	int write = (ttm->page_flags & DRM_TTM_PAGE_WRITE) != 0;
+
+	BUG_ON(num_pages != ttm->num_pages);
+	BUG_ON((ttm->page_flags & DRM_TTM_PAGE_USER) == 0);
+
+	down_read(&mm->mmap_sem);
+	ret = get_user_pages(tsk, mm, start, num_pages,
+			     write, 0, ttm->pages, NULL);
+	up_read(&mm->mmap_sem);
+
+	if (ret != num_pages && write) {
+		drm_ttm_free_user_pages(ttm);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+
+
+/**
+ * drm_ttm_populate:
+ *
+ * @ttm: the object to allocate pages for
+ *
+ * Allocate pages for all unset page entries, then
+ * call the backend to create the hardware mappings
+ */
+int drm_ttm_populate(struct drm_ttm *ttm)
+{
+	struct page *page;
+	unsigned long i;
+	struct drm_ttm_backend *be;
+
+	if (ttm->state != ttm_unpopulated)
+		return 0;
+
+	be = ttm->be;
+	if (ttm->page_flags & DRM_TTM_PAGE_WRITE) {
+		for (i = 0; i < ttm->num_pages; ++i) {
+			page = drm_ttm_get_page(ttm, i);
+			if (!page)
+				return -ENOMEM;
+		}
+	}
+	be->func->populate(be, ttm->num_pages, ttm->pages, ttm->dummy_read_page);
+	ttm->state = ttm_unbound;
+	return 0;
+}
+
+/**
+ * drm_ttm_create:
+ *
+ * @dev: the drm_device
+ *
+ * @size: The size (in bytes) of the desired object
+ *
+ * @page_flags: various DRM_TTM_PAGE_* flags. See drm_object.h.
+ *
+ * Allocate and initialize a ttm, leaving it unpopulated at this time
+ */
+
+struct drm_ttm *drm_ttm_create(struct drm_device *dev, unsigned long size,
+			       uint32_t page_flags, struct page *dummy_read_page)
+{
+	struct drm_bo_driver *bo_driver = dev->driver->bo_driver;
+	struct drm_ttm *ttm;
+
+	if (!bo_driver)
+		return NULL;
+
+	ttm = drm_ctl_calloc(1, sizeof(*ttm), DRM_MEM_TTM);
+	if (!ttm)
+		return NULL;
+
+	ttm->dev = dev;
+	atomic_set(&ttm->vma_count, 0);
+
+	ttm->destroy = 0;
+	ttm->num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+
+	ttm->page_flags = page_flags;
+
+	ttm->dummy_read_page = dummy_read_page;
+
+	/*
+	 * Account also for AGP module memory usage.
+	 */
+
+	drm_ttm_alloc_pages(ttm);
+	if (!ttm->pages) {
+		drm_ttm_destroy(ttm);
+		DRM_ERROR("Failed allocating page table\n");
+		return NULL;
+	}
+	ttm->be = bo_driver->create_ttm_backend_entry(dev);
+	if (!ttm->be) {
+		drm_ttm_destroy(ttm);
+		DRM_ERROR("Failed creating ttm backend entry\n");
+		return NULL;
+	}
+	ttm->state = ttm_unpopulated;
+	return ttm;
+}
+
+/**
+ * drm_ttm_evict:
+ *
+ * @ttm: the object to be unbound from the aperture.
+ *
+ * Transition a ttm from bound to evicted, where it
+ * isn't present in the aperture, but various caches may
+ * not be consistent.
+ */
+void drm_ttm_evict(struct drm_ttm *ttm)
+{
+	struct drm_ttm_backend *be = ttm->be;
+	int ret;
+
+	if (ttm->state == ttm_bound) {
+		ret = be->func->unbind(be);
+		BUG_ON(ret);
+	}
+
+	ttm->state = ttm_evicted;
+}
+
+/**
+ * drm_ttm_fixup_caching:
+ *
+ * @ttm: the object to set unbound
+ *
+ * XXX this function is misnamed. Transition a ttm from evicted to
+ * unbound, flushing caches as appropriate.
+ */
+void drm_ttm_fixup_caching(struct drm_ttm *ttm)
+{
+
+	if (ttm->state == ttm_evicted) {
+		struct drm_ttm_backend *be = ttm->be;
+		if (be->func->needs_ub_cache_adjust(be))
+			drm_ttm_set_caching(ttm, 0);
+		ttm->state = ttm_unbound;
+	}
+}
+
+/**
+ * drm_ttm_unbind:
+ *
+ * @ttm: the object to unbind from the graphics device
+ *
+ * Unbind an object from the aperture. This removes the mappings
+ * from the graphics device and flushes caches if necessary.
+ */
+void drm_ttm_unbind(struct drm_ttm *ttm)
+{
+	if (ttm->state == ttm_bound)
+		drm_ttm_evict(ttm);
+
+	drm_ttm_fixup_caching(ttm);
+}
+
+/**
+ * drm_ttm_bind:
+ *
+ * @ttm: the ttm object to bind to the graphics device
+ *
+ * @bo_mem: the aperture memory region which will hold the object
+ *
+ * Bind a ttm object to the aperture. This ensures that the necessary
+ * pages are allocated, flushes CPU caches as needed and marks the
+ * ttm as DRM_TTM_PAGE_USER_DIRTY to indicate that it may have been
+ * modified by the GPU
+ */
+int drm_ttm_bind(struct drm_ttm *ttm, struct drm_bo_mem_reg *bo_mem)
+{
+	struct drm_bo_driver *bo_driver = ttm->dev->driver->bo_driver;
+	int ret = 0;
+	struct drm_ttm_backend *be;
+
+	if (!ttm)
+		return -EINVAL;
+	if (ttm->state == ttm_bound)
+		return 0;
+
+	be = ttm->be;
+
+	ret = drm_ttm_populate(ttm);
+	if (ret)
+		return ret;
+
+	if (ttm->state == ttm_unbound && !(bo_mem->flags & DRM_BO_FLAG_CACHED))
+		drm_ttm_set_caching(ttm, DRM_TTM_PAGE_UNCACHED);
+	else if ((bo_mem->flags & DRM_BO_FLAG_CACHED_MAPPED) &&
+		   bo_driver->ttm_cache_flush)
+		bo_driver->ttm_cache_flush(ttm);
+
+	ret = be->func->bind(be, bo_mem);
+	if (ret) {
+		ttm->state = ttm_evicted;
+		DRM_ERROR("Couldn't bind backend.\n");
+		return ret;
+	}
+
+	ttm->state = ttm_bound;
+	if (ttm->page_flags & DRM_TTM_PAGE_USER)
+		ttm->page_flags |= DRM_TTM_PAGE_USER_DIRTY;
+	return 0;
+}
+EXPORT_SYMBOL(drm_ttm_bind);
diff --git a/drivers/char/drm/drm_vm.c b/drivers/char/drm/drm_vm.c
index c234c6f..94803ed 100644
--- a/drivers/char/drm/drm_vm.c
+++ b/drivers/char/drm/drm_vm.c
@@ -40,6 +40,10 @@
 
 static void drm_vm_open(struct vm_area_struct *vma);
 static void drm_vm_close(struct vm_area_struct *vma);
+static int drm_bo_mmap_locked(struct vm_area_struct *vma,
+			      struct file *filp,
+			      drm_local_map_t *map);
+
 
 static pgprot_t drm_io_prot(uint32_t map_type, struct vm_area_struct *vma)
 {
@@ -225,7 +229,7 @@ static void drm_vm_shm_close(struct vm_area_struct *vma)
 			found_maps++;
 		if (pt->vma == vma) {
 			list_del(&pt->head);
-			drm_free(pt, sizeof(*pt), DRM_MEM_VMAS);
+			drm_ctl_free(pt, sizeof(*pt), DRM_MEM_VMAS);
 		}
 	}
 
@@ -267,6 +271,9 @@ static void drm_vm_shm_close(struct vm_area_struct *vma)
 				dmah.size = map->size;
 				__drm_pci_free(dev, &dmah);
 				break;
+			case _DRM_TTM:
+				BUG_ON(1);
+				break;
 			}
 			drm_free(map, sizeof(*map), DRM_MEM_MAPS);
 		}
@@ -409,7 +416,7 @@ static void drm_vm_open_locked(struct vm_area_struct *vma)
 		  vma->vm_start, vma->vm_end - vma->vm_start);
 	atomic_inc(&dev->vma_count);
 
-	vma_entry = drm_alloc(sizeof(*vma_entry), DRM_MEM_VMAS);
+	vma_entry = drm_ctl_alloc(sizeof(*vma_entry), DRM_MEM_VMAS);
 	if (vma_entry) {
 		vma_entry->vma = vma;
 		vma_entry->pid = current->pid;
@@ -449,7 +456,7 @@ static void drm_vm_close(struct vm_area_struct *vma)
 	list_for_each_entry_safe(pt, temp, &dev->vmalist, head) {
 		if (pt->vma == vma) {
 			list_del(&pt->head);
-			drm_free(pt, sizeof(*pt), DRM_MEM_VMAS);
+			drm_ctl_free(pt, sizeof(*pt), DRM_MEM_VMAS);
 			break;
 		}
 	}
@@ -647,6 +654,8 @@ static int drm_mmap_locked(struct file *filp, struct vm_area_struct *vma)
 		vma->vm_flags |= VM_RESERVED;
 		vma->vm_page_prot = drm_dma_prot(map->type, vma);
 		break;
+	case _DRM_TTM:
+		return drm_bo_mmap_locked(vma, filp, map);
 	default:
 		return -EINVAL;	/* This should never happen. */
 	}
@@ -671,3 +680,189 @@ int drm_mmap(struct file *filp, struct vm_area_struct *vma)
 	return ret;
 }
 EXPORT_SYMBOL(drm_mmap);
+
+/**
+ * buffer object vm functions.
+ */
+
+/**
+ * \c Pagefault method for buffer objects.
+ *
+ * \param vma Virtual memory area.
+ * \param address File offset.
+ * \return Error or refault. The pfn is manually inserted.
+ *
+ * It's important that pfns are inserted while holding the bo->mutex lock.
+ * otherwise we might race with unmap_mapping_range() which is always
+ * called with the bo->mutex lock held.
+ *
+ * We're modifying the page attribute bits of the vma->vm_page_prot field,
+ * without holding the mmap_sem in write mode. Only in read mode.
+ * These bits are not used by the mm subsystem code, and we consider them
+ * protected by the bo->mutex lock.
+ */
+
+static unsigned long drm_bo_vm_nopfn(struct vm_area_struct *vma,
+				     unsigned long address)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	unsigned long page_offset;
+	struct page *page = NULL;
+	struct drm_ttm *ttm;
+	struct drm_device *dev;
+	unsigned long pfn;
+	int err;
+	unsigned long bus_base;
+	unsigned long bus_offset;
+	unsigned long bus_size;
+	unsigned long ret = NOPFN_REFAULT;
+
+	if (address > vma->vm_end)
+		return NOPFN_SIGBUS;
+
+	dev = bo->dev;
+	err = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (err)
+		return NOPFN_REFAULT;
+
+	err = mutex_lock_interruptible(&bo->mutex);
+	if (err) {
+		drm_bo_read_unlock(&dev->bm.bm_lock);
+		return NOPFN_REFAULT;
+	}
+
+	err = drm_bo_wait(bo, 0, 0, 0);
+	if (err) {
+		ret = (err != -EAGAIN) ? NOPFN_SIGBUS : NOPFN_REFAULT;
+		goto out_unlock;
+	}
+
+	/*
+	 * If buffer happens to be in a non-mappable location,
+	 * move it to a mappable.
+	 */
+
+	if (!(bo->mem.flags & DRM_BO_FLAG_MAPPABLE)) {
+		uint32_t new_flags = bo->mem.proposed_flags |
+			DRM_BO_FLAG_MAPPABLE |
+			DRM_BO_FLAG_FORCE_MAPPABLE;
+		err = drm_bo_move_buffer(bo, new_flags, 0, 0);
+		if (err) {
+			ret = (err != -EAGAIN) ? NOPFN_SIGBUS : NOPFN_REFAULT;
+			goto out_unlock;
+		}
+	}
+
+	err = drm_bo_pci_offset(dev, &bo->mem, &bus_base, &bus_offset,
+				&bus_size);
+
+	if (err) {
+		ret = NOPFN_SIGBUS;
+		goto out_unlock;
+	}
+
+	page_offset = (address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (bus_size) {
+		struct drm_mem_type_manager *man = &dev->bm.man[bo->mem.mem_type];
+
+		pfn = ((bus_base + bus_offset) >> PAGE_SHIFT) + page_offset;
+		vma->vm_page_prot = drm_io_prot(man->drm_bus_maptype, vma);
+	} else {
+		ttm = bo->ttm;
+
+		drm_ttm_fixup_caching(ttm);
+		page = drm_ttm_get_page(ttm, page_offset);
+		if (!page) {
+			ret = NOPFN_OOM;
+			goto out_unlock;
+		}
+		pfn = page_to_pfn(page);
+		vma->vm_page_prot = (bo->mem.flags & DRM_BO_FLAG_CACHED) ?
+			vm_get_page_prot(vma->vm_flags) :
+			drm_io_prot(_DRM_TTM, vma);
+	}
+
+	err = vm_insert_pfn(vma, address, pfn);
+	if (err) {
+		ret = (err != -EAGAIN) ? NOPFN_OOM : NOPFN_REFAULT;
+		goto out_unlock;
+	}
+out_unlock:
+	mutex_unlock(&bo->mutex);
+	drm_bo_read_unlock(&dev->bm.bm_lock);
+	return ret;
+}
+
+static void drm_bo_vm_open_locked(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+
+	drm_vm_open_locked(vma);
+	atomic_inc(&bo->usage);
+}
+
+/**
+ * \c vma open method for buffer objects.
+ *
+ * \param vma virtual memory area.
+ */
+
+static void drm_bo_vm_open(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	struct drm_device *dev = bo->dev;
+
+	mutex_lock(&dev->struct_mutex);
+	drm_bo_vm_open_locked(vma);
+	mutex_unlock(&dev->struct_mutex);
+}
+
+/**
+ * \c vma close method for buffer objects.
+ *
+ * \param vma virtual memory area.
+ */
+
+static void drm_bo_vm_close(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	struct drm_device *dev = bo->dev;
+
+	drm_vm_close(vma);
+	if (bo) {
+		mutex_lock(&dev->struct_mutex);
+		drm_bo_usage_deref_locked((struct drm_buffer_object **)
+					  &vma->vm_private_data);
+		mutex_unlock(&dev->struct_mutex);
+	}
+	return;
+}
+
+static struct vm_operations_struct drm_bo_vm_ops = {
+	.nopfn = drm_bo_vm_nopfn,
+	.open = drm_bo_vm_open,
+	.close = drm_bo_vm_close,
+};
+
+/**
+ * mmap buffer object memory.
+ *
+ * \param vma virtual memory area.
+ * \param file_priv DRM file private.
+ * \param map The buffer object drm map.
+ * \return zero on success or a negative number on failure.
+ */
+
+int drm_bo_mmap_locked(struct vm_area_struct *vma,
+		       struct file *filp,
+		       drm_local_map_t *map)
+{
+	vma->vm_ops = &drm_bo_vm_ops;
+	vma->vm_private_data = map->handle;
+	vma->vm_file = filp;
+	vma->vm_flags |= VM_RESERVED | VM_IO;
+	vma->vm_flags |= VM_PFNMAP;
+	drm_bo_vm_open_locked(vma);
+	return 0;
+}
diff --git a/drivers/char/drm/i915_buffer.c b/drivers/char/drm/i915_buffer.c
new file mode 100644
index 0000000..4c9168c
--- /dev/null
+++ b/drivers/char/drm/i915_buffer.c
@@ -0,0 +1,198 @@
+/**************************************************************************
+ *
+ * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrm <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+struct drm_ttm_backend *i915_create_ttm_backend_entry(struct drm_device *dev)
+{
+	return drm_agp_init_ttm(dev);
+}
+
+int i915_fence_type(struct drm_buffer_object *bo,
+		     uint32_t *fclass,
+		     uint32_t *type)
+{
+	if (bo->mem.proposed_flags & (DRM_BO_FLAG_READ | DRM_BO_FLAG_WRITE))
+		*type = 3;
+	else
+		*type = 1;
+	return 0;
+}
+
+int i915_invalidate_caches(struct drm_device *dev, uint64_t flags)
+{
+	/*
+	 * FIXME: Only emit once per batchbuffer submission.
+	 */
+
+	uint32_t flush_cmd = MI_NO_WRITE_FLUSH;
+
+	if (flags & DRM_BO_FLAG_READ)
+		flush_cmd |= MI_READ_FLUSH;
+	if (flags & DRM_BO_FLAG_EXE)
+		flush_cmd |= MI_EXE_FLUSH;
+
+	return i915_emit_mi_flush(dev, flush_cmd);
+}
+
+int i915_init_mem_type(struct drm_device *dev, uint32_t type,
+		       struct drm_mem_type_manager *man)
+{
+	switch (type) {
+	case DRM_BO_MEM_LOCAL:
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CACHED;
+		man->drm_bus_maptype = 0;
+		man->gpu_offset = 0;
+		break;
+	case DRM_BO_MEM_TT:
+		if (!(drm_core_has_AGP(dev) && dev->agp)) {
+			DRM_ERROR("AGP is not enabled for memory type %u\n",
+				  (unsigned)type);
+			return -EINVAL;
+		}
+		man->io_offset = dev->agp->agp_info.aper_base;
+		man->io_size = dev->agp->agp_info.aper_size * 1024 * 1024;
+		man->io_addr = NULL;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_NEEDS_IOREMAP;
+		man->drm_bus_maptype = _DRM_AGP;
+		man->gpu_offset = 0;
+		break;
+	case DRM_BO_MEM_VRAM:
+		if (!(drm_core_has_AGP(dev) && dev->agp)) {
+			DRM_ERROR("AGP is not enabled for memory type %u\n",
+				  (unsigned)type);
+			return -EINVAL;
+		}
+		man->io_offset = dev->agp->agp_info.aper_base;
+		man->io_size = dev->agp->agp_info.aper_size * 1024 * 1024;
+		man->io_addr = NULL;
+		man->flags =  _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_FIXED | _DRM_FLAG_NEEDS_IOREMAP;
+		man->drm_bus_maptype = _DRM_AGP;
+		man->gpu_offset = 0;
+		break;
+	case DRM_BO_MEM_PRIV0:
+		DRM_ERROR("PRIV0 not used yet.\n");
+		break;
+	default:
+		DRM_ERROR("Unsupported memory type %u\n", (unsigned)type);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * i915_evict_flags:
+ *
+ * @bo: the buffer object to be evicted
+ *
+ * Return the bo flags for a buffer which is not mapped to the hardware.
+ * These will be placed in proposed_flags so that when the move is
+ * finished, they'll end up in bo->mem.flags
+ */
+uint64_t i915_evict_flags(struct drm_buffer_object *bo)
+{
+	switch (bo->mem.mem_type) {
+	case DRM_BO_MEM_LOCAL:
+	case DRM_BO_MEM_TT:
+		return DRM_BO_FLAG_MEM_LOCAL;
+	default:
+		return DRM_BO_FLAG_MEM_TT | DRM_BO_FLAG_CACHED;
+	}
+}
+
+
+/*
+ * Disable i915_move_flip for now, since we can't guarantee that the hardware lock
+ * is held here. To re-enable we need to make sure either
+ * a) The X server is using DRM to submit commands to the ring, or
+ * b) DRM can use the HP ring for these blits. This means i915 needs to
+ * implement a new ring submission mechanism and fence class.
+ */
+
+int i915_move(struct drm_buffer_object *bo,
+	      int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+
+	if (old_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	} else if (new_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		if (0 /*i915_move_flip(bo, evict, no_wait, new_mem)*/)
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	} else {
+		if (0 /*i915_move_blit(bo, evict, no_wait, new_mem)*/)
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	}
+	return 0;
+}
+
+
+static inline void drm_cache_flush_addr(void *virt)
+{
+	int i;
+
+	for (i = 0; i < PAGE_SIZE; i += boot_cpu_data.x86_clflush_size)
+		clflush(virt+i);
+}
+
+static inline void drm_cache_flush_page(struct page *p)
+{
+	drm_cache_flush_addr(page_address(p));
+}
+
+void i915_flush_ttm(struct drm_ttm *ttm)
+{
+	int i;
+
+	if (!ttm)
+		return;
+
+	DRM_MEMORYBARRIER();
+
+#ifdef CONFIG_X86_32
+	/* Hopefully nobody has built an x86-64 processor without clflush */
+	if (!cpu_has_clflush) {
+		wbinvd();
+		DRM_MEMORYBARRIER();
+		return;
+	}
+#endif
+
+	for (i = ttm->num_pages - 1; i >= 0; i--)
+		drm_cache_flush_page(drm_ttm_get_page(ttm, i));
+
+	DRM_MEMORYBARRIER();
+}
diff --git a/drivers/char/drm/i915_dma.c b/drivers/char/drm/i915_dma.c
index 8897434..e3fbb9a 100644
--- a/drivers/char/drm/i915_dma.c
+++ b/drivers/char/drm/i915_dma.c
@@ -36,10 +36,10 @@
  * the head pointer changes, so that EBUSY only happens if the ring
  * actually stalls for (eg) 3 seconds.
  */
-int i915_wait_ring(struct drm_device * dev, int n, const char *caller)
+int i915_wait_ring(struct drm_device *dev, int n, const char *caller)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_ring_buffer_t *ring = &(dev_priv->ring);
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_ring_buffer *ring = &(dev_priv->ring);
 	u32 last_head = I915_READ(LP_RING + RING_HEAD) & HEAD_ADDR;
 	int i;
 
@@ -51,7 +51,7 @@ int i915_wait_ring(struct drm_device * dev, int n, const char *caller)
 		if (ring->space >= n)
 			return 0;
 
-		dev_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
+		//dev_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
 
 		if (ring->head != last_head)
 			i = 0;
@@ -62,10 +62,15 @@ int i915_wait_ring(struct drm_device * dev, int n, const char *caller)
 	return -EBUSY;
 }
 
-void i915_kernel_lost_context(struct drm_device * dev)
+void i915_kernel_lost_context(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_ring_buffer_t *ring = &(dev_priv->ring);
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_ring_buffer *ring = &(dev_priv->ring);
+
+	/* we should never lose context on the ring with modesetting 
+	 * as we don't expose it to userspace */
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return;
 
 	ring->head = I915_READ(LP_RING + RING_HEAD) & HEAD_ADDR;
 	ring->tail = I915_READ(LP_RING + RING_TAIL) & TAIL_ADDR;
@@ -73,13 +78,17 @@ void i915_kernel_lost_context(struct drm_device * dev)
 	if (ring->space < 0)
 		ring->space += ring->Size;
 
-	if (ring->head == ring->tail)
-		dev_priv->sarea_priv->perf_boxes |= I915_BOX_RING_EMPTY;
+//	if (ring->head == ring->tail)
+//		dev_priv->sarea_priv->perf_boxes |= I915_BOX_RING_EMPTY;
 }
 
-static int i915_dma_cleanup(struct drm_device * dev)
+int i915_dma_cleanup(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return 0;
+
 	/* Make sure interrupts are disabled here because the uninstall ioctl
 	 * may not have been called from userspace and after dev_private
 	 * is freed, it's too late.
@@ -92,6 +101,7 @@ static int i915_dma_cleanup(struct drm_device * dev)
 		dev_priv->ring.virtual_start = 0;
 		dev_priv->ring.map.handle = 0;
 		dev_priv->ring.map.size = 0;
+		dev_priv->ring.Size = 0;
 	}
 
 	if (dev_priv->status_page_dmah) {
@@ -110,54 +120,124 @@ static int i915_dma_cleanup(struct drm_device * dev)
 	return 0;
 }
 
-static int i915_initialize(struct drm_device * dev, drm_i915_init_t * init)
+#define DRI2_SAREA_BLOCK_TYPE(b) ((b) >> 16)
+#define DRI2_SAREA_BLOCK_SIZE(b) ((b) & 0xffff)
+#define DRI2_SAREA_BLOCK_NEXT(p)				\
+	((void *) ((unsigned char *) (p) +			\
+		   DRI2_SAREA_BLOCK_SIZE(*(unsigned int *) p)))
+
+#define DRI2_SAREA_BLOCK_END		0x0000
+#define DRI2_SAREA_BLOCK_LOCK		0x0001
+#define DRI2_SAREA_BLOCK_EVENT_BUFFER	0x0002
+
+static int
+setup_dri2_sarea(struct drm_device * dev,
+		 struct drm_file *file_priv,
+		 drm_i915_init_t * init)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret;
+	unsigned int *p, *end, *next;
 
-	dev_priv->sarea = drm_getsarea(dev);
-	if (!dev_priv->sarea) {
-		DRM_ERROR("can not find sarea!\n");
-		i915_dma_cleanup(dev);
+	mutex_lock(&dev->struct_mutex);
+	dev_priv->sarea_bo =
+		drm_lookup_buffer_object(file_priv,
+					 init->sarea_handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!dev_priv->sarea_bo) {
+		DRM_ERROR("did not find sarea bo\n");
 		return -EINVAL;
 	}
 
-	dev_priv->mmio_map = drm_core_findmap(dev, init->mmio_offset);
-	if (!dev_priv->mmio_map) {
-		i915_dma_cleanup(dev);
-		DRM_ERROR("can not find mmio map!\n");
-		return -EINVAL;
+	ret = drm_bo_kmap(dev_priv->sarea_bo, 0,
+			  dev_priv->sarea_bo->num_pages,
+			  &dev_priv->sarea_kmap);
+	if (ret) {
+		DRM_ERROR("could not map sarea bo\n");
+		return ret;
 	}
 
-	dev_priv->sarea_priv = (drm_i915_sarea_t *)
-	    ((u8 *) dev_priv->sarea->handle + init->sarea_priv_offset);
+	p = dev_priv->sarea_kmap.virtual;
+	end = (void *) p + (dev_priv->sarea_bo->num_pages << PAGE_SHIFT);
+	while (p < end && DRI2_SAREA_BLOCK_TYPE(*p) != DRI2_SAREA_BLOCK_END) {
+		switch (DRI2_SAREA_BLOCK_TYPE(*p)) {
+		case DRI2_SAREA_BLOCK_LOCK:
+			dev->primary->master->lock.hw_lock = (void *) (p + 1);
+			dev->sigdata.lock = dev->primary->master->lock.hw_lock;
+			break;
+		}
+		next = DRI2_SAREA_BLOCK_NEXT(p);
+		if (next <= p || end < next) {
+			DRM_ERROR("malformed dri2 sarea: next is %p should be within %p-%p\n",
+				  next, p, end);
+			return -EINVAL;
+		}
+		p = next;
+	}
 
-	dev_priv->ring.Start = init->ring_start;
-	dev_priv->ring.End = init->ring_end;
-	dev_priv->ring.Size = init->ring_size;
-	dev_priv->ring.tail_mask = dev_priv->ring.Size - 1;
+	return 0;
+}
 
-	dev_priv->ring.map.offset = init->ring_start;
-	dev_priv->ring.map.size = init->ring_size;
-	dev_priv->ring.map.type = 0;
-	dev_priv->ring.map.flags = 0;
-	dev_priv->ring.map.mtrr = 0;
+static int i915_initialize(struct drm_device *dev,
+			   struct drm_file *file_priv,
+			   drm_i915_init_t * init)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
 
-	drm_core_ioremap(&dev_priv->ring.map, dev);
+	if (!drm_core_check_feature(dev, DRIVER_MODESET)) {
+		if (init->mmio_offset != 0)
+			dev_priv->mmio_map = drm_core_findmap(dev, init->mmio_offset);
+		if (!dev_priv->mmio_map) {
+			i915_dma_cleanup(dev);
+			DRM_ERROR("can not find mmio map!\n");
+			return -EINVAL;
+		}
+	}
 
-	if (dev_priv->ring.map.handle == NULL) {
-		i915_dma_cleanup(dev);
-		DRM_ERROR("can not ioremap virtual address for"
-			  " ring buffer\n");
-		return -ENOMEM;
+
+	dev_priv->max_validate_buffers = I915_MAX_VALIDATE_BUFFERS;
+
+#if 0
+	if (init->sarea_priv_offset)
+		dev_priv->sarea_priv = (drm_i915_sarea_t *)
+			((u8 *) dev_priv->sarea->handle +
+			 init->sarea_priv_offset);
+	else {
+		/* No sarea_priv for you! */
+		dev_priv->sarea_priv = NULL;
+	}
+#endif
+
+	if (!dev_priv->ring.Size) {
+		dev_priv->ring.Start = init->ring_start;
+		dev_priv->ring.End = init->ring_end;
+		dev_priv->ring.Size = init->ring_size;
+		dev_priv->ring.tail_mask = dev_priv->ring.Size - 1;
+		
+		dev_priv->ring.map.offset = init->ring_start;
+		dev_priv->ring.map.size = init->ring_size;
+		dev_priv->ring.map.type = 0;
+		dev_priv->ring.map.flags = 0;
+		dev_priv->ring.map.mtrr = 0;
+		
+		drm_core_ioremap(&dev_priv->ring.map, dev);
+		
+		if (dev_priv->ring.map.handle == NULL) {
+			i915_dma_cleanup(dev);
+			DRM_ERROR("can not ioremap virtual address for"
+				  " ring buffer\n");
+			return -ENOMEM;
+		}
+		dev_priv->ring.virtual_start = dev_priv->ring.map.handle;
 	}
 
-	dev_priv->ring.virtual_start = dev_priv->ring.map.handle;
 
 	dev_priv->cpp = init->cpp;
-	dev_priv->back_offset = init->back_offset;
-	dev_priv->front_offset = init->front_offset;
-	dev_priv->current_page = 0;
-	dev_priv->sarea_priv->pf_current_page = dev_priv->current_page;
+
+	if (master_priv->sarea_priv)
+		master_priv->sarea_priv->pf_current_page = 0;	
 
 	/* We are using separate values as placeholders for mechanisms for
 	 * private backbuffer/depthbuffer usage.
@@ -170,6 +250,10 @@ static int i915_initialize(struct drm_device * dev, drm_i915_init_t * init)
 	 */
 	dev_priv->allow_batchbuffer = 1;
 
+	/* Enable vblank on pipe A for older X servers
+	 */
+	dev_priv->vblank_pipe = DRM_I915_VBLANK_PIPE_A;
+
 	/* Program Hardware Status Page */
 	if (!I915_NEED_GFX_HWS(dev)) {
 		dev_priv->status_page_dmah =
@@ -187,19 +271,31 @@ static int i915_initialize(struct drm_device * dev, drm_i915_init_t * init)
 		I915_WRITE(0x02080, dev_priv->dma_status_page);
 	}
 	DRM_DEBUG("Enabled hardware status page\n");
+	mutex_init(&dev_priv->cmdbuf_mutex);
+
+	if (init->func == I915_INIT_DMA2) {
+		int ret = setup_dri2_sarea(dev, file_priv, init);
+		if (ret) {
+			i915_dma_cleanup(dev);
+			DRM_ERROR("could not set up dri2 sarea\n");
+			return ret;
+		}
+	}
 	return 0;
 }
 
-static int i915_dma_resume(struct drm_device * dev)
+static int i915_dma_resume(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 
 	DRM_DEBUG("%s\n", __func__);
 
+#if 0
 	if (!dev_priv->sarea) {
 		DRM_ERROR("can not find sarea!\n");
 		return -EINVAL;
 	}
+#endif
 
 	if (!dev_priv->mmio_map) {
 		DRM_ERROR("can not find mmio map!\n");
@@ -236,7 +332,8 @@ static int i915_dma_init(struct drm_device *dev, void *data,
 
 	switch (init->func) {
 	case I915_INIT_DMA:
-		retcode = i915_initialize(dev, init);
+	case I915_INIT_DMA2:
+		retcode = i915_initialize(dev, file_priv, init);
 		break;
 	case I915_CLEANUP_DMA:
 		retcode = i915_dma_cleanup(dev);
@@ -328,9 +425,9 @@ static int validate_cmd(int cmd)
 	return ret;
 }
 
-static int i915_emit_cmds(struct drm_device * dev, int __user * buffer, int dwords)
+static int i915_emit_cmds(struct drm_device *dev, int __user * buffer, int dwords)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	int i;
 	RING_LOCALS;
 
@@ -367,11 +464,11 @@ static int i915_emit_cmds(struct drm_device * dev, int __user * buffer, int dwor
 	return 0;
 }
 
-static int i915_emit_box(struct drm_device * dev,
+static int i915_emit_box(struct drm_device *dev,
 			 struct drm_clip_rect __user * boxes,
 			 int i, int DR1, int DR4)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_clip_rect box;
 	RING_LOCALS;
 
@@ -410,15 +507,22 @@ static int i915_emit_box(struct drm_device * dev,
  * emit. For now, do it in both places:
  */
 
-static void i915_emit_breadcrumb(struct drm_device *dev)
+void i915_emit_breadcrumb(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv;
 	RING_LOCALS;
 
-	dev_priv->sarea_priv->last_enqueue = ++dev_priv->counter;
+	if (++dev_priv->counter > BREADCRUMB_MASK) {
+		dev_priv->counter = 1;
+		DRM_DEBUG("Breadcrumb counter wrapped around\n");
+	}
 
-	if (dev_priv->counter > 0x7FFFFFFFUL)
-		dev_priv->sarea_priv->last_enqueue = dev_priv->counter = 1;
+	if (dev->primary && dev->primary->master) {
+		master_priv = dev->primary->master->driver_priv;
+		if (master_priv->sarea_priv)
+			master_priv->sarea_priv->last_enqueue = dev_priv->counter;
+	}
 
 	BEGIN_LP_RING(4);
 	OUT_RING(CMD_STORE_DWORD_IDX);
@@ -428,9 +532,32 @@ static void i915_emit_breadcrumb(struct drm_device *dev)
 	ADVANCE_LP_RING();
 }
 
-static int i915_dispatch_cmdbuffer(struct drm_device * dev,
+
+int i915_emit_mi_flush(struct drm_device *dev, uint32_t flush)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t flush_cmd = CMD_MI_FLUSH;
+	RING_LOCALS;
+
+	flush_cmd |= flush;
+
+	i915_kernel_lost_context(dev);
+
+	BEGIN_LP_RING(4);
+	OUT_RING(flush_cmd);
+	OUT_RING(0);
+	OUT_RING(0);
+	OUT_RING(0);
+	ADVANCE_LP_RING();
+
+	return 0;
+}
+
+
+static int i915_dispatch_cmdbuffer(struct drm_device *dev,
 				   drm_i915_cmdbuffer_t * cmd)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	int nbox = cmd->num_cliprects;
 	int i = 0, count, ret;
 
@@ -457,13 +584,15 @@ static int i915_dispatch_cmdbuffer(struct drm_device * dev,
 	}
 
 	i915_emit_breadcrumb(dev);
+	if (unlikely((dev_priv->counter & 0xFF) == 0))
+		drm_fence_flush_old(dev, 0, dev_priv->counter);
 	return 0;
 }
 
-static int i915_dispatch_batchbuffer(struct drm_device * dev,
+static int i915_dispatch_batchbuffer(struct drm_device *dev,
 				     drm_i915_batchbuffer_t * batch)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_clip_rect __user *boxes = batch->cliprects;
 	int nbox = batch->num_cliprects;
 	int i = 0, count;
@@ -507,61 +636,88 @@ static int i915_dispatch_batchbuffer(struct drm_device * dev,
 	}
 
 	i915_emit_breadcrumb(dev);
+	if (unlikely((dev_priv->counter & 0xFF) == 0))
+		drm_fence_flush_old(dev, 0, dev_priv->counter);
 
 	return 0;
 }
 
-static int i915_dispatch_flip(struct drm_device * dev)
+static void i915_do_dispatch_flip(struct drm_device *dev, int plane, int sync)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	u32 num_pages, current_page, next_page, dspbase;
+	int shift = 2 * plane, x, y;
 	RING_LOCALS;
 
-	DRM_DEBUG("%s: page=%d pfCurrentPage=%d\n",
-		  __FUNCTION__,
-		  dev_priv->current_page,
-		  dev_priv->sarea_priv->pf_current_page);
+	/* Calculate display base offset */
+	num_pages = master_priv->sarea_priv->third_handle ? 3 : 2;
+	current_page = (master_priv->sarea_priv->pf_current_page >> shift) & 0x3;
+	next_page = (current_page + 1) % num_pages;
 
-	i915_kernel_lost_context(dev);
-
-	BEGIN_LP_RING(2);
-	OUT_RING(INST_PARSER_CLIENT | INST_OP_FLUSH | INST_FLUSH_MAP_CACHE);
-	OUT_RING(0);
-	ADVANCE_LP_RING();
+	switch (next_page) {
+	default:
+	case 0:
+		dspbase = master_priv->sarea_priv->front_offset;
+		break;
+	case 1:
+		dspbase = master_priv->sarea_priv->back_offset;
+		break;
+	case 2:
+		dspbase = master_priv->sarea_priv->third_offset;
+		break;
+	}
 
-	BEGIN_LP_RING(6);
-	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | ASYNC_FLIP);
-	OUT_RING(0);
-	if (dev_priv->current_page == 0) {
-		OUT_RING(dev_priv->back_offset);
-		dev_priv->current_page = 1;
+	if (plane == 0) {
+		x = master_priv->sarea_priv->planeA_x;
+		y = master_priv->sarea_priv->planeA_y;
 	} else {
-		OUT_RING(dev_priv->front_offset);
-		dev_priv->current_page = 0;
+		x = master_priv->sarea_priv->planeB_x;
+		y = master_priv->sarea_priv->planeB_y;
 	}
-	OUT_RING(0);
-	ADVANCE_LP_RING();
 
-	BEGIN_LP_RING(2);
-	OUT_RING(MI_WAIT_FOR_EVENT | MI_WAIT_FOR_PLANE_A_FLIP);
-	OUT_RING(0);
-	ADVANCE_LP_RING();
+	dspbase += (y * master_priv->sarea_priv->pitch + x) * dev_priv->cpp;
 
-	dev_priv->sarea_priv->last_enqueue = dev_priv->counter++;
+	DRM_DEBUG("plane=%d current_page=%d dspbase=0x%x\n", plane, current_page,
+		  dspbase);
 
 	BEGIN_LP_RING(4);
-	OUT_RING(CMD_STORE_DWORD_IDX);
-	OUT_RING(20);
-	OUT_RING(dev_priv->counter);
-	OUT_RING(0);
+	OUT_RING(sync ? 0 :
+		 (MI_WAIT_FOR_EVENT | (plane ? MI_WAIT_FOR_PLANE_B_FLIP :
+				       MI_WAIT_FOR_PLANE_A_FLIP)));
+	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | (sync ? 0 : ASYNC_FLIP) |
+		 (plane ? DISPLAY_PLANE_B : DISPLAY_PLANE_A));
+	OUT_RING(master_priv->sarea_priv->pitch * dev_priv->cpp);
+	OUT_RING(dspbase);
 	ADVANCE_LP_RING();
 
-	dev_priv->sarea_priv->pf_current_page = dev_priv->current_page;
-	return 0;
+	master_priv->sarea_priv->pf_current_page &= ~(0x3 << shift);
+	master_priv->sarea_priv->pf_current_page |= next_page << shift;
+}
+
+void i915_dispatch_flip(struct drm_device *dev, int planes, int sync)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	int i;
+
+	DRM_DEBUG("planes=0x%x pfCurrentPage=%d\n",
+		  planes, master_priv->sarea_priv->pf_current_page);
+
+	i915_emit_mi_flush(dev, MI_READ_FLUSH | MI_EXE_FLUSH);
+
+	for (i = 0; i < 2; i++)
+		if (planes & (1 << i))
+			i915_do_dispatch_flip(dev, i, sync);
+
+	i915_emit_breadcrumb(dev);
+	if (unlikely(!sync && ((dev_priv->counter & 0xFF) == 0)))
+		drm_fence_flush_old(dev, 0, dev_priv->counter);
 }
 
-static int i915_quiescent(struct drm_device * dev)
+static int i915_quiescent(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
 	i915_kernel_lost_context(dev);
 	return i915_wait_ring(dev, dev_priv->ring.Size - 8, __func__);
@@ -578,10 +734,10 @@ static int i915_flush_ioctl(struct drm_device *dev, void *data,
 static int i915_batchbuffer(struct drm_device *dev, void *data,
 			    struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
-	u32 *hw_status = dev_priv->hw_status_page;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
 	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
-	    dev_priv->sarea_priv;
+	    master_priv->sarea_priv;
 	drm_i915_batchbuffer_t *batch = data;
 	int ret;
 
@@ -602,18 +758,18 @@ static int i915_batchbuffer(struct drm_device *dev, void *data,
 
 	ret = i915_dispatch_batchbuffer(dev, batch);
 
-	sarea_priv->last_dispatch = (int)hw_status[5];
+	sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
 	return ret;
 }
 
 static int i915_cmdbuffer(struct drm_device *dev, void *data,
 			  struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
-	u32 *hw_status = dev_priv->hw_status_page;
-	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
-	    dev_priv->sarea_priv;
-	drm_i915_cmdbuffer_t *cmdbuf = data;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	struct drm_i915_sarea *sarea_priv = (struct drm_i915_sarea *)
+		master_priv->sarea_priv;
+	struct drm_i915_cmdbuffer *cmdbuf = data;
 	int ret;
 
 	DRM_DEBUG("i915 cmdbuffer, buf %p sz %d cliprects %d\n",
@@ -635,24 +791,611 @@ static int i915_cmdbuffer(struct drm_device *dev, void *data,
 		return ret;
 	}
 
-	sarea_priv->last_dispatch = (int)hw_status[5];
+	sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
 	return 0;
 }
 
-static int i915_flip_bufs(struct drm_device *dev, void *data,
-			  struct drm_file *file_priv)
+#if DRM_DEBUG_CODE
+#define DRM_DEBUG_RELOCATION  (drm_debug != 0)
+#else
+#define DRM_DEBUG_RELOCATION  0
+#endif
+
+struct i915_relocatee_info {
+	struct drm_buffer_object *buf;
+	unsigned long offset;
+	u32 *data_page;
+	unsigned page_offset;
+	struct drm_bo_kmap_obj kmap;
+	int is_iomem;
+	int idle;
+};
+
+struct drm_i915_validate_buffer {
+	struct drm_buffer_object *buffer;
+	struct drm_bo_info_rep rep;
+	int presumed_offset_correct;
+	void __user *data;
+	int ret;
+};
+
+static void i915_dereference_buffers_locked(struct drm_i915_validate_buffer *buffers,
+					    unsigned num_buffers)
+{
+	while (num_buffers--)
+		drm_bo_usage_deref_locked(&buffers[num_buffers].buffer);
+}
+
+int i915_apply_reloc(struct drm_file *file_priv, int num_buffers,
+		     struct drm_i915_validate_buffer *buffers,
+		     struct i915_relocatee_info *relocatee,
+		     uint32_t *reloc)
+{
+	unsigned index;
+	unsigned long new_cmd_offset;
+	u32 val;
+	int ret, i;
+	int buf_index = -1;
+
+	/*
+	 * FIXME: O(relocs * buffers) complexity.
+	 */
+
+	for (i = 0; i <= num_buffers; i++)
+		if (buffers[i].buffer)
+			if (reloc[2] == buffers[i].buffer->base.hash.key)
+				buf_index = i;
+
+	if (buf_index == -1) {
+		DRM_ERROR("Illegal relocation buffer %08X\n", reloc[2]);
+		return -EINVAL;
+	}
+
+	/*
+	 * Short-circuit relocations that were correctly
+	 * guessed by the client
+	 */
+	if (buffers[buf_index].presumed_offset_correct && !DRM_DEBUG_RELOCATION)
+		return 0;
+
+	new_cmd_offset = reloc[0];
+	if (!relocatee->data_page ||
+	    !drm_bo_same_page(relocatee->offset, new_cmd_offset)) {
+		drm_bo_kunmap(&relocatee->kmap);
+		relocatee->data_page = NULL;
+		relocatee->offset = new_cmd_offset;
+		
+		if (unlikely(!relocatee->idle)) {
+			ret = drm_bo_wait(relocatee->buf, 0, 0, 0);
+			if (ret)
+				return ret;
+			relocatee->idle = 1;
+		}
+
+		ret = drm_bo_kmap(relocatee->buf, new_cmd_offset >> PAGE_SHIFT,
+				  1, &relocatee->kmap);
+		if (ret) {
+			DRM_ERROR("Could not map command buffer to apply relocs\n %08lx", new_cmd_offset);
+			return ret;
+		}
+		relocatee->data_page = drm_bmo_virtual(&relocatee->kmap,
+						       &relocatee->is_iomem);
+		relocatee->page_offset = (relocatee->offset & PAGE_MASK);
+	}
+
+	val = buffers[buf_index].buffer->offset;
+	index = (reloc[0] - relocatee->page_offset) >> 2;
+
+	/* add in validate */
+	val = val + reloc[1];
+
+	if (DRM_DEBUG_RELOCATION) {
+		if (buffers[buf_index].presumed_offset_correct &&
+		    relocatee->data_page[index] != val) {
+			DRM_DEBUG ("Relocation mismatch source %d target %d buffer %d user %08x kernel %08x\n",
+				   reloc[0], reloc[1], buf_index, relocatee->data_page[index], val);
+		}
+	}
+
+	if (relocatee->is_iomem)
+		iowrite32(val, relocatee->data_page + index);
+	else
+		relocatee->data_page[index] = val;
+	return 0;
+}
+
+int i915_process_relocs(struct drm_file *file_priv,
+			uint32_t buf_handle,
+			uint32_t __user **reloc_user_ptr,
+			struct i915_relocatee_info *relocatee,
+			struct drm_i915_validate_buffer *buffers,
+			uint32_t num_buffers)
+{
+	int ret, reloc_stride;
+	uint32_t cur_offset;
+	uint32_t reloc_count;
+	uint32_t reloc_type;
+	uint32_t reloc_buf_size;
+	uint32_t *reloc_buf = NULL;
+	int i;
+
+	/* do a copy from user from the user ptr */
+	ret = get_user(reloc_count, *reloc_user_ptr);
+	if (ret) {
+		DRM_ERROR("Could not map relocation buffer.\n");
+		goto out;
+	}
+
+	ret = get_user(reloc_type, (*reloc_user_ptr)+1);
+	if (ret) {
+		DRM_ERROR("Could not map relocation buffer.\n");
+		goto out;
+	}
+
+	if (reloc_type != 0) {
+		DRM_ERROR("Unsupported relocation type requested\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	reloc_buf_size = (I915_RELOC_HEADER + (reloc_count * I915_RELOC0_STRIDE)) * sizeof(uint32_t);
+	reloc_buf = kmalloc(reloc_buf_size, GFP_KERNEL);
+	if (!reloc_buf) {
+		DRM_ERROR("Out of memory for reloc buffer\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(reloc_buf, *reloc_user_ptr, reloc_buf_size)) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	/* get next relocate buffer handle */
+	*reloc_user_ptr = (uint32_t *)*(unsigned long *)&reloc_buf[2];
+
+	reloc_stride = I915_RELOC0_STRIDE * sizeof(uint32_t); /* may be different for other types of relocs */
+
+	DRM_DEBUG("num relocs is %d, next is %p\n", reloc_count, *reloc_user_ptr);
+
+	for (i = 0; i < reloc_count; i++) {
+		cur_offset = I915_RELOC_HEADER + (i * I915_RELOC0_STRIDE);
+		  
+		ret = i915_apply_reloc(file_priv, num_buffers, buffers,
+				       relocatee, reloc_buf + cur_offset);
+		if (ret)
+			goto out;
+	}
+
+out:
+	if (reloc_buf)
+		kfree(reloc_buf);
+
+	if (relocatee->data_page) {		
+		drm_bo_kunmap(&relocatee->kmap);
+		relocatee->data_page = NULL;
+	}
+
+	return ret;
+}
+
+static int i915_exec_reloc(struct drm_file *file_priv, drm_handle_t buf_handle,
+			   uint32_t __user *reloc_user_ptr,
+			   struct drm_i915_validate_buffer *buffers,
+			   uint32_t buf_count)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct i915_relocatee_info relocatee;
+	int ret = 0;
+	int b;
+
+	/*
+	 * Short circuit relocations when all previous
+	 * buffers offsets were correctly guessed by
+	 * the client
+	 */
+	if (!DRM_DEBUG_RELOCATION) {
+		for (b = 0; b < buf_count; b++)
+			if (!buffers[b].presumed_offset_correct)
+				break;
+
+		if (b == buf_count)
+			return 0;
+	}
+
+	memset(&relocatee, 0, sizeof(relocatee));
+
+	mutex_lock(&dev->struct_mutex);
+	relocatee.buf = drm_lookup_buffer_object(file_priv, buf_handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+	if (!relocatee.buf) {
+		DRM_DEBUG("relocatee buffer invalid %08x\n", buf_handle);
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	mutex_lock (&relocatee.buf->mutex);
+	while (reloc_user_ptr) {
+		ret = i915_process_relocs(file_priv, buf_handle, &reloc_user_ptr, &relocatee, buffers, buf_count);
+		if (ret) {
+			DRM_ERROR("process relocs failed\n");
+			goto out_err1;
+		}
+	}
+
+out_err1:
+	mutex_unlock (&relocatee.buf->mutex);
+	drm_bo_usage_deref_unlocked(&relocatee.buf);
+out_err:
+	return ret;
+}
+
+static int i915_check_presumed(struct drm_i915_op_arg *arg,
+			       struct drm_buffer_object *bo,
+			       uint32_t __user *data,
+			       int *presumed_ok)
+{
+	struct drm_bo_op_req *req = &arg->d.req;
+	uint32_t hint_offset;
+	uint32_t hint = req->bo_req.hint;
+
+	*presumed_ok = 0;
+
+	if (!(hint & DRM_BO_HINT_PRESUMED_OFFSET))
+		return 0;
+	if (bo->offset == req->bo_req.presumed_offset) {
+		*presumed_ok = 1;
+		return 0;
+	}
+
+	/*
+	 * We need to turn off the HINT_PRESUMED_OFFSET for this buffer in
+	 * the user-space IOCTL argument list, since the buffer has moved,
+	 * we're about to apply relocations and we might subsequently
+	 * hit an -EAGAIN. In that case the argument list will be reused by
+	 * user-space, but the presumed offset is no longer valid.
+	 *
+	 * Needless to say, this is a bit ugly.
+	 */
+
+       	hint_offset = (uint32_t *)&req->bo_req.hint - (uint32_t *)arg;
+	hint &= ~DRM_BO_HINT_PRESUMED_OFFSET;
+	return __put_user(hint, data + hint_offset);
+}
+
+
+/*
+ * Validate, add fence and relocate a block of bos from a userspace list
+ */
+int i915_validate_buffer_list(struct drm_file *file_priv,
+			      unsigned int fence_class, uint64_t data,
+			      struct drm_i915_validate_buffer *buffers,
+			      uint32_t *num_buffers)
+{
+	struct drm_i915_op_arg arg;
+	struct drm_bo_op_req *req = &arg.d.req;
+	int ret = 0;
+	unsigned buf_count = 0;
+	uint32_t buf_handle;
+	uint32_t __user *reloc_user_ptr;
+	struct drm_i915_validate_buffer *item = buffers;
+
+	do {
+		if (buf_count >= *num_buffers) {
+			DRM_ERROR("Buffer count exceeded %d\n.", *num_buffers);
+			ret = -EINVAL;
+			goto out_err;
+		}
+		item = buffers + buf_count;
+		item->buffer = NULL;
+		item->presumed_offset_correct = 0;
+
+		buffers[buf_count].buffer = NULL;
+
+		if (copy_from_user(&arg, (void __user *)(unsigned long)data, sizeof(arg))) {
+			ret = -EFAULT;
+			goto out_err;
+		}
+
+		ret = 0;
+		if (req->op != drm_bo_validate) {
+			DRM_ERROR
+			    ("Buffer object operation wasn't \"validate\".\n");
+			ret = -EINVAL;
+			goto out_err;
+		}
+		item->ret = 0;
+		item->data = (void __user *) (unsigned long) data;
+
+		buf_handle = req->bo_req.handle;
+		reloc_user_ptr = (uint32_t *)(unsigned long)arg.reloc_ptr;
+
+		if (reloc_user_ptr) {
+			ret = i915_exec_reloc(file_priv, buf_handle, reloc_user_ptr, buffers, buf_count);
+			if (ret)
+				goto out_err;
+			DRM_MEMORYBARRIER();
+		}
+
+		ret = drm_bo_handle_validate(file_priv, req->bo_req.handle,
+					     req->bo_req.flags, req->bo_req.mask,
+					     req->bo_req.hint,
+					     req->bo_req.fence_class, 0,
+					     &item->rep,
+					     &item->buffer);
+
+		if (ret) {
+			DRM_ERROR("error on handle validate %d\n", ret);
+			goto out_err;
+		}
+
+		buf_count++;
+
+		ret = i915_check_presumed(&arg, item->buffer,
+					  (uint32_t __user *)
+					  (unsigned long) data,
+					  &item->presumed_offset_correct);
+		if (ret)
+			goto out_err;
+
+		data = arg.next;
+	} while (data != 0);
+	*num_buffers = buf_count;
+	return 0;
+out_err:
+	*num_buffers = 0;
+	item->ret = (ret != -EAGAIN) ? ret : 0;
+	return ret;
+}
+
+/*
+ * Remove all buffers from the unfenced list.
+ * If the execbuffer operation was aborted, for example due to a signal,
+ * this also make sure that buffers retain their original state and
+ * fence pointers.
+ * Copy back buffer information to user-space unless we were interrupted
+ * by a signal. In which case the IOCTL must be rerun.
+ */
+
+static int i915_handle_copyback(struct drm_device *dev,
+				struct drm_i915_validate_buffer *buffers,
+				unsigned int num_buffers, int ret)
+{
+	int err = ret;
+	int i;
+	struct drm_i915_op_arg arg;
+
+	if (ret)
+		drm_putback_buffer_objects(dev);
+
+	if (ret != -EAGAIN) {
+		for (i = 0; i < num_buffers; ++i) {
+			arg.handled = 1;
+			arg.d.rep.ret = buffers->ret;
+			arg.d.rep.bo_info = buffers->rep;
+			if (__copy_to_user(buffers->data, &arg, sizeof(arg)))
+				err = -EFAULT;
+			buffers++;
+		}
+	}
+
+	return err;
+}
+
+/*
+ * Create a fence object, and if that fails, pretend that everything is
+ * OK and just idle the GPU.
+ */
+
+void i915_fence_or_sync(struct drm_file *file_priv,
+			uint32_t fence_flags,
+			struct drm_fence_arg *fence_arg,
+			struct drm_fence_object **fence_p)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	int ret;
+	struct drm_fence_object *fence;
+
+	ret = drm_fence_buffer_objects(dev, NULL, fence_flags,
+			 NULL, &fence);
+
+	if (ret) {
+
+		/*
+		 * Fence creation failed.
+		 * Fall back to synchronous operation and idle the engine.
+		 */
+
+		(void) i915_emit_mi_flush(dev, MI_READ_FLUSH);
+		(void) i915_quiescent(dev);
+
+		if (!(fence_flags & DRM_FENCE_FLAG_NO_USER)) {
+
+			/*
+			 * Communicate to user-space that
+			 * fence creation has failed and that
+			 * the engine is idle.
+			 */
+
+			fence_arg->handle = ~0;
+			fence_arg->error = ret;
+		}
+
+		drm_putback_buffer_objects(dev);
+		if (fence_p)
+		    *fence_p = NULL;
+		return;
+	}
+
+	if (!(fence_flags & DRM_FENCE_FLAG_NO_USER)) {
+
+		ret = drm_fence_add_user_object(file_priv, fence,
+						fence_flags &
+						DRM_FENCE_FLAG_SHAREABLE);
+		if (!ret)
+			drm_fence_fill_arg(fence, fence_arg);
+		else {
+			/*
+			 * Fence user object creation failed.
+			 * We must idle the engine here as well, as user-
+			 * space expects a fence object to wait on. Since we
+			 * have a fence object we wait for it to signal
+			 * to indicate engine "sufficiently" idle.
+			 */
+
+			(void) drm_fence_object_wait(fence, 0, 1,
+						     fence->type);
+			drm_fence_usage_deref_unlocked(&fence);
+			fence_arg->handle = ~0;
+			fence_arg->error = ret;
+		}
+	}
+
+	if (fence_p)
+		*fence_p = fence;
+	else if (fence)
+		drm_fence_usage_deref_unlocked(&fence);
+}
+
+static int i915_execbuffer(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
+		master_priv->sarea_priv;
+	struct drm_i915_execbuffer *exec_buf = data;
+	struct drm_i915_batchbuffer *batch = &exec_buf->batch;
+	struct drm_fence_arg *fence_arg = &exec_buf->fence_arg;
+	int num_buffers;
+	int ret;
+	struct drm_i915_validate_buffer *buffers;
+
+	if (!dev_priv->allow_batchbuffer) {
+		DRM_ERROR("Batchbuffer ioctl disabled\n");
+		return -EINVAL;
+	}
+
+
+	if (batch->num_cliprects && DRM_VERIFYAREA_READ(batch->cliprects,
+							batch->num_cliprects *
+							sizeof(struct drm_clip_rect)))
+		return -EFAULT;
+
+	if (exec_buf->num_buffers > dev_priv->max_validate_buffers)
+		return -EINVAL;
+
+
+	ret = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	/*
+	 * The cmdbuf_mutex makes sure the validate-submit-fence
+	 * operation is atomic.
+	 */
+
+	ret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);
+	if (ret) {
+		drm_bo_read_unlock(&dev->bm.bm_lock);
+		return -EAGAIN;
+	}
+
+	num_buffers = exec_buf->num_buffers;
+
+	buffers = drm_calloc(num_buffers, sizeof(struct drm_i915_validate_buffer), DRM_MEM_DRIVER);
+	if (!buffers) {
+		drm_bo_read_unlock(&dev->bm.bm_lock);
+		mutex_unlock(&dev_priv->cmdbuf_mutex);
+		return -ENOMEM;
+	}
+
+	/* validate buffer list + fixup relocations */
+	ret = i915_validate_buffer_list(file_priv, 0, exec_buf->ops_list,
+					buffers, &num_buffers);
+	if (ret)
+		goto out_err0;
+
+	/* make sure all previous memory operations have passed */
+	DRM_MEMORYBARRIER();
+	drm_agp_chipset_flush(dev);
+
+	/* submit buffer */
+	batch->start = buffers[num_buffers-1].buffer->offset;
+
+	DRM_DEBUG("i915 exec batchbuffer, start %x used %d cliprects %d\n",
+		  batch->start, batch->used, batch->num_cliprects);
+
+	ret = i915_dispatch_batchbuffer(dev, batch);
+	if (ret)
+		goto out_err0;
+
+	if (sarea_priv)
+		sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+
+	i915_fence_or_sync(file_priv, fence_arg->flags, fence_arg, NULL);
+out_err0:
+
+	/* handle errors */
+	ret = i915_handle_copyback(dev, buffers, num_buffers, ret);
+	mutex_lock(&dev->struct_mutex);
+	i915_dereference_buffers_locked(buffers, num_buffers);
+	mutex_unlock(&dev->struct_mutex);
+
+	drm_free(buffers, (exec_buf->num_buffers * sizeof(struct drm_buffer_object *)), DRM_MEM_DRIVER);
+
+	mutex_unlock(&dev_priv->cmdbuf_mutex);
+	drm_bo_read_unlock(&dev->bm.bm_lock);
+	return ret;
+}
+
+static int i915_do_cleanup_pageflip(struct drm_device *dev)
 {
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	int i, planes, num_pages;
+
+	DRM_DEBUG("\n");
+	num_pages = master_priv->sarea_priv->third_handle ? 3 : 2;
+	for (i = 0, planes = 0; i < 2; i++) {
+		if (master_priv->sarea_priv->pf_current_page & (0x3 << (2 * i))) {
+			master_priv->sarea_priv->pf_current_page =
+				(master_priv->sarea_priv->pf_current_page &
+				 ~(0x3 << (2 * i))) | ((num_pages - 1) << (2 * i));
+
+			planes |= 1 << i;
+		}
+	}
+
+	if (planes)
+		i915_dispatch_flip(dev, planes, 0);
+
+	return 0;
+}
+
+static int i915_flip_bufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	drm_i915_flip_t *param = data;
+
 	DRM_DEBUG("%s\n", __FUNCTION__);
 
 	LOCK_TEST_WITH_RETURN(dev, file_priv);
 
-	return i915_dispatch_flip(dev);
+	/* This is really planes */
+	if (param->pipes & ~0x3) {
+		DRM_ERROR("Invalid planes 0x%x, only <= 0x3 is valid\n",
+			  param->pipes);
+		return -EINVAL;
+	}
+
+	i915_dispatch_flip(dev, param->pipes, 0);
+
+	return 0;
 }
 
 static int i915_getparam(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_getparam_t *param = data;
 	int value;
 
@@ -671,6 +1414,9 @@ static int i915_getparam(struct drm_device *dev, void *data,
 	case I915_PARAM_LAST_DISPATCH:
 		value = READ_BREADCRUMB(dev_priv);
 		break;
+	case I915_PARAM_CHIPSET_ID:
+		value = dev->pci_device;
+		break;
 	default:
 		DRM_ERROR("Unknown parameter %d\n", param->param);
 		return -EINVAL;
@@ -687,7 +1433,7 @@ static int i915_getparam(struct drm_device *dev, void *data,
 static int i915_setparam(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_setparam_t *param = data;
 
 	if (!dev_priv) {
@@ -714,10 +1460,67 @@ static int i915_setparam(struct drm_device *dev, void *data,
 	return 0;
 }
 
+drm_i915_mmio_entry_t mmio_table[] = {
+	[MMIO_REGS_PS_DEPTH_COUNT] = {
+		I915_MMIO_MAY_READ|I915_MMIO_MAY_WRITE,
+		0x2350,
+		8
+	}
+};
+
+static int mmio_table_size = sizeof(mmio_table)/sizeof(drm_i915_mmio_entry_t);
+
+static int i915_mmio(struct drm_device *dev, void *data,
+		     struct drm_file *file_priv)
+{
+	uint32_t buf[8];
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	drm_i915_mmio_entry_t *e;
+	drm_i915_mmio_t *mmio = data;
+	void __iomem *base;
+	int i;
+
+	if (!dev_priv) {
+		DRM_ERROR("called with no initialization\n");
+		return -EINVAL;
+	}
+
+	if (mmio->reg >= mmio_table_size)
+		return -EINVAL;
+
+	e = &mmio_table[mmio->reg];
+	base = (u8 *) dev_priv->mmio_map->handle + e->offset;
+
+	switch (mmio->read_write) {
+	case I915_MMIO_READ:
+		if (!(e->flag & I915_MMIO_MAY_READ))
+			return -EINVAL;
+		for (i = 0; i < e->size / 4; i++)
+			buf[i] = I915_READ(e->offset + i * 4);
+		if (DRM_COPY_TO_USER(mmio->data, buf, e->size)) {
+			DRM_ERROR("DRM_COPY_TO_USER failed\n");
+			return -EFAULT;
+		}
+		break;
+
+	case I915_MMIO_WRITE:
+		if (!(e->flag & I915_MMIO_MAY_WRITE))
+			return -EINVAL;
+		if (DRM_COPY_FROM_USER(buf, mmio->data, e->size)) {
+			DRM_ERROR("DRM_COPY_TO_USER failed\n");
+			return -EFAULT;
+		}
+		for (i = 0; i < e->size / 4; i++)
+			I915_WRITE(e->offset + i * 4, buf[i]);
+		break;
+	}
+	return 0;
+}
+
 static int i915_set_status_page(struct drm_device *dev, void *data,
 				struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_hws_addr_t *hws = data;
 
 	if (!I915_NEED_GFX_HWS(dev))
@@ -756,69 +1559,6 @@ static int i915_set_status_page(struct drm_device *dev, void *data,
 	return 0;
 }
 
-int i915_driver_load(struct drm_device *dev, unsigned long flags)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long base, size;
-	int ret = 0, mmio_bar = IS_I9XX(dev) ? 0 : 1;
-
-	/* i915 has 4 more counters */
-	dev->counters += 4;
-	dev->types[6] = _DRM_STAT_IRQ;
-	dev->types[7] = _DRM_STAT_PRIMARY;
-	dev->types[8] = _DRM_STAT_SECONDARY;
-	dev->types[9] = _DRM_STAT_DMA;
-
-	dev_priv = drm_alloc(sizeof(drm_i915_private_t), DRM_MEM_DRIVER);
-	if (dev_priv == NULL)
-		return -ENOMEM;
-
-	memset(dev_priv, 0, sizeof(drm_i915_private_t));
-
-	dev->dev_private = (void *)dev_priv;
-
-	/* Add register map (needed for suspend/resume) */
-	base = drm_get_resource_start(dev, mmio_bar);
-	size = drm_get_resource_len(dev, mmio_bar);
-
-	ret = drm_addmap(dev, base, size, _DRM_REGISTERS,
-			 _DRM_KERNEL | _DRM_DRIVER,
-			 &dev_priv->mmio_map);
-	return ret;
-}
-
-int i915_driver_unload(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (dev_priv->mmio_map)
-		drm_rmmap(dev, dev_priv->mmio_map);
-
-	drm_free(dev->dev_private, sizeof(drm_i915_private_t),
-		 DRM_MEM_DRIVER);
-
-	return 0;
-}
-
-void i915_driver_lastclose(struct drm_device * dev)
-{
-	drm_i915_private_t *dev_priv = dev->dev_private;
-
-	if (!dev_priv)
-		return;
-
-	if (dev_priv->agp_heap)
-		i915_mem_takedown(&(dev_priv->agp_heap));
-
-	i915_dma_cleanup(dev);
-}
-
-void i915_driver_preclose(struct drm_device * dev, struct drm_file *file_priv)
-{
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	i915_mem_release(dev, file_priv, dev_priv->agp_heap);
-}
-
 struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF(DRM_I915_INIT, i915_dma_init, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
 	DRM_IOCTL_DEF(DRM_I915_FLUSH, i915_flush_ioctl, DRM_AUTH),
@@ -836,7 +1576,9 @@ struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF(DRM_I915_SET_VBLANK_PIPE,  i915_vblank_pipe_set, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY ),
 	DRM_IOCTL_DEF(DRM_I915_GET_VBLANK_PIPE,  i915_vblank_pipe_get, DRM_AUTH ),
 	DRM_IOCTL_DEF(DRM_I915_VBLANK_SWAP, i915_vblank_swap, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_I915_MMIO, i915_mmio, DRM_AUTH),
 	DRM_IOCTL_DEF(DRM_I915_HWS_ADDR, i915_set_status_page, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_I915_EXECBUFFER, i915_execbuffer, DRM_AUTH),
 };
 
 int i915_max_ioctl = DRM_ARRAY_SIZE(i915_ioctls);
@@ -852,7 +1594,8 @@ int i915_max_ioctl = DRM_ARRAY_SIZE(i915_ioctls);
  * \returns
  * A value of 1 is always retured to indictate every i9x5 is AGP.
  */
-int i915_driver_device_is_agp(struct drm_device * dev)
+int i915_driver_device_is_agp(struct drm_device *dev)
 {
 	return 1;
 }
+
diff --git a/drivers/char/drm/i915_drm.h b/drivers/char/drm/i915_drm.h
index 05c66cf..1c45711 100644
--- a/drivers/char/drm/i915_drm.h
+++ b/drivers/char/drm/i915_drm.h
@@ -43,7 +43,12 @@ typedef struct _drm_i915_init {
 	enum {
 		I915_INIT_DMA = 0x01,
 		I915_CLEANUP_DMA = 0x02,
-		I915_RESUME_DMA = 0x03
+		I915_RESUME_DMA = 0x03,
+
+		/* Since this struct isn't versioned, just used a new
+		 * 'func' code to indicate the presence of dri2 sarea
+		 * info. */
+		I915_INIT_DMA2 = 0x04
 	} func;
 	unsigned int mmio_offset;
 	int sarea_priv_offset;
@@ -61,9 +66,10 @@ typedef struct _drm_i915_init {
 	unsigned int depth_pitch;
 	unsigned int cpp;
 	unsigned int chipset;
+	unsigned int sarea_handle;
 } drm_i915_init_t;
 
-typedef struct _drm_i915_sarea {
+typedef struct drm_i915_sarea {
 	struct drm_tex_region texList[I915_NR_TEX_REGIONS + 1];
 	int last_upload;	/* last time texture was uploaded */
 	int last_enqueue;	/* last time a buffer was enqueued */
@@ -105,16 +111,41 @@ typedef struct _drm_i915_sarea {
 	unsigned int rotated_tiled;
 	unsigned int rotated2_tiled;
 
-	int pipeA_x;
-	int pipeA_y;
-	int pipeA_w;
-	int pipeA_h;
-	int pipeB_x;
-	int pipeB_y;
-	int pipeB_w;
-	int pipeB_h;
+	int planeA_x;
+	int planeA_y;
+	int planeA_w;
+	int planeA_h;
+	int planeB_x;
+	int planeB_y;
+	int planeB_w;
+	int planeB_h;
+
+	/* Triple buffering */
+	drm_handle_t third_handle;
+	int third_offset;
+	int third_size;
+	unsigned int third_tiled;
+
+	/* buffer object handles for the static buffers.  May change
+	 * over the lifetime of the client, though it doesn't in our current
+	 * implementation.
+	 */
+	unsigned int front_bo_handle;
+	unsigned int back_bo_handle;
+	unsigned int third_bo_handle;
+	unsigned int depth_bo_handle;
 } drm_i915_sarea_t;
 
+/* Driver specific fence types and classes.
+ */
+
+/* The only fence class we support */
+#define DRM_I915_FENCE_CLASS_ACCEL 0
+/* Fence type that guarantees read-write flush */
+#define DRM_I915_FENCE_TYPE_RW 2
+/* MI_FLUSH programmed just before the fence */
+#define DRM_I915_FENCE_FLAG_FLUSHED 0x01000000
+
 /* Flags for perf_boxes
  */
 #define I915_BOX_RING_EMPTY    0x1
@@ -142,11 +173,13 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_SET_VBLANK_PIPE	0x0d
 #define DRM_I915_GET_VBLANK_PIPE	0x0e
 #define DRM_I915_VBLANK_SWAP	0x0f
+#define DRM_I915_MMIO		0x10
 #define DRM_I915_HWS_ADDR	0x11
+#define DRM_I915_EXECBUFFER	0x12
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
-#define DRM_IOCTL_I915_FLIP		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLIP)
+#define DRM_IOCTL_I915_FLIP		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_FLIP, drm_i915_flip_t)
 #define DRM_IOCTL_I915_BATCHBUFFER	DRM_IOW( DRM_COMMAND_BASE + DRM_I915_BATCHBUFFER, drm_i915_batchbuffer_t)
 #define DRM_IOCTL_I915_IRQ_EMIT         DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_IRQ_EMIT, drm_i915_irq_emit_t)
 #define DRM_IOCTL_I915_IRQ_WAIT         DRM_IOW( DRM_COMMAND_BASE + DRM_I915_IRQ_WAIT, drm_i915_irq_wait_t)
@@ -160,11 +193,25 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_SET_VBLANK_PIPE	DRM_IOW( DRM_COMMAND_BASE + DRM_I915_SET_VBLANK_PIPE, drm_i915_vblank_pipe_t)
 #define DRM_IOCTL_I915_GET_VBLANK_PIPE	DRM_IOR( DRM_COMMAND_BASE + DRM_I915_GET_VBLANK_PIPE, drm_i915_vblank_pipe_t)
 #define DRM_IOCTL_I915_VBLANK_SWAP	DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_VBLANK_SWAP, drm_i915_vblank_swap_t)
+#define DRM_IOCTL_I915_MMIO             DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_MMIO, drm_i915_vblank_swap_t)
+#define DRM_IOCTL_I915_EXECBUFFER	DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_EXECBUFFER, struct drm_i915_execbuffer)
+
+/* Asynchronous page flipping:
+ */
+typedef struct drm_i915_flip {
+	/*
+	 * This is really talking about planes, and we could rename it
+	 * except for the fact that some of the duplicated i915_drm.h files
+	 * out there check for HAVE_I915_FLIP and so might pick up this
+	 * version.
+	 */
+	int pipes;
+} drm_i915_flip_t;
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
  */
-typedef struct _drm_i915_batchbuffer {
+typedef struct drm_i915_batchbuffer {
 	int start;		/* agp offset */
 	int used;		/* nr bytes in use */
 	int DR1;		/* hw flags for GFX_OP_DRAWRECT_INFO */
@@ -176,7 +223,7 @@ typedef struct _drm_i915_batchbuffer {
 /* As above, but pass a pointer to userspace buffer which can be
  * validated by the kernel prior to sending to hardware.
  */
-typedef struct _drm_i915_cmdbuffer {
+typedef struct drm_i915_cmdbuffer {
 	char __user *buf;	/* pointer to userspace command buffer */
 	int sz;			/* nr bytes in buf */
 	int DR1;		/* hw flags for GFX_OP_DRAWRECT_INFO */
@@ -200,6 +247,7 @@ typedef struct drm_i915_irq_wait {
 #define I915_PARAM_IRQ_ACTIVE            1
 #define I915_PARAM_ALLOW_BATCHBUFFER     2
 #define I915_PARAM_LAST_DISPATCH         3
+#define I915_PARAM_CHIPSET_ID            4
 
 typedef struct drm_i915_getparam {
 	int param;
@@ -263,8 +311,73 @@ typedef struct drm_i915_vblank_swap {
 	unsigned int sequence;
 } drm_i915_vblank_swap_t;
 
+#define I915_MMIO_READ	0
+#define I915_MMIO_WRITE 1
+
+#define I915_MMIO_MAY_READ	0x1
+#define I915_MMIO_MAY_WRITE	0x2
+
+#define MMIO_REGS_IA_PRIMATIVES_COUNT		0
+#define MMIO_REGS_IA_VERTICES_COUNT		1
+#define MMIO_REGS_VS_INVOCATION_COUNT		2
+#define MMIO_REGS_GS_PRIMITIVES_COUNT		3
+#define MMIO_REGS_GS_INVOCATION_COUNT		4
+#define MMIO_REGS_CL_PRIMITIVES_COUNT		5
+#define MMIO_REGS_CL_INVOCATION_COUNT		6
+#define MMIO_REGS_PS_INVOCATION_COUNT		7
+#define MMIO_REGS_PS_DEPTH_COUNT		8
+
+typedef struct drm_i915_mmio_entry {
+	unsigned int flag;
+	unsigned int offset;
+	unsigned int size;
+} drm_i915_mmio_entry_t;
+
+typedef struct drm_i915_mmio {
+	unsigned int read_write:1;
+	unsigned int reg:31;
+	void __user *data;
+} drm_i915_mmio_t;
+
 typedef struct drm_i915_hws_addr {
 	uint64_t addr;
 } drm_i915_hws_addr_t;
 
+/*
+ * Relocation header is 4 uint32_ts
+ * 0 - 32 bit reloc count
+ * 1 - 32-bit relocation type
+ * 2-3 - 64-bit user buffer handle ptr for another list of relocs.
+ */
+#define I915_RELOC_HEADER 4
+
+/*
+ * type 0 relocation has 4-uint32_t stride
+ * 0 - offset into buffer
+ * 1 - delta to add in
+ * 2 - buffer handle
+ * 3 - reserved (for optimisations later).
+ */
+#define I915_RELOC_TYPE_0 0
+#define I915_RELOC0_STRIDE 4
+
+struct drm_i915_op_arg {
+	uint64_t next;
+	uint64_t reloc_ptr;
+	int handled;
+	union {
+		struct drm_bo_op_req req;
+		struct drm_bo_arg_rep rep;
+	} d;
+
+};
+
+struct drm_i915_execbuffer {
+	uint64_t ops_list;
+	uint32_t num_buffers;
+	struct drm_i915_batchbuffer batch;
+	drm_context_t context; /* for lockless use in the future */
+	struct drm_fence_arg fence_arg;
+};
+
 #endif				/* _I915_DRM_H_ */
diff --git a/drivers/char/drm/i915_drv.c b/drivers/char/drm/i915_drv.c
index 93aed1c..98f7b25 100644
--- a/drivers/char/drm/i915_drv.c
+++ b/drivers/char/drm/i915_drv.c
@@ -34,9 +34,33 @@
 
 #include "drm_pciids.h"
 
+#include "intel_drv.h"
 static struct pci_device_id pciidlist[] = {
 	i915_PCI_IDS
 };
+MODULE_DEVICE_TABLE(pci, pciidlist);
+extern struct drm_fence_driver i915_fence_driver;
+
+unsigned int i915_modeset = 0;
+module_param_named(modeset, i915_modeset, int, 0400);
+
+static uint32_t i915_mem_prios[] = {DRM_BO_MEM_VRAM, DRM_BO_MEM_TT, DRM_BO_MEM_LOCAL};
+static uint32_t i915_busy_prios[] = {DRM_BO_MEM_TT, DRM_BO_MEM_VRAM, DRM_BO_MEM_LOCAL};
+
+static struct drm_bo_driver i915_bo_driver = {
+	.mem_type_prio = i915_mem_prios,
+	.mem_busy_prio = i915_busy_prios,
+	.num_mem_type_prio = sizeof(i915_mem_prios)/sizeof(uint32_t),
+	.num_mem_busy_prio = sizeof(i915_busy_prios)/sizeof(uint32_t),
+	.create_ttm_backend_entry = i915_create_ttm_backend_entry,
+	.fence_type = i915_fence_type,
+	.invalidate_caches = i915_invalidate_caches,
+	.init_mem_type = i915_init_mem_type,
+	.evict_flags = i915_evict_flags,
+	.move = i915_move,
+	.ttm_cache_flush = i915_flush_ttm,
+	.command_stream_barrier = NULL,
+};
 
 enum pipe {
     PIPE_A = 0,
@@ -545,6 +569,7 @@ static struct drm_driver driver = {
 	    DRIVER_IRQ_VBL2,
 	.load = i915_driver_load,
 	.unload = i915_driver_unload,
+	.firstopen = i915_driver_firstopen,
 	.lastclose = i915_driver_lastclose,
 	.preclose = i915_driver_preclose,
 	.suspend = i915_suspend,
@@ -559,6 +584,11 @@ static struct drm_driver driver = {
 	.reclaim_buffers = drm_core_reclaim_buffers,
 	.get_map_ofs = drm_core_get_map_ofs,
 	.get_reg_ofs = drm_core_get_reg_ofs,
+	.fb_probe = intelfb_probe,
+	.fb_remove = intelfb_remove,
+	.fb_resize = intelfb_resize,
+	.master_create = i915_master_create,
+	.master_destroy = i915_master_destroy,
 	.ioctls = i915_ioctls,
 	.fops = {
 		 .owner = THIS_MODULE,
@@ -577,7 +607,8 @@ static struct drm_driver driver = {
 		 .name = DRIVER_NAME,
 		 .id_table = pciidlist,
 	},
-
+	.fence_driver = &i915_fence_driver,
+	.bo_driver = &i915_bo_driver,
 	.name = DRIVER_NAME,
 	.desc = DRIVER_DESC,
 	.date = DRIVER_DATE,
@@ -589,6 +620,8 @@ static struct drm_driver driver = {
 static int __init i915_init(void)
 {
 	driver.num_ioctls = i915_max_ioctl;
+	if (i915_modeset == 1)
+		driver.driver_features |= DRIVER_MODESET;
 	return drm_init(&driver);
 }
 
diff --git a/drivers/char/drm/i915_drv.h b/drivers/char/drm/i915_drv.h
index d7326d9..a33b78c 100644
--- a/drivers/char/drm/i915_drv.h
+++ b/drivers/char/drm/i915_drv.h
@@ -37,7 +37,7 @@
 
 #define DRIVER_NAME		"i915"
 #define DRIVER_DESC		"Intel Graphics"
-#define DRIVER_DATE		"20060119"
+#define DRIVER_DATE		"20071122"
 
 /* Interface history:
  *
@@ -48,12 +48,19 @@
  * 1.5: Add vblank pipe configuration
  * 1.6: - New ioctl for scheduling buffer swaps on vertical blank
  *      - Support vertical blank on secondary display pipe
+ * 1.8: New ioctl for ARB_Occlusion_Query
+ * 1.9: Usable page flipping and triple buffering
+ * 1.10: Plane/pipe disentangling
+ * 1.11: TTM superioctl
+ * 1.12: TTM relocation optimization
  */
 #define DRIVER_MAJOR		1
-#define DRIVER_MINOR		6
+#define DRIVER_MINOR		12
 #define DRIVER_PATCHLEVEL	0
 
-typedef struct _drm_i915_ring_buffer {
+#define I915_MAX_VALIDATE_BUFFERS 4096
+
+typedef struct drm_i915_ring_buffer {
 	int tail_mask;
 	unsigned long Start;
 	unsigned long End;
@@ -76,29 +83,35 @@ struct mem_block {
 typedef struct _drm_i915_vbl_swap {
 	struct list_head head;
 	drm_drawable_t drw_id;
-	unsigned int pipe;
+	unsigned int plane;
 	unsigned int sequence;
+	int flip;
+	struct drm_minor *minor;
 } drm_i915_vbl_swap_t;
 
-typedef struct drm_i915_private {
+struct drm_i915_master_private {
 	drm_local_map_t *sarea;
+	struct drm_i915_sarea *sarea_priv;
+};
+	
+struct drm_i915_private {
+	struct drm_buffer_object *ring_buffer;
+
 	drm_local_map_t *mmio_map;
 
-	drm_i915_sarea_t *sarea_priv;
-	drm_i915_ring_buffer_t ring;
+	unsigned long mmiobase;
+	unsigned long mmiolen;
+
+	struct drm_i915_ring_buffer ring;
 
-	drm_dma_handle_t *status_page_dmah;
+	struct drm_dma_handle *status_page_dmah;
 	void *hw_status_page;
 	dma_addr_t dma_status_page;
-	unsigned long counter;
+	uint32_t counter;
 	unsigned int status_gfx_addr;
 	drm_local_map_t hws_map;
 
 	unsigned int cpp;
-	int back_offset;
-	int front_offset;
-	int current_page;
-	int page_flipping;
 	int use_mi_batchbuffer_start;
 
 	wait_queue_head_t irq_queue;
@@ -110,11 +123,38 @@ typedef struct drm_i915_private {
 	struct mem_block *agp_heap;
 	unsigned int sr01, adpa, ppcr, dvob, dvoc, lvds;
 	int vblank_pipe;
+	spinlock_t user_irq_lock;
+	int user_irq_refcount;
+	int fence_irq_on;
+	uint32_t irq_enable_reg;
+	int irq_enabled;
+
+	struct workqueue_struct *wq;
+
+	bool cursor_needs_physical;
+
+	uint32_t flush_sequence;
+	uint32_t flush_flags;
+	uint32_t flush_pending;
+	uint32_t saved_flush_status;
+	void *agp_iomap;
+	unsigned int max_validate_buffers;
+	struct mutex cmdbuf_mutex;
+	size_t stolen_base;
 
 	spinlock_t swaps_lock;
 	drm_i915_vbl_swap_t vbl_swaps;
 	unsigned int swaps_pending;
 
+	/* LVDS info */
+	int backlight_duty_cycle;  /* restore backlight to this value */
+	bool panel_wants_dither;
+	struct drm_display_mode *panel_fixed_mode;
+
+	/* DRI2 sarea */
+	struct drm_buffer_object *sarea_bo;
+	struct drm_bo_kmap_obj sarea_kmap;
+
 	/* Register state */
 	u8 saveLBB;
 	u32 saveDSPACNTR;
@@ -203,22 +243,34 @@ typedef struct drm_i915_private {
 	u8 saveDACMASK;
 	u8 saveDACDATA[256*3]; /* 256 3-byte colors */
 	u8 saveCR[37];
-} drm_i915_private_t;
+};
 
+enum intel_chip_family {
+	CHIP_I8XX = 0x01,
+	CHIP_I9XX = 0x02,
+	CHIP_I915 = 0x04,
+	CHIP_I965 = 0x08,
+};
 extern struct drm_ioctl_desc i915_ioctls[];
 extern int i915_max_ioctl;
 
+extern int i915_master_create(struct drm_device *dev, struct drm_master *master);
+extern void i915_master_destroy(struct drm_device *dev, struct drm_master *master);
 				/* i915_dma.c */
 extern void i915_kernel_lost_context(struct drm_device * dev);
 extern int i915_driver_load(struct drm_device *, unsigned long flags);
 extern int i915_driver_unload(struct drm_device *);
 extern void i915_driver_lastclose(struct drm_device * dev);
+extern int i915_driver_firstopen(struct drm_device *dev);
 extern void i915_driver_preclose(struct drm_device *dev,
 				 struct drm_file *file_priv);
 extern int i915_driver_device_is_agp(struct drm_device * dev);
 extern long i915_compat_ioctl(struct file *filp, unsigned int cmd,
 			      unsigned long arg);
-
+extern void i915_emit_breadcrumb(struct drm_device *dev);
+extern void i915_dispatch_flip(struct drm_device * dev, int pipes, int sync);
+extern int i915_emit_mi_flush(struct drm_device *dev, uint32_t flush);
+extern int i915_dma_cleanup(struct drm_device *dev);
 /* i915_irq.c */
 extern int i915_irq_emit(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv);
@@ -235,9 +287,12 @@ extern int i915_vblank_pipe_set(struct drm_device *dev, void *data,
 				struct drm_file *file_priv);
 extern int i915_vblank_pipe_get(struct drm_device *dev, void *data,
 				struct drm_file *file_priv);
+extern int i915_emit_irq(struct drm_device *dev);
+extern void i915_user_irq_on(struct drm_i915_private *dev_priv);
+extern void i915_user_irq_off(struct drm_i915_private *dev_priv);
 extern int i915_vblank_swap(struct drm_device *dev, void *data,
 			    struct drm_file *file_priv);
-
+extern void i915_enable_interrupt (struct drm_device *dev);
 /* i915_mem.c */
 extern int i915_mem_alloc(struct drm_device *dev, void *data,
 			  struct drm_file *file_priv);
@@ -249,7 +304,30 @@ extern int i915_mem_destroy_heap(struct drm_device *dev, void *data,
 				 struct drm_file *file_priv);
 extern void i915_mem_takedown(struct mem_block **heap);
 extern void i915_mem_release(struct drm_device * dev,
-			     struct drm_file *file_priv, struct mem_block *heap);
+			     struct drm_file *file_priv,
+			     struct mem_block *heap);
+/* i915_fence.c */
+extern void i915_fence_handler(struct drm_device *dev);
+extern void i915_invalidate_reported_sequence(struct drm_device *dev);
+
+/* i915_buffer.c */
+extern struct drm_ttm_backend *i915_create_ttm_backend_entry(struct drm_device *dev);
+extern int i915_fence_type(struct drm_buffer_object *bo, uint32_t *fclass,
+			   uint32_t *type);
+extern int i915_invalidate_caches(struct drm_device *dev, uint64_t buffer_flags);
+extern int i915_init_mem_type(struct drm_device *dev, uint32_t type,
+			       struct drm_mem_type_manager *man);
+extern uint64_t i915_evict_flags(struct drm_buffer_object *bo);
+extern int i915_move(struct drm_buffer_object *bo, int evict,
+		int no_wait, struct drm_bo_mem_reg *new_mem);
+void i915_flush_ttm(struct drm_ttm *ttm);
+
+extern void intel_init_chipset_flush_compat(struct drm_device *dev);
+extern void intel_fini_chipset_flush_compat(struct drm_device *dev);
+
+/* modesetting */
+extern void intel_modeset_init(struct drm_device *dev);
+extern void intel_modeset_cleanup(struct drm_device *dev);
 
 #define I915_READ(reg)          DRM_READ32(dev_priv->mmio_map, (reg))
 #define I915_WRITE(reg,val)     DRM_WRITE32(dev_priv->mmio_map, (reg), (val))
@@ -287,8 +365,31 @@ extern void i915_mem_release(struct drm_device * dev,
 	I915_WRITE(LP_RING + RING_TAIL, outring);			\
 } while(0)
 
+#define MI_NOOP	(0x00 << 23)
+
 extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 
+/*
+ * The Bridge device's PCI config space has information about the
+ * fb aperture size and the amount of pre-reserved memory.
+ */
+#define INTEL_GMCH_CTRL		0x52
+#define INTEL_GMCH_ENABLED	0x4
+#define INTEL_GMCH_MEM_MASK	0x1
+#define INTEL_GMCH_MEM_64M	0x1
+#define INTEL_GMCH_MEM_128M	0
+
+#define INTEL_855_GMCH_GMS_MASK		(0x7 << 4)
+#define INTEL_855_GMCH_GMS_DISABLED	(0x0 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_1M	(0x1 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_4M	(0x2 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_8M	(0x3 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_16M	(0x4 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_32M	(0x5 << 4)
+
+#define INTEL_915G_GMCH_GMS_STOLEN_48M	(0x6 << 4)
+#define INTEL_915G_GMCH_GMS_STOLEN_64M	(0x7 << 4)
+
 /* Extended config space */
 #define LBB 0xf4
 
@@ -339,15 +440,24 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define CMD_STORE_DWORD_IDX		((0x21<<23) | 0x1)
 #define CMD_OP_BATCH_BUFFER  ((0x0<<29)|(0x30<<23)|0x1)
 
-#define INST_PARSER_CLIENT   0x00000000
-#define INST_OP_FLUSH        0x02000000
-#define INST_FLUSH_MAP_CACHE 0x00000001
+#define CMD_MI_FLUSH         (0x04 << 23)
+#define MI_NO_WRITE_FLUSH    (1 << 2)
+#define MI_READ_FLUSH        (1 << 0)
+#define MI_EXE_FLUSH         (1 << 1)
+#define MI_END_SCENE         (1 << 4) /* flush binner and incr scene count */
+#define MI_SCENE_COUNT       (1 << 3) /* just increment scene count */
+
+/* Packet to load a register value from the ring/batch command stream:
+ */
+#define CMD_MI_LOAD_REGISTER_IMM	((0x22 << 23)|0x1)
 
 #define BB1_START_ADDR_MASK   (~0x7)
 #define BB1_PROTECTED         (1<<0)
 #define BB1_UNPROTECTED       (0<<0)
 #define BB2_END_ADDR_MASK     (~0x7)
 
+#define I915REG_HWS_PGA		0x02080
+
 /* Framebuffer compression */
 #define FBC_CFB_BASE		0x03200 /* 4k page aligned */
 #define FBC_LL_BASE		0x03204 /* 4k page aligned */
@@ -390,13 +500,104 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define I915REG_INT_IDENTITY_R	0x020a4
 #define I915REG_INT_MASK_R	0x020a8
 #define I915REG_INT_ENABLE_R	0x020a0
+#define I915REG_INSTPM	        0x020c0
+
+#define PIPEADSL		0x70000
+#define PIPEBDSL		0x71000
 
 #define I915REG_PIPEASTAT	0x70024
 #define I915REG_PIPEBSTAT	0x71024
+/*
+ * The two pipe frame counter registers are not synchronized, so
+ * reading a stable value is somewhat tricky. The following code 
+ * should work:
+ *
+ *  do {
+ *    high1 = ((INREG(PIPEAFRAMEHIGH) & PIPE_FRAME_HIGH_MASK) >>
+ *             PIPE_FRAME_HIGH_SHIFT;
+ *    low1 =  ((INREG(PIPEAFRAMEPIXEL) & PIPE_FRAME_LOW_MASK) >>
+ *             PIPE_FRAME_LOW_SHIFT);
+ *    high2 = ((INREG(PIPEAFRAMEHIGH) & PIPE_FRAME_HIGH_MASK) >>
+ *             PIPE_FRAME_HIGH_SHIFT);
+ *  } while (high1 != high2);
+ *  frame = (high1 << 8) | low1;
+ */
+#define PIPEAFRAMEHIGH          0x70040
+#define PIPEBFRAMEHIGH		0x71040
+#define PIPE_FRAME_HIGH_MASK    0x0000ffff
+#define PIPE_FRAME_HIGH_SHIFT   0
+#define PIPEAFRAMEPIXEL         0x70044
+#define PIPEBFRAMEPIXEL		0x71044
+
+#define PIPE_FRAME_LOW_MASK     0xff000000
+#define PIPE_FRAME_LOW_SHIFT    24
+/*
+ * Pixel within the current frame is counted in the PIPEAFRAMEPIXEL register
+ * and is 24 bits wide.
+ */
+#define PIPE_PIXEL_MASK         0x00ffffff
+#define PIPE_PIXEL_SHIFT        0
 
 #define I915_VBLANK_INTERRUPT_ENABLE	(1UL<<17)
 #define I915_VBLANK_CLEAR		(1UL<<1)
 
+#define GPIOA			0x5010
+#define GPIOB			0x5014
+#define GPIOC			0x5018
+#define GPIOD			0x501c
+#define GPIOE			0x5020
+#define GPIOF			0x5024
+#define GPIOG			0x5028
+#define GPIOH			0x502c
+# define GPIO_CLOCK_DIR_MASK		(1 << 0)
+# define GPIO_CLOCK_DIR_IN		(0 << 1)
+# define GPIO_CLOCK_DIR_OUT		(1 << 1)
+# define GPIO_CLOCK_VAL_MASK		(1 << 2)
+# define GPIO_CLOCK_VAL_OUT		(1 << 3)
+# define GPIO_CLOCK_VAL_IN		(1 << 4)
+# define GPIO_CLOCK_PULLUP_DISABLE	(1 << 5)
+# define GPIO_DATA_DIR_MASK		(1 << 8)
+# define GPIO_DATA_DIR_IN		(0 << 9)
+# define GPIO_DATA_DIR_OUT		(1 << 9)
+# define GPIO_DATA_VAL_MASK		(1 << 10)
+# define GPIO_DATA_VAL_OUT		(1 << 11)
+# define GPIO_DATA_VAL_IN		(1 << 12)
+# define GPIO_DATA_PULLUP_DISABLE	(1 << 13)
+
+/* p317, 319
+ */
+#define VCLK2_VCO_M        0x6008 /* treat as 16 bit? (includes msbs) */
+#define VCLK2_VCO_N        0x600a
+#define VCLK2_VCO_DIV_SEL  0x6012
+
+#define VCLK_DIVISOR_VGA0   0x6000
+#define VCLK_DIVISOR_VGA1   0x6004
+#define VCLK_POST_DIV	    0x6010
+/** Selects a post divisor of 4 instead of 2. */
+# define VGA1_PD_P2_DIV_4	(1 << 15)
+/** Overrides the p2 post divisor field */
+# define VGA1_PD_P1_DIV_2	(1 << 13)
+# define VGA1_PD_P1_SHIFT	8
+/** P1 value is 2 greater than this field */
+# define VGA1_PD_P1_MASK	(0x1f << 8)
+/** Selects a post divisor of 4 instead of 2. */
+# define VGA0_PD_P2_DIV_4	(1 << 7)
+/** Overrides the p2 post divisor field */
+# define VGA0_PD_P1_DIV_2	(1 << 5)
+# define VGA0_PD_P1_SHIFT	0
+/** P1 value is 2 greater than this field */
+# define VGA0_PD_P1_MASK	(0x1f << 0)
+
+#define POST_DIV_SELECT        0x70
+#define POST_DIV_1             0x00
+#define POST_DIV_2             0x10
+#define POST_DIV_4             0x20
+#define POST_DIV_8             0x30
+#define POST_DIV_16            0x40
+#define POST_DIV_32            0x50
+#define VCO_LOOP_DIV_BY_4M     0x00
+#define VCO_LOOP_DIV_BY_16M    0x04
+
 #define SRX_INDEX		0x3c4
 #define SRX_DATA		0x3c5
 #define SR01			1
@@ -405,6 +606,8 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define PPCR			0x61204
 #define PPCR_ON			(1<<0)
 
+#define DVOA			0x61120
+#define DVOA_ON			(1<<31)
 #define DVOB			0x61140
 #define DVOB_ON			(1<<31)
 #define DVOC			0x61160
@@ -562,14 +765,20 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define GFX_OP_DESTBUFFER_VARS   ((0x3<<29)|(0x1d<<24)|(0x85<<16)|0x0)
 #define GFX_OP_DRAWRECT_INFO     ((0x3<<29)|(0x1d<<24)|(0x80<<16)|(0x3))
 
-#define GFX_OP_DRAWRECT_INFO_I965 ((0x7900<<16)|0x2)
+#define GFX_OP_DRAWRECT_INFO_I965  ((0x7900<<16)|0x2)
 
 #define SRC_COPY_BLT_CMD                ((2<<29)|(0x43<<22)|4)
 #define XY_SRC_COPY_BLT_CMD		((2<<29)|(0x53<<22)|6)
+#define XY_MONO_SRC_COPY_IMM_BLT	((2<<29)|(0x71<<22)|5)
 #define XY_SRC_COPY_BLT_WRITE_ALPHA	(1<<21)
 #define XY_SRC_COPY_BLT_WRITE_RGB	(1<<20)
 #define XY_SRC_COPY_BLT_SRC_TILED	(1<<15)
 #define XY_SRC_COPY_BLT_DST_TILED	(1<<11)
+#define   BLT_DEPTH_8			(0<<24)
+#define   BLT_DEPTH_16_565		(1<<24)
+#define   BLT_DEPTH_16_1555		(2<<24)
+#define   BLT_DEPTH_32			(3<<24)
+#define   BLT_ROP_GXCOPY		(0xcc<<16)
 
 #define MI_BATCH_BUFFER		((0x30<<23)|1)
 #define MI_BATCH_BUFFER_START	(0x31<<23)
@@ -600,13 +809,6 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 
 #define CMD_OP_DESTBUFFER_INFO	 ((0x3<<29)|(0x1d<<24)|(0x8e<<16)|1)
 
-#define CMD_MI_FLUSH         (0x04 << 23)
-#define MI_NO_WRITE_FLUSH    (1 << 2)
-#define MI_READ_FLUSH        (1 << 0)
-#define MI_EXE_FLUSH         (1 << 1)
-#define MI_END_SCENE         (1 << 4) /* flush binner and incr scene count */
-#define MI_SCENE_COUNT       (1 << 3) /* just increment scene count */
-
 #define BREADCRUMB_BITS 31
 #define BREADCRUMB_MASK ((1U << BREADCRUMB_BITS) - 1)
 
@@ -697,6 +899,14 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define BCLRPAT_B	0x61020
 #define VSYNCSHIFT_B	0x61028
 
+#define HACTIVE_MASK	0x00000fff
+#define VTOTAL_MASK	0x00001fff
+#define VTOTAL_SHIFT	16
+#define VACTIVE_MASK	0x00000fff
+#define VBLANK_END_MASK	0x00001fff
+#define VBLANK_END_SHIFT 16
+#define VBLANK_START_MASK 0x00001fff
+
 #define PP_STATUS	0x61200
 # define PP_ON					(1 << 31)
 /**
@@ -969,6 +1179,579 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 # define LVDS_B0B3_POWER_DOWN		(0 << 2)
 # define LVDS_B0B3_POWER_UP		(3 << 2)
 
+#define TV_CTL			0x68000
+/** Enables the TV encoder */
+# define TV_ENC_ENABLE			(1 << 31)
+/** Sources the TV encoder input from pipe B instead of A. */
+# define TV_ENC_PIPEB_SELECT		(1 << 30)
+/** Outputs composite video (DAC A only) */
+# define TV_ENC_OUTPUT_COMPOSITE	(0 << 28)
+/** Outputs SVideo video (DAC B/C) */
+# define TV_ENC_OUTPUT_SVIDEO		(1 << 28)
+/** Outputs Component video (DAC A/B/C) */
+# define TV_ENC_OUTPUT_COMPONENT	(2 << 28)
+/** Outputs Composite and SVideo (DAC A/B/C) */
+# define TV_ENC_OUTPUT_SVIDEO_COMPOSITE	(3 << 28)
+# define TV_TRILEVEL_SYNC		(1 << 21)
+/** Enables slow sync generation (945GM only) */
+# define TV_SLOW_SYNC			(1 << 20)
+/** Selects 4x oversampling for 480i and 576p */
+# define TV_OVERSAMPLE_4X		(0 << 18)
+/** Selects 2x oversampling for 720p and 1080i */
+# define TV_OVERSAMPLE_2X		(1 << 18)
+/** Selects no oversampling for 1080p */
+# define TV_OVERSAMPLE_NONE		(2 << 18)
+/** Selects 8x oversampling */
+# define TV_OVERSAMPLE_8X		(3 << 18)
+/** Selects progressive mode rather than interlaced */
+# define TV_PROGRESSIVE			(1 << 17)
+/** Sets the colorburst to PAL mode.  Required for non-M PAL modes. */
+# define TV_PAL_BURST			(1 << 16)
+/** Field for setting delay of Y compared to C */
+# define TV_YC_SKEW_MASK		(7 << 12)
+/** Enables a fix for 480p/576p standard definition modes on the 915GM only */
+# define TV_ENC_SDP_FIX			(1 << 11)
+/**
+ * Enables a fix for the 915GM only.
+ *
+ * Not sure what it does.
+ */
+# define TV_ENC_C0_FIX			(1 << 10)
+/** Bits that must be preserved by software */
+# define TV_CTL_SAVE			((3 << 8) | (3 << 6))
+# define TV_FUSE_STATE_MASK		(3 << 4)
+/** Read-only state that reports all features enabled */
+# define TV_FUSE_STATE_ENABLED		(0 << 4)
+/** Read-only state that reports that Macrovision is disabled in hardware*/
+# define TV_FUSE_STATE_NO_MACROVISION	(1 << 4)
+/** Read-only state that reports that TV-out is disabled in hardware. */
+# define TV_FUSE_STATE_DISABLED		(2 << 4)
+/** Normal operation */
+# define TV_TEST_MODE_NORMAL		(0 << 0)
+/** Encoder test pattern 1 - combo pattern */
+# define TV_TEST_MODE_PATTERN_1		(1 << 0)
+/** Encoder test pattern 2 - full screen vertical 75% color bars */
+# define TV_TEST_MODE_PATTERN_2		(2 << 0)
+/** Encoder test pattern 3 - full screen horizontal 75% color bars */
+# define TV_TEST_MODE_PATTERN_3		(3 << 0)
+/** Encoder test pattern 4 - random noise */
+# define TV_TEST_MODE_PATTERN_4		(4 << 0)
+/** Encoder test pattern 5 - linear color ramps */
+# define TV_TEST_MODE_PATTERN_5		(5 << 0)
+/**
+ * This test mode forces the DACs to 50% of full output.
+ *
+ * This is used for load detection in combination with TVDAC_SENSE_MASK
+ */
+# define TV_TEST_MODE_MONITOR_DETECT	(7 << 0)
+# define TV_TEST_MODE_MASK		(7 << 0)
+/** @} */
+
+/** @defgroup TV_DAC
+ * @{
+ */
+#define TV_DAC			0x68004
+/**
+ * Reports that DAC state change logic has reported change (RO).
+ *
+ * This gets cleared when TV_DAC_STATE_EN is cleared
+*/
+# define TVDAC_STATE_CHG		(1 << 31)
+# define TVDAC_SENSE_MASK		(7 << 28)
+/** Reports that DAC A voltage is above the detect threshold */
+# define TVDAC_A_SENSE			(1 << 30)
+/** Reports that DAC B voltage is above the detect threshold */
+# define TVDAC_B_SENSE			(1 << 29)
+/** Reports that DAC C voltage is above the detect threshold */
+# define TVDAC_C_SENSE			(1 << 28)
+/**
+ * Enables DAC state detection logic, for load-based TV detection.
+ *
+ * The PLL of the chosen pipe (in TV_CTL) must be running, and the encoder set
+ * to off, for load detection to work.
+ */
+# define TVDAC_STATE_CHG_EN		(1 << 27)
+/** Sets the DAC A sense value to high */
+# define TVDAC_A_SENSE_CTL		(1 << 26)
+/** Sets the DAC B sense value to high */
+# define TVDAC_B_SENSE_CTL		(1 << 25)
+/** Sets the DAC C sense value to high */
+# define TVDAC_C_SENSE_CTL		(1 << 24)
+/** Overrides the ENC_ENABLE and DAC voltage levels */
+# define DAC_CTL_OVERRIDE		(1 << 7)
+/** Sets the slew rate.  Must be preserved in software */
+# define ENC_TVDAC_SLEW_FAST		(1 << 6)
+# define DAC_A_1_3_V			(0 << 4)
+# define DAC_A_1_1_V			(1 << 4)
+# define DAC_A_0_7_V			(2 << 4)
+# define DAC_A_OFF			(3 << 4)
+# define DAC_B_1_3_V			(0 << 2)
+# define DAC_B_1_1_V			(1 << 2)
+# define DAC_B_0_7_V			(2 << 2)
+# define DAC_B_OFF			(3 << 2)
+# define DAC_C_1_3_V			(0 << 0)
+# define DAC_C_1_1_V			(1 << 0)
+# define DAC_C_0_7_V			(2 << 0)
+# define DAC_C_OFF			(3 << 0)
+/** @} */
+
+/**
+ * CSC coefficients are stored in a floating point format with 9 bits of
+ * mantissa and 2 or 3 bits of exponent.  The exponent is represented as 2**-n,
+ * where 2-bit exponents are unsigned n, and 3-bit exponents are signed n with
+ * -1 (0x3) being the only legal negative value.
+ */
+#define TV_CSC_Y		0x68010
+# define TV_RY_MASK			0x07ff0000
+# define TV_RY_SHIFT			16
+# define TV_GY_MASK			0x00000fff
+# define TV_GY_SHIFT			0
+
+#define TV_CSC_Y2		0x68014
+# define TV_BY_MASK			0x07ff0000
+# define TV_BY_SHIFT			16
+/**
+ * Y attenuation for component video.
+ *
+ * Stored in 1.9 fixed point.
+ */
+# define TV_AY_MASK			0x000003ff
+# define TV_AY_SHIFT			0
+
+#define TV_CSC_U		0x68018
+# define TV_RU_MASK			0x07ff0000
+# define TV_RU_SHIFT			16
+# define TV_GU_MASK			0x000007ff
+# define TV_GU_SHIFT			0
+
+#define TV_CSC_U2		0x6801c
+# define TV_BU_MASK			0x07ff0000
+# define TV_BU_SHIFT			16
+/**
+ * U attenuation for component video.
+ *
+ * Stored in 1.9 fixed point.
+ */
+# define TV_AU_MASK			0x000003ff
+# define TV_AU_SHIFT			0
+
+#define TV_CSC_V		0x68020
+# define TV_RV_MASK			0x0fff0000
+# define TV_RV_SHIFT			16
+# define TV_GV_MASK			0x000007ff
+# define TV_GV_SHIFT			0
+
+#define TV_CSC_V2		0x68024
+# define TV_BV_MASK			0x07ff0000
+# define TV_BV_SHIFT			16
+/**
+ * V attenuation for component video.
+ *
+ * Stored in 1.9 fixed point.
+ */
+# define TV_AV_MASK			0x000007ff
+# define TV_AV_SHIFT			0
+
+/** @defgroup TV_CSC_KNOBS
+ * @{
+ */
+#define TV_CLR_KNOBS		0x68028
+/** 2s-complement brightness adjustment */
+# define TV_BRIGHTNESS_MASK		0xff000000
+# define TV_BRIGHTNESS_SHIFT		24
+/** Contrast adjustment, as a 2.6 unsigned floating point number */
+# define TV_CONTRAST_MASK		0x00ff0000
+# define TV_CONTRAST_SHIFT		16
+/** Saturation adjustment, as a 2.6 unsigned floating point number */
+# define TV_SATURATION_MASK		0x0000ff00
+# define TV_SATURATION_SHIFT		8
+/** Hue adjustment, as an integer phase angle in degrees */
+# define TV_HUE_MASK			0x000000ff
+# define TV_HUE_SHIFT			0
+/** @} */
+
+/** @defgroup TV_CLR_LEVEL
+ * @{
+ */
+#define TV_CLR_LEVEL		0x6802c
+/** Controls the DAC level for black */
+# define TV_BLACK_LEVEL_MASK		0x01ff0000
+# define TV_BLACK_LEVEL_SHIFT		16
+/** Controls the DAC level for blanking */
+# define TV_BLANK_LEVEL_MASK		0x000001ff
+# define TV_BLANK_LEVEL_SHIFT		0
+/* @} */
+
+/** @defgroup TV_H_CTL_1
+ * @{
+ */
+#define TV_H_CTL_1		0x68030
+/** Number of pixels in the hsync. */
+# define TV_HSYNC_END_MASK		0x1fff0000
+# define TV_HSYNC_END_SHIFT		16
+/** Total number of pixels minus one in the line (display and blanking). */
+# define TV_HTOTAL_MASK			0x00001fff
+# define TV_HTOTAL_SHIFT		0
+/** @} */
+
+/** @defgroup TV_H_CTL_2
+ * @{
+ */
+#define TV_H_CTL_2		0x68034
+/** Enables the colorburst (needed for non-component color) */
+# define TV_BURST_ENA			(1 << 31)
+/** Offset of the colorburst from the start of hsync, in pixels minus one. */
+# define TV_HBURST_START_SHIFT		16
+# define TV_HBURST_START_MASK		0x1fff0000
+/** Length of the colorburst */
+# define TV_HBURST_LEN_SHIFT		0
+# define TV_HBURST_LEN_MASK		0x0001fff
+/** @} */
+
+/** @defgroup TV_H_CTL_3
+ * @{
+ */
+#define TV_H_CTL_3		0x68038
+/** End of hblank, measured in pixels minus one from start of hsync */
+# define TV_HBLANK_END_SHIFT		16
+# define TV_HBLANK_END_MASK		0x1fff0000
+/** Start of hblank, measured in pixels minus one from start of hsync */
+# define TV_HBLANK_START_SHIFT		0
+# define TV_HBLANK_START_MASK		0x0001fff
+/** @} */
+
+/** @defgroup TV_V_CTL_1
+ * @{
+ */
+#define TV_V_CTL_1		0x6803c
+/** XXX */
+# define TV_NBR_END_SHIFT		16
+# define TV_NBR_END_MASK		0x07ff0000
+/** XXX */
+# define TV_VI_END_F1_SHIFT		8
+# define TV_VI_END_F1_MASK		0x00003f00
+/** XXX */
+# define TV_VI_END_F2_SHIFT		0
+# define TV_VI_END_F2_MASK		0x0000003f
+/** @} */
+
+/** @defgroup TV_V_CTL_2
+ * @{
+ */
+#define TV_V_CTL_2		0x68040
+/** Length of vsync, in half lines */
+# define TV_VSYNC_LEN_MASK		0x07ff0000
+# define TV_VSYNC_LEN_SHIFT		16
+/** Offset of the start of vsync in field 1, measured in one less than the
+ * number of half lines.
+ */
+# define TV_VSYNC_START_F1_MASK		0x00007f00
+# define TV_VSYNC_START_F1_SHIFT	8
+/**
+ * Offset of the start of vsync in field 2, measured in one less than the
+ * number of half lines.
+ */
+# define TV_VSYNC_START_F2_MASK		0x0000007f
+# define TV_VSYNC_START_F2_SHIFT	0
+/** @} */
+
+/** @defgroup TV_V_CTL_3
+ * @{
+ */
+#define TV_V_CTL_3		0x68044
+/** Enables generation of the equalization signal */
+# define TV_EQUAL_ENA			(1 << 31)
+/** Length of vsync, in half lines */
+# define TV_VEQ_LEN_MASK		0x007f0000
+# define TV_VEQ_LEN_SHIFT		16
+/** Offset of the start of equalization in field 1, measured in one less than
+ * the number of half lines.
+ */
+# define TV_VEQ_START_F1_MASK		0x0007f00
+# define TV_VEQ_START_F1_SHIFT		8
+/**
+ * Offset of the start of equalization in field 2, measured in one less than
+ * the number of half lines.
+ */
+# define TV_VEQ_START_F2_MASK		0x000007f
+# define TV_VEQ_START_F2_SHIFT		0
+/** @} */
+
+/** @defgroup TV_V_CTL_4
+ * @{
+ */
+#define TV_V_CTL_4		0x68048
+/**
+ * Offset to start of vertical colorburst, measured in one less than the
+ * number of lines from vertical start.
+ */
+# define TV_VBURST_START_F1_MASK	0x003f0000
+# define TV_VBURST_START_F1_SHIFT	16
+/**
+ * Offset to the end of vertical colorburst, measured in one less than the
+ * number of lines from the start of NBR.
+ */
+# define TV_VBURST_END_F1_MASK		0x000000ff
+# define TV_VBURST_END_F1_SHIFT		0
+/** @} */
+
+/** @defgroup TV_V_CTL_5
+ * @{
+ */
+#define TV_V_CTL_5		0x6804c
+/**
+ * Offset to start of vertical colorburst, measured in one less than the
+ * number of lines from vertical start.
+ */
+# define TV_VBURST_START_F2_MASK	0x003f0000
+# define TV_VBURST_START_F2_SHIFT	16
+/**
+ * Offset to the end of vertical colorburst, measured in one less than the
+ * number of lines from the start of NBR.
+ */
+# define TV_VBURST_END_F2_MASK		0x000000ff
+# define TV_VBURST_END_F2_SHIFT		0
+/** @} */
+
+/** @defgroup TV_V_CTL_6
+ * @{
+ */
+#define TV_V_CTL_6		0x68050
+/**
+ * Offset to start of vertical colorburst, measured in one less than the
+ * number of lines from vertical start.
+ */
+# define TV_VBURST_START_F3_MASK	0x003f0000
+# define TV_VBURST_START_F3_SHIFT	16
+/**
+ * Offset to the end of vertical colorburst, measured in one less than the
+ * number of lines from the start of NBR.
+ */
+# define TV_VBURST_END_F3_MASK		0x000000ff
+# define TV_VBURST_END_F3_SHIFT		0
+/** @} */
+
+/** @defgroup TV_V_CTL_7
+ * @{
+ */
+#define TV_V_CTL_7		0x68054
+/**
+ * Offset to start of vertical colorburst, measured in one less than the
+ * number of lines from vertical start.
+ */
+# define TV_VBURST_START_F4_MASK	0x003f0000
+# define TV_VBURST_START_F4_SHIFT	16
+/**
+ * Offset to the end of vertical colorburst, measured in one less than the
+ * number of lines from the start of NBR.
+ */
+# define TV_VBURST_END_F4_MASK		0x000000ff
+# define TV_VBURST_END_F4_SHIFT		0
+/** @} */
+
+/** @defgroup TV_SC_CTL_1
+ * @{
+ */
+#define TV_SC_CTL_1		0x68060
+/** Turns on the first subcarrier phase generation DDA */
+# define TV_SC_DDA1_EN			(1 << 31)
+/** Turns on the first subcarrier phase generation DDA */
+# define TV_SC_DDA2_EN			(1 << 30)
+/** Turns on the first subcarrier phase generation DDA */
+# define TV_SC_DDA3_EN			(1 << 29)
+/** Sets the subcarrier DDA to reset frequency every other field */
+# define TV_SC_RESET_EVERY_2		(0 << 24)
+/** Sets the subcarrier DDA to reset frequency every fourth field */
+# define TV_SC_RESET_EVERY_4		(1 << 24)
+/** Sets the subcarrier DDA to reset frequency every eighth field */
+# define TV_SC_RESET_EVERY_8		(2 << 24)
+/** Sets the subcarrier DDA to never reset the frequency */
+# define TV_SC_RESET_NEVER		(3 << 24)
+/** Sets the peak amplitude of the colorburst.*/
+# define TV_BURST_LEVEL_MASK		0x00ff0000
+# define TV_BURST_LEVEL_SHIFT		16
+/** Sets the increment of the first subcarrier phase generation DDA */
+# define TV_SCDDA1_INC_MASK		0x00000fff
+# define TV_SCDDA1_INC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_SC_CTL_2
+ * @{
+ */
+#define TV_SC_CTL_2		0x68064
+/** Sets the rollover for the second subcarrier phase generation DDA */
+# define TV_SCDDA2_SIZE_MASK		0x7fff0000
+# define TV_SCDDA2_SIZE_SHIFT		16
+/** Sets the increent of the second subcarrier phase generation DDA */
+# define TV_SCDDA2_INC_MASK		0x00007fff
+# define TV_SCDDA2_INC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_SC_CTL_3
+ * @{
+ */
+#define TV_SC_CTL_3		0x68068
+/** Sets the rollover for the third subcarrier phase generation DDA */
+# define TV_SCDDA3_SIZE_MASK		0x7fff0000
+# define TV_SCDDA3_SIZE_SHIFT		16
+/** Sets the increent of the third subcarrier phase generation DDA */
+# define TV_SCDDA3_INC_MASK		0x00007fff
+# define TV_SCDDA3_INC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_WIN_POS
+ * @{
+ */
+#define TV_WIN_POS		0x68070
+/** X coordinate of the display from the start of horizontal active */
+# define TV_XPOS_MASK			0x1fff0000
+# define TV_XPOS_SHIFT			16
+/** Y coordinate of the display from the start of vertical active (NBR) */
+# define TV_YPOS_MASK			0x00000fff
+# define TV_YPOS_SHIFT			0
+/** @} */
+
+/** @defgroup TV_WIN_SIZE
+ * @{
+ */
+#define TV_WIN_SIZE		0x68074
+/** Horizontal size of the display window, measured in pixels*/
+# define TV_XSIZE_MASK			0x1fff0000
+# define TV_XSIZE_SHIFT			16
+/**
+ * Vertical size of the display window, measured in pixels.
+ *
+ * Must be even for interlaced modes.
+ */
+# define TV_YSIZE_MASK			0x00000fff
+# define TV_YSIZE_SHIFT			0
+/** @} */
+
+/** @defgroup TV_FILTER_CTL_1
+ * @{
+ */
+#define TV_FILTER_CTL_1		0x68080
+/**
+ * Enables automatic scaling calculation.
+ *
+ * If set, the rest of the registers are ignored, and the calculated values can
+ * be read back from the register.
+ */
+# define TV_AUTO_SCALE			(1 << 31)
+/**
+ * Disables the vertical filter.
+ *
+ * This is required on modes more than 1024 pixels wide */
+# define TV_V_FILTER_BYPASS		(1 << 29)
+/** Enables adaptive vertical filtering */
+# define TV_VADAPT			(1 << 28)
+# define TV_VADAPT_MODE_MASK		(3 << 26)
+/** Selects the least adaptive vertical filtering mode */
+# define TV_VADAPT_MODE_LEAST		(0 << 26)
+/** Selects the moderately adaptive vertical filtering mode */
+# define TV_VADAPT_MODE_MODERATE	(1 << 26)
+/** Selects the most adaptive vertical filtering mode */
+# define TV_VADAPT_MODE_MOST		(3 << 26)
+/**
+ * Sets the horizontal scaling factor.
+ *
+ * This should be the fractional part of the horizontal scaling factor divided
+ * by the oversampling rate.  TV_HSCALE should be less than 1, and set to:
+ *
+ * (src width - 1) / ((oversample * dest width) - 1)
+ */
+# define TV_HSCALE_FRAC_MASK		0x00003fff
+# define TV_HSCALE_FRAC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_FILTER_CTL_2
+ * @{
+ */
+#define TV_FILTER_CTL_2		0x68084
+/**
+ * Sets the integer part of the 3.15 fixed-point vertical scaling factor.
+ *
+ * TV_VSCALE should be (src height - 1) / ((interlace * dest height) - 1)
+ */
+# define TV_VSCALE_INT_MASK		0x00038000
+# define TV_VSCALE_INT_SHIFT		15
+/**
+ * Sets the fractional part of the 3.15 fixed-point vertical scaling factor.
+ *
+ * \sa TV_VSCALE_INT_MASK
+ */
+# define TV_VSCALE_FRAC_MASK		0x00007fff
+# define TV_VSCALE_FRAC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_FILTER_CTL_3
+ * @{
+ */
+#define TV_FILTER_CTL_3		0x68088
+/**
+ * Sets the integer part of the 3.15 fixed-point vertical scaling factor.
+ *
+ * TV_VSCALE should be (src height - 1) / (1/4 * (dest height - 1))
+ *
+ * For progressive modes, TV_VSCALE_IP_INT should be set to zeroes.
+ */
+# define TV_VSCALE_IP_INT_MASK		0x00038000
+# define TV_VSCALE_IP_INT_SHIFT		15
+/**
+ * Sets the fractional part of the 3.15 fixed-point vertical scaling factor.
+ *
+ * For progressive modes, TV_VSCALE_IP_INT should be set to zeroes.
+ *
+ * \sa TV_VSCALE_IP_INT_MASK
+ */
+# define TV_VSCALE_IP_FRAC_MASK		0x00007fff
+# define TV_VSCALE_IP_FRAC_SHIFT		0
+/** @} */
+
+/** @defgroup TV_CC_CONTROL
+ * @{
+ */
+#define TV_CC_CONTROL		0x68090
+# define TV_CC_ENABLE			(1 << 31)
+/**
+ * Specifies which field to send the CC data in.
+ *
+ * CC data is usually sent in field 0.
+ */
+# define TV_CC_FID_MASK			(1 << 27)
+# define TV_CC_FID_SHIFT		27
+/** Sets the horizontal position of the CC data.  Usually 135. */
+# define TV_CC_HOFF_MASK		0x03ff0000
+# define TV_CC_HOFF_SHIFT		16
+/** Sets the vertical position of the CC data.  Usually 21 */
+# define TV_CC_LINE_MASK		0x0000003f
+# define TV_CC_LINE_SHIFT		0
+/** @} */
+
+/** @defgroup TV_CC_DATA
+ * @{
+ */
+#define TV_CC_DATA		0x68094
+# define TV_CC_RDY			(1 << 31)
+/** Second word of CC data to be transmitted. */
+# define TV_CC_DATA_2_MASK		0x007f0000
+# define TV_CC_DATA_2_SHIFT		16
+/** First word of CC data to be transmitted. */
+# define TV_CC_DATA_1_MASK		0x0000007f
+# define TV_CC_DATA_1_SHIFT		0
+/** @}
+ */
+
+/** @{ */
+#define TV_H_LUMA_0		0x68100
+#define TV_H_LUMA_59		0x681ec
+#define TV_H_CHROMA_0		0x68200
+#define TV_H_CHROMA_59		0x682ec
+#define TV_V_LUMA_0		0x68300
+#define TV_V_LUMA_42		0x683a8
+#define TV_V_CHROMA_0		0x68400
+#define TV_V_CHROMA_42		0x684a8
+
 #define PIPEACONF 0x70008
 #define PIPEACONF_ENABLE	(1<<31)
 #define PIPEACONF_DISABLE	0
@@ -1066,13 +1849,17 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
  */
 
 #define SWF0			0x71410
+#define SWF1			0x71414
+#define SWF2			0x71418
+#define SWF3			0x7141c
+#define SWF4			0x71420
+#define SWF5			0x71424
+#define SWF6			0x71428
 
-/*
- * 855 scratch registers.
- */
 #define SWF10			0x70410
-
 #define SWF30			0x72414
+#define SWF31			0x72418
+#define SWF32			0x7241c
 
 /*
  * Overlay registers.  These are overlay registers accessed via MMIO.
@@ -1105,7 +1892,8 @@ extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
 #define IS_I915GM(dev) ((dev)->pci_device == 0x2592)
 #define IS_I945G(dev) ((dev)->pci_device == 0x2772)
 #define IS_I945GM(dev) ((dev)->pci_device == 0x27A2 ||\
-		        (dev)->pci_device == 0x27AE)
+			(dev)->pci_device == 0x27AE)
+
 #define IS_I965G(dev) ((dev)->pci_device == 0x2972 || \
 		       (dev)->pci_device == 0x2982 || \
 		       (dev)->pci_device == 0x2992 || \
diff --git a/drivers/char/drm/i915_fence.c b/drivers/char/drm/i915_fence.c
new file mode 100644
index 0000000..803cfe8
--- /dev/null
+++ b/drivers/char/drm/i915_fence.c
@@ -0,0 +1,269 @@
+/**************************************************************************
+ *
+ * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrm <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+/*
+ * Initiate a sync flush if it's not already pending.
+ */
+
+static inline void i915_initiate_rwflush(struct drm_i915_private *dev_priv,
+					 struct drm_fence_class_manager *fc)
+{
+	if ((fc->pending_flush & DRM_I915_FENCE_TYPE_RW) &&
+	    !dev_priv->flush_pending) {
+		dev_priv->flush_sequence = (uint32_t) READ_BREADCRUMB(dev_priv);
+		dev_priv->flush_flags = fc->pending_flush;
+		dev_priv->saved_flush_status = READ_HWSP(dev_priv, 0);
+		I915_WRITE(I915REG_INSTPM, (1 << 5) | (1 << 21));
+		dev_priv->flush_pending = 1;
+		fc->pending_flush &= ~DRM_I915_FENCE_TYPE_RW;
+	}
+}
+
+static inline void i915_report_rwflush(struct drm_device *dev,
+				       struct drm_i915_private *dev_priv)
+{
+	if (unlikely(dev_priv->flush_pending)) {
+
+		uint32_t flush_flags;
+		uint32_t i_status;
+		uint32_t flush_sequence;
+
+		i_status = READ_HWSP(dev_priv, 0);
+		if ((i_status & (1 << 12)) !=
+		    (dev_priv->saved_flush_status & (1 << 12))) {
+			flush_flags = dev_priv->flush_flags;
+			flush_sequence = dev_priv->flush_sequence;
+			dev_priv->flush_pending = 0;
+			drm_fence_handler(dev, 0, flush_sequence,
+					  flush_flags, 0);
+		}
+	}
+}
+
+static void i915_fence_flush(struct drm_device *dev,
+			     uint32_t fence_class)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[0];
+	unsigned long irq_flags;
+
+	if (unlikely(!dev_priv))
+		return;
+
+	write_lock_irqsave(&fm->lock, irq_flags);
+	i915_initiate_rwflush(dev_priv, fc);
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+}
+
+
+static void i915_fence_poll(struct drm_device *dev, uint32_t fence_class,
+			    uint32_t waiting_types)
+{
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[0];
+	uint32_t sequence;
+
+	if (unlikely(!dev_priv))
+		return;
+
+	/*
+	 * First, report any executed sync flush:
+	 */
+
+	i915_report_rwflush(dev, dev_priv);
+
+	/*
+	 * Report A new breadcrumb, and adjust IRQs.
+	 */
+
+	if (waiting_types & DRM_FENCE_TYPE_EXE) {
+
+		sequence = READ_BREADCRUMB(dev_priv);
+		drm_fence_handler(dev, 0, sequence,
+				  DRM_FENCE_TYPE_EXE, 0);
+
+		if (dev_priv->fence_irq_on &&
+		    !(fc->waiting_types & DRM_FENCE_TYPE_EXE)) {
+			i915_user_irq_off(dev_priv);
+			dev_priv->fence_irq_on = 0;
+		} else if (!dev_priv->fence_irq_on &&
+			   (fc->waiting_types & DRM_FENCE_TYPE_EXE)) {
+			i915_user_irq_on(dev_priv);
+			dev_priv->fence_irq_on = 1;
+		}
+	}
+
+	/*
+	 * There may be new RW flushes pending. Start them.
+	 */
+	i915_initiate_rwflush(dev_priv, fc);
+
+	/*
+	 * And possibly, but unlikely, they finish immediately.
+	 */
+
+	i915_report_rwflush(dev, dev_priv);
+
+}
+
+static int i915_fence_emit_sequence(struct drm_device *dev, uint32_t class,
+			     uint32_t flags, uint32_t *sequence,
+			     uint32_t *native_type)
+{
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	if (unlikely(!dev_priv))
+		return -EINVAL;
+
+	i915_emit_irq(dev);
+	*sequence = (uint32_t) dev_priv->counter;
+	*native_type = DRM_FENCE_TYPE_EXE;
+	if (flags & DRM_I915_FENCE_FLAG_FLUSHED)
+		*native_type |= DRM_I915_FENCE_TYPE_RW;
+
+	return 0;
+}
+
+void i915_fence_handler(struct drm_device *dev)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[0];
+
+	write_lock(&fm->lock);
+	i915_fence_poll(dev, 0, fc->waiting_types);
+	write_unlock(&fm->lock);
+}
+
+/*
+ * We need a separate wait function since we need to poll for
+ * sync flushes.
+ */
+
+static int i915_fence_wait(struct drm_fence_object *fence,
+			   int lazy, int interruptible, uint32_t mask)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[0];
+	int ret;
+	unsigned long  _end = jiffies + 3 * DRM_HZ;
+
+	drm_fence_object_flush(fence, mask);
+	if (likely(interruptible))
+		ret = wait_event_interruptible_timeout
+			(fc->fence_queue, drm_fence_object_signaled(fence, DRM_FENCE_TYPE_EXE),
+			 3 * DRM_HZ);
+	else
+		ret = wait_event_timeout
+			(fc->fence_queue, drm_fence_object_signaled(fence, DRM_FENCE_TYPE_EXE),
+			 3 * DRM_HZ);
+
+	if (unlikely(ret == -ERESTARTSYS))
+		return -EAGAIN;
+
+	if (unlikely(ret == 0))
+		return -EBUSY;
+
+	if (likely(mask == DRM_FENCE_TYPE_EXE ||
+		   drm_fence_object_signaled(fence, mask)))
+		return 0;
+
+	/*
+	 * Remove this code snippet when fixed. HWSTAM doesn't let
+	 * flush info through...
+	 */
+
+	if (unlikely(dev_priv && !dev_priv->irq_enabled)) {
+		unsigned long irq_flags;
+
+		DRM_ERROR("X server disabled IRQs before releasing frame buffer.\n");
+		msleep(100);
+		dev_priv->flush_pending = 0;
+		write_lock_irqsave(&fm->lock, irq_flags);
+		drm_fence_handler(dev, fence->fence_class,
+				  fence->sequence, fence->type, 0);
+		write_unlock_irqrestore(&fm->lock, irq_flags);
+	}
+
+	/*
+	 * Poll for sync flush completion.
+	 */
+
+	return drm_fence_wait_polling(fence, lazy, interruptible, mask, _end);
+}
+
+static uint32_t i915_fence_needed_flush(struct drm_fence_object *fence)
+{
+	uint32_t flush_flags = fence->waiting_types &
+		~(DRM_FENCE_TYPE_EXE | fence->signaled_types);
+
+	if (likely(flush_flags == 0 ||
+		   ((flush_flags & ~fence->native_types) == 0) ||
+		   (fence->signaled_types != DRM_FENCE_TYPE_EXE)))
+		return 0;
+	else {
+		struct drm_device *dev = fence->dev;
+		struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+		struct drm_fence_driver *driver = dev->driver->fence_driver;
+
+		if (unlikely(!dev_priv))
+			return 0;
+
+		if (dev_priv->flush_pending) {
+			uint32_t diff = (dev_priv->flush_sequence - fence->sequence) & driver->sequence_mask;
+
+			if (diff < driver->wrap_diff)
+				return 0;
+		}
+	}
+	return flush_flags;
+}
+
+struct drm_fence_driver i915_fence_driver = {
+	.num_classes = 1,
+	.wrap_diff = (1U << (BREADCRUMB_BITS - 1)),
+	.flush_diff = (1U << (BREADCRUMB_BITS - 2)),
+	.sequence_mask = BREADCRUMB_MASK,
+	.has_irq = NULL,
+	.emit = i915_fence_emit_sequence,
+	.flush = i915_fence_flush,
+	.poll = i915_fence_poll,
+	.needed_flush = i915_fence_needed_flush,
+	.wait = i915_fence_wait,
+};
diff --git a/drivers/char/drm/i915_init.c b/drivers/char/drm/i915_init.c
new file mode 100644
index 0000000..4a53fc8
--- /dev/null
+++ b/drivers/char/drm/i915_init.c
@@ -0,0 +1,411 @@
+/*
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ *
+ * Copyright  2002, 2003 David Dawes <dawes@xfree86.org>
+ *                   2004 Sylvain Meyer
+ *
+ * GPL/BSD dual license
+ */
+#include "drmP.h"
+#include "drm.h"
+#include "drm_sarea.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+/**
+ * i915_probe_agp - get AGP bootup configuration
+ * @pdev: PCI device
+ * @aperture_size: returns AGP aperture configured size
+ * @preallocated_size: returns size of BIOS preallocated AGP space
+ *
+ * Since Intel integrated graphics are UMA, the BIOS has to set aside
+ * some RAM for the framebuffer at early boot.  This code figures out
+ * how much was set aside so we can use it for our own purposes.
+ */
+int i915_probe_agp(struct pci_dev *pdev, unsigned long *aperture_size,
+		   unsigned long *preallocated_size)
+{
+	struct pci_dev *bridge_dev;
+	u16 tmp = 0;
+	unsigned long overhead;
+
+	bridge_dev = pci_get_bus_and_slot(0, PCI_DEVFN(0,0));
+	if (!bridge_dev) {
+		DRM_ERROR("bridge device not found\n");
+		return -1;
+	}
+
+	/* Get the fb aperture size and "stolen" memory amount. */
+	pci_read_config_word(bridge_dev, INTEL_GMCH_CTRL, &tmp);
+	pci_dev_put(bridge_dev);
+
+	*aperture_size = 1024 * 1024;
+	*preallocated_size = 1024 * 1024;
+
+	switch (pdev->device) {
+	case PCI_DEVICE_ID_INTEL_82830_CGC:
+	case PCI_DEVICE_ID_INTEL_82845G_IG:
+	case PCI_DEVICE_ID_INTEL_82855GM_IG:
+	case PCI_DEVICE_ID_INTEL_82865_IG:
+		if ((tmp & INTEL_GMCH_MEM_MASK) == INTEL_GMCH_MEM_64M)
+			*aperture_size *= 64;
+		else
+			*aperture_size *= 128;
+		break;
+	default:
+		/* 9xx supports large sizes, just look at the length */
+		*aperture_size = pci_resource_len(pdev, 2);
+		break;
+	}
+
+	/*
+	 * Some of the preallocated space is taken by the GTT
+	 * and popup.  GTT is 1K per MB of aperture size, and popup is 4K.
+	 */
+	overhead = (*aperture_size / 1024) + 4096;
+	switch (tmp & INTEL_855_GMCH_GMS_MASK) {
+	case INTEL_855_GMCH_GMS_STOLEN_1M:
+		break; /* 1M already */
+	case INTEL_855_GMCH_GMS_STOLEN_4M:
+		*preallocated_size *= 4;
+		break;
+	case INTEL_855_GMCH_GMS_STOLEN_8M:
+		*preallocated_size *= 8;
+		break;
+	case INTEL_855_GMCH_GMS_STOLEN_16M:
+		*preallocated_size *= 16;
+		break;
+	case INTEL_855_GMCH_GMS_STOLEN_32M:
+		*preallocated_size *= 32;
+		break;
+	case INTEL_915G_GMCH_GMS_STOLEN_48M:
+		*preallocated_size *= 48;
+		break;
+	case INTEL_915G_GMCH_GMS_STOLEN_64M:
+		*preallocated_size *= 64;
+		break;
+	case INTEL_855_GMCH_GMS_DISABLED:
+		DRM_ERROR("video memory is disabled\n");
+		return -1;
+	default:
+		DRM_ERROR("unexpected GMCH_GMS value: 0x%02x\n",
+			tmp & INTEL_855_GMCH_GMS_MASK);
+		return -1;
+	}
+	*preallocated_size -= overhead;
+
+	return 0;
+}
+
+/**
+ * i915_driver_load - setup chip and create an initial config
+ * @dev: DRM device
+ * @flags: startup flags
+ *
+ * The driver load routine has to do several things:
+ *   - drive output discovery via intel_modeset_init()
+ *   - initialize the memory manager
+ *   - allocate initial config memory
+ *   - setup the DRM framebuffer with the allocated memory
+ */
+int i915_driver_load(struct drm_device *dev, unsigned long flags)
+{
+	struct drm_i915_private *dev_priv;
+	unsigned long agp_size, prealloc_size;
+	int size, ret;
+
+	dev_priv = drm_alloc(sizeof(struct drm_i915_private), DRM_MEM_DRIVER);
+	if (dev_priv == NULL)
+		return -ENOMEM;
+
+	memset(dev_priv, 0, sizeof(struct drm_i915_private));
+	dev->dev_private = (void *)dev_priv;
+//	dev_priv->flags = flags;
+
+	/* i915 has 4 more counters */
+	dev->counters += 4;
+	dev->types[6] = _DRM_STAT_IRQ;
+	dev->types[7] = _DRM_STAT_PRIMARY;
+	dev->types[8] = _DRM_STAT_SECONDARY;
+	dev->types[9] = _DRM_STAT_DMA;
+
+	if (IS_MOBILE(dev) || IS_I9XX(dev))
+		dev_priv->cursor_needs_physical = true;
+	else
+		dev_priv->cursor_needs_physical = false;
+
+	if (IS_I965G(dev) || IS_G33(dev))
+		dev_priv->cursor_needs_physical = false;
+
+	if (IS_I9XX(dev)) {
+		pci_read_config_dword(dev->pdev, 0x5C, &dev_priv->stolen_base);
+		DRM_DEBUG("stolen base %p\n", (void*)dev_priv->stolen_base);
+	}
+
+	if (IS_I9XX(dev)) {
+		dev_priv->mmiobase = drm_get_resource_start(dev, 0);
+		dev_priv->mmiolen = drm_get_resource_len(dev, 0);
+		dev->mode_config.fb_base =
+			drm_get_resource_start(dev, 2) & 0xff000000;
+	} else if (drm_get_resource_start(dev, 1)) {
+		dev_priv->mmiobase = drm_get_resource_start(dev, 1);
+		dev_priv->mmiolen = drm_get_resource_len(dev, 1);
+		dev->mode_config.fb_base =
+			drm_get_resource_start(dev, 0) & 0xff000000;
+	} else {
+		DRM_ERROR("Unable to find MMIO registers\n");
+		return -ENODEV;
+	}
+
+	DRM_DEBUG("fb_base: 0x%08lx\n", dev->mode_config.fb_base);
+
+	ret = drm_addmap(dev, dev_priv->mmiobase, dev_priv->mmiolen,
+			 _DRM_REGISTERS, _DRM_KERNEL|_DRM_READ_ONLY|_DRM_DRIVER, &dev_priv->mmio_map);
+	if (ret != 0) {
+		DRM_ERROR("Cannot add mapping for MMIO registers\n");
+		return ret;
+	}
+
+	/*
+	 * Initialize the memory manager for local and AGP space
+	 */
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+		drm_bo_driver_init(dev);
+
+		i915_probe_agp(dev->pdev, &agp_size, &prealloc_size);
+		printk("setting up %ld bytes of VRAM space\n", prealloc_size);
+		printk("setting up %ld bytes of TT space\n", (agp_size - prealloc_size));
+		drm_bo_init_mm(dev, DRM_BO_MEM_VRAM, 0, prealloc_size >> PAGE_SHIFT, 1);
+		drm_bo_init_mm(dev, DRM_BO_MEM_TT, prealloc_size >> PAGE_SHIFT, (agp_size - prealloc_size) >> PAGE_SHIFT, 1);
+		
+		I915_WRITE(LP_RING + RING_LEN, 0);
+		I915_WRITE(LP_RING + RING_HEAD, 0);
+		I915_WRITE(LP_RING + RING_TAIL, 0);
+
+		size = PRIMARY_RINGBUFFER_SIZE;
+		ret = drm_buffer_object_create(dev, size, drm_bo_type_kernel,
+					       DRM_BO_FLAG_READ | DRM_BO_FLAG_WRITE |
+					       DRM_BO_FLAG_MEM_VRAM |
+					       DRM_BO_FLAG_NO_EVICT,
+					       DRM_BO_HINT_DONT_FENCE, 0x1, 0,
+					       &dev_priv->ring_buffer);
+		if (ret < 0) {
+			DRM_ERROR("Unable to allocate or pin ring buffer\n");
+			return -EINVAL;
+		}
+		
+		/* remap the buffer object properly */
+		dev_priv->ring.Start = dev_priv->ring_buffer->offset;
+		dev_priv->ring.End = dev_priv->ring.Start + size;
+		dev_priv->ring.Size = size;
+		dev_priv->ring.tail_mask = dev_priv->ring.Size - 1;
+
+		/* FIXME: need wrapper with PCI mem checks */
+		ret = drm_mem_reg_ioremap(dev, &dev_priv->ring_buffer->mem,
+					  (void **) &dev_priv->ring.virtual_start);
+		if (ret)
+			DRM_ERROR("error mapping ring buffer: %d\n", ret);
+		
+		DRM_DEBUG("ring start %08lX, %p, %08lX\n", dev_priv->ring.Start,
+			  dev_priv->ring.virtual_start, dev_priv->ring.Size);
+
+	//
+
+		memset((void *)(dev_priv->ring.virtual_start), 0, dev_priv->ring.Size);
+		
+		I915_WRITE(LP_RING + RING_START, dev_priv->ring.Start);
+		I915_WRITE(LP_RING + RING_LEN,
+			   ((dev_priv->ring.Size - 4096) & RING_NR_PAGES) |
+			   (RING_NO_REPORT | RING_VALID));
+
+		/* We are using separate values as placeholders for mechanisms for
+		 * private backbuffer/depthbuffer usage.
+		 */
+		dev_priv->use_mi_batchbuffer_start = 0;
+		
+		/* Allow hardware batchbuffers unless told otherwise.
+		 */
+		dev_priv->allow_batchbuffer = 1;
+
+		/* Program Hardware Status Page */
+		if (!IS_G33(dev)) {
+			dev_priv->status_page_dmah = 
+				drm_pci_alloc(dev, PAGE_SIZE, PAGE_SIZE, 0xffffffff);
+			
+			if (!dev_priv->status_page_dmah) {
+				dev->dev_private = (void *)dev_priv;
+				i915_dma_cleanup(dev);
+				DRM_ERROR("Can not allocate hardware status page\n");
+				return -ENOMEM;
+			}
+			dev_priv->hw_status_page = dev_priv->status_page_dmah->vaddr;
+			dev_priv->dma_status_page = dev_priv->status_page_dmah->busaddr;
+			
+			memset(dev_priv->hw_status_page, 0, PAGE_SIZE);
+			
+			I915_WRITE(I915REG_HWS_PGA, dev_priv->dma_status_page);
+		}
+		DRM_DEBUG("Enabled hardware status page\n");
+
+		dev_priv->wq = create_singlethread_workqueue("i915");
+		if (dev_priv == 0) {
+		  DRM_DEBUG("Error\n");
+		}
+
+		intel_modeset_init(dev);
+		drm_initial_config(dev, false);
+
+		//		drm_mm_print(&dev->bm.man[DRM_BO_MEM_VRAM].manager, "VRAM");
+		//		drm_mm_print(&dev->bm.man[DRM_BO_MEM_TT].manager, "TT");
+
+		drm_irq_install(dev);
+	}
+
+	return 0;
+}
+
+int i915_driver_unload(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	I915_WRITE(LP_RING + RING_LEN, 0);
+
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+		drm_irq_uninstall(dev);
+		intel_modeset_cleanup(dev);
+	}
+
+#if 0
+	if (dev_priv->ring.virtual_start) {
+		drm_core_ioremapfree(&dev_priv->ring.map, dev);
+	}
+#endif
+	if (dev_priv->sarea_kmap.virtual) {
+		drm_bo_kunmap(&dev_priv->sarea_kmap);
+		dev_priv->sarea_kmap.virtual = NULL;
+		dev->sigdata.lock = NULL;
+	}
+
+	if (dev_priv->sarea_bo) {
+		mutex_lock(&dev->struct_mutex);
+		drm_bo_usage_deref_locked(&dev_priv->sarea_bo);
+		mutex_unlock(&dev->struct_mutex);
+		dev_priv->sarea_bo = NULL;
+	}
+
+	if (dev_priv->status_page_dmah) {
+		drm_pci_free(dev, dev_priv->status_page_dmah);
+		dev_priv->status_page_dmah = NULL;
+		dev_priv->hw_status_page = NULL;
+		dev_priv->dma_status_page = 0;
+		/* Need to rewrite hardware status page */
+		I915_WRITE(I915REG_HWS_PGA, 0x1ffff000);
+	}
+
+	if (dev_priv->status_gfx_addr) {
+		dev_priv->status_gfx_addr = 0;
+		drm_core_ioremapfree(&dev_priv->hws_map, dev);
+		I915_WRITE(I915REG_HWS_PGA, 0x1ffff000);
+	}
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+		drm_mem_reg_iounmap(dev, &dev_priv->ring_buffer->mem,
+				    dev_priv->ring.virtual_start);
+
+		DRM_DEBUG("usage is %d\n", atomic_read(&dev_priv->ring_buffer->usage));
+		mutex_lock(&dev->struct_mutex);
+		drm_bo_usage_deref_locked(&dev_priv->ring_buffer);
+
+		if (drm_bo_clean_mm(dev, DRM_BO_MEM_TT, 1)) {
+			DRM_ERROR("Memory manager type 3 not clean. "
+				  "Delaying takedown\n");
+		}
+		if (drm_bo_clean_mm(dev, DRM_BO_MEM_VRAM, 1)) {
+			DRM_ERROR("Memory manager type 3 not clean. "
+				  "Delaying takedown\n");
+		}
+		mutex_unlock(&dev->struct_mutex);
+	}
+
+	drm_bo_driver_finish(dev);
+
+        DRM_DEBUG("%p\n", dev_priv->mmio_map);
+        drm_rmmap(dev, dev_priv->mmio_map);
+
+	drm_free(dev_priv, sizeof(*dev_priv), DRM_MEM_DRIVER);
+
+	dev->dev_private = NULL;
+	return 0;
+}
+
+int i915_master_create(struct drm_device *dev, struct drm_master *master)
+{
+	struct drm_i915_master_private *master_priv;
+	unsigned long sareapage;
+	int ret;
+
+	master_priv = drm_calloc(1, sizeof(*master_priv), DRM_MEM_DRIVER);
+	if (!master_priv)
+		return -ENOMEM;
+
+	/* prebuild the SAREA */
+	sareapage = max(SAREA_MAX, PAGE_SIZE);
+	ret = drm_addmap(dev, 0, sareapage, _DRM_SHM, _DRM_CONTAINS_LOCK|_DRM_DRIVER,
+			 &master_priv->sarea);
+	if (ret) {
+		DRM_ERROR("SAREA setup failed\n");
+		return ret;
+	}
+	master_priv->sarea_priv = master_priv->sarea->handle + sizeof(struct drm_sarea);
+	master_priv->sarea_priv->pf_current_page = 0;
+
+	master->driver_priv = master_priv;
+	return 0;
+}
+
+void i915_master_destroy(struct drm_device *dev, struct drm_master *master)
+{
+	struct drm_i915_master_private *master_priv = master->driver_priv;
+
+	if (!master_priv)
+		return;
+
+        drm_rmmap(dev, master_priv->sarea);
+	drm_free(master_priv, sizeof(*master_priv), DRM_MEM_DRIVER);
+
+	master->driver_priv = NULL;
+}
+
+void i915_driver_preclose(struct drm_device * dev, struct drm_file *file_priv)
+{
+        struct drm_i915_private *dev_priv = dev->dev_private;
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		i915_mem_release(dev, file_priv, dev_priv->agp_heap);
+}
+
+void i915_driver_lastclose(struct drm_device * dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (!dev_priv)
+		return;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return;
+
+	if (dev_priv->agp_heap)
+		i915_mem_takedown(&(dev_priv->agp_heap));
+	
+	i915_dma_cleanup(dev);
+}
+
+int i915_driver_firstopen(struct drm_device *dev)
+{
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return 0;
+
+	drm_bo_driver_init(dev);
+	return 0;
+}
diff --git a/drivers/char/drm/i915_ioc32.c b/drivers/char/drm/i915_ioc32.c
index 1fe68a2..725fad2 100644
--- a/drivers/char/drm/i915_ioc32.c
+++ b/drivers/char/drm/i915_ioc32.c
@@ -182,12 +182,70 @@ static int compat_i915_alloc(struct file *file, unsigned int cmd,
 			 DRM_IOCTL_I915_ALLOC, (unsigned long)request);
 }
 
+typedef struct drm_i915_execbuffer32 {
+	uint64_t ops_list;
+	uint32_t num_buffers;
+	struct _drm_i915_batchbuffer32 batch;
+	drm_context_t context; 
+	struct drm_fence_arg fence_arg;
+} drm_i915_execbuffer32_t;
+
+static int compat_i915_execbuffer(struct file *file, unsigned int cmd,
+			     unsigned long arg)
+{
+	drm_i915_execbuffer32_t req32;
+	struct drm_i915_execbuffer __user *request;
+	int err;
+
+	if (copy_from_user(&req32, (void __user *) arg, sizeof(req32)))
+		return -EFAULT;
+
+	request = compat_alloc_user_space(sizeof(*request));
+
+	if (!access_ok(VERIFY_WRITE, request, sizeof(*request))
+       || __put_user(req32.ops_list, &request->ops_list)
+       || __put_user(req32.num_buffers, &request->num_buffers)
+       || __put_user(req32.context, &request->context)
+       || __copy_to_user(&request->fence_arg, &req32.fence_arg, 
+                         sizeof(req32.fence_arg))
+       || __put_user(req32.batch.start, &request->batch.start)
+       || __put_user(req32.batch.used, &request->batch.used)
+       || __put_user(req32.batch.DR1, &request->batch.DR1)
+       || __put_user(req32.batch.DR4, &request->batch.DR4)
+       || __put_user(req32.batch.num_cliprects,
+                     &request->batch.num_cliprects)
+       || __put_user((int __user *)(unsigned long)req32.batch.cliprects,
+                     &request->batch.cliprects))
+		return -EFAULT;
+
+	err = drm_ioctl(file->f_dentry->d_inode, file,
+			 DRM_IOCTL_I915_EXECBUFFER, (unsigned long)request);
+
+	if (err)
+		return err;
+
+	if (__get_user(req32.fence_arg.handle, &request->fence_arg.handle)
+	    || __get_user(req32.fence_arg.fence_class, &request->fence_arg.fence_class)
+	    || __get_user(req32.fence_arg.type, &request->fence_arg.type)
+	    || __get_user(req32.fence_arg.flags, &request->fence_arg.flags)
+	    || __get_user(req32.fence_arg.signaled, &request->fence_arg.signaled)
+	    || __get_user(req32.fence_arg.error, &request->fence_arg.error)
+	    || __get_user(req32.fence_arg.sequence, &request->fence_arg.sequence))
+		return -EFAULT;
+
+	if (copy_to_user((void __user *)arg, &req32, sizeof(req32)))
+		return -EFAULT;
+
+	return 0;
+}
+
 drm_ioctl_compat_t *i915_compat_ioctls[] = {
 	[DRM_I915_BATCHBUFFER] = compat_i915_batchbuffer,
 	[DRM_I915_CMDBUFFER] = compat_i915_cmdbuffer,
 	[DRM_I915_GETPARAM] = compat_i915_getparam,
 	[DRM_I915_IRQ_EMIT] = compat_i915_irq_emit,
-	[DRM_I915_ALLOC] = compat_i915_alloc
+	[DRM_I915_ALLOC] = compat_i915_alloc,
+	[DRM_I915_EXECBUFFER] = compat_i915_execbuffer,
 };
 
 /**
diff --git a/drivers/char/drm/i915_irq.c b/drivers/char/drm/i915_irq.c
index df03611..1e12d62 100644
--- a/drivers/char/drm/i915_irq.c
+++ b/drivers/char/drm/i915_irq.c
@@ -38,53 +38,113 @@
 #define MAX_NOPID ((u32)~0)
 
 /**
+ * i915_get_pipe - return the the pipe associated with a given plane
+ * @dev: DRM device
+ * @plane: plane to look for
+ *
+ * We need to get the pipe associated with a given plane to correctly perform
+ * vblank driven swapping, and they may not always be equal.  So look up the
+ * pipe associated with @plane here.
+ */
+static int
+i915_get_pipe(struct drm_device *dev, int plane)
+{
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	u32 dspcntr;
+
+	dspcntr = plane ? I915_READ(DSPBCNTR) : I915_READ(DSPACNTR);
+
+	return dspcntr & DISPPLANE_SEL_PIPE_MASK ? 1 : 0;
+}
+
+/**
+ * Emit a synchronous flip.
+ *
+ * This function must be called with the drawable spinlock held.
+ */
+static void
+i915_dispatch_vsync_flip(struct drm_device *dev, struct drm_drawable_info *drw,
+			 int plane)
+{
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	struct drm_i915_sarea *sarea_priv = master_priv->sarea_priv;
+	u16 x1, y1, x2, y2;
+	int pf_planes = 1 << plane;
+
+	/* If the window is visible on the other plane, we have to flip on that
+	 * plane as well.
+	 */
+	if (plane == 1) {
+		x1 = sarea_priv->planeA_x;
+		y1 = sarea_priv->planeA_y;
+		x2 = x1 + sarea_priv->planeA_w;
+		y2 = y1 + sarea_priv->planeA_h;
+	} else {
+		x1 = sarea_priv->planeB_x;
+		y1 = sarea_priv->planeB_y;
+		x2 = x1 + sarea_priv->planeB_w;
+		y2 = y1 + sarea_priv->planeB_h;
+	}
+
+	if (x2 > 0 && y2 > 0) {
+		int i, num_rects = drw->num_rects;
+		struct drm_clip_rect *rect = drw->rects;
+
+		for (i = 0; i < num_rects; i++)
+			if (!(rect[i].x1 >= x2 || rect[i].y1 >= y2 ||
+			      rect[i].x2 <= x1 || rect[i].y2 <= y1)) {
+				pf_planes = 0x3;
+
+				break;
+			}
+	}
+
+	i915_dispatch_flip(dev, pf_planes, 1);
+}
+
+/**
  * Emit blits for scheduled buffer swaps.
  *
  * This function will be called with the HW lock held.
  */
 static void i915_vblank_tasklet(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
-	unsigned long irqflags;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
 	struct list_head *list, *tmp, hits, *hit;
-	int nhits, nrects, slice[2], upper[2], lower[2], i;
+	int nhits, nrects, slice[2], upper[2], lower[2], i, num_pages;
 	unsigned counter[2] = { atomic_read(&dev->vbl_received),
 				atomic_read(&dev->vbl_received2) };
 	struct drm_drawable_info *drw;
-	drm_i915_sarea_t *sarea_priv = dev_priv->sarea_priv;
-	u32 cpp = dev_priv->cpp;
+	struct drm_i915_sarea *sarea_priv;
+	u32 cpp = dev_priv->cpp,  offsets[3];
 	u32 cmd = (cpp == 4) ? (XY_SRC_COPY_BLT_CMD |
 				XY_SRC_COPY_BLT_WRITE_ALPHA |
 				XY_SRC_COPY_BLT_WRITE_RGB)
 			     : XY_SRC_COPY_BLT_CMD;
-	u32 src_pitch = sarea_priv->pitch * cpp;
-	u32 dst_pitch = sarea_priv->pitch * cpp;
-	u32 ropcpp = (0xcc << 16) | ((cpp - 1) << 24);
+	u32 ropcpp, src_pitch, dst_pitch;
 	RING_LOCALS;
 
-	if (IS_I965G(dev) && sarea_priv->front_tiled) {
-		cmd |= XY_SRC_COPY_BLT_DST_TILED;
-		dst_pitch >>= 2;
-	}
-	if (IS_I965G(dev) && sarea_priv->back_tiled) {
-		cmd |= XY_SRC_COPY_BLT_SRC_TILED;
-		src_pitch >>= 2;
-	}
-
 	DRM_DEBUG("\n");
 
 	INIT_LIST_HEAD(&hits);
 
 	nhits = nrects = 0;
 
-	spin_lock_irqsave(&dev_priv->swaps_lock, irqflags);
+	/* No irqsave/restore necessary.  This tasklet may be run in an
+	 * interrupt context or normal context, but we don't have to worry
+	 * about getting interrupted by something acquiring the lock, because
+	 * we are the interrupt context thing that acquires the lock.
+	 */
+	spin_lock(&dev_priv->swaps_lock);
 
 	/* Find buffer swaps scheduled for this vertical blank */
 	list_for_each_safe(list, tmp, &dev_priv->vbl_swaps.head) {
 		drm_i915_vbl_swap_t *vbl_swap =
 			list_entry(list, drm_i915_vbl_swap_t, head);
+		int pipe = i915_get_pipe(dev, vbl_swap->plane);
 
-		if ((counter[vbl_swap->pipe] - vbl_swap->sequence) > (1<<23))
+		if ((counter[pipe] - vbl_swap->sequence) > (1<<23))
 			continue;
 
 		list_del(list);
@@ -126,44 +186,12 @@ static void i915_vblank_tasklet(struct drm_device *dev)
 		spin_lock(&dev_priv->swaps_lock);
 	}
 
-	if (nhits == 0) {
-		spin_unlock_irqrestore(&dev_priv->swaps_lock, irqflags);
-		return;
-	}
-
 	spin_unlock(&dev_priv->swaps_lock);
+	if (nhits == 0)
+		return;
 
 	i915_kernel_lost_context(dev);
 
-	if (IS_I965G(dev)) {
-		BEGIN_LP_RING(4);
-
-		OUT_RING(GFX_OP_DRAWRECT_INFO_I965);
-		OUT_RING(0);
-		OUT_RING(((sarea_priv->width - 1) & 0xffff) | ((sarea_priv->height - 1) << 16));
-		OUT_RING(0);
-		ADVANCE_LP_RING();
-	} else {
-		BEGIN_LP_RING(6);
-
-		OUT_RING(GFX_OP_DRAWRECT_INFO);
-		OUT_RING(0);
-		OUT_RING(0);
-		OUT_RING(sarea_priv->width | sarea_priv->height << 16);
-		OUT_RING(sarea_priv->width | sarea_priv->height << 16);
-		OUT_RING(0);
-
-		ADVANCE_LP_RING();
-	}
-
-	sarea_priv->ctxOwner = DRM_KERNEL_CONTEXT;
-
-	upper[0] = upper[1] = 0;
-	slice[0] = max(sarea_priv->pipeA_h / nhits, 1);
-	slice[1] = max(sarea_priv->pipeB_h / nhits, 1);
-	lower[0] = sarea_priv->pipeA_y + slice[0];
-	lower[1] = sarea_priv->pipeB_y + slice[0];
-
 	spin_lock(&dev->drw_lock);
 
 	/* Emit blits for buffer swaps, partitioning both outputs into as many
@@ -174,14 +202,14 @@ static void i915_vblank_tasklet(struct drm_device *dev)
 	for (i = 0; i++ < nhits;
 	     upper[0] = lower[0], lower[0] += slice[0],
 	     upper[1] = lower[1], lower[1] += slice[1]) {
-		if (i == nhits)
-			lower[0] = lower[1] = sarea_priv->height;
+		int init_drawrect = 1;
+
 
 		list_for_each(hit, &hits) {
 			drm_i915_vbl_swap_t *swap_hit =
 				list_entry(hit, drm_i915_vbl_swap_t, head);
 			struct drm_clip_rect *rect;
-			int num_rects, pipe;
+			int num_rects, plane, front, back;
 			unsigned short top, bottom;
 
 			drw = drm_get_drawable_info(dev, swap_hit->drw_id);
@@ -189,10 +217,80 @@ static void i915_vblank_tasklet(struct drm_device *dev)
 			if (!drw)
 				continue;
 
+			if (swap_hit->flip) {
+				i915_dispatch_vsync_flip(dev, drw, plane);
+				continue;
+			}
+
+			master_priv = swap_hit->minor->master->driver_priv;
+			sarea_priv = master_priv->sarea_priv;
+
+			src_pitch = sarea_priv->pitch * cpp;
+			dst_pitch = sarea_priv->pitch * cpp;
+
+			if (sarea_priv->front_tiled) {
+				cmd |= XY_SRC_COPY_BLT_DST_TILED;
+				dst_pitch >>=2;
+			}
+
+			if (sarea_priv->back_tiled) {
+				cmd |= XY_SRC_COPY_BLT_SRC_TILED;
+				src_pitch >>=2;
+			}
+
+			upper[0] = upper[1] = 0;
+			slice[0] = max(sarea_priv->planeA_h / nhits, 1);
+			slice[1] = max(sarea_priv->planeB_h / nhits, 1);
+			lower[0] = sarea_priv->planeA_y + slice[0];
+			lower[1] = sarea_priv->planeB_y + slice[0];
+
+			offsets[0] = sarea_priv->front_offset;
+			offsets[1] = sarea_priv->back_offset;
+			offsets[2] = sarea_priv->third_offset;
+			num_pages = sarea_priv->third_handle ? 3 : 2;
+			if (i == nhits)
+				lower[0] = lower[1] = sarea_priv->height;
+
+			ropcpp = (0xcc << 16) | ((cpp - 1) << 24);
+
+			plane = swap_hit->plane;
+
+			if (init_drawrect) {
+				if (IS_I965G(dev)) {
+					BEGIN_LP_RING(4);
+
+					OUT_RING(GFX_OP_DRAWRECT_INFO_I965);
+					OUT_RING(0);
+					OUT_RING(((sarea_priv->width - 1) & 0xffff) | ((sarea_priv->height - 1) << 16));
+					OUT_RING(0);
+					ADVANCE_LP_RING();
+				} else {
+					BEGIN_LP_RING(6);
+
+					OUT_RING(GFX_OP_DRAWRECT_INFO);
+					OUT_RING(0);
+					OUT_RING(0);
+					OUT_RING(sarea_priv->width | sarea_priv->height << 16);
+					OUT_RING(sarea_priv->width | sarea_priv->height << 16);
+					OUT_RING(0);
+
+					ADVANCE_LP_RING();
+				}
+
+				sarea_priv->ctxOwner = DRM_KERNEL_CONTEXT;
+
+				init_drawrect = 0;
+			}
+
 			rect = drw->rects;
-			pipe = swap_hit->pipe;
-			top = upper[pipe];
-			bottom = lower[pipe];
+			top = upper[plane];
+			bottom = lower[plane];
+
+			front = (master_priv->sarea_priv->pf_current_page >>
+				 (2 * plane)) & 0x3;
+			back = (front + 1) % num_pages;
+
+		
 
 			for (num_rects = drw->num_rects; num_rects--; rect++) {
 				int y1 = max(rect->y1, top);
@@ -207,17 +305,17 @@ static void i915_vblank_tasklet(struct drm_device *dev)
 				OUT_RING(ropcpp | dst_pitch);
 				OUT_RING((y1 << 16) | rect->x1);
 				OUT_RING((y2 << 16) | rect->x2);
-				OUT_RING(sarea_priv->front_offset);
+				OUT_RING(offsets[front]);
 				OUT_RING((y1 << 16) | rect->x1);
 				OUT_RING(src_pitch);
-				OUT_RING(sarea_priv->back_offset);
+				OUT_RING(offsets[back]);
 
 				ADVANCE_LP_RING();
 			}
 		}
 	}
 
-	spin_unlock_irqrestore(&dev->drw_lock, irqflags);
+	spin_unlock(&dev->drw_lock);
 
 	list_for_each_safe(hit, tmp, &hits) {
 		drm_i915_vbl_swap_t *swap_hit =
@@ -232,7 +330,8 @@ static void i915_vblank_tasklet(struct drm_device *dev)
 irqreturn_t i915_driver_irq_handler(DRM_IRQ_ARGS)
 {
 	struct drm_device *dev = (struct drm_device *) arg;
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_master_private *master_priv;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 	u16 temp;
 	u32 pipea_stats, pipeb_stats;
 
@@ -240,10 +339,7 @@ irqreturn_t i915_driver_irq_handler(DRM_IRQ_ARGS)
 	pipeb_stats = I915_READ(I915REG_PIPEBSTAT);
 
 	temp = I915_READ16(I915REG_INT_IDENTITY_R);
-
-	temp &= (USER_INT_FLAG | VSYNC_PIPEA_FLAG | VSYNC_PIPEB_FLAG);
-
-	DRM_DEBUG("%s flag=%08x\n", __FUNCTION__, temp);
+	temp &= (dev_priv->irq_enable_reg | USER_INT_FLAG);
 
 	if (temp == 0)
 		return IRQ_NONE;
@@ -252,10 +348,14 @@ irqreturn_t i915_driver_irq_handler(DRM_IRQ_ARGS)
 	(void) I915_READ16(I915REG_INT_IDENTITY_R);
 	DRM_READMEMORYBARRIER();
 
-	dev_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
-
-	if (temp & USER_INT_FLAG)
+	if (dev->primary->master) {
+		master_priv = dev->primary->master->driver_priv;
+		master_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+	}
+	if (temp & USER_INT_FLAG) {
 		DRM_WAKEUP(&dev_priv->irq_queue);
+		i915_fence_handler(dev);
+	}
 
 	if (temp & (VSYNC_PIPEA_FLAG | VSYNC_PIPEB_FLAG)) {
 		int vblank_pipe = dev_priv->vblank_pipe;
@@ -289,25 +389,18 @@ irqreturn_t i915_driver_irq_handler(DRM_IRQ_ARGS)
 	return IRQ_HANDLED;
 }
 
-static int i915_emit_irq(struct drm_device * dev)
+int i915_emit_irq(struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	RING_LOCALS;
 
 	i915_kernel_lost_context(dev);
 
 	DRM_DEBUG("\n");
 
-	dev_priv->sarea_priv->last_enqueue = ++dev_priv->counter;
-
-	if (dev_priv->counter > 0x7FFFFFFFUL)
-		dev_priv->sarea_priv->last_enqueue = dev_priv->counter = 1;
+	i915_emit_breadcrumb(dev);
 
-	BEGIN_LP_RING(6);
-	OUT_RING(CMD_STORE_DWORD_IDX);
-	OUT_RING(20);
-	OUT_RING(dev_priv->counter);
-	OUT_RING(0);
+	BEGIN_LP_RING(2);
 	OUT_RING(0);
 	OUT_RING(GFX_OP_USER_INTERRUPT);
 	ADVANCE_LP_RING();
@@ -315,9 +408,31 @@ static int i915_emit_irq(struct drm_device * dev)
 	return dev_priv->counter;
 }
 
+void i915_user_irq_on(struct drm_i915_private *dev_priv)
+{
+	spin_lock(&dev_priv->user_irq_lock);
+	if (dev_priv->irq_enabled && (++dev_priv->user_irq_refcount == 1)) {
+		dev_priv->irq_enable_reg |= USER_INT_FLAG;
+		I915_WRITE16(I915REG_INT_ENABLE_R, dev_priv->irq_enable_reg);
+	}
+	spin_unlock(&dev_priv->user_irq_lock);
+}
+
+void i915_user_irq_off(struct drm_i915_private *dev_priv)
+{
+	spin_lock(&dev_priv->user_irq_lock);
+	if (dev_priv->irq_enabled && (--dev_priv->user_irq_refcount == 0)) {
+		/*		dev_priv->irq_enable_reg &= ~USER_INT_FLAG;
+		  I915_WRITE16(I915REG_INT_ENABLE_R, dev_priv->irq_enable_reg);*/
+	}
+	spin_unlock(&dev_priv->user_irq_lock);
+}
+
+
 static int i915_wait_irq(struct drm_device * dev, int irq_nr)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_master_private *master_priv;
 	int ret = 0;
 
 	DRM_DEBUG("irq_nr=%d breadcrumb=%d\n", irq_nr,
@@ -326,24 +441,29 @@ static int i915_wait_irq(struct drm_device * dev, int irq_nr)
 	if (READ_BREADCRUMB(dev_priv) >= irq_nr)
 		return 0;
 
-	dev_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
-
+	i915_user_irq_on(dev_priv);
 	DRM_WAIT_ON(ret, dev_priv->irq_queue, 3 * DRM_HZ,
 		    READ_BREADCRUMB(dev_priv) >= irq_nr);
+	i915_user_irq_off(dev_priv);
 
 	if (ret == -EBUSY) {
 		DRM_ERROR("EBUSY -- rec: %d emitted: %d\n",
 			  READ_BREADCRUMB(dev_priv), (int)dev_priv->counter);
 	}
 
-	dev_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+	if (dev->primary->master) {
+		master_priv = dev->primary->master->driver_priv;
+		master_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+	}
+
 	return ret;
 }
 
-static int i915_driver_vblank_do_wait(struct drm_device *dev, unsigned int *sequence,
+static int i915_driver_vblank_do_wait(struct drm_device *dev,
+				      unsigned int *sequence,
 				      atomic_t *counter)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	unsigned int cur_vblank;
 	int ret = 0;
 
@@ -377,7 +497,7 @@ int i915_driver_vblank_wait2(struct drm_device *dev, unsigned int *sequence)
 int i915_irq_emit(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_irq_emit_t *emit = data;
 	int result;
 
@@ -403,7 +523,7 @@ int i915_irq_emit(struct drm_device *dev, void *data,
 int i915_irq_wait(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_irq_wait_t *irqwait = data;
 
 	if (!dev_priv) {
@@ -414,18 +534,18 @@ int i915_irq_wait(struct drm_device *dev, void *data,
 	return i915_wait_irq(dev, irqwait->irq_seq);
 }
 
-static void i915_enable_interrupt (struct drm_device *dev)
+void i915_enable_interrupt (struct drm_device *dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
-	u16 flag;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 
-	flag = 0;
+	dev_priv->irq_enable_reg = USER_INT_FLAG;
 	if (dev_priv->vblank_pipe & DRM_I915_VBLANK_PIPE_A)
-		flag |= VSYNC_PIPEA_FLAG;
+		dev_priv->irq_enable_reg |= VSYNC_PIPEA_FLAG;
 	if (dev_priv->vblank_pipe & DRM_I915_VBLANK_PIPE_B)
-		flag |= VSYNC_PIPEB_FLAG;
+		dev_priv->irq_enable_reg |= VSYNC_PIPEB_FLAG;
 
-	I915_WRITE16(I915REG_INT_ENABLE_R, USER_INT_FLAG | flag);
+	I915_WRITE16(I915REG_INT_ENABLE_R, dev_priv->irq_enable_reg);
+	dev_priv->irq_enabled = 1;
 }
 
 /* Set the vblank monitor pipe
@@ -433,7 +553,7 @@ static void i915_enable_interrupt (struct drm_device *dev)
 int i915_vblank_pipe_set(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_vblank_pipe_t *pipe = data;
 
 	if (!dev_priv) {
@@ -456,7 +576,7 @@ int i915_vblank_pipe_set(struct drm_device *dev, void *data,
 int i915_vblank_pipe_get(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_vblank_pipe_t *pipe = data;
 	u16 flag;
 
@@ -481,10 +601,11 @@ int i915_vblank_pipe_get(struct drm_device *dev, void *data,
 int i915_vblank_swap(struct drm_device *dev, void *data,
 		     struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv;
 	drm_i915_vblank_swap_t *swap = data;
 	drm_i915_vbl_swap_t *vbl_swap;
-	unsigned int pipe, seqtype, curseq;
+	unsigned int pipe, seqtype, curseq, plane;
 	unsigned long irqflags;
 	struct list_head *list;
 
@@ -493,18 +614,25 @@ int i915_vblank_swap(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	if (dev_priv->sarea_priv->rotation) {
+	if (!dev->primary->master)
+		return -EINVAL;
+
+	master_priv = dev->primary->master->driver_priv;
+
+	if (master_priv->sarea_priv->rotation) {
 		DRM_DEBUG("Rotation not supported\n");
 		return -EINVAL;
 	}
 
 	if (swap->seqtype & ~(_DRM_VBLANK_RELATIVE | _DRM_VBLANK_ABSOLUTE |
-			     _DRM_VBLANK_SECONDARY | _DRM_VBLANK_NEXTONMISS)) {
+			     _DRM_VBLANK_SECONDARY | _DRM_VBLANK_NEXTONMISS |
+			     _DRM_VBLANK_FLIP)) {
 		DRM_ERROR("Invalid sequence type 0x%x\n", swap->seqtype);
 		return -EINVAL;
 	}
 
-	pipe = (swap->seqtype & _DRM_VBLANK_SECONDARY) ? 1 : 0;
+	plane = (swap->seqtype & _DRM_VBLANK_SECONDARY) ? 1 : 0;
+	pipe = i915_get_pipe(dev, plane);
 
 	seqtype = swap->seqtype & (_DRM_VBLANK_RELATIVE | _DRM_VBLANK_ABSOLUTE);
 
@@ -515,6 +643,11 @@ int i915_vblank_swap(struct drm_device *dev, void *data,
 
 	spin_lock_irqsave(&dev->drw_lock, irqflags);
 
+	/* It makes no sense to schedule a swap for a drawable that doesn't have
+	 * valid information at this point. E.g. this could mean that the X
+	 * server is too old to push drawable information to the DRM, in which
+	 * case all such swaps would become ineffective.
+	 */
 	if (!drm_get_drawable_info(dev, swap->drawable)) {
 		spin_unlock_irqrestore(&dev->drw_lock, irqflags);
 		DRM_DEBUG("Invalid drawable ID %d\n", swap->drawable);
@@ -537,14 +670,43 @@ int i915_vblank_swap(struct drm_device *dev, void *data,
 		}
 	}
 
+	if (swap->seqtype & _DRM_VBLANK_FLIP) {
+		swap->sequence--;
+
+		if ((curseq - swap->sequence) <= (1<<23)) {
+			struct drm_drawable_info *drw;
+
+			LOCK_TEST_WITH_RETURN(dev, file_priv);
+
+			spin_lock_irqsave(&dev->drw_lock, irqflags);
+
+			drw = drm_get_drawable_info(dev, swap->drawable);
+
+			if (!drw) {
+				spin_unlock_irqrestore(&dev->drw_lock,
+				    irqflags);
+				DRM_DEBUG("Invalid drawable ID %d\n",
+					  swap->drawable);
+				return -EINVAL;
+			}
+
+			i915_dispatch_vsync_flip(dev, drw, plane);
+
+			spin_unlock_irqrestore(&dev->drw_lock, irqflags);
+
+			return 0;
+		}
+	}
+
 	spin_lock_irqsave(&dev_priv->swaps_lock, irqflags);
 
 	list_for_each(list, &dev_priv->vbl_swaps.head) {
 		vbl_swap = list_entry(list, drm_i915_vbl_swap_t, head);
 
 		if (vbl_swap->drw_id == swap->drawable &&
-		    vbl_swap->pipe == pipe &&
+		    vbl_swap->plane == plane &&
 		    vbl_swap->sequence == swap->sequence) {
+			vbl_swap->flip = (swap->seqtype & _DRM_VBLANK_FLIP);
 			spin_unlock_irqrestore(&dev_priv->swaps_lock, irqflags);
 			DRM_DEBUG("Already scheduled\n");
 			return 0;
@@ -568,8 +730,13 @@ int i915_vblank_swap(struct drm_device *dev, void *data,
 	DRM_DEBUG("\n");
 
 	vbl_swap->drw_id = swap->drawable;
-	vbl_swap->pipe = pipe;
+	vbl_swap->plane = plane;
 	vbl_swap->sequence = swap->sequence;
+	vbl_swap->flip = (swap->seqtype & _DRM_VBLANK_FLIP);
+	vbl_swap->minor = file_priv->minor;
+
+	if (vbl_swap->flip)
+		swap->sequence++;
 
 	spin_lock_irqsave(&dev_priv->swaps_lock, irqflags);
 
@@ -585,35 +752,43 @@ int i915_vblank_swap(struct drm_device *dev, void *data,
 */
 void i915_driver_irq_preinstall(struct drm_device * dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 
-	I915_WRITE16(I915REG_HWSTAM, 0xfffe);
+	I915_WRITE16(I915REG_HWSTAM, 0xeffe);
 	I915_WRITE16(I915REG_INT_MASK_R, 0x0);
 	I915_WRITE16(I915REG_INT_ENABLE_R, 0x0);
 }
 
 void i915_driver_irq_postinstall(struct drm_device * dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 
 	spin_lock_init(&dev_priv->swaps_lock);
 	INIT_LIST_HEAD(&dev_priv->vbl_swaps.head);
 	dev_priv->swaps_pending = 0;
 
-	if (!dev_priv->vblank_pipe)
-		dev_priv->vblank_pipe = DRM_I915_VBLANK_PIPE_A;
+	spin_lock_init(&dev_priv->user_irq_lock);
+	dev_priv->user_irq_refcount = 0;
+
 	i915_enable_interrupt(dev);
 	DRM_INIT_WAITQUEUE(&dev_priv->irq_queue);
+
+	/*
+	 * Initialize the hardware status page IRQ location.
+	 */
+
+	I915_WRITE(I915REG_INSTPM, (1 << 5) | (1 << 21));
 }
 
 void i915_driver_irq_uninstall(struct drm_device * dev)
 {
-	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
+	struct drm_i915_private *dev_priv = (struct drm_i915_private *) dev->dev_private;
 	u16 temp;
 
 	if (!dev_priv)
 		return;
 
+	dev_priv->irq_enabled = 0;
 	I915_WRITE16(I915REG_HWSTAM, 0xffff);
 	I915_WRITE16(I915REG_INT_MASK_R, 0xffff);
 	I915_WRITE16(I915REG_INT_ENABLE_R, 0x0);
diff --git a/drivers/char/drm/i915_mem.c b/drivers/char/drm/i915_mem.c
index 6126a60..15d63de 100644
--- a/drivers/char/drm/i915_mem.c
+++ b/drivers/char/drm/i915_mem.c
@@ -45,8 +45,9 @@
  */
 static void mark_block(struct drm_device * dev, struct mem_block *p, int in_use)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_sarea_t *sarea_priv = dev_priv->sarea_priv;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+	struct drm_i915_sarea *sarea_priv = master_priv->sarea_priv;
 	struct drm_tex_region *list;
 	unsigned shift, nr;
 	unsigned start;
@@ -256,7 +257,7 @@ void i915_mem_takedown(struct mem_block **heap)
 	*heap = NULL;
 }
 
-static struct mem_block **get_heap(drm_i915_private_t * dev_priv, int region)
+static struct mem_block **get_heap(struct drm_i915_private * dev_priv, int region)
 {
 	switch (region) {
 	case I915_MEM_REGION_AGP:
@@ -271,8 +272,8 @@ static struct mem_block **get_heap(drm_i915_private_t * dev_priv, int region)
 int i915_mem_alloc(struct drm_device *dev, void *data,
 		   struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_mem_alloc_t *alloc = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_mem_alloc *alloc = data;
 	struct mem_block *block, **heap;
 
 	if (!dev_priv) {
@@ -309,8 +310,8 @@ int i915_mem_alloc(struct drm_device *dev, void *data,
 int i915_mem_free(struct drm_device *dev, void *data,
 		  struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_mem_free_t *memfree = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_mem_free *memfree = data;
 	struct mem_block *block, **heap;
 
 	if (!dev_priv) {
@@ -337,8 +338,8 @@ int i915_mem_free(struct drm_device *dev, void *data,
 int i915_mem_init_heap(struct drm_device *dev, void *data,
 		       struct drm_file *file_priv)
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_mem_init_heap_t *initheap = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_mem_init_heap *initheap = data;
 	struct mem_block **heap;
 
 	if (!dev_priv) {
@@ -361,8 +362,8 @@ int i915_mem_init_heap(struct drm_device *dev, void *data,
 int i915_mem_destroy_heap( struct drm_device *dev, void *data,
 			   struct drm_file *file_priv )
 {
-	drm_i915_private_t *dev_priv = dev->dev_private;
-	drm_i915_mem_destroy_heap_t *destroyheap = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_mem_destroy_heap *destroyheap = data;
 	struct mem_block **heap;
 
 	if ( !dev_priv ) {
diff --git a/drivers/char/drm/intel_crt.c b/drivers/char/drm/intel_crt.c
new file mode 100644
index 0000000..a9fb50a
--- /dev/null
+++ b/drivers/char/drm/intel_crt.c
@@ -0,0 +1,274 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+#include <linux/i2c.h>
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+static void intel_crt_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 temp;
+	
+	temp = I915_READ(ADPA);
+	temp &= ~(ADPA_HSYNC_CNTL_DISABLE | ADPA_VSYNC_CNTL_DISABLE);
+	temp &= ~ADPA_DAC_ENABLE;
+	
+	switch(mode) {
+	case DPMSModeOn:
+		temp |= ADPA_DAC_ENABLE;
+		break;
+	case DPMSModeStandby:
+		temp |= ADPA_DAC_ENABLE | ADPA_HSYNC_CNTL_DISABLE;
+		break;
+	case DPMSModeSuspend:
+		temp |= ADPA_DAC_ENABLE | ADPA_VSYNC_CNTL_DISABLE;
+		break;
+	case DPMSModeOff:
+		temp |= ADPA_HSYNC_CNTL_DISABLE | ADPA_VSYNC_CNTL_DISABLE;
+		break;
+	}
+	
+	I915_WRITE(ADPA, temp);
+}
+
+static void intel_crt_save(struct drm_output *output)
+{
+	
+}
+
+static void intel_crt_restore(struct drm_output *output)
+{
+
+}
+
+static int intel_crt_mode_valid(struct drm_output *output,
+				struct drm_display_mode *mode)
+{
+	if (mode->flags & V_DBLSCAN)
+		return MODE_NO_DBLESCAN;
+
+	if (mode->clock > 400000 || mode->clock < 25000)
+		return MODE_CLOCK_RANGE;
+
+	return MODE_OK;
+}
+
+static bool intel_crt_mode_fixup(struct drm_output *output,
+				 struct drm_display_mode *mode,
+				 struct drm_display_mode *adjusted_mode)
+{
+	return true;
+}
+
+static void intel_crt_mode_set(struct drm_output *output,
+			       struct drm_display_mode *mode,
+			       struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int dpll_md_reg;
+	u32 adpa, dpll_md;
+
+	if (intel_crtc->pipe == 0) 
+		dpll_md_reg = DPLL_A_MD;
+	else
+		dpll_md_reg = DPLL_B_MD;
+
+	/*
+	 * Disable separate mode multiplier used when cloning SDVO to CRT
+	 * XXX this needs to be adjusted when we really are cloning
+	 */
+	if (IS_I965G(dev)) {
+		dpll_md = I915_READ(dpll_md_reg);
+		I915_WRITE(dpll_md_reg,
+			   dpll_md & ~DPLL_MD_UDI_MULTIPLIER_MASK);
+	}
+	
+	adpa = 0;
+	if (adjusted_mode->flags & V_PHSYNC)
+		adpa |= ADPA_HSYNC_ACTIVE_HIGH;
+	if (adjusted_mode->flags & V_PVSYNC)
+		adpa |= ADPA_VSYNC_ACTIVE_HIGH;
+	
+	if (intel_crtc->pipe == 0)
+		adpa |= ADPA_PIPE_A_SELECT;
+	else
+		adpa |= ADPA_PIPE_B_SELECT;
+	
+	I915_WRITE(ADPA, adpa);
+}
+
+/**
+ * Uses CRT_HOTPLUG_EN and CRT_HOTPLUG_STAT to detect CRT presence.
+ *
+ * Only for I945G/GM.
+ *
+ * \return TRUE if CRT is connected.
+ * \return FALSE if CRT is disconnected.
+ */
+static bool intel_crt_detect_hotplug(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 temp;
+#if 1
+	unsigned long timeout = jiffies + msecs_to_jiffies(1000);
+
+	temp = I915_READ(PORT_HOTPLUG_EN);
+
+	I915_WRITE(PORT_HOTPLUG_EN,
+		   temp | CRT_HOTPLUG_FORCE_DETECT | (1 << 5));
+
+	do {
+		if (!(I915_READ(PORT_HOTPLUG_EN) & CRT_HOTPLUG_FORCE_DETECT))
+			break;
+		msleep(1);
+	} while (time_after(timeout, jiffies));
+
+	if ((I915_READ(PORT_HOTPLUG_STAT) & CRT_HOTPLUG_MONITOR_MASK) ==
+	    CRT_HOTPLUG_MONITOR_COLOR)
+		return true;
+
+	return false;
+#else
+	temp = I915_READ(PORT_HOTPLUG_STAT);
+	DRM_DEBUG("HST 0x%08x\n", temp);
+
+	if (temp & (1 << 8) && temp & (1 << 9))
+		return true;
+
+	return false;
+#endif
+}
+
+static bool intel_crt_detect_ddc(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	/* CRT should always be at 0, but check anyway */
+	if (intel_output->type != INTEL_OUTPUT_ANALOG)
+		return false;
+	
+	return intel_ddc_probe(output);
+}
+
+static enum drm_output_status intel_crt_detect(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	
+	if (IS_I945G(dev) || IS_I945GM(dev) || IS_I965G(dev)) {
+		if (intel_crt_detect_hotplug(output))
+			return output_status_connected;
+		else
+			return output_status_disconnected;
+	}
+
+	if (intel_crt_detect_ddc(output))
+		return output_status_connected;
+
+	/* TODO use load detect */
+	return output_status_unknown;
+}
+
+static void intel_crt_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	intel_i2c_destroy(intel_output->ddc_bus);
+	kfree(output->driver_private);
+}
+
+static int intel_crt_get_modes(struct drm_output *output)
+{
+	return intel_ddc_get_modes(output);
+}
+
+static bool intel_crt_set_property(struct drm_output *output,
+				  struct drm_property *property,
+				  uint64_t value)
+{
+	struct drm_device *dev = output->dev;
+
+	if (property == dev->mode_config.dpms_property)
+		intel_crt_dpms(output, (uint32_t)(value & 0xf));
+
+	return true;
+}
+
+/*
+ * Routines for controlling stuff on the analog port
+ */
+static const struct drm_output_funcs intel_crt_output_funcs = {
+	.dpms = intel_crt_dpms,
+	.save = intel_crt_save,
+	.restore = intel_crt_restore,
+	.mode_valid = intel_crt_mode_valid,
+	.mode_fixup = intel_crt_mode_fixup,
+	.prepare = intel_output_prepare,
+	.mode_set = intel_crt_mode_set,
+	.commit = intel_output_commit,
+	.detect = intel_crt_detect,
+	.get_modes = intel_crt_get_modes,
+	.cleanup = intel_crt_destroy,
+	.set_property = intel_crt_set_property,
+};
+
+void intel_crt_init(struct drm_device *dev)
+{
+	struct drm_output *output;
+	struct intel_output *intel_output;
+
+	output = drm_output_create(dev, &intel_crt_output_funcs,
+				   DRM_MODE_OUTPUT_DAC);
+
+	intel_output = kmalloc(sizeof(struct intel_output), GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+	/* Set up the DDC bus. */
+	intel_output->ddc_bus = intel_i2c_create(dev, GPIOA, "CRTDDC_A");
+	if (!intel_output->ddc_bus) {
+		dev_printk(KERN_ERR, &dev->pdev->dev, "DDC bus registration "
+			   "failed.\n");
+		return;
+	}
+
+	intel_output->type = INTEL_OUTPUT_ANALOG;
+	output->driver_private = intel_output;
+	output->interlace_allowed = 0;
+	output->doublescan_allowed = 0;
+
+	drm_output_attach_property(output, dev->mode_config.connector_type_property, ConnectorVGA);
+}
diff --git a/drivers/char/drm/intel_display.c b/drivers/char/drm/intel_display.c
new file mode 100644
index 0000000..2e566d9
--- /dev/null
+++ b/drivers/char/drm/intel_display.c
@@ -0,0 +1,1370 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+#include <linux/i2c.h>
+#include "drmP.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+bool intel_pipe_has_type (struct drm_crtc *crtc, int type);
+
+typedef struct {
+    /* given values */    
+    int n;
+    int m1, m2;
+    int p1, p2;
+    /* derived values */
+    int	dot;
+    int	vco;
+    int	m;
+    int	p;
+} intel_clock_t;
+
+typedef struct {
+    int	min, max;
+} intel_range_t;
+
+typedef struct {
+    int	dot_limit;
+    int	p2_slow, p2_fast;
+} intel_p2_t;
+
+#define INTEL_P2_NUM		      2
+
+typedef struct {
+    intel_range_t   dot, vco, n, m, m1, m2, p, p1;
+    intel_p2_t	    p2;
+} intel_limit_t;
+
+#define I8XX_DOT_MIN		  25000
+#define I8XX_DOT_MAX		 350000
+#define I8XX_VCO_MIN		 930000
+#define I8XX_VCO_MAX		1400000
+#define I8XX_N_MIN		      3
+#define I8XX_N_MAX		     16
+#define I8XX_M_MIN		     96
+#define I8XX_M_MAX		    140
+#define I8XX_M1_MIN		     18
+#define I8XX_M1_MAX		     26
+#define I8XX_M2_MIN		      6
+#define I8XX_M2_MAX		     16
+#define I8XX_P_MIN		      4
+#define I8XX_P_MAX		    128
+#define I8XX_P1_MIN		      2
+#define I8XX_P1_MAX		     33
+#define I8XX_P1_LVDS_MIN	      1
+#define I8XX_P1_LVDS_MAX	      6
+#define I8XX_P2_SLOW		      4
+#define I8XX_P2_FAST		      2
+#define I8XX_P2_LVDS_SLOW	      14
+#define I8XX_P2_LVDS_FAST	      14 /* No fast option */
+#define I8XX_P2_SLOW_LIMIT	 165000
+
+#define I9XX_DOT_MIN		  20000
+#define I9XX_DOT_MAX		 400000
+#define I9XX_VCO_MIN		1400000
+#define I9XX_VCO_MAX		2800000
+#define I9XX_N_MIN		      3
+#define I9XX_N_MAX		      8
+#define I9XX_M_MIN		     70
+#define I9XX_M_MAX		    120
+#define I9XX_M1_MIN		     10
+#define I9XX_M1_MAX		     20
+#define I9XX_M2_MIN		      5
+#define I9XX_M2_MAX		      9
+#define I9XX_P_SDVO_DAC_MIN	      5
+#define I9XX_P_SDVO_DAC_MAX	     80
+#define I9XX_P_LVDS_MIN		      7
+#define I9XX_P_LVDS_MAX		     98
+#define I9XX_P1_MIN		      1
+#define I9XX_P1_MAX		      8
+#define I9XX_P2_SDVO_DAC_SLOW		     10
+#define I9XX_P2_SDVO_DAC_FAST		      5
+#define I9XX_P2_SDVO_DAC_SLOW_LIMIT	 200000
+#define I9XX_P2_LVDS_SLOW		     14
+#define I9XX_P2_LVDS_FAST		      7
+#define I9XX_P2_LVDS_SLOW_LIMIT		 112000
+
+#define INTEL_LIMIT_I8XX_DVO_DAC    0
+#define INTEL_LIMIT_I8XX_LVDS	    1
+#define INTEL_LIMIT_I9XX_SDVO_DAC   2
+#define INTEL_LIMIT_I9XX_LVDS	    3
+
+static const intel_limit_t intel_limits[] = {
+    { /* INTEL_LIMIT_I8XX_DVO_DAC */
+        .dot = { .min = I8XX_DOT_MIN,		.max = I8XX_DOT_MAX },
+        .vco = { .min = I8XX_VCO_MIN,		.max = I8XX_VCO_MAX },
+        .n   = { .min = I8XX_N_MIN,		.max = I8XX_N_MAX },
+        .m   = { .min = I8XX_M_MIN,		.max = I8XX_M_MAX },
+        .m1  = { .min = I8XX_M1_MIN,		.max = I8XX_M1_MAX },
+        .m2  = { .min = I8XX_M2_MIN,		.max = I8XX_M2_MAX },
+        .p   = { .min = I8XX_P_MIN,		.max = I8XX_P_MAX },
+        .p1  = { .min = I8XX_P1_MIN,		.max = I8XX_P1_MAX },
+	.p2  = { .dot_limit = I8XX_P2_SLOW_LIMIT,
+		 .p2_slow = I8XX_P2_SLOW,	.p2_fast = I8XX_P2_FAST },
+    },
+    { /* INTEL_LIMIT_I8XX_LVDS */
+        .dot = { .min = I8XX_DOT_MIN,		.max = I8XX_DOT_MAX },
+        .vco = { .min = I8XX_VCO_MIN,		.max = I8XX_VCO_MAX },
+        .n   = { .min = I8XX_N_MIN,		.max = I8XX_N_MAX },
+        .m   = { .min = I8XX_M_MIN,		.max = I8XX_M_MAX },
+        .m1  = { .min = I8XX_M1_MIN,		.max = I8XX_M1_MAX },
+        .m2  = { .min = I8XX_M2_MIN,		.max = I8XX_M2_MAX },
+        .p   = { .min = I8XX_P_MIN,		.max = I8XX_P_MAX },
+        .p1  = { .min = I8XX_P1_LVDS_MIN,	.max = I8XX_P1_LVDS_MAX },
+	.p2  = { .dot_limit = I8XX_P2_SLOW_LIMIT,
+		 .p2_slow = I8XX_P2_LVDS_SLOW,	.p2_fast = I8XX_P2_LVDS_FAST },
+    },
+    { /* INTEL_LIMIT_I9XX_SDVO_DAC */
+        .dot = { .min = I9XX_DOT_MIN,		.max = I9XX_DOT_MAX },
+        .vco = { .min = I9XX_VCO_MIN,		.max = I9XX_VCO_MAX },
+        .n   = { .min = I9XX_N_MIN,		.max = I9XX_N_MAX },
+        .m   = { .min = I9XX_M_MIN,		.max = I9XX_M_MAX },
+        .m1  = { .min = I9XX_M1_MIN,		.max = I9XX_M1_MAX },
+        .m2  = { .min = I9XX_M2_MIN,		.max = I9XX_M2_MAX },
+        .p   = { .min = I9XX_P_SDVO_DAC_MIN,	.max = I9XX_P_SDVO_DAC_MAX },
+        .p1  = { .min = I9XX_P1_MIN,		.max = I9XX_P1_MAX },
+	.p2  = { .dot_limit = I9XX_P2_SDVO_DAC_SLOW_LIMIT,
+		 .p2_slow = I9XX_P2_SDVO_DAC_SLOW,	.p2_fast = I9XX_P2_SDVO_DAC_FAST },
+    },
+    { /* INTEL_LIMIT_I9XX_LVDS */
+        .dot = { .min = I9XX_DOT_MIN,		.max = I9XX_DOT_MAX },
+        .vco = { .min = I9XX_VCO_MIN,		.max = I9XX_VCO_MAX },
+        .n   = { .min = I9XX_N_MIN,		.max = I9XX_N_MAX },
+        .m   = { .min = I9XX_M_MIN,		.max = I9XX_M_MAX },
+        .m1  = { .min = I9XX_M1_MIN,		.max = I9XX_M1_MAX },
+        .m2  = { .min = I9XX_M2_MIN,		.max = I9XX_M2_MAX },
+        .p   = { .min = I9XX_P_LVDS_MIN,	.max = I9XX_P_LVDS_MAX },
+        .p1  = { .min = I9XX_P1_MIN,		.max = I9XX_P1_MAX },
+	/* The single-channel range is 25-112Mhz, and dual-channel
+	 * is 80-224Mhz.  Prefer single channel as much as possible.
+	 */
+	.p2  = { .dot_limit = I9XX_P2_LVDS_SLOW_LIMIT,
+		 .p2_slow = I9XX_P2_LVDS_SLOW,	.p2_fast = I9XX_P2_LVDS_FAST },
+    },
+};
+
+static const intel_limit_t *intel_limit(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	const intel_limit_t *limit;
+	
+	if (IS_I9XX(dev)) {
+		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+			limit = &intel_limits[INTEL_LIMIT_I9XX_LVDS];
+		else
+			limit = &intel_limits[INTEL_LIMIT_I9XX_SDVO_DAC];
+	} else {
+		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+			limit = &intel_limits[INTEL_LIMIT_I8XX_LVDS];
+		else
+			limit = &intel_limits[INTEL_LIMIT_I8XX_DVO_DAC];
+	}
+	return limit;
+}
+
+/** Derive the pixel clock for the given refclk and divisors for 8xx chips. */
+
+static void i8xx_clock(int refclk, intel_clock_t *clock)
+{
+	clock->m = 5 * (clock->m1 + 2) + (clock->m2 + 2);
+	clock->p = clock->p1 * clock->p2;
+	clock->vco = refclk * clock->m / (clock->n + 2);
+	clock->dot = clock->vco / clock->p;
+}
+
+/** Derive the pixel clock for the given refclk and divisors for 9xx chips. */
+
+static void i9xx_clock(int refclk, intel_clock_t *clock)
+{
+	clock->m = 5 * (clock->m1 + 2) + (clock->m2 + 2);
+	clock->p = clock->p1 * clock->p2;
+	clock->vco = refclk * clock->m / (clock->n + 2);
+	clock->dot = clock->vco / clock->p;
+}
+
+static void intel_clock(struct drm_device *dev, int refclk,
+			intel_clock_t *clock)
+{
+	if (IS_I9XX(dev))
+		return i9xx_clock (refclk, clock);
+	else
+		return i8xx_clock (refclk, clock);
+}
+
+/**
+ * Returns whether any output on the specified pipe is of the specified type
+ */
+bool intel_pipe_has_type (struct drm_crtc *crtc, int type)
+{
+    struct drm_device *dev = crtc->dev;
+    struct drm_mode_config *mode_config = &dev->mode_config;
+    struct drm_output *l_entry;
+
+    list_for_each_entry(l_entry, &mode_config->output_list, head) {
+	    if (l_entry->crtc == crtc) {
+		    struct intel_output *intel_output = l_entry->driver_private;
+		    if (intel_output->type == type)
+			    return true;
+	    }
+    }
+    return false;
+}
+
+#define INTELPllInvalid(s)   { /* ErrorF (s) */; return false; }
+/**
+ * Returns whether the given set of divisors are valid for a given refclk with
+ * the given outputs.
+ */
+
+static bool intel_PLL_is_valid(struct drm_crtc *crtc, intel_clock_t *clock)
+{
+	const intel_limit_t *limit = intel_limit (crtc);
+	
+	if (clock->p1  < limit->p1.min  || limit->p1.max  < clock->p1)
+		INTELPllInvalid ("p1 out of range\n");
+	if (clock->p   < limit->p.min   || limit->p.max   < clock->p)
+		INTELPllInvalid ("p out of range\n");
+	if (clock->m2  < limit->m2.min  || limit->m2.max  < clock->m2)
+		INTELPllInvalid ("m2 out of range\n");
+	if (clock->m1  < limit->m1.min  || limit->m1.max  < clock->m1)
+		INTELPllInvalid ("m1 out of range\n");
+	if (clock->m1 <= clock->m2)
+		INTELPllInvalid ("m1 <= m2\n");
+	if (clock->m   < limit->m.min   || limit->m.max   < clock->m)
+		INTELPllInvalid ("m out of range\n");
+	if (clock->n   < limit->n.min   || limit->n.max   < clock->n)
+		INTELPllInvalid ("n out of range\n");
+	if (clock->vco < limit->vco.min || limit->vco.max < clock->vco)
+		INTELPllInvalid ("vco out of range\n");
+	/* XXX: We may need to be checking "Dot clock" depending on the multiplier,
+	 * output, etc., rather than just a single range.
+	 */
+	if (clock->dot < limit->dot.min || limit->dot.max < clock->dot)
+		INTELPllInvalid ("dot out of range\n");
+	
+	return true;
+}
+
+/**
+ * Returns a set of divisors for the desired target clock with the given
+ * refclk, or FALSE.  The returned values represent the clock equation:
+ * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.
+ */
+static bool intel_find_best_PLL(struct drm_crtc *crtc, int target,
+				int refclk, intel_clock_t *best_clock)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	intel_clock_t clock;
+	const intel_limit_t *limit = intel_limit(crtc);
+	int err = target;
+
+	if (IS_I9XX(dev) && intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
+	    (I915_READ(LVDS) & LVDS_PORT_EN) != 0) {
+		/*
+		 * For LVDS, if the panel is on, just rely on its current
+		 * settings for dual-channel.  We haven't figured out how to
+		 * reliably set up different single/dual channel state, if we
+		 * even can.
+		 */
+		if ((I915_READ(LVDS) & LVDS_CLKB_POWER_MASK) ==
+		    LVDS_CLKB_POWER_UP)
+			clock.p2 = limit->p2.p2_fast;
+		else
+			clock.p2 = limit->p2.p2_slow;
+	} else {
+		if (target < limit->p2.dot_limit)
+			clock.p2 = limit->p2.p2_slow;
+		else
+			clock.p2 = limit->p2.p2_fast;
+	}
+	
+	memset (best_clock, 0, sizeof (*best_clock));
+	
+	for (clock.m1 = limit->m1.min; clock.m1 <= limit->m1.max; clock.m1++) {
+		for (clock.m2 = limit->m2.min; clock.m2 < clock.m1 &&
+			     clock.m2 <= limit->m2.max; clock.m2++) {
+			for (clock.n = limit->n.min; clock.n <= limit->n.max;
+			     clock.n++) {
+				for (clock.p1 = limit->p1.min;
+				     clock.p1 <= limit->p1.max; clock.p1++) {
+					int this_err;
+					
+					intel_clock(dev, refclk, &clock);
+					
+					if (!intel_PLL_is_valid(crtc, &clock))
+						continue;
+					
+					this_err = abs(clock.dot - target);
+					if (this_err < err) {
+						*best_clock = clock;
+						err = this_err;
+					}
+				}
+			}
+		}
+	}
+
+	return (err != target);
+}
+
+void
+intel_set_vblank(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc;
+	struct intel_crtc *intel_crtc;
+	int vbl_pipe = 0;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		intel_crtc = crtc->driver_private;
+
+		if (crtc->enabled)
+			vbl_pipe |= (1<<intel_crtc->pipe);
+	}
+
+	dev_priv->vblank_pipe = vbl_pipe;
+	i915_enable_interrupt(dev);
+}
+void
+intel_wait_for_vblank(struct drm_device *dev)
+{
+	/* Wait for 20ms, i.e. one cycle at 50hz. */
+	udelay(20000);
+}
+
+void
+intel_pipe_set_base(struct drm_crtc *crtc, int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_master_private *master_priv;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	unsigned long Start, Offset;
+	int dspbase = (pipe == 0 ? DSPABASE : DSPBBASE);
+	int dspsurf = (pipe == 0 ? DSPASURF : DSPBSURF);
+	int dspstride = (pipe == 0) ? DSPASTRIDE : DSPBSTRIDE;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	u32 dspcntr;
+
+	/* no fb bound */
+	if (!crtc->fb) {
+		DRM_DEBUG("No FB bound\n");
+		return;
+	}
+
+	Start = crtc->fb->bo->offset;
+	Offset = y * crtc->fb->pitch + x * (crtc->fb->bits_per_pixel / 8);
+
+	I915_WRITE(dspstride, crtc->fb->pitch);
+
+	dspcntr = I915_READ(dspcntr_reg);
+	switch (crtc->fb->bits_per_pixel) {
+	case 8:
+		dspcntr |= DISPPLANE_8BPP;
+		break;
+	case 16:
+		if (crtc->fb->depth == 15)
+			dspcntr |= DISPPLANE_15_16BPP;
+		else
+			dspcntr |= DISPPLANE_16BPP;
+		break;
+	case 24:
+	case 32:
+		dspcntr |= DISPPLANE_32BPP_NO_ALPHA;
+		break;
+	default:
+		DRM_ERROR("Unknown color depth\n");
+		return;
+	}
+	I915_WRITE(dspcntr_reg, dspcntr);
+
+	DRM_DEBUG("Writing base %08lX %08lX %d %d\n", Start, Offset, x, y);
+	if (IS_I965G(dev)) {
+		I915_WRITE(dspbase, Offset);
+		I915_READ(dspbase);
+		I915_WRITE(dspsurf, Start);
+		I915_READ(dspsurf);
+	} else {
+		I915_WRITE(dspbase, Start + Offset);
+		I915_READ(dspbase);
+	}
+	
+
+	if (!dev->primary->master)
+		return;
+
+	master_priv = dev->primary->master->driver_priv;
+	if (!master_priv->sarea_priv) 
+		return;
+		
+	switch (pipe) {
+	case 0:
+		master_priv->sarea_priv->planeA_x = x;
+		master_priv->sarea_priv->planeA_y = y;
+		break;
+	case 1:
+		master_priv->sarea_priv->planeB_x = x;
+		master_priv->sarea_priv->planeB_y = y;
+		break;
+	default:
+		DRM_ERROR("Can't update pipe %d in SAREA\n", pipe);
+		break;
+	}
+}
+
+
+
+/**
+ * Sets the power management mode of the pipe and plane.
+ *
+ * This code should probably grow support for turning the cursor off and back
+ * on appropriately at the same time as we're turning the pipe off/on.
+ */
+static void intel_crtc_dpms(struct drm_crtc *crtc, int mode)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_master_private *master_priv;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int dspbase_reg = (pipe == 0) ? DSPABASE : DSPBBASE;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	u32 temp;
+	bool enabled;
+
+	/* XXX: When our outputs are all unaware of DPMS modes other than off
+	 * and on, we should map those modes to DPMSModeOff in the CRTC.
+	 */
+	switch (mode) {
+	case DPMSModeOn:
+	case DPMSModeStandby:
+	case DPMSModeSuspend:
+		/* Enable the DPLL */
+		temp = I915_READ(dpll_reg);
+		if ((temp & DPLL_VCO_ENABLE) == 0) {
+			I915_WRITE(dpll_reg, temp);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+			I915_WRITE(dpll_reg, temp | DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+			I915_WRITE(dpll_reg, temp | DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+		}
+		
+		/* Enable the pipe */
+		temp = I915_READ(pipeconf_reg);
+		if ((temp & PIPEACONF_ENABLE) == 0)
+			I915_WRITE(pipeconf_reg, temp | PIPEACONF_ENABLE);
+		
+		/* Enable the plane */
+		temp = I915_READ(dspcntr_reg);
+		if ((temp & DISPLAY_PLANE_ENABLE) == 0) {
+			I915_WRITE(dspcntr_reg, temp | DISPLAY_PLANE_ENABLE);
+			/* Flush the plane changes */
+			I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+		}
+		
+		intel_crtc_load_lut(crtc);
+		
+		/* Give the overlay scaler a chance to enable if it's on this pipe */
+		//intel_crtc_dpms_video(crtc, true); TODO
+	break;
+	case DPMSModeOff:
+		/* Give the overlay scaler a chance to disable if it's on this pipe */
+		//intel_crtc_dpms_video(crtc, false); TODO
+		
+		/* Disable the VGA plane that we never use */
+		I915_WRITE(VGACNTRL, VGA_DISP_DISABLE);
+		
+		/* Disable display plane */
+		temp = I915_READ(dspcntr_reg);
+		if ((temp & DISPLAY_PLANE_ENABLE) != 0) {
+			I915_WRITE(dspcntr_reg, temp & ~DISPLAY_PLANE_ENABLE);
+			/* Flush the plane changes */
+			I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+			I915_READ(dspbase_reg);
+		}
+		
+		if (!IS_I9XX(dev)) {
+			/* Wait for vblank for the disable to take effect */
+			intel_wait_for_vblank(dev);
+		}
+		
+		/* Next, disable display pipes */
+		temp = I915_READ(pipeconf_reg);
+		if ((temp & PIPEACONF_ENABLE) != 0) {
+			I915_WRITE(pipeconf_reg, temp & ~PIPEACONF_ENABLE);
+			I915_READ(pipeconf_reg);
+		}
+		
+		/* Wait for vblank for the disable to take effect. */
+		intel_wait_for_vblank(dev);
+		
+		temp = I915_READ(dpll_reg);
+		if ((temp & DPLL_VCO_ENABLE) != 0) {
+			I915_WRITE(dpll_reg, temp & ~DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+		}
+		
+		/* Wait for the clocks to turn off. */
+		udelay(150);
+		break;
+	}
+
+	if (!dev->primary->master)
+		return;	
+
+	master_priv = dev->primary->master->driver_priv;
+	if (!master_priv->sarea_priv)
+		return;
+
+	enabled = crtc->enabled && mode != DPMSModeOff;
+	
+	switch (pipe) {
+	case 0:
+		master_priv->sarea_priv->planeA_w = enabled ? crtc->mode.hdisplay : 0;
+		master_priv->sarea_priv->planeA_h = enabled ? crtc->mode.vdisplay : 0;
+		break;
+	case 1:
+		master_priv->sarea_priv->planeB_w = enabled ? crtc->mode.hdisplay : 0;
+		master_priv->sarea_priv->planeB_h = enabled ? crtc->mode.vdisplay : 0;
+		break;
+	default:
+		DRM_ERROR("Can't update pipe %d in SAREA\n", pipe);
+		break;
+	}
+}
+
+static bool intel_crtc_lock(struct drm_crtc *crtc)
+{
+   /* Sync the engine before mode switch */
+//   i830WaitSync(crtc->scrn);
+
+#if 0 // TODO def XF86DRI
+    return I830DRILock(crtc->scrn);
+#else
+    return false;
+#endif
+}
+
+static void intel_crtc_unlock (struct drm_crtc *crtc)
+{
+#if 0 // TODO def XF86DRI
+    I830DRIUnlock (crtc->scrn);
+#endif
+}
+
+static void intel_crtc_prepare (struct drm_crtc *crtc)
+{
+	crtc->funcs->dpms(crtc, DPMSModeOff);
+}
+
+static void intel_crtc_commit (struct drm_crtc *crtc)
+{
+	crtc->funcs->dpms(crtc, DPMSModeOn);
+}
+
+void intel_output_prepare (struct drm_output *output)
+{
+	/* lvds has its own version of prepare see intel_lvds_prepare */
+	output->funcs->dpms(output, DPMSModeOff);
+}
+
+void intel_output_commit (struct drm_output *output)
+{
+	/* lvds has its own version of commit see intel_lvds_commit */
+	output->funcs->dpms(output, DPMSModeOn);
+}
+
+static bool intel_crtc_mode_fixup(struct drm_crtc *crtc,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	return true;
+}
+
+
+/** Returns the core display clock speed for i830 - i945 */
+static int intel_get_core_clock_speed(struct drm_device *dev)
+{
+
+	/* Core clock values taken from the published datasheets.
+	 * The 830 may go up to 166 Mhz, which we should check.
+	 */
+	if (IS_I945G(dev))
+		return 400000;
+	else if (IS_I915G(dev))
+		return 333000;
+	else if (IS_I945GM(dev) || IS_845G(dev))
+		return 200000;
+	else if (IS_I915GM(dev)) {
+		u16 gcfgc = 0;
+
+		pci_read_config_word(dev->pdev, I915_GCFGC, &gcfgc);
+		
+		if (gcfgc & I915_LOW_FREQUENCY_ENABLE)
+			return 133000;
+		else {
+			switch (gcfgc & I915_DISPLAY_CLOCK_MASK) {
+			case I915_DISPLAY_CLOCK_333_MHZ:
+				return 333000;
+			default:
+			case I915_DISPLAY_CLOCK_190_200_MHZ:
+				return 190000;
+			}
+		}
+	} else if (IS_I865G(dev))
+		return 266000;
+	else if (IS_I855(dev)) {
+#if 0
+		PCITAG bridge = pciTag(0, 0, 0); /* This is always the host bridge */
+		u16 hpllcc = pciReadWord(bridge, I855_HPLLCC);
+		
+#endif
+		u16 hpllcc = 0;
+		/* Assume that the hardware is in the high speed state.  This
+		 * should be the default.
+		 */
+		switch (hpllcc & I855_CLOCK_CONTROL_MASK) {
+		case I855_CLOCK_133_200:
+		case I855_CLOCK_100_200:
+			return 200000;
+		case I855_CLOCK_166_250:
+			return 250000;
+		case I855_CLOCK_100_133:
+			return 133000;
+		}
+	} else /* 852, 830 */
+		return 133000;
+	
+	return 0; /* Silence gcc warning */
+}
+
+
+/**
+ * Return the pipe currently connected to the panel fitter,
+ * or -1 if the panel fitter is not present or not in use
+ */
+static int intel_panel_fitter_pipe (struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32  pfit_control;
+    
+	/* i830 doesn't have a panel fitter */
+	if (IS_I830(dev))
+		return -1;
+    
+	pfit_control = I915_READ(PFIT_CONTROL);
+    
+	/* See if the panel fitter is in use */
+	if ((pfit_control & PFIT_ENABLE) == 0)
+		return -1;
+	
+	/* 965 can place panel fitter on either pipe */
+	if (IS_I965G(dev))
+		return (pfit_control >> 29) & 0x3;
+	
+	/* older chips can only use pipe 1 */
+	return 1;
+}
+
+static void intel_crtc_mode_set(struct drm_crtc *crtc,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode,
+				int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int fp_reg = (pipe == 0) ? FPA0 : FPB0;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dpll_md_reg = (intel_crtc->pipe == 0) ? DPLL_A_MD : DPLL_B_MD;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	int htot_reg = (pipe == 0) ? HTOTAL_A : HTOTAL_B;
+	int hblank_reg = (pipe == 0) ? HBLANK_A : HBLANK_B;
+	int hsync_reg = (pipe == 0) ? HSYNC_A : HSYNC_B;
+	int vtot_reg = (pipe == 0) ? VTOTAL_A : VTOTAL_B;
+	int vblank_reg = (pipe == 0) ? VBLANK_A : VBLANK_B;
+	int vsync_reg = (pipe == 0) ? VSYNC_A : VSYNC_B;
+	int dspsize_reg = (pipe == 0) ? DSPASIZE : DSPBSIZE;
+	int dsppos_reg = (pipe == 0) ? DSPAPOS : DSPBPOS;
+	int pipesrc_reg = (pipe == 0) ? PIPEASRC : PIPEBSRC;
+	int refclk;
+	intel_clock_t clock;
+	u32 dpll = 0, fp = 0, dspcntr, pipeconf;
+	bool ok, is_sdvo = false, is_dvo = false;
+	bool is_crt = false, is_lvds = false, is_tv = false;
+	struct drm_mode_config *mode_config = &dev->mode_config;
+	struct drm_output *output;
+
+	list_for_each_entry(output, &mode_config->output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+
+		if (output->crtc != crtc)
+			continue;
+
+		switch (intel_output->type) {
+		case INTEL_OUTPUT_LVDS:
+			is_lvds = true;
+			break;
+		case INTEL_OUTPUT_SDVO:
+			is_sdvo = true;
+			break;
+		case INTEL_OUTPUT_DVO:
+			is_dvo = true;
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			is_tv = true;
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			is_crt = true;
+			break;
+		}
+	}
+	
+	if (IS_I9XX(dev)) {
+		refclk = 96000;
+	} else {
+		refclk = 48000;
+	}
+
+	ok = intel_find_best_PLL(crtc, adjusted_mode->clock, refclk, &clock);
+	if (!ok) {
+		DRM_ERROR("Couldn't find PLL settings for mode!\n");
+		return;
+	}
+
+	fp = clock.n << 16 | clock.m1 << 8 | clock.m2;
+	
+	dpll = DPLL_VGA_MODE_DIS;
+	if (IS_I9XX(dev)) {
+		if (is_lvds)
+			dpll |= DPLLB_MODE_LVDS;
+		else
+			dpll |= DPLLB_MODE_DAC_SERIAL;
+		if (is_sdvo) {
+			dpll |= DPLL_DVO_HIGH_SPEED;
+			if (IS_I945G(dev) || IS_I945GM(dev)) {
+				int sdvo_pixel_multiply = adjusted_mode->clock / mode->clock;
+				dpll |= (sdvo_pixel_multiply - 1) << SDVO_MULTIPLIER_SHIFT_HIRES;
+			}
+		}
+		
+		/* compute bitmask from p1 value */
+		dpll |= (1 << (clock.p1 - 1)) << 16;
+		switch (clock.p2) {
+		case 5:
+			dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_5;
+			break;
+		case 7:
+			dpll |= DPLLB_LVDS_P2_CLOCK_DIV_7;
+			break;
+		case 10:
+			dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_10;
+			break;
+		case 14:
+			dpll |= DPLLB_LVDS_P2_CLOCK_DIV_14;
+			break;
+		}
+		if (IS_I965G(dev))
+			dpll |= (6 << PLL_LOAD_PULSE_PHASE_SHIFT);
+	} else {
+		if (is_lvds) {
+			dpll |= (1 << (clock.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+		} else {
+			if (clock.p1 == 2)
+				dpll |= PLL_P1_DIVIDE_BY_TWO;
+			else
+				dpll |= (clock.p1 - 2) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+			if (clock.p2 == 4)
+				dpll |= PLL_P2_DIVIDE_BY_4;
+		}
+	}
+	
+	if (is_tv) {
+		/* XXX: just matching BIOS for now */
+/*	dpll |= PLL_REF_INPUT_TVCLKINBC; */
+		dpll |= 3;
+	}
+#if 0
+	else if (is_lvds)
+		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
+#endif
+	else
+		dpll |= PLL_REF_INPUT_DREFCLK;
+	
+	/* setup pipeconf */
+	pipeconf = I915_READ(pipeconf_reg);
+
+	/* Set up the display plane register */
+	dspcntr = DISPPLANE_GAMMA_ENABLE;
+
+	if (pipe == 0)
+		dspcntr |= DISPPLANE_SEL_PIPE_A;
+	else
+		dspcntr |= DISPPLANE_SEL_PIPE_B;
+	
+	if (pipe == 0 && !IS_I965G(dev)) {
+		/* Enable pixel doubling when the dot clock is > 90% of the (display)
+		 * core speed.
+		 *
+		 * XXX: No double-wide on 915GM pipe B. Is that the only reason for the
+		 * pipe == 0 check?
+		 */
+		if (mode->clock > intel_get_core_clock_speed(dev) * 9 / 10)
+			pipeconf |= PIPEACONF_DOUBLE_WIDE;
+		else
+			pipeconf &= ~PIPEACONF_DOUBLE_WIDE;
+	}
+
+	dspcntr |= DISPLAY_PLANE_ENABLE;
+	pipeconf |= PIPEACONF_ENABLE;
+	dpll |= DPLL_VCO_ENABLE;
+
+	
+	/* Disable the panel fitter if it was on our pipe */
+	if (intel_panel_fitter_pipe(dev) == pipe)
+		I915_WRITE(PFIT_CONTROL, 0);
+
+	DRM_DEBUG("Mode for pipe %c:\n", pipe == 0 ? 'A' : 'B');
+	drm_mode_debug_printmodeline(dev, mode);
+	
+#if 0
+	if (!xf86ModesEqual(mode, adjusted_mode)) {
+		xf86DrvMsg(pScrn->scrnIndex, X_INFO,
+			   "Adjusted mode for pipe %c:\n", pipe == 0 ? 'A' : 'B');
+		xf86PrintModeline(pScrn->scrnIndex, mode);
+	}
+	i830PrintPll("chosen", &clock);
+#endif
+
+	if (dpll & DPLL_VCO_ENABLE) {
+		I915_WRITE(fp_reg, fp);
+		I915_WRITE(dpll_reg, dpll & ~DPLL_VCO_ENABLE);
+		I915_READ(dpll_reg);
+		udelay(150);
+	}
+	
+	/* The LVDS pin pair needs to be on before the DPLLs are enabled.
+	 * This is an exception to the general rule that mode_set doesn't turn
+	 * things on.
+	 */
+	if (is_lvds) {
+		u32 lvds = I915_READ(LVDS);
+		
+		lvds |= LVDS_PORT_EN | LVDS_A0A2_CLKA_POWER_UP | LVDS_PIPEB_SELECT;
+		/* Set the B0-B3 data pairs corresponding to whether we're going to
+		 * set the DPLLs for dual-channel mode or not.
+		 */
+		if (clock.p2 == 7)
+			lvds |= LVDS_B0B3_POWER_UP | LVDS_CLKB_POWER_UP;
+		else
+			lvds &= ~(LVDS_B0B3_POWER_UP | LVDS_CLKB_POWER_UP);
+		
+		/* It would be nice to set 24 vs 18-bit mode (LVDS_A3_POWER_UP)
+		 * appropriately here, but we need to look more thoroughly into how
+		 * panels behave in the two modes.
+		 */
+		
+		I915_WRITE(LVDS, lvds);
+		I915_READ(LVDS);
+	}
+	
+	I915_WRITE(fp_reg, fp);
+	I915_WRITE(dpll_reg, dpll);
+	I915_READ(dpll_reg);
+	/* Wait for the clocks to stabilize. */
+	udelay(150);
+	
+	if (IS_I965G(dev)) {
+		int sdvo_pixel_multiply = adjusted_mode->clock / mode->clock;
+		I915_WRITE(dpll_md_reg, (0 << DPLL_MD_UDI_DIVIDER_SHIFT) |
+			   ((sdvo_pixel_multiply - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT));
+	} else {
+		/* write it again -- the BIOS does, after all */
+		I915_WRITE(dpll_reg, dpll);
+	}
+	I915_READ(dpll_reg);
+	/* Wait for the clocks to stabilize. */
+	udelay(150);
+	
+	I915_WRITE(htot_reg, (adjusted_mode->crtc_hdisplay - 1) |
+		   ((adjusted_mode->crtc_htotal - 1) << 16));
+	I915_WRITE(hblank_reg, (adjusted_mode->crtc_hblank_start - 1) |
+		   ((adjusted_mode->crtc_hblank_end - 1) << 16));
+	I915_WRITE(hsync_reg, (adjusted_mode->crtc_hsync_start - 1) |
+		   ((adjusted_mode->crtc_hsync_end - 1) << 16));
+	I915_WRITE(vtot_reg, (adjusted_mode->crtc_vdisplay - 1) |
+		   ((adjusted_mode->crtc_vtotal - 1) << 16));
+	I915_WRITE(vblank_reg, (adjusted_mode->crtc_vblank_start - 1) |
+		   ((adjusted_mode->crtc_vblank_end - 1) << 16));
+	I915_WRITE(vsync_reg, (adjusted_mode->crtc_vsync_start - 1) |
+		   ((adjusted_mode->crtc_vsync_end - 1) << 16));
+	/* pipesrc and dspsize control the size that is scaled from, which should
+	 * always be the user's requested size.
+	 */
+	I915_WRITE(dspsize_reg, ((mode->vdisplay - 1) << 16) | (mode->hdisplay - 1));
+	I915_WRITE(dsppos_reg, 0);
+	I915_WRITE(pipesrc_reg, ((mode->hdisplay - 1) << 16) | (mode->vdisplay - 1));
+	I915_WRITE(pipeconf_reg, pipeconf);
+	I915_READ(pipeconf_reg);
+	
+	intel_wait_for_vblank(dev);
+	
+	I915_WRITE(dspcntr_reg, dspcntr);
+	
+	/* Flush the plane changes */
+	intel_pipe_set_base(crtc, x, y);
+	
+	intel_set_vblank(dev);
+
+	intel_wait_for_vblank(dev);    
+}
+
+/** Loads the palette/gamma unit for the CRTC with the prepared values */
+void intel_crtc_load_lut(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int palreg = (intel_crtc->pipe == 0) ? PALETTE_A : PALETTE_B;
+	int i;
+
+	/* The clocks have to be on to load the palette. */
+	if (!crtc->enabled)
+		return;
+
+	for (i = 0; i < 256; i++) {
+		I915_WRITE(palreg + 4 * i,
+			   (intel_crtc->lut_r[i] << 16) |
+			   (intel_crtc->lut_g[i] << 8) |
+			   intel_crtc->lut_b[i]);
+	}
+}
+
+#define CURSOR_A_CONTROL        0x70080
+#define CURSOR_A_BASE           0x70084
+#define CURSOR_A_POSITION       0x70088
+
+#define CURSOR_B_CONTROL        0x700C0
+#define CURSOR_B_BASE           0x700C4
+#define CURSOR_B_POSITION       0x700C8
+
+#define CURSOR_MODE_DISABLE     0x00
+#define CURSOR_MODE_64_32B_AX   0x07
+#define CURSOR_MODE_64_ARGB_AX  ((1 << 5) | CURSOR_MODE_64_32B_AX)
+#define MCURSOR_GAMMA_ENABLE    (1 << 26)
+
+#define CURSOR_POS_MASK         0x007FF
+#define CURSOR_POS_SIGN         0x8000
+#define CURSOR_X_SHIFT          0
+#define CURSOR_Y_SHIFT          16
+
+static int intel_crtc_cursor_set(struct drm_crtc *crtc,
+				 struct drm_buffer_object *bo,
+				 uint32_t width, uint32_t height)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	uint32_t control = (pipe == 0) ? CURSOR_A_CONTROL : CURSOR_B_CONTROL;
+	uint32_t base = (pipe == 0) ? CURSOR_A_BASE : CURSOR_B_BASE;
+	uint32_t temp;
+	size_t addr;
+
+	DRM_DEBUG("\n");
+
+	/* if we want to turn of the cursor ignore width and height */
+	if (!bo) {
+		DRM_DEBUG("cursor off\n");
+		/* turn of the cursor */
+		temp = 0;
+		temp |= CURSOR_MODE_DISABLE;
+
+		I915_WRITE(control, temp);
+		I915_WRITE(base, 0);
+		return 0;
+	}
+
+	/* Currently we only support 64x64 cursors */
+	if (width != 64 || height != 64) {
+		DRM_ERROR("we currently only support 64x64 cursors\n");
+		return -EINVAL;
+	}
+
+	if ((bo->mem.flags & DRM_BO_MASK_MEM) != DRM_BO_FLAG_MEM_VRAM) {
+		DRM_ERROR("buffer needs to be in VRAM\n");
+		return -ENOMEM;
+	}
+
+	if (bo->mem.size < width * height * 4) {
+		DRM_ERROR("buffer is to small\n");
+		return -ENOMEM;
+	}
+
+	if (dev_priv->cursor_needs_physical)
+		addr = dev_priv->stolen_base + bo->offset;
+	else
+		addr = bo->offset;
+
+	intel_crtc->cursor_addr = addr;
+	temp = 0;
+	/* set the pipe for the cursor */
+	temp |= (pipe << 28);
+	temp |= CURSOR_MODE_64_ARGB_AX | MCURSOR_GAMMA_ENABLE;
+
+	DRM_DEBUG("cusror base %x\n", addr);
+
+	I915_WRITE(control, temp);
+	I915_WRITE(base, addr);
+
+	return 0;
+}
+
+static int intel_crtc_cursor_move(struct drm_crtc *crtc, int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	uint32_t temp = 0;
+	uint32_t adder;
+
+	if (x < 0) {
+		temp |= (CURSOR_POS_SIGN << CURSOR_X_SHIFT);
+		x = -x;
+	}
+	if (y < 0) {
+		temp |= (CURSOR_POS_SIGN << CURSOR_Y_SHIFT);
+		y = -y;
+	}
+
+	temp |= ((x & CURSOR_POS_MASK) << CURSOR_X_SHIFT);
+	temp |= ((y & CURSOR_POS_MASK) << CURSOR_Y_SHIFT);
+
+	adder = intel_crtc->cursor_addr;
+	I915_WRITE((pipe == 0) ? CURSOR_A_POSITION : CURSOR_B_POSITION, temp);
+	I915_WRITE((pipe == 0) ? CURSOR_A_BASE : CURSOR_B_BASE, adder);
+
+	return 0;
+}
+
+/** Sets the color ramps on behalf of RandR */
+static void intel_crtc_gamma_set(struct drm_crtc *crtc, u16 red, u16 green,
+				 u16 blue, int regno)
+{
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	
+	intel_crtc->lut_r[regno] = red >> 8;
+	intel_crtc->lut_g[regno] = green >> 8;
+	intel_crtc->lut_b[regno] = blue >> 8;
+}
+
+/* Returns the clock of the currently programmed mode of the given pipe. */
+static int intel_crtc_clock_get(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	u32 dpll = I915_READ((pipe == 0) ? DPLL_A : DPLL_B);
+	u32 fp;
+	intel_clock_t clock;
+
+	if ((dpll & DISPLAY_RATE_SELECT_FPA1) == 0)
+		fp = I915_READ((pipe == 0) ? FPA0 : FPB0);
+	else
+		fp = I915_READ((pipe == 0) ? FPA1 : FPB1);
+
+	clock.m1 = (fp & FP_M1_DIV_MASK) >> FP_M1_DIV_SHIFT;
+	clock.m2 = (fp & FP_M2_DIV_MASK) >> FP_M2_DIV_SHIFT;
+	clock.n = (fp & FP_N_DIV_MASK) >> FP_N_DIV_SHIFT;
+	if (IS_I9XX(dev)) {
+		clock.p1 = ffs((dpll & DPLL_FPA01_P1_POST_DIV_MASK) >>
+			       DPLL_FPA01_P1_POST_DIV_SHIFT);
+
+		switch (dpll & DPLL_MODE_MASK) {
+		case DPLLB_MODE_DAC_SERIAL:
+			clock.p2 = dpll & DPLL_DAC_SERIAL_P2_CLOCK_DIV_5 ?
+				5 : 10;
+			break;
+		case DPLLB_MODE_LVDS:
+			clock.p2 = dpll & DPLLB_LVDS_P2_CLOCK_DIV_7 ?
+				7 : 14;
+			break;
+		default:
+			DRM_DEBUG("Unknown DPLL mode %08x in programmed "
+				  "mode\n", (int)(dpll & DPLL_MODE_MASK));
+			return 0;
+		}
+
+		/* XXX: Handle the 100Mhz refclk */
+		i9xx_clock(96000, &clock);
+	} else {
+		bool is_lvds = (pipe == 1) && (I915_READ(LVDS) & LVDS_PORT_EN);
+
+		if (is_lvds) {
+			clock.p1 = ffs((dpll & DPLL_FPA01_P1_POST_DIV_MASK_I830_LVDS) >>
+				       DPLL_FPA01_P1_POST_DIV_SHIFT);
+			clock.p2 = 14;
+
+			if ((dpll & PLL_REF_INPUT_MASK) ==
+			    PLLB_REF_INPUT_SPREADSPECTRUMIN) {
+				/* XXX: might not be 66MHz */
+				i8xx_clock(66000, &clock);
+			} else
+				i8xx_clock(48000, &clock);		
+		} else {
+			if (dpll & PLL_P1_DIVIDE_BY_TWO)
+				clock.p1 = 2;
+			else {
+				clock.p1 = ((dpll & DPLL_FPA01_P1_POST_DIV_MASK_I830) >>
+					    DPLL_FPA01_P1_POST_DIV_SHIFT) + 2;
+			}
+			if (dpll & PLL_P2_DIVIDE_BY_4)
+				clock.p2 = 4;
+			else
+				clock.p2 = 2;
+
+			i8xx_clock(48000, &clock);
+		}
+	}
+
+	/* XXX: It would be nice to validate the clocks, but we can't reuse
+	 * i830PllIsValid() because it relies on the xf86_config output
+	 * configuration being accurate, which it isn't necessarily.
+	 */
+
+	return clock.dot;
+}
+
+/** Returns the currently programmed mode of the given pipe. */
+struct drm_display_mode *intel_crtc_mode_get(struct drm_device *dev,
+					     struct drm_crtc *crtc)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	struct drm_display_mode *mode;
+	int htot = I915_READ((pipe == 0) ? HTOTAL_A : HTOTAL_B);
+	int hsync = I915_READ((pipe == 0) ? HSYNC_A : HSYNC_B);
+	int vtot = I915_READ((pipe == 0) ? VTOTAL_A : VTOTAL_B);
+	int vsync = I915_READ((pipe == 0) ? VSYNC_A : VSYNC_B);
+
+	mode = kzalloc(sizeof(*mode), GFP_KERNEL);
+	if (!mode)
+		return NULL;
+
+	mode->clock = intel_crtc_clock_get(dev, crtc);
+	mode->hdisplay = (htot & 0xffff) + 1;
+	mode->htotal = ((htot & 0xffff0000) >> 16) + 1;
+	mode->hsync_start = (hsync & 0xffff) + 1;
+	mode->hsync_end = ((hsync & 0xffff0000) >> 16) + 1;
+	mode->vdisplay = (vtot & 0xffff) + 1;
+	mode->vtotal = ((vtot & 0xffff0000) >> 16) + 1;
+	mode->vsync_start = (vsync & 0xffff) + 1;
+	mode->vsync_end = ((vsync & 0xffff0000) >> 16) + 1;
+
+	drm_mode_set_name(mode);
+	drm_mode_set_crtcinfo(mode, 0);
+
+	return mode;
+}
+
+static const struct drm_crtc_funcs intel_crtc_funcs = {
+	.dpms = intel_crtc_dpms,
+	.lock = intel_crtc_lock,
+	.unlock = intel_crtc_unlock,
+	.mode_fixup = intel_crtc_mode_fixup,
+	.mode_set = intel_crtc_mode_set,
+	.mode_set_base = intel_pipe_set_base,
+	.cursor_set = intel_crtc_cursor_set,
+	.cursor_move = intel_crtc_cursor_move,
+	.gamma_set = intel_crtc_gamma_set,
+	.prepare = intel_crtc_prepare,
+	.commit = intel_crtc_commit,
+};
+
+
+void intel_crtc_init(struct drm_device *dev, int pipe)
+{
+	struct drm_crtc *crtc;
+	struct intel_crtc *intel_crtc;
+	int i;
+
+	crtc = drm_crtc_create(dev, &intel_crtc_funcs);
+	if (crtc == NULL)
+		return;
+
+	intel_crtc = kzalloc(sizeof(struct intel_crtc), GFP_KERNEL);
+	if (intel_crtc == NULL) {
+		kfree(crtc);
+		return;
+	}
+
+	intel_crtc->pipe = pipe;
+	for (i = 0; i < 256; i++) {
+		intel_crtc->lut_r[i] = i;
+		intel_crtc->lut_g[i] = i;
+		intel_crtc->lut_b[i] = i;
+	}
+
+	intel_crtc->cursor_addr = 0;
+
+	crtc->driver_private = intel_crtc;
+}
+
+struct drm_crtc *intel_get_crtc_from_pipe(struct drm_device *dev, int pipe)
+{
+	struct drm_crtc *crtc = NULL;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		struct intel_crtc *intel_crtc = crtc->driver_private;
+		if (intel_crtc->pipe == pipe)
+			break;
+	}
+	return crtc;
+}
+
+int intel_output_clones(struct drm_device *dev, int type_mask)
+{
+	int index_mask = 0;
+	struct drm_output *output;
+	int entry = 0;
+
+        list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+		if (type_mask & (1 << intel_output->type))
+			index_mask |= (1 << entry);
+		entry++;
+	}
+	return index_mask;
+}
+
+
+static void intel_setup_outputs(struct drm_device *dev)
+{
+	struct drm_output *output;
+
+	intel_crt_init(dev);
+
+	/* Set up integrated LVDS */
+	if (IS_MOBILE(dev) && !IS_I830(dev))
+		intel_lvds_init(dev);
+
+	if (IS_I9XX(dev)) {
+		intel_sdvo_init(dev, SDVOB);
+		intel_sdvo_init(dev, SDVOC);
+	}
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+		int crtc_mask = 0, clone_mask = 0;
+		
+		/* valid crtcs */
+		switch(intel_output->type) {
+		case INTEL_OUTPUT_DVO:
+		case INTEL_OUTPUT_SDVO:
+			crtc_mask = ((1 << 0)|
+				     (1 << 1));
+			clone_mask = ((1 << INTEL_OUTPUT_ANALOG) |
+				      (1 << INTEL_OUTPUT_DVO) |
+				      (1 << INTEL_OUTPUT_SDVO));
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			crtc_mask = ((1 << 0)|
+				     (1 << 1));
+			clone_mask = ((1 << INTEL_OUTPUT_ANALOG) |
+				      (1 << INTEL_OUTPUT_DVO) |
+				      (1 << INTEL_OUTPUT_SDVO));
+			break;
+		case INTEL_OUTPUT_LVDS:
+			crtc_mask = (1 << 1);
+			clone_mask = (1 << INTEL_OUTPUT_LVDS);
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			crtc_mask = ((1 << 0) |
+				     (1 << 1));
+			clone_mask = (1 << INTEL_OUTPUT_TVOUT);
+			break;
+		}
+		output->possible_crtcs = crtc_mask;
+		output->possible_clones = intel_output_clones(dev, clone_mask);
+	}
+}
+
+void intel_modeset_init(struct drm_device *dev)
+{
+	int num_pipe;
+	int i;
+
+	drm_mode_config_init(dev);
+
+	dev->mode_config.min_width = 0;
+	dev->mode_config.min_height = 0;
+
+	dev->mode_config.max_width = 4096;
+	dev->mode_config.max_height = 4096;
+
+	/* set memory base */
+	if (IS_I9XX(dev))
+		dev->mode_config.fb_base = pci_resource_start(dev->pdev, 2);
+	else
+		dev->mode_config.fb_base = pci_resource_start(dev->pdev, 0);
+
+	if (IS_MOBILE(dev) || IS_I9XX(dev))
+		num_pipe = 2;
+	else
+		num_pipe = 1;
+	DRM_DEBUG("%d display pipe%s available.\n",
+		  num_pipe, num_pipe > 1 ? "s" : "");
+
+	for (i = 0; i < num_pipe; i++) {
+		intel_crtc_init(dev, i);
+	}
+
+	intel_setup_outputs(dev);
+
+	//drm_initial_config(dev, false);
+}
+
+void intel_modeset_cleanup(struct drm_device *dev)
+{
+	drm_mode_config_cleanup(dev);
+}
diff --git a/drivers/char/drm/intel_drv.h b/drivers/char/drm/intel_drv.h
new file mode 100644
index 0000000..a36fd3f
--- /dev/null
+++ b/drivers/char/drm/intel_drv.h
@@ -0,0 +1,89 @@
+/*
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+#ifndef __INTEL_DRV_H__
+#define __INTEL_DRV_H__
+
+#include <linux/i2c.h>
+#include <linux/i2c-id.h>
+#include <linux/i2c-algo-bit.h>
+#include "drm_crtc.h"
+
+/*
+ * Display related stuff
+ */
+
+/* store information about an Ixxx DVO */
+/* The i830->i865 use multiple DVOs with multiple i2cs */
+/* the i915, i945 have a single sDVO i2c bus - which is different */
+#define MAX_OUTPUTS 6
+
+#define INTEL_I2C_BUS_DVO 1
+#define INTEL_I2C_BUS_SDVO 2
+
+/* these are outputs from the chip - integrated only 
+   external chips are via DVO or SDVO output */
+#define INTEL_OUTPUT_UNUSED 0
+#define INTEL_OUTPUT_ANALOG 1
+#define INTEL_OUTPUT_DVO 2
+#define INTEL_OUTPUT_SDVO 3
+#define INTEL_OUTPUT_LVDS 4
+#define INTEL_OUTPUT_TVOUT 5
+
+#define INTEL_DVO_CHIP_NONE 0
+#define INTEL_DVO_CHIP_LVDS 1
+#define INTEL_DVO_CHIP_TMDS 2
+#define INTEL_DVO_CHIP_TVOUT 4
+
+struct intel_i2c_chan {
+	struct drm_device *drm_dev; /* for getting at dev. private (mmio etc.) */
+	u32 reg; /* GPIO reg */
+	struct i2c_adapter adapter;
+	struct i2c_algo_bit_data algo;
+        u8 slave_addr;
+};
+
+struct intel_output {
+	int type;
+	struct intel_i2c_chan *i2c_bus; /* for control functions */
+	struct intel_i2c_chan *ddc_bus; /* for DDC only stuff */
+	bool load_detect_temp;
+	void *dev_priv;
+};
+
+struct intel_crtc {
+	int pipe;
+	int plane;
+	uint32_t cursor_addr;
+	u8 lut_r[256], lut_g[256], lut_b[256];
+};
+
+struct intel_i2c_chan *intel_i2c_create(struct drm_device *dev, const u32 reg,
+					const char *name);
+void intel_i2c_destroy(struct intel_i2c_chan *chan);
+int intel_ddc_get_modes(struct drm_output *output);
+extern bool intel_ddc_probe(struct drm_output *output);
+
+extern void intel_crt_init(struct drm_device *dev);
+extern void intel_sdvo_init(struct drm_device *dev, int output_device);
+extern void intel_lvds_init(struct drm_device *dev);
+
+extern void intel_crtc_load_lut(struct drm_crtc *crtc);
+extern void intel_output_prepare (struct drm_output *output);
+extern void intel_output_commit (struct drm_output *output);
+extern struct drm_display_mode *intel_crtc_mode_get(struct drm_device *dev,
+ 						    struct drm_crtc *crtc);
+extern void intel_wait_for_vblank(struct drm_device *dev);
+extern struct drm_crtc *intel_get_crtc_from_pipe(struct drm_device *dev, int pipe);
+
+extern struct drm_output* intel_sdvo_find(struct drm_device *dev, int sdvoB);
+extern int intel_sdvo_supports_hotplug(struct drm_output *output);
+extern void intel_sdvo_set_hotplug(struct drm_output *output, int enable);
+
+extern int intelfb_probe(struct drm_device *dev, struct drm_crtc *crtc);
+extern int intelfb_remove(struct drm_device *dev, struct drm_crtc *crtc);
+extern int intelfb_resize(struct drm_device *dev, struct drm_crtc *crtc);
+
+#endif /* __INTEL_DRV_H__ */
diff --git a/drivers/char/drm/intel_fb.c b/drivers/char/drm/intel_fb.c
new file mode 100644
index 0000000..28cc324
--- /dev/null
+++ b/drivers/char/drm/intel_fb.c
@@ -0,0 +1,776 @@
+/*
+ * Copyright  2007 David Airlie
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *     David Airlie
+ */
+    /*
+     *  Modularization
+     */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/tty.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/fb.h>
+#include <linux/init.h>
+
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+struct intelfb_par {
+	struct drm_device *dev;
+	struct drm_crtc *crtc;
+        struct drm_display_mode *fb_mode;
+	struct drm_framebuffer *fb;
+};
+/*
+static int
+var_to_refresh(const struct fb_var_screeninfo *var)
+{
+	int xtot = var->xres + var->left_margin + var->right_margin +
+		   var->hsync_len;
+	int ytot = var->yres + var->upper_margin + var->lower_margin +
+		   var->vsync_len;
+
+	return (1000000000 / var->pixclock * 1000 + 500) / xtot / ytot;
+}*/
+
+static int intelfb_setcolreg(unsigned regno, unsigned red, unsigned green,
+			   unsigned blue, unsigned transp,
+			   struct fb_info *info)
+{
+	struct intelfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->fb;
+	struct drm_crtc *crtc = par->crtc;
+
+	if (regno > 255)
+		return 1;
+
+	if (fb->depth == 8) {
+		if (crtc->funcs->gamma_set)
+			crtc->funcs->gamma_set(crtc, red, green, blue, regno);
+		return 0;
+	}
+
+	if (regno < 16) {
+		switch (fb->depth) {
+		case 15:
+			fb->pseudo_palette[regno] = ((red & 0xf800) >>  1) |
+				((green & 0xf800) >>  6) |
+				((blue & 0xf800) >> 11);
+			break;
+		case 16:
+			fb->pseudo_palette[regno] = (red & 0xf800) |
+				((green & 0xfc00) >>  5) |
+				((blue  & 0xf800) >> 11);
+			break;
+		case 24:
+		case 32:
+			fb->pseudo_palette[regno] = ((red & 0xff00) << 8) |
+				(green & 0xff00) |
+				((blue  & 0xff00) >> 8);
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static int intelfb_check_var(struct fb_var_screeninfo *var,
+			     struct fb_info *info)
+{
+        struct intelfb_par *par = info->par;
+        /*struct drm_device *dev = par->dev;*/
+	struct drm_framebuffer *fb = par->fb;
+        /*struct drm_output *output;*/
+        int depth/*, found = 0*/;
+
+        if (!var->pixclock)
+                return -EINVAL;
+
+        /* Need to resize the fb object !!! */
+        if (var->xres > fb->width || var->yres > fb->height) {
+                DRM_ERROR("Requested width/height is greater than current fb object %dx%d > %dx%d\n",var->xres,var->yres,fb->width,fb->height);
+                DRM_ERROR("Need resizing code.\n");
+                return -EINVAL;
+        }
+
+        switch (var->bits_per_pixel) {
+        case 16:
+                depth = (var->green.length == 6) ? 16 : 15;
+                break;
+        case 32:
+                depth = (var->transp.length > 0) ? 32 : 24;
+                break;
+        default:
+                depth = var->bits_per_pixel;
+                break;
+        }
+                
+        switch (depth) {
+        case 8:
+                var->red.offset = 0;
+                var->green.offset = 0;
+                var->blue.offset = 0;
+                var->red.length = 8;
+                var->green.length = 8;
+                var->blue.length = 8;
+                var->transp.length = 0;
+                var->transp.offset = 0;
+                break;
+        case 15:
+                var->red.offset = 10;
+                var->green.offset = 5;
+                var->blue.offset = 0;
+                var->red.length = 5;
+                var->green.length = 5;
+                var->blue.length = 5;
+                var->transp.length = 1;
+                var->transp.offset = 15;
+                break;
+        case 16:
+                var->red.offset = 11;
+                var->green.offset = 6;
+                var->blue.offset = 0;
+                var->red.length = 5;
+                var->green.length = 6;
+                var->blue.length = 5;
+                var->transp.length = 0;
+                var->transp.offset = 0;
+                break;
+        case 24:
+                var->red.offset = 16;
+                var->green.offset = 8;
+                var->blue.offset = 0;
+                var->red.length = 8;
+                var->green.length = 8;
+                var->blue.length = 8;
+                var->transp.length = 0;
+                var->transp.offset = 0;
+                break;
+        case 32:
+                var->red.offset = 16;
+                var->green.offset = 8;
+                var->blue.offset = 0;
+                var->red.length = 8;
+                var->green.length = 8;
+                var->blue.length = 8;
+                var->transp.length = 8;
+                var->transp.offset = 24;
+                break;
+        default:
+                return -EINVAL; 
+        }
+
+#if 0
+        /* Here we walk the output mode list and look for modes. If we haven't
+         * got it, then bail. Not very nice, so this is disabled.
+         * In the set_par code, we create our mode based on the incoming
+         * parameters. Nicer, but may not be desired by some.
+         */
+        list_for_each_entry(output, &dev->mode_config.output_list, head) {
+                if (output->crtc == par->crtc)
+                        break;
+        }
+    
+        list_for_each_entry(drm_mode, &output->modes, head) {
+                if (drm_mode->hdisplay == var->xres &&
+                    drm_mode->vdisplay == var->yres &&
+                    (((PICOS2KHZ(var->pixclock))/1000) >= ((drm_mode->clock/1000)-1)) &&
+                    (((PICOS2KHZ(var->pixclock))/1000) <= ((drm_mode->clock/1000)+1))) {
+			found = 1;
+			break;
+		}
+	}
+ 
+        if (!found)
+                return -EINVAL;
+#endif
+
+	return 0;
+}
+
+/* this will let fbcon do the mode init */
+/* FIXME: take mode config lock? */
+static int intelfb_set_par(struct fb_info *info)
+{
+	struct intelfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->fb;
+	struct drm_device *dev = par->dev;
+        struct drm_display_mode *drm_mode, *search_mode;
+        struct drm_output *output = NULL;
+        struct fb_var_screeninfo *var = &info->var;
+	int found = 0;
+	int changed = 0;
+	int move_or_flip = 0;
+
+	DRM_DEBUG("\n");
+
+        switch (var->bits_per_pixel) {
+        case 16:
+                fb->depth = (var->green.length == 6) ? 16 : 15;
+                break;
+        case 32:
+                fb->depth = (var->transp.length > 0) ? 32 : 24;
+                break;
+        default:
+                fb->depth = var->bits_per_pixel;
+                break;
+        }
+
+        fb->bits_per_pixel = var->bits_per_pixel;
+
+        info->fix.line_length = fb->pitch;
+        info->fix.smem_len = info->fix.line_length * fb->height;
+        info->fix.visual = (fb->depth == 8) ? FB_VISUAL_PSEUDOCOLOR : FB_VISUAL_TRUECOLOR;
+
+        info->screen_size = info->fix.smem_len; /* ??? */
+
+	/* create a drm mode */
+        drm_mode = drm_mode_create(dev);
+        drm_mode->hdisplay = var->xres;
+        drm_mode->hsync_start = drm_mode->hdisplay + var->right_margin;
+        drm_mode->hsync_end = drm_mode->hsync_start + var->hsync_len;
+        drm_mode->htotal = drm_mode->hsync_end + var->left_margin;
+        drm_mode->vdisplay = var->yres;
+        drm_mode->vsync_start = drm_mode->vdisplay + var->lower_margin;
+        drm_mode->vsync_end = drm_mode->vsync_start + var->vsync_len;
+        drm_mode->vtotal = drm_mode->vsync_end + var->upper_margin;
+        drm_mode->clock = PICOS2KHZ(var->pixclock);
+        drm_mode->vrefresh = drm_mode_vrefresh(drm_mode);
+	drm_mode->flags = 0;
+	drm_mode->flags |= var->sync & FB_SYNC_HOR_HIGH_ACT ? V_PHSYNC : V_NHSYNC;
+	drm_mode->flags |= var->sync & FB_SYNC_VERT_HIGH_ACT ? V_PVSYNC : V_NVSYNC;
+
+        drm_mode_set_name(drm_mode);
+	drm_mode_set_crtcinfo(drm_mode, CRTC_INTERLACE_HALVE_V);
+
+	found = 0;
+        list_for_each_entry(output, &dev->mode_config.output_list, head) {
+                if (output->crtc == par->crtc){
+			found = 1;
+                        break;
+		}
+        }
+
+	/* no output bound, bail */
+	if (!found)
+		return -EINVAL;
+
+	found = 0;
+	drm_mode_debug_printmodeline(dev, drm_mode);    
+        list_for_each_entry(search_mode, &output->modes, head) {
+		drm_mode_debug_printmodeline(dev, search_mode);
+		if (drm_mode_equal(drm_mode, search_mode)) {
+			drm_mode_destroy(dev, drm_mode);
+			drm_mode = search_mode;
+			found = 1;
+			break;
+		}
+	}
+	
+	/* If we didn't find a matching mode that exists on our output,
+	 * create a new attachment for the incoming user specified mode
+	 */
+	if (!found) {
+		if (par->fb_mode) {
+			/* this also destroys the mode */
+			drm_mode_detachmode_crtc(dev, par->fb_mode);
+		}
+	
+		par->fb_mode = drm_mode;
+		drm_mode_debug_printmodeline(dev, drm_mode);
+		/* attach mode */
+		drm_mode_attachmode_crtc(dev, par->crtc, par->fb_mode);
+	}
+
+	/* re-attach fb */
+	if (par->crtc->fb != par->fb) {
+		par->crtc->fb = par->fb;
+		move_or_flip = 1;
+	}
+
+	if (par->crtc->x != var->xoffset || par->crtc->y != var->yoffset)
+		move_or_flip = 1;
+
+	if (!drm_mode_equal(drm_mode, &par->crtc->mode))
+		changed = 1;
+
+	if (changed)
+		if (!drm_crtc_set_mode(par->crtc, drm_mode, var->xoffset, var->yoffset))
+			return -EINVAL;
+
+	if (move_or_flip) {
+		par->crtc->funcs->mode_set_base(par->crtc, var->xoffset, var->yoffset);
+	}
+		
+
+	return 0;
+}
+
+#if 0
+static void intelfb_copyarea(struct fb_info *info,
+			     const struct fb_copyarea *region)
+{
+        struct intelfb_par *par = info->par;
+	struct drm_device *dev = par->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 src_x1, src_y1, dst_x1, dst_y1, dst_x2, dst_y2, offset;
+	u32 cmd, rop_depth_pitch, src_pitch;
+	RING_LOCALS;
+
+	cmd = XY_SRC_COPY_BLT_CMD;
+	src_x1 = region->sx;
+	src_y1 = region->sy;
+	dst_x1 = region->dx;
+	dst_y1 = region->dy;
+	dst_x2 = region->dx + region->width;
+	dst_y2 = region->dy + region->height;
+	offset = par->fb->offset;
+	rop_depth_pitch = BLT_ROP_GXCOPY | par->fb->pitch;
+	src_pitch = par->fb->pitch;
+
+	switch (par->fb->bits_per_pixel) {
+	case 16:
+		rop_depth_pitch |= BLT_DEPTH_16_565;
+		break;
+	case 32:
+		rop_depth_pitch |= BLT_DEPTH_32;
+		cmd |= XY_SRC_COPY_BLT_WRITE_ALPHA | XY_SRC_COPY_BLT_WRITE_RGB;
+		break;
+	}
+
+	BEGIN_LP_RING(8);
+	OUT_RING(cmd);
+	OUT_RING(rop_depth_pitch);
+	OUT_RING((dst_y1 << 16) | (dst_x1 & 0xffff));
+	OUT_RING((dst_y2 << 16) | (dst_x2 & 0xffff));
+	OUT_RING(offset);
+	OUT_RING((src_y1 << 16) | (src_x1 & 0xffff));
+	OUT_RING(src_pitch);
+	OUT_RING(offset);
+	ADVANCE_LP_RING();
+}
+
+#define ROUND_UP_TO(x, y)	(((x) + (y) - 1) / (y) * (y))
+#define ROUND_DOWN_TO(x, y)	((x) / (y) * (y))
+
+void intelfb_imageblit(struct fb_info *info, const struct fb_image *image)
+{
+        struct intelfb_par *par = info->par;
+	struct drm_device *dev = par->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 cmd, rop_pitch_depth, tmp;
+	int nbytes, ndwords, pad;
+	u32 dst_x1, dst_y1, dst_x2, dst_y2, offset, bg, fg;
+	int dat, ix, iy, iw;
+	int i, j;
+	RING_LOCALS;
+
+	/* size in bytes of a padded scanline */
+	nbytes = ROUND_UP_TO(image->width, 16) / 8;
+
+	/* Total bytes of padded scanline data to write out. */
+	nbytes *= image->height;
+
+	/*
+	 * Check if the glyph data exceeds the immediate mode limit.
+	 * It would take a large font (1K pixels) to hit this limit.
+	 */
+	if (nbytes > 128 || image->depth != 1)
+		return cfb_imageblit(info, image);
+
+	/* Src data is packaged a dword (32-bit) at a time. */
+	ndwords = ROUND_UP_TO(nbytes, 4) / 4;
+
+	/*
+	 * Ring has to be padded to a quad word. But because the command starts
+	   with 7 bytes, pad only if there is an even number of ndwords
+	 */
+	pad = !(ndwords % 2);
+
+	DRM_DEBUG("imageblit %dx%dx%d to (%d,%d)\n", image->width,
+		  image->height, image->depth, image->dx, image->dy);
+	DRM_DEBUG("nbytes: %d, ndwords: %d, pad: %d\n", nbytes, ndwords, pad);
+
+	tmp = (XY_MONO_SRC_COPY_IMM_BLT & 0xff) + ndwords;
+	cmd = (XY_MONO_SRC_COPY_IMM_BLT & ~0xff) | tmp;
+	offset = par->fb->offset;
+	dst_x1 = image->dx;
+	dst_y1 = image->dy;
+	dst_x2 = image->dx + image->width;
+	dst_y2 = image->dy + image->height;
+	rop_pitch_depth = BLT_ROP_GXCOPY | par->fb->pitch;
+
+	switch (par->fb->bits_per_pixel) {
+	case 8:
+		rop_pitch_depth |= BLT_DEPTH_8;
+		fg = image->fg_color;
+		bg = image->bg_color;
+		break;
+	case 16:
+		rop_pitch_depth |= BLT_DEPTH_16_565;
+		fg = par->fb->pseudo_palette[image->fg_color];
+		bg = par->fb->pseudo_palette[image->bg_color];
+		break;
+	case 32:
+		rop_pitch_depth |= BLT_DEPTH_32;
+		cmd |= XY_SRC_COPY_BLT_WRITE_ALPHA | XY_SRC_COPY_BLT_WRITE_RGB;
+		fg = par->fb->pseudo_palette[image->fg_color];
+		bg = par->fb->pseudo_palette[image->bg_color];
+		break;
+	default:
+		DRM_ERROR("unknown depth %d\n", par->fb->bits_per_pixel);
+		break;
+	}
+	
+	BEGIN_LP_RING(8 + ndwords);
+	OUT_RING(cmd);
+	OUT_RING(rop_pitch_depth);
+	OUT_RING((dst_y1 << 16) | (dst_x1 & 0xffff));
+	OUT_RING((dst_y2 << 16) | (dst_x2 & 0xffff));
+	OUT_RING(offset);
+	OUT_RING(bg);
+	OUT_RING(fg);
+	ix = iy = 0;
+	iw = ROUND_UP_TO(image->width, 8) / 8;
+	while (ndwords--) {
+		dat = 0;
+		for (j = 0; j < 2; ++j) {
+			for (i = 0; i < 2; ++i) {
+				if (ix != iw || i == 0)
+					dat |= image->data[iy*iw + ix++] << (i+j*2)*8;
+			}
+			if (ix == iw && iy != (image->height - 1)) {
+				ix = 0;
+				++iy;
+			}
+		}
+		OUT_RING(dat);
+	}
+	if (pad)
+		OUT_RING(MI_NOOP);
+	ADVANCE_LP_RING();
+}
+#endif
+static int intelfb_pan_display(struct fb_var_screeninfo *var,
+			       struct fb_info *info)
+{
+	struct intelfb_par *par = info->par;
+	struct drm_crtc *crtc = par->crtc;
+	int changed = 0;
+
+	DRM_DEBUG("\n");
+
+	/* TODO add check size and pos*/
+
+	/* re-attach fb */
+	if (crtc->fb != par->fb) {
+		crtc->fb = par->fb;
+		changed = 1;
+	}
+
+	if (par->crtc->x != var->xoffset || par->crtc->y != var->yoffset)
+		changed = 1;
+
+	if (changed)
+		drm_crtc_set_mode(crtc, &crtc->mode, var->xoffset, var->yoffset);
+
+	info->var.xoffset = var->xoffset;
+	info->var.yoffset = var->yoffset;
+
+	return 0;
+}
+
+static struct fb_ops intelfb_ops = {
+	.owner = THIS_MODULE,
+	//	.fb_open = intelfb_open,
+	//	.fb_read = intelfb_read,
+	//	.fb_write = intelfb_write,
+	//	.fb_release = intelfb_release,
+	//	.fb_ioctl = intelfb_ioctl,
+	.fb_check_var = intelfb_check_var,
+	.fb_set_par = intelfb_set_par,
+	.fb_setcolreg = intelfb_setcolreg,
+	.fb_fillrect = cfb_fillrect,
+	.fb_copyarea = cfb_copyarea, //intelfb_copyarea,
+	.fb_imageblit = cfb_imageblit, //intelfb_imageblit,
+	.fb_pan_display = intelfb_pan_display,
+};
+
+/**
+ * Curretly it is assumed that the old framebuffer is reused.
+ *
+ * LOCKING
+ * caller should hold the mode config lock.
+ *
+ */
+int intelfb_resize(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct fb_info *info;
+	struct drm_framebuffer *fb;
+	struct drm_display_mode *mode = crtc->desired_mode;
+
+	fb = crtc->fb;
+	if (!fb)
+		return 1;
+
+	info = fb->fbdev;
+	if (!info)
+		return 1;
+
+	if (!mode)
+		return 1;
+
+	info->var.xres = mode->hdisplay;
+	info->var.right_margin = mode->hsync_start - mode->hdisplay;
+	info->var.hsync_len = mode->hsync_end - mode->hsync_start;
+	info->var.left_margin = mode->htotal - mode->hsync_end;
+	info->var.yres = mode->vdisplay;
+	info->var.lower_margin = mode->vsync_start - mode->vdisplay;
+	info->var.vsync_len = mode->vsync_end - mode->vsync_start;
+	info->var.upper_margin = mode->vtotal - mode->vsync_end;
+	info->var.pixclock = 10000000 / mode->htotal * 1000 / mode->vtotal * 100;
+	/* avoid overflow */
+	info->var.pixclock = info->var.pixclock * 1000 / mode->vrefresh;
+	info->var.pixclock = KHZ2PICOS(mode->clock);
+
+	return 0;
+}
+EXPORT_SYMBOL(intelfb_resize);
+
+int intelfb_probe(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct fb_info *info;
+	struct intelfb_par *par;
+	struct device *device = &dev->pdev->dev; 
+	struct drm_framebuffer *fb;
+	struct drm_display_mode *mode = crtc->desired_mode;
+	struct drm_buffer_object *fbo = NULL;
+	int ret;
+
+	info = framebuffer_alloc(sizeof(struct intelfb_par), device);
+	if (!info){
+		return -EINVAL;
+	}
+
+	fb = drm_framebuffer_create(dev);
+	if (!fb) {
+		framebuffer_release(info);
+		DRM_ERROR("failed to allocate fb.\n");
+		return -EINVAL;
+	}
+	crtc->fb = fb;
+
+	/* To allow resizeing without swapping buffers */
+	fb->width = 2048;/* crtc->desired_mode->hdisplay; */
+	fb->height = 2048;/* crtc->desired_mode->vdisplay; */
+
+	fb->bits_per_pixel = 32;
+	fb->pitch = fb->width * ((fb->bits_per_pixel + 1) / 8);
+	fb->depth = 24;
+	ret = drm_buffer_object_create(dev, fb->pitch * fb->height, 
+				       drm_bo_type_kernel,
+				       DRM_BO_FLAG_READ |
+				       DRM_BO_FLAG_WRITE |
+				       DRM_BO_FLAG_MEM_TT |
+				       DRM_BO_FLAG_MEM_VRAM |
+				       DRM_BO_FLAG_NO_EVICT,
+				       DRM_BO_HINT_DONT_FENCE, 0, 0,
+				       &fbo);
+	if (ret || !fbo) {
+		printk(KERN_ERR "failed to allocate framebuffer\n");
+		drm_framebuffer_destroy(fb);
+		framebuffer_release(info);
+		return -EINVAL;
+	}
+
+	fb->bo = fbo;
+	printk("allocated %dx%d fb: 0x%08lx, bo %p\n", fb->width,
+		       fb->height, fbo->offset, fbo);
+
+
+	fb->fbdev = info;
+		
+	par = info->par;
+
+	par->dev = dev;
+	par->crtc = crtc;
+	par->fb = fb;
+
+	info->fbops = &intelfb_ops;
+
+	strcpy(info->fix.id, "intelfb");
+	info->fix.type = FB_TYPE_PACKED_PIXELS;
+	info->fix.visual = FB_VISUAL_TRUECOLOR;
+	info->fix.type_aux = 0;
+	info->fix.xpanstep = 1; /* doing it in hw */
+	info->fix.ypanstep = 1; /* doing it in hw */
+	info->fix.ywrapstep = 0;
+	info->fix.accel = FB_ACCEL_I830;
+	info->fix.type_aux = 0;
+ 
+ 	if (IS_I9XX(dev)) {
+ 		info->fix.mmio_start = pci_resource_start(dev->pdev, 0);
+ 		info->fix.mmio_len = pci_resource_len(dev->pdev, 0);
+ 	} else {
+ 		info->fix.mmio_start = pci_resource_start(dev->pdev, 1);
+ 		info->fix.mmio_len = pci_resource_len(dev->pdev, 1);
+ 	}
+
+	info->fix.line_length = fb->pitch;
+	info->fix.smem_start = fb->bo->offset + dev->mode_config.fb_base;
+	info->fix.smem_len = info->fix.line_length * fb->height;
+
+	info->flags = FBINFO_DEFAULT;
+
+ 	ret = drm_bo_kmap(fb->bo, 0, fb->bo->num_pages, &fb->kmap);
+  	if (ret)
+  		DRM_ERROR("error mapping fb: %d\n", ret);
+  
+ 	info->screen_base = fb->kmap.virtual;
+	info->screen_size = info->fix.smem_len; /* FIXME */
+	info->pseudo_palette = fb->pseudo_palette;
+	info->var.xres_virtual = fb->width;
+	info->var.yres_virtual = fb->height;
+	info->var.bits_per_pixel = fb->bits_per_pixel;
+	info->var.xoffset = 0;
+	info->var.yoffset = 0;
+	info->var.activate = FB_ACTIVATE_NOW;
+	info->var.height = -1;
+	info->var.width = -1;
+
+        info->var.xres = mode->hdisplay;
+        info->var.right_margin = mode->hsync_start - mode->hdisplay;
+        info->var.hsync_len = mode->hsync_end - mode->hsync_start;
+        info->var.left_margin = mode->htotal - mode->hsync_end;
+        info->var.yres = mode->vdisplay;
+        info->var.lower_margin = mode->vsync_start - mode->vdisplay;
+        info->var.vsync_len = mode->vsync_end - mode->vsync_start;
+	info->var.upper_margin = mode->vtotal - mode->vsync_end;
+	info->var.pixclock = KHZ2PICOS(mode->clock);
+
+	if (mode->flags & V_PHSYNC)
+		info->var.sync |= FB_SYNC_HOR_HIGH_ACT;
+
+	if (mode->flags & V_PVSYNC)
+		info->var.sync |= FB_SYNC_VERT_HIGH_ACT;
+
+	if (mode->flags & V_INTERLACE)
+		info->var.vmode = FB_VMODE_INTERLACED;
+	else if (mode->flags & V_DBLSCAN)
+		info->var.vmode = FB_VMODE_DOUBLE;
+	else
+		info->var.vmode = FB_VMODE_NONINTERLACED;
+
+	info->pixmap.size = 64*1024;
+	info->pixmap.buf_align = 8;
+	info->pixmap.access_align = 32;
+	info->pixmap.flags = FB_PIXMAP_SYSTEM;
+	info->pixmap.scan_align = 1;
+
+	DRM_DEBUG("fb depth is %d\n", fb->depth);
+	DRM_DEBUG("   pitch is %d\n", fb->pitch);
+	switch(fb->depth) {
+	case 8:
+                info->var.red.offset = 0;
+                info->var.green.offset = 0;
+                info->var.blue.offset = 0;
+                info->var.red.length = 8; /* 8bit DAC */
+                info->var.green.length = 8;
+                info->var.blue.length = 8;
+                info->var.transp.offset = 0;
+                info->var.transp.length = 0;
+                break;
+ 	case 15:
+                info->var.red.offset = 10;
+                info->var.green.offset = 5;
+                info->var.blue.offset = 0;
+                info->var.red.length = info->var.green.length =
+                        info->var.blue.length = 5;
+                info->var.transp.offset = 15;
+                info->var.transp.length = 1;
+                break;
+	case 16:
+                info->var.red.offset = 11;
+                info->var.green.offset = 5;
+                info->var.blue.offset = 0;
+                info->var.red.length = 5;
+                info->var.green.length = 6;
+                info->var.blue.length = 5;
+                info->var.transp.offset = 0;
+ 		break;
+	case 24:
+                info->var.red.offset = 16;
+                info->var.green.offset = 8;
+                info->var.blue.offset = 0;
+                info->var.red.length = info->var.green.length =
+                        info->var.blue.length = 8;
+                info->var.transp.offset = 0;
+                info->var.transp.length = 0;
+                break;
+	case 32:
+		info->var.red.offset = 16;
+		info->var.green.offset = 8;
+		info->var.blue.offset = 0;
+		info->var.red.length = info->var.green.length =
+			info->var.blue.length = 8;
+		info->var.transp.offset = 24;
+		info->var.transp.length = 8;
+		break;
+	default:
+		break;
+	}
+
+	if (register_framebuffer(info) < 0)
+		return -EINVAL;
+
+	printk(KERN_INFO "fb%d: %s frame buffer device\n", info->node,
+	       info->fix.id);
+	return 0;
+}
+EXPORT_SYMBOL(intelfb_probe);
+
+int intelfb_remove(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct drm_framebuffer *fb = crtc->fb;
+	struct fb_info *info = fb->fbdev;
+	
+	if (info) {
+		unregister_framebuffer(info);
+		drm_bo_kunmap(&fb->kmap);
+		drm_bo_usage_deref_unlocked(&fb->bo);
+		drm_framebuffer_destroy(fb);
+		framebuffer_release(info);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(intelfb_remove);
+MODULE_LICENSE("GPL");
diff --git a/drivers/char/drm/intel_i2c.c b/drivers/char/drm/intel_i2c.c
new file mode 100644
index 0000000..efcbf65
--- /dev/null
+++ b/drivers/char/drm/intel_i2c.c
@@ -0,0 +1,190 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+/*
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/i2c-id.h>
+#include <linux/i2c-algo-bit.h>
+#include "drmP.h"
+#include "drm.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+/*
+ * Intel GPIO access functions
+ */
+
+#define I2C_RISEFALL_TIME 20
+
+static int get_clock(void *data)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_i915_private *dev_priv = chan->drm_dev->dev_private;
+	u32 val;
+
+	val = I915_READ(chan->reg);
+	return ((val & GPIO_CLOCK_VAL_IN) != 0);
+}
+
+static int get_data(void *data)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_i915_private *dev_priv = chan->drm_dev->dev_private;
+	u32 val;
+
+	val = I915_READ(chan->reg);
+	return ((val & GPIO_DATA_VAL_IN) != 0);
+}
+
+static void set_clock(void *data, int state_high)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_device *dev = chan->drm_dev;
+	struct drm_i915_private *dev_priv = chan->drm_dev->dev_private;
+	u32 reserved = 0, clock_bits;
+
+	/* On most chips, these bits must be preserved in software. */
+	if (!IS_I830(dev) && !IS_845G(dev))
+		reserved = I915_READ(chan->reg) & (GPIO_DATA_PULLUP_DISABLE |
+						   GPIO_CLOCK_PULLUP_DISABLE);
+
+	if (state_high)
+		clock_bits = GPIO_CLOCK_DIR_IN | GPIO_CLOCK_DIR_MASK;
+	else
+		clock_bits = GPIO_CLOCK_DIR_OUT | GPIO_CLOCK_DIR_MASK |
+			GPIO_CLOCK_VAL_MASK;
+	I915_WRITE(chan->reg, reserved | clock_bits);
+	udelay(I2C_RISEFALL_TIME); /* wait for the line to change state */
+}
+
+static void set_data(void *data, int state_high)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_device *dev = chan->drm_dev;
+	struct drm_i915_private *dev_priv = chan->drm_dev->dev_private;
+	u32 reserved = 0, data_bits;
+
+	/* On most chips, these bits must be preserved in software. */
+	if (!IS_I830(dev) && !IS_845G(dev))
+		reserved = I915_READ(chan->reg) & (GPIO_DATA_PULLUP_DISABLE |
+						   GPIO_CLOCK_PULLUP_DISABLE);
+
+	if (state_high)
+		data_bits = GPIO_DATA_DIR_IN | GPIO_DATA_DIR_MASK;
+	else
+		data_bits = GPIO_DATA_DIR_OUT | GPIO_DATA_DIR_MASK |
+			GPIO_DATA_VAL_MASK;
+
+	I915_WRITE(chan->reg, reserved | data_bits);
+	udelay(I2C_RISEFALL_TIME); /* wait for the line to change state */
+}
+
+/**
+ * intel_i2c_create - instantiate an Intel i2c bus using the specified GPIO reg
+ * @dev: DRM device
+ * @output: driver specific output device
+ * @reg: GPIO reg to use
+ * @name: name for this bus
+ *
+ * Creates and registers a new i2c bus with the Linux i2c layer, for use
+ * in output probing and control (e.g. DDC or SDVO control functions).
+ *
+ * Possible values for @reg include:
+ *   %GPIOA
+ *   %GPIOB
+ *   %GPIOC
+ *   %GPIOD
+ *   %GPIOE
+ *   %GPIOF
+ *   %GPIOG
+ *   %GPIOH
+ * see PRM for details on how these different busses are used.
+ */
+struct intel_i2c_chan *intel_i2c_create(struct drm_device *dev, const u32 reg,
+					const char *name)
+{
+	struct intel_i2c_chan *chan;
+
+	chan = kzalloc(sizeof(struct intel_i2c_chan), GFP_KERNEL);
+	if (!chan)
+		goto out_free;
+
+	chan->drm_dev = dev;
+	chan->reg = reg;
+	snprintf(chan->adapter.name, I2C_NAME_SIZE, "intel drm %s", name);
+	chan->adapter.owner = THIS_MODULE;
+#ifndef I2C_HW_B_INTELFB
+#define I2C_HW_B_INTELFB I2C_HW_B_I810
+#endif
+	chan->adapter.id = I2C_HW_B_INTELFB;
+	chan->adapter.algo_data	= &chan->algo;
+	chan->adapter.dev.parent = &dev->pdev->dev;
+	chan->algo.setsda = set_data;
+	chan->algo.setscl = set_clock;
+	chan->algo.getsda = get_data;
+	chan->algo.getscl = get_clock;
+	chan->algo.udelay = 20;
+	chan->algo.timeout = usecs_to_jiffies(2200);
+	chan->algo.data = chan;
+
+	i2c_set_adapdata(&chan->adapter, chan);
+
+	if(i2c_bit_add_bus(&chan->adapter))
+		goto out_free;
+
+	/* JJJ:  raise SCL and SDA? */
+	set_data(chan, 1);
+	set_clock(chan, 1);
+	udelay(20);
+
+	return chan;
+
+out_free:
+	kfree(chan);
+	return NULL;
+}
+
+/**
+ * intel_i2c_destroy - unregister and free i2c bus resources
+ * @output: channel to free
+ *
+ * Unregister the adapter from the i2c layer, then free the structure.
+ */
+void intel_i2c_destroy(struct intel_i2c_chan *chan)
+{
+	if (!chan)
+		return;
+
+	i2c_del_adapter(&chan->adapter);
+	kfree(chan);
+}
+
+	
+	
diff --git a/drivers/char/drm/intel_lvds.c b/drivers/char/drm/intel_lvds.c
new file mode 100644
index 0000000..81d624f
--- /dev/null
+++ b/drivers/char/drm/intel_lvds.c
@@ -0,0 +1,509 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ *      Dave Airlie <airlied@linux.ie>
+ *      Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "drm_edid.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+/**
+ * Sets the backlight level.
+ *
+ * \param level backlight level, from 0 to intel_lvds_get_max_backlight().
+ */
+static void intel_lvds_set_backlight(struct drm_device *dev, int level)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 blc_pwm_ctl;
+
+	blc_pwm_ctl = I915_READ(BLC_PWM_CTL) & ~BACKLIGHT_DUTY_CYCLE_MASK;
+	I915_WRITE(BLC_PWM_CTL, (blc_pwm_ctl |
+				 (level << BACKLIGHT_DUTY_CYCLE_SHIFT)));
+}
+
+/**
+ * Returns the maximum level of the backlight duty cycle field.
+ */
+static u32 intel_lvds_get_max_backlight(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+    
+	return ((I915_READ(BLC_PWM_CTL) & BACKLIGHT_MODULATION_FREQ_MASK) >>
+		BACKLIGHT_MODULATION_FREQ_SHIFT) * 2;
+}
+
+/**
+ * Sets the power state for the panel.
+ */
+static void intel_lvds_set_power(struct drm_device *dev, bool on)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 pp_status;
+
+	if (on) {
+		I915_WRITE(PP_CONTROL, I915_READ(PP_CONTROL) |
+			   POWER_TARGET_ON);
+		do {
+			pp_status = I915_READ(PP_STATUS);
+		} while ((pp_status & PP_ON) == 0);
+
+		intel_lvds_set_backlight(dev, dev_priv->backlight_duty_cycle);
+	} else {
+		intel_lvds_set_backlight(dev, 0);
+
+		I915_WRITE(PP_CONTROL, I915_READ(PP_CONTROL) &
+			   ~POWER_TARGET_ON);
+		do {
+			pp_status = I915_READ(PP_STATUS);
+		} while (pp_status & PP_ON);
+	}
+}
+
+static void intel_lvds_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+
+	if (mode == DPMSModeOn)
+		intel_lvds_set_power(dev, true);
+	else
+		intel_lvds_set_power(dev, false);
+
+	/* XXX: We never power down the LVDS pairs. */
+}
+
+static void intel_lvds_save(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	dev_priv->savePP_ON = I915_READ(LVDSPP_ON);
+	dev_priv->savePP_OFF = I915_READ(LVDSPP_OFF);
+	dev_priv->savePP_CONTROL = I915_READ(PP_CONTROL);
+	dev_priv->savePP_CYCLE = I915_READ(PP_CYCLE);
+	dev_priv->saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
+	dev_priv->backlight_duty_cycle = (dev_priv->saveBLC_PWM_CTL &
+				       BACKLIGHT_DUTY_CYCLE_MASK);
+
+	/*
+	 * If the light is off at server startup, just make it full brightness
+	 */
+	if (dev_priv->backlight_duty_cycle == 0)
+		dev_priv->backlight_duty_cycle =
+			intel_lvds_get_max_backlight(dev);
+}
+
+static void intel_lvds_restore(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	I915_WRITE(BLC_PWM_CTL, dev_priv->saveBLC_PWM_CTL);
+	I915_WRITE(LVDSPP_ON, dev_priv->savePP_ON);
+	I915_WRITE(LVDSPP_OFF, dev_priv->savePP_OFF);
+	I915_WRITE(PP_CYCLE, dev_priv->savePP_CYCLE);
+	I915_WRITE(PP_CONTROL, dev_priv->savePP_CONTROL);
+	if (dev_priv->savePP_CONTROL & POWER_TARGET_ON)
+		intel_lvds_set_power(dev, true);
+	else
+		intel_lvds_set_power(dev, false);
+}
+
+static int intel_lvds_mode_valid(struct drm_output *output,
+				 struct drm_display_mode *mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_display_mode *fixed_mode = dev_priv->panel_fixed_mode;
+
+	if (fixed_mode)	{
+		if (mode->hdisplay > fixed_mode->hdisplay)
+			return MODE_PANEL;
+		if (mode->vdisplay > fixed_mode->vdisplay)
+			return MODE_PANEL;
+	}
+
+	return MODE_OK;
+}
+
+static bool intel_lvds_mode_fixup(struct drm_output *output,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = output->crtc->driver_private;
+	struct drm_output *tmp_output;
+
+	/* Should never happen!! */
+	if (!IS_I965G(dev) && intel_crtc->pipe == 0) {
+		printk(KERN_ERR "Can't support LVDS on pipe A\n");
+		return false;
+	}
+
+	/* Should never happen!! */
+	list_for_each_entry(tmp_output, &dev->mode_config.output_list, head) {
+		if (tmp_output != output && tmp_output->crtc == output->crtc) {
+			printk(KERN_ERR "Can't enable LVDS and another "
+			       "output on the same pipe\n");
+			return false;
+		}
+	}
+
+	/*
+	 * If we have timings from the BIOS for the panel, put them in
+	 * to the adjusted mode.  The CRTC will be set up for this mode,
+	 * with the panel scaling set up to source from the H/VDisplay
+	 * of the original mode.
+	 */
+	if (dev_priv->panel_fixed_mode != NULL) {
+		adjusted_mode->hdisplay = dev_priv->panel_fixed_mode->hdisplay;
+		adjusted_mode->hsync_start =
+			dev_priv->panel_fixed_mode->hsync_start;
+		adjusted_mode->hsync_end =
+			dev_priv->panel_fixed_mode->hsync_end;
+		adjusted_mode->htotal = dev_priv->panel_fixed_mode->htotal;
+		adjusted_mode->vdisplay = dev_priv->panel_fixed_mode->vdisplay;
+		adjusted_mode->vsync_start =
+			dev_priv->panel_fixed_mode->vsync_start;
+		adjusted_mode->vsync_end =
+			dev_priv->panel_fixed_mode->vsync_end;
+		adjusted_mode->vtotal = dev_priv->panel_fixed_mode->vtotal;
+		adjusted_mode->clock = dev_priv->panel_fixed_mode->clock;
+		drm_mode_set_crtcinfo(adjusted_mode, CRTC_INTERLACE_HALVE_V);
+	}
+
+	/*
+	 * XXX: It would be nice to support lower refresh rates on the
+	 * panels to reduce power consumption, and perhaps match the
+	 * user's requested refresh rate.
+	 */
+
+	return true;
+}
+
+static void intel_lvds_prepare(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	dev_priv->saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
+	dev_priv->backlight_duty_cycle = (dev_priv->saveBLC_PWM_CTL &
+				       BACKLIGHT_DUTY_CYCLE_MASK);
+
+	intel_lvds_set_power(dev, false);
+}
+
+static void intel_lvds_commit( struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (dev_priv->backlight_duty_cycle == 0)
+		dev_priv->backlight_duty_cycle =
+			intel_lvds_get_max_backlight(dev);
+
+	intel_lvds_set_power(dev, true);
+}
+
+static void intel_lvds_mode_set(struct drm_output *output,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = output->crtc->driver_private;
+	u32 pfit_control;
+
+	/*
+	 * The LVDS pin pair will already have been turned on in the
+	 * intel_crtc_mode_set since it has a large impact on the DPLL
+	 * settings.
+	 */
+
+	/*
+	 * Enable automatic panel scaling so that non-native modes fill the
+	 * screen.  Should be enabled before the pipe is enabled, according to
+	 * register description and PRM.
+	 */
+	if (mode->hdisplay != adjusted_mode->hdisplay ||
+	    mode->vdisplay != adjusted_mode->vdisplay)
+		pfit_control = (PFIT_ENABLE | VERT_AUTO_SCALE |
+				HORIZ_AUTO_SCALE | VERT_INTERP_BILINEAR |
+				HORIZ_INTERP_BILINEAR);
+	else
+		pfit_control = 0;
+
+	if (!IS_I965G(dev)) {
+		if (dev_priv->panel_wants_dither)
+			pfit_control |= PANEL_8TO6_DITHER_ENABLE;
+	}
+	else
+		pfit_control |= intel_crtc->pipe << PFIT_PIPE_SHIFT;
+
+	I915_WRITE(PFIT_CONTROL, pfit_control);
+}
+
+/**
+ * Detect the LVDS connection.
+ *
+ * This always returns OUTPUT_STATUS_CONNECTED.  This output should only have
+ * been set up if the LVDS was actually connected anyway.
+ */
+static enum drm_output_status intel_lvds_detect(struct drm_output *output)
+{
+	return output_status_connected;
+}
+
+/**
+ * Return the list of DDC modes if available, or the BIOS fixed mode otherwise.
+ */
+static int intel_lvds_get_modes(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret = 0;
+
+	ret = intel_ddc_get_modes(output);
+
+	if (ret)
+		return ret;
+
+	/* Didn't get an EDID */
+	if (!output->monitor_info) {
+		struct drm_display_info *dspinfo;
+		dspinfo = kzalloc(sizeof(*output->monitor_info), GFP_KERNEL);
+		if (!dspinfo)
+			goto out;
+
+		/* Set wide sync ranges so we get all modes
+		 * handed to valid_mode for checking
+		 */
+		dspinfo->min_vfreq = 0;
+		dspinfo->max_vfreq = 200;
+		dspinfo->min_hfreq = 0;
+		dspinfo->max_hfreq = 200;
+		output->monitor_info = dspinfo;
+	}
+
+out:
+	if (dev_priv->panel_fixed_mode != NULL) {
+		struct drm_display_mode *mode =
+			drm_mode_duplicate(dev, dev_priv->panel_fixed_mode);
+		drm_mode_probed_add(output, mode);
+		return 1;
+	}
+
+	return 0;
+}
+
+/**
+ * intel_lvds_destroy - unregister and free LVDS structures
+ * @output: output to free
+ *
+ * Unregister the DDC bus for this output then free the driver private
+ * structure.
+ */
+static void intel_lvds_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	intel_i2c_destroy(intel_output->ddc_bus);
+	kfree(output->driver_private);
+}
+
+static const struct drm_output_funcs intel_lvds_output_funcs = {
+	.dpms = intel_lvds_dpms,
+	.save = intel_lvds_save,
+	.restore = intel_lvds_restore,
+	.mode_valid = intel_lvds_mode_valid,
+	.mode_fixup = intel_lvds_mode_fixup,
+	.prepare = intel_lvds_prepare,
+	.mode_set = intel_lvds_mode_set,
+	.commit = intel_lvds_commit,
+	.detect = intel_lvds_detect,
+	.get_modes = intel_lvds_get_modes,
+	.cleanup = intel_lvds_destroy
+};
+
+/**
+ * intel_lvds_init - setup LVDS outputs on this device
+ * @dev: drm device
+ *
+ * Create the output, register the LVDS DDC bus, and try to figure out what
+ * modes we can display on the LVDS panel (if present).
+ */
+void intel_lvds_init(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_output *output;
+	struct intel_output *intel_output;
+	struct drm_display_mode *scan; /* *modes, *bios_mode; */
+	struct drm_crtc *crtc;
+	u32 lvds;
+	int pipe;
+
+	output = drm_output_create(dev, &intel_lvds_output_funcs,
+				   DRM_MODE_OUTPUT_LVDS);
+	if (!output)
+		return;
+
+	intel_output = kmalloc(sizeof(struct intel_output), GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	intel_output->type = INTEL_OUTPUT_LVDS;
+	output->driver_private = intel_output;
+	output->subpixel_order = SubPixelHorizontalRGB;
+	output->interlace_allowed = false;
+	output->doublescan_allowed = false;
+
+	/* Set up the DDC bus. */
+	intel_output->ddc_bus = intel_i2c_create(dev, GPIOC, "LVDSDDC_C");
+	if (!intel_output->ddc_bus) {
+		dev_printk(KERN_ERR, &dev->pdev->dev, "DDC bus registration "
+			   "failed.\n");
+		return;
+	}
+
+	/*
+	 * Attempt to get the fixed panel mode from DDC.  Assume that the
+	 * preferred mode is the right one.
+	 */
+	intel_ddc_get_modes(output);
+
+	list_for_each_entry(scan, &output->probed_modes, head) {
+		if (scan->type & DRM_MODE_TYPE_PREFERRED) {
+			dev_priv->panel_fixed_mode = 
+				drm_mode_duplicate(dev, scan);
+			goto out; /* FIXME: check for quirks */
+		}
+	}
+
+	/*
+	 * If we didn't get EDID, try checking if the panel is already turned
+	 * on.  If so, assume that whatever is currently programmed is the
+	 * correct mode.
+	 */
+	lvds = I915_READ(LVDS);
+	pipe = (lvds & LVDS_PIPEB_SELECT) ? 1 : 0;
+	crtc = intel_get_crtc_from_pipe(dev, pipe);
+		
+	if (crtc && (lvds & LVDS_PORT_EN)) {
+		dev_priv->panel_fixed_mode = intel_crtc_mode_get(dev, crtc);
+		if (dev_priv->panel_fixed_mode) {
+			dev_priv->panel_fixed_mode->type |=
+				DRM_MODE_TYPE_PREFERRED;
+			goto out; /* FIXME: check for quirks */
+		}
+	}
+
+	/* If we still don't have a mode after all that, give up. */
+	if (!dev_priv->panel_fixed_mode)
+		goto failed;
+
+	/* FIXME: probe the BIOS for modes and check for LVDS quirks */
+#if 0
+	/* Get the LVDS fixed mode out of the BIOS.  We should support LVDS
+	 * with the BIOS being unavailable or broken, but lack the
+	 * configuration options for now.
+	 */
+	bios_mode = intel_bios_get_panel_mode(pScrn);
+	if (bios_mode != NULL) {
+		if (dev_priv->panel_fixed_mode != NULL) {
+			if (dev_priv->debug_modes &&
+			    !xf86ModesEqual(dev_priv->panel_fixed_mode,
+					    bios_mode))
+			{
+				xf86DrvMsg(pScrn->scrnIndex, X_WARNING,
+					   "BIOS panel mode data doesn't match probed data, "
+					   "continuing with probed.\n");
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO, "BIOS mode:\n");
+				xf86PrintModeline(pScrn->scrnIndex, bios_mode);
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO, "probed mode:\n");
+				xf86PrintModeline(pScrn->scrnIndex, dev_priv->panel_fixed_mode);
+				xfree(bios_mode->name);
+				xfree(bios_mode);
+			}
+		}  else {
+			dev_priv->panel_fixed_mode = bios_mode;
+		}
+	} else {
+		xf86DrvMsg(pScrn->scrnIndex, X_WARNING,
+			   "Couldn't detect panel mode.  Disabling panel\n");
+		goto disable_exit;
+	}
+
+	/*
+	 * Blacklist machines with BIOSes that list an LVDS panel without
+	 * actually having one.
+	 */
+	if (dev_priv->PciInfo->chipType == PCI_CHIP_I945_GM) {
+		/* aopen mini pc */
+		if (dev_priv->PciInfo->subsysVendor == 0xa0a0)
+			goto disable_exit;
+
+		if ((dev_priv->PciInfo->subsysVendor == 0x8086) &&
+		    (dev_priv->PciInfo->subsysCard == 0x7270)) {
+			/* It's a Mac Mini or Macbook Pro.
+			 *
+			 * Apple hardware is out to get us.  The macbook pro
+			 * has a real LVDS panel, but the mac mini does not,
+			 * and they have the same device IDs.  We'll
+			 * distinguish by panel size, on the assumption
+			 * that Apple isn't about to make any machines with an
+			 * 800x600 display.
+			 */
+
+			if (dev_priv->panel_fixed_mode != NULL &&
+			    dev_priv->panel_fixed_mode->HDisplay == 800 &&
+			    dev_priv->panel_fixed_mode->VDisplay == 600)
+			{
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO,
+					   "Suspected Mac Mini, ignoring the LVDS\n");
+				goto disable_exit;
+			}
+		}
+	}
+
+#endif
+
+out:
+	drm_output_attach_property(output, dev->mode_config.connector_type_property, ConnectorLVDS);
+	return;
+
+failed:
+        DRM_DEBUG("No LVDS modes found, disabling.\n");
+	drm_output_destroy(output); /* calls intel_lvds_destroy above */
+}
diff --git a/drivers/char/drm/intel_modes.c b/drivers/char/drm/intel_modes.c
new file mode 100644
index 0000000..f8bf496
--- /dev/null
+++ b/drivers/char/drm/intel_modes.c
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/fb.h>
+#include "drmP.h"
+#include "intel_drv.h"
+
+/**
+ * intel_ddc_probe
+ *
+ */
+bool intel_ddc_probe(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	u8 out_buf[] = { 0x0, 0x0};
+	u8 buf[2];
+	int ret;
+	struct i2c_msg msgs[] = {
+		{
+			.addr = 0x50,
+			.flags = 0,
+			.len = 1,
+			.buf = out_buf,
+		},
+		{
+			.addr = 0x50,
+			.flags = I2C_M_RD,
+			.len = 1,
+			.buf = buf,
+		}
+	};
+
+	ret = i2c_transfer(&intel_output->ddc_bus->adapter, msgs, 2);
+	if (ret == 2)
+		return true;
+
+	return false;
+}
+
+/**
+ * intel_ddc_get_modes - get modelist from monitor
+ * @output: DRM output device to use
+ *
+ * Fetch the EDID information from @output using the DDC bus.
+ */
+int intel_ddc_get_modes(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct edid *edid;
+	int ret = 0;
+
+	edid = drm_get_edid(output, &intel_output->ddc_bus->adapter);
+	if (edid) {
+		drm_mode_output_update_edid_property(output, edid);
+		ret = drm_add_edid_modes(output, edid);
+		kfree(edid);
+	}
+	return ret;
+}
diff --git a/drivers/char/drm/intel_sdvo.c b/drivers/char/drm/intel_sdvo.c
new file mode 100644
index 0000000..3887df0
--- /dev/null
+++ b/drivers/char/drm/intel_sdvo.c
@@ -0,0 +1,1168 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+/*
+ * Copyright 2006 Dave Airlie <airlied@linux.ie>
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/delay.h>
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+#include "intel_sdvo_regs.h"
+
+struct intel_sdvo_priv {
+	struct intel_i2c_chan *i2c_bus;
+	int slaveaddr;
+	int output_device;
+
+	u16 active_outputs;
+
+	struct intel_sdvo_caps caps;
+	int pixel_clock_min, pixel_clock_max;
+
+	int save_sdvo_mult;
+	u16 save_active_outputs;
+	struct intel_sdvo_dtd save_input_dtd_1, save_input_dtd_2;
+	struct intel_sdvo_dtd save_output_dtd[16];
+	u32 save_SDVOX;
+	int hotplug_enabled;
+};
+
+/**
+ * Writes the SDVOB or SDVOC with the given value, but always writes both
+ * SDVOB and SDVOC to work around apparent hardware issues (according to
+ * comments in the BIOS).
+ */
+void intel_sdvo_write_sdvox(struct drm_output *output, u32 val)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv   *sdvo_priv = intel_output->dev_priv;
+	u32 bval = val, cval = val;
+	int i;
+
+	if (sdvo_priv->output_device == SDVOB) {
+		cval = I915_READ(SDVOC);
+
+		if (sdvo_priv->hotplug_enabled)
+			bval = bval | (1 << 26);
+	} else {
+		bval = I915_READ(SDVOB);
+
+		if (sdvo_priv->hotplug_enabled)
+			cval = cval | (1 << 26);
+	}
+	/*
+	 * Write the registers twice for luck. Sometimes,
+	 * writing them only once doesn't appear to 'stick'.
+	 * The BIOS does this too. Yay, magic
+	 */
+	for (i = 0; i < 2; i++)
+	{
+		I915_WRITE(SDVOB, bval);
+		I915_READ(SDVOB);
+		I915_WRITE(SDVOC, cval);
+		I915_READ(SDVOC);
+	}
+}
+
+static bool intel_sdvo_read_byte(struct drm_output *output, u8 addr,
+				 u8 *ch)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u8 out_buf[2];
+	u8 buf[2];
+	int ret;
+
+	struct i2c_msg msgs[] = {
+		{ 
+			.addr = sdvo_priv->i2c_bus->slave_addr,
+			.flags = 0,
+			.len = 1,
+			.buf = out_buf,
+		}, 
+		{
+			.addr = sdvo_priv->i2c_bus->slave_addr,
+			.flags = I2C_M_RD,
+			.len = 1,
+			.buf = buf,
+		}
+	};
+
+	out_buf[0] = addr;
+	out_buf[1] = 0;
+
+	if ((ret = i2c_transfer(&sdvo_priv->i2c_bus->adapter, msgs, 2)) == 2)
+	{
+//		DRM_DEBUG("got back from addr %02X = %02x\n", out_buf[0], buf[0]); 
+		*ch = buf[0];
+		return true;
+	}
+
+	DRM_DEBUG("i2c transfer returned %d\n", ret);
+	return false;
+}
+
+
+static bool intel_sdvo_read_byte_quiet(struct drm_output *output, int addr,
+				       u8 *ch)
+{
+	return true;
+
+}
+
+static bool intel_sdvo_write_byte(struct drm_output *output, int addr,
+				  u8 ch)
+{
+	struct intel_output *intel_output = output->driver_private;
+	u8 out_buf[2];
+	struct i2c_msg msgs[] = {
+		{ 
+			.addr = intel_output->i2c_bus->slave_addr,
+			.flags = 0,
+			.len = 2,
+			.buf = out_buf,
+		}
+	};
+
+	out_buf[0] = addr;
+	out_buf[1] = ch;
+
+	if (i2c_transfer(&intel_output->i2c_bus->adapter, msgs, 1) == 1)
+	{
+		return true;
+	}
+	return false;
+}
+
+#define SDVO_CMD_NAME_ENTRY(cmd) {cmd, #cmd}
+/** Mapping of command numbers to names, for debug output */
+const static struct _sdvo_cmd_name {
+    u8 cmd;
+    char *name;
+} sdvo_cmd_names[] = {
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_RESET),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_DEVICE_CAPS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_FIRMWARE_REV),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_TRAINED_INPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ACTIVE_OUTPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_ACTIVE_OUTPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_IN_OUT_MAP),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_IN_OUT_MAP),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ATTACHED_DISPLAYS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_HOT_PLUG_SUPPORT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_ACTIVE_HOT_PLUG),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ACTIVE_HOT_PLUG),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INTERRUPT_EVENT_SOURCE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TARGET_INPUT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TARGET_OUTPUT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_OUTPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_OUTPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_CREATE_PREFERRED_INPUT_TIMING),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_PIXEL_CLOCK_RANGE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_SUPPORTED_CLOCK_RATE_MULTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_CLOCK_RATE_MULT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_CLOCK_RATE_MULT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_SUPPORTED_TV_FORMATS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_TV_FORMAT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TV_FORMAT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TV_RESOLUTION_SUPPORT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_CONTROL_BUS_SWITCH),
+};
+
+#define SDVO_NAME(dev_priv) ((dev_priv)->output_device == SDVOB ? "SDVOB" : "SDVOC")
+#define SDVO_PRIV(output)   ((struct intel_sdvo_priv *) (output)->dev_priv)
+
+static void intel_sdvo_write_cmd(struct drm_output *output, u8 cmd,
+				 void *args, int args_len)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int i;
+
+        if (1) {
+                DRM_DEBUG("%s: W: %02X ", SDVO_NAME(sdvo_priv), cmd);
+                for (i = 0; i < args_len; i++)
+                        printk("%02X ", ((u8 *)args)[i]);
+                for (; i < 8; i++)
+                        printk("   ");
+                for (i = 0; i < sizeof(sdvo_cmd_names) / sizeof(sdvo_cmd_names[0]); i++) {
+                        if (cmd == sdvo_cmd_names[i].cmd) {
+                                printk("(%s)", sdvo_cmd_names[i].name);
+                                break;
+                        }
+                }
+                if (i == sizeof(sdvo_cmd_names)/ sizeof(sdvo_cmd_names[0]))
+                        printk("(%02X)",cmd);
+                printk("\n");
+        }
+                        
+	for (i = 0; i < args_len; i++) {
+		intel_sdvo_write_byte(output, SDVO_I2C_ARG_0 - i, ((u8*)args)[i]);
+	}
+
+	intel_sdvo_write_byte(output, SDVO_I2C_OPCODE, cmd);
+}
+
+static const char *cmd_status_names[] = {
+	"Power on",
+	"Success",
+	"Not supported",
+	"Invalid arg",
+	"Pending",
+	"Target not specified",
+	"Scaling not supported"
+};
+
+static u8 intel_sdvo_read_response(struct drm_output *output, void *response,
+				   int response_len)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int i;
+	u8 status;
+	u8 retry = 50;
+
+	while (retry--) {
+		/* Read the command response */
+		for (i = 0; i < response_len; i++) {
+			intel_sdvo_read_byte(output, SDVO_I2C_RETURN_0 + i,
+				     &((u8 *)response)[i]);
+		}
+
+		/* read the return status */
+		intel_sdvo_read_byte(output, SDVO_I2C_CMD_STATUS, &status);
+
+	        if (1) {
+			DRM_DEBUG("%s: R: ", SDVO_NAME(sdvo_priv));
+       			for (i = 0; i < response_len; i++)
+                        	printk("%02X ", ((u8 *)response)[i]);
+                	for (; i < 8; i++)
+                        	printk("   ");
+                	if (status <= SDVO_CMD_STATUS_SCALING_NOT_SUPP)
+                        	printk("(%s)", cmd_status_names[status]);
+                	else
+                        	printk("(??? %d)", status);
+                	printk("\n");
+        	}
+
+		if (status != SDVO_CMD_STATUS_PENDING)
+			return status;
+
+		mdelay(50);
+	}
+
+	return status;
+}
+
+int intel_sdvo_get_pixel_multiplier(struct drm_display_mode *mode)
+{
+	if (mode->clock >= 100000)
+		return 1;
+	else if (mode->clock >= 50000)
+		return 2;
+	else
+		return 4;
+}
+
+/**
+ * Don't check status code from this as it switches the bus back to the
+ * SDVO chips which defeats the purpose of doing a bus switch in the first
+ * place.
+ */
+void intel_sdvo_set_control_bus_switch(struct drm_output *output, u8 target)
+{
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_CONTROL_BUS_SWITCH, &target, 1);
+}
+
+static bool intel_sdvo_set_target_input(struct drm_output *output, bool target_0, bool target_1)
+{
+	struct intel_sdvo_set_target_input_args targets = {0};
+	u8 status;
+
+	if (target_0 && target_1)
+		return SDVO_CMD_STATUS_NOTSUPP;
+
+	if (target_1)
+		targets.target_1 = 1;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_TARGET_INPUT, &targets,
+			     sizeof(targets));
+
+	status = intel_sdvo_read_response(output, NULL, 0);
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+/**
+ * Return whether each input is trained.
+ *
+ * This function is making an assumption about the layout of the response,
+ * which should be checked against the docs.
+ */
+static bool intel_sdvo_get_trained_inputs(struct drm_output *output, bool *input_1, bool *input_2)
+{
+	struct intel_sdvo_get_trained_inputs_response response;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_TRAINED_INPUTS, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, sizeof(response));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	*input_1 = response.input0_trained;
+	*input_2 = response.input1_trained;
+	return true;
+}
+
+static bool intel_sdvo_get_active_outputs(struct drm_output *output,
+					  u16 *outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ACTIVE_OUTPUTS, NULL, 0);
+	status = intel_sdvo_read_response(output, outputs, sizeof(*outputs));
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_set_active_outputs(struct drm_output *output,
+					  u16 outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_ACTIVE_OUTPUTS, &outputs,
+			     sizeof(outputs));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_set_encoder_power_state(struct drm_output *output,
+					       int mode)
+{
+	u8 status, state = SDVO_ENCODER_STATE_ON;
+
+	switch (mode) {
+	case DPMSModeOn:
+		state = SDVO_ENCODER_STATE_ON;
+		break;
+	case DPMSModeStandby:
+		state = SDVO_ENCODER_STATE_STANDBY;
+		break;
+	case DPMSModeSuspend:
+		state = SDVO_ENCODER_STATE_SUSPEND;
+		break;
+	case DPMSModeOff:
+		state = SDVO_ENCODER_STATE_OFF;
+		break;
+	}
+	
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_ENCODER_POWER_STATE, &state,
+			     sizeof(state));
+	status = intel_sdvo_read_response(output, NULL, 0);
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_get_input_pixel_clock_range(struct drm_output *output,
+						   int *clock_min,
+						   int *clock_max)
+{
+	struct intel_sdvo_pixel_clock_range clocks;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE,
+			     NULL, 0);
+
+	status = intel_sdvo_read_response(output, &clocks, sizeof(clocks));
+
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	/* Convert the values from units of 10 kHz to kHz. */
+	*clock_min = clocks.min * 10;
+	*clock_max = clocks.max * 10;
+
+	return true;
+}
+
+static bool intel_sdvo_set_target_output(struct drm_output *output,
+					 u16 outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_TARGET_OUTPUT, &outputs,
+			     sizeof(outputs));
+
+	status = intel_sdvo_read_response(output, NULL, 0);
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_get_timing(struct drm_output *output, u8 cmd,
+				  struct intel_sdvo_dtd *dtd)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, cmd, NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part1,
+					  sizeof(dtd->part1));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, cmd + 1, NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part2,
+					  sizeof(dtd->part2));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_get_input_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_get_timing(output,
+				     SDVO_CMD_GET_INPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_get_output_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_get_timing(output,
+				     SDVO_CMD_GET_OUTPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_set_timing(struct drm_output *output, u8 cmd,
+				  struct intel_sdvo_dtd *dtd)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, cmd, &dtd->part1, sizeof(dtd->part1));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, cmd + 1, &dtd->part2, sizeof(dtd->part2));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_set_input_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_set_timing(output,
+				     SDVO_CMD_SET_INPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_set_output_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_set_timing(output,
+				     SDVO_CMD_SET_OUTPUT_TIMINGS_PART1, dtd);
+}
+
+#if 0
+static bool intel_sdvo_get_preferred_input_timing(struct drm_output *output,
+						  struct intel_sdvo_dtd *dtd)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1,
+			     NULL, 0);
+
+	status = intel_sdvo_read_response(output, &dtd->part1,
+					  sizeof(dtd->part1));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2,
+			     NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part2,
+					  sizeof(dtd->part2));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+#endif
+
+static int intel_sdvo_get_clock_rate_mult(struct drm_output *output)
+{
+	u8 response, status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_CLOCK_RATE_MULT, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 1);
+
+	if (status != SDVO_CMD_STATUS_SUCCESS) {
+		DRM_DEBUG("Couldn't get SDVO clock rate multiplier\n");
+		return SDVO_CLOCK_RATE_MULT_1X;
+	} else {
+		DRM_DEBUG("Current clock rate multiplier: %d\n", response);
+	}
+
+	return response;
+}
+
+static bool intel_sdvo_set_clock_rate_mult(struct drm_output *output, u8 val)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_CLOCK_RATE_MULT, &val, 1);
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_mode_fixup(struct drm_output *output,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	/* Make the CRTC code factor in the SDVO pixel multiplier.  The SDVO
+	 * device will be told of the multiplier during mode_set.
+	 */
+	adjusted_mode->clock *= intel_sdvo_get_pixel_multiplier(mode);
+	return true;
+}
+
+static void intel_sdvo_mode_set(struct drm_output *output,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u16 width, height;
+	u16 h_blank_len, h_sync_len, v_blank_len, v_sync_len;
+	u16 h_sync_offset, v_sync_offset;
+	u32 sdvox;
+	struct intel_sdvo_dtd output_dtd;
+	int sdvo_pixel_multiply;
+
+	if (!mode)
+		return;
+
+	width = mode->crtc_hdisplay;
+	height = mode->crtc_vdisplay;
+
+	/* do some mode translations */
+	h_blank_len = mode->crtc_hblank_end - mode->crtc_hblank_start;
+	h_sync_len = mode->crtc_hsync_end - mode->crtc_hsync_start;
+
+	v_blank_len = mode->crtc_vblank_end - mode->crtc_vblank_start;
+	v_sync_len = mode->crtc_vsync_end - mode->crtc_vsync_start;
+
+	h_sync_offset = mode->crtc_hsync_start - mode->crtc_hblank_start;
+	v_sync_offset = mode->crtc_vsync_start - mode->crtc_vblank_start;
+
+	output_dtd.part1.clock = mode->clock / 10;
+	output_dtd.part1.h_active = width & 0xff;
+	output_dtd.part1.h_blank = h_blank_len & 0xff;
+	output_dtd.part1.h_high = (((width >> 8) & 0xf) << 4) |
+		((h_blank_len >> 8) & 0xf);
+	output_dtd.part1.v_active = height & 0xff;
+	output_dtd.part1.v_blank = v_blank_len & 0xff;
+	output_dtd.part1.v_high = (((height >> 8) & 0xf) << 4) |
+		((v_blank_len >> 8) & 0xf);
+	
+	output_dtd.part2.h_sync_off = h_sync_offset;
+	output_dtd.part2.h_sync_width = h_sync_len & 0xff;
+	output_dtd.part2.v_sync_off_width = (v_sync_offset & 0xf) << 4 |
+		(v_sync_len & 0xf);
+	output_dtd.part2.sync_off_width_high = ((h_sync_offset & 0x300) >> 2) |
+		((h_sync_len & 0x300) >> 4) | ((v_sync_offset & 0x30) >> 2) |
+		((v_sync_len & 0x30) >> 4);
+	
+	output_dtd.part2.dtd_flags = 0x18;
+	if (mode->flags & V_PHSYNC)
+		output_dtd.part2.dtd_flags |= 0x2;
+	if (mode->flags & V_PVSYNC)
+		output_dtd.part2.dtd_flags |= 0x4;
+
+	output_dtd.part2.sdvo_flags = 0;
+	output_dtd.part2.v_sync_off_high = v_sync_offset & 0xc0;
+	output_dtd.part2.reserved = 0;
+
+	/* Set the output timing to the screen */
+	intel_sdvo_set_target_output(output, sdvo_priv->active_outputs);
+	intel_sdvo_set_output_timing(output, &output_dtd);
+
+	/* Set the input timing to the screen. Assume always input 0. */
+	intel_sdvo_set_target_input(output, true, false);
+
+	/* We would like to use i830_sdvo_create_preferred_input_timing() to
+	 * provide the device with a timing it can support, if it supports that
+	 * feature.  However, presumably we would need to adjust the CRTC to
+	 * output the preferred timing, and we don't support that currently.
+	 */
+#if 0
+	success = intel_sdvo_create_preferred_input_timing(output, clock,
+							   width, height);
+	if (success) {
+		struct intel_sdvo_dtd *input_dtd;
+		
+		intel_sdvo_get_preferred_input_timing(output, &input_dtd);
+		intel_sdvo_set_input_timing(output, &input_dtd);
+	}
+#else
+	intel_sdvo_set_input_timing(output, &output_dtd);
+#endif	
+
+	switch (intel_sdvo_get_pixel_multiplier(mode)) {
+	case 1:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_1X);
+		break;
+	case 2:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_2X);
+		break;
+	case 4:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_4X);
+		break;
+	}	
+
+	/* Set the SDVO control regs. */
+        if (0/*IS_I965GM(dev)*/) {
+                sdvox = SDVO_BORDER_ENABLE;
+        } else {
+                sdvox = I915_READ(sdvo_priv->output_device);
+                switch (sdvo_priv->output_device) {
+                case SDVOB:
+                        sdvox &= SDVOB_PRESERVE_MASK;
+                        break;
+                case SDVOC:
+                        sdvox &= SDVOC_PRESERVE_MASK;
+                        break;
+                }
+                sdvox |= (9 << 19) | SDVO_BORDER_ENABLE;
+        }
+	if (intel_crtc->pipe == 1)
+		sdvox |= SDVO_PIPE_B_SELECT;
+
+	sdvo_pixel_multiply = intel_sdvo_get_pixel_multiplier(mode);
+	if (IS_I965G(dev)) {
+		/* done in crtc_mode_set as the dpll_md reg must be written 
+		   early */
+	} else if (IS_I945G(dev) || IS_I945GM(dev)) {
+		/* done in crtc_mode_set as it lives inside the 
+		   dpll register */
+	} else {
+		sdvox |= (sdvo_pixel_multiply - 1) << SDVO_PORT_MULTIPLY_SHIFT;
+	}
+
+	intel_sdvo_write_sdvox(output, sdvox);
+}
+
+static void intel_sdvo_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u32 temp;
+
+	if (mode != DPMSModeOn) {
+		intel_sdvo_set_active_outputs(output, 0);
+		if (0)
+			intel_sdvo_set_encoder_power_state(output, mode);
+
+		if (mode == DPMSModeOff) {
+			temp = I915_READ(sdvo_priv->output_device);
+			if ((temp & SDVO_ENABLE) != 0) {
+				intel_sdvo_write_sdvox(output, temp & ~SDVO_ENABLE);
+			}
+		}
+	} else {
+		bool input1, input2;
+		int i;
+		u8 status;
+		
+		temp = I915_READ(sdvo_priv->output_device);
+		if ((temp & SDVO_ENABLE) == 0)
+			intel_sdvo_write_sdvox(output, temp | SDVO_ENABLE);
+		for (i = 0; i < 2; i++)
+		  intel_wait_for_vblank(dev);
+		
+		status = intel_sdvo_get_trained_inputs(output, &input1,
+						       &input2);
+
+		
+		/* Warn if the device reported failure to sync. 
+		 * A lot of SDVO devices fail to notify of sync, but it's
+		 * a given it the status is a success, we succeeded.
+		 */
+		if (status == SDVO_CMD_STATUS_SUCCESS && !input1) {
+			DRM_DEBUG("First %s output reported failure to sync\n",
+				   SDVO_NAME(sdvo_priv));
+		}
+		
+		if (0)
+			intel_sdvo_set_encoder_power_state(output, mode);
+		intel_sdvo_set_active_outputs(output, sdvo_priv->active_outputs);
+	}	
+	return;
+}
+
+static void intel_sdvo_save(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int o;
+
+	sdvo_priv->save_sdvo_mult = intel_sdvo_get_clock_rate_mult(output);
+	intel_sdvo_get_active_outputs(output, &sdvo_priv->save_active_outputs);
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x1) {
+		intel_sdvo_set_target_input(output, true, false);
+		intel_sdvo_get_input_timing(output,
+					    &sdvo_priv->save_input_dtd_1);
+	}
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x2) {
+		intel_sdvo_set_target_input(output, false, true);
+		intel_sdvo_get_input_timing(output,
+					    &sdvo_priv->save_input_dtd_2);
+	}
+
+	for (o = SDVO_OUTPUT_FIRST; o <= SDVO_OUTPUT_LAST; o++)
+	{
+	        u16  this_output = (1 << o);
+		if (sdvo_priv->caps.output_flags & this_output)
+		{
+			intel_sdvo_set_target_output(output, this_output);
+			intel_sdvo_get_output_timing(output,
+						     &sdvo_priv->save_output_dtd[o]);
+		}
+	}
+
+	sdvo_priv->save_SDVOX = I915_READ(sdvo_priv->output_device);
+}
+
+static void intel_sdvo_restore(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int o;
+	int i;
+	bool input1, input2;
+	u8 status;
+
+	intel_sdvo_set_active_outputs(output, 0);
+
+	for (o = SDVO_OUTPUT_FIRST; o <= SDVO_OUTPUT_LAST; o++)
+	{
+		u16  this_output = (1 << o);
+		if (sdvo_priv->caps.output_flags & this_output) {
+			intel_sdvo_set_target_output(output, this_output);
+			intel_sdvo_set_output_timing(output, &sdvo_priv->save_output_dtd[o]);
+		}
+	}
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x1) {
+		intel_sdvo_set_target_input(output, true, false);
+		intel_sdvo_set_input_timing(output, &sdvo_priv->save_input_dtd_1);
+	}
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x2) {
+		intel_sdvo_set_target_input(output, false, true);
+		intel_sdvo_set_input_timing(output, &sdvo_priv->save_input_dtd_2);
+	}
+	
+	intel_sdvo_set_clock_rate_mult(output, sdvo_priv->save_sdvo_mult);
+	
+	I915_WRITE(sdvo_priv->output_device, sdvo_priv->save_SDVOX);
+	
+	if (sdvo_priv->save_SDVOX & SDVO_ENABLE)
+	{
+		for (i = 0; i < 2; i++)
+			intel_wait_for_vblank(dev);
+		status = intel_sdvo_get_trained_inputs(output, &input1, &input2);
+		if (status == SDVO_CMD_STATUS_SUCCESS && !input1)
+			DRM_DEBUG("First %s output reported failure to sync\n",
+				   SDVO_NAME(sdvo_priv));
+	}
+	
+	intel_sdvo_set_active_outputs(output, sdvo_priv->save_active_outputs);
+}
+
+static int intel_sdvo_mode_valid(struct drm_output *output,
+				 struct drm_display_mode *mode)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	if (mode->flags & V_DBLSCAN)
+		return MODE_NO_DBLESCAN;
+
+	if (sdvo_priv->pixel_clock_min > mode->clock)
+		return MODE_CLOCK_LOW;
+
+	if (sdvo_priv->pixel_clock_max < mode->clock)
+		return MODE_CLOCK_HIGH;
+
+	return MODE_OK;
+}
+
+static bool intel_sdvo_get_capabilities(struct drm_output *output, struct intel_sdvo_caps *caps)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_DEVICE_CAPS, NULL, 0);
+	status = intel_sdvo_read_response(output, caps, sizeof(*caps));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+
+static void intel_sdvo_dump_cmd(struct drm_output *output, int opcode)
+{
+
+
+}
+
+static void intel_sdvo_dump_device(struct drm_output *output)
+{
+
+}
+
+void intel_sdvo_dump(void)
+{
+
+}
+
+struct drm_output* intel_sdvo_find(struct drm_device *dev, int sdvoB)
+{
+	struct drm_output *output = 0;
+	struct intel_output *iout = 0;
+	struct intel_sdvo_priv *sdvo;
+
+	/* find the sdvo output */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		iout = output->driver_private;
+
+		if (iout->type != INTEL_OUTPUT_SDVO)
+			continue;
+
+		sdvo = iout->dev_priv;
+
+		if (sdvo->output_device == SDVOB && sdvoB)
+			return output;
+
+		if (sdvo->output_device == SDVOC && !sdvoB)
+			return output;
+
+    }
+
+	return 0;
+}
+
+int intel_sdvo_supports_hotplug(struct drm_output *output)
+{
+	u8 response[2];
+	u8 status;
+	DRM_DEBUG("\n");
+
+	if (!output)
+		return 0;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_HOT_PLUG_SUPPORT, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 2);
+
+	if (response[0] !=0)
+		return 1;
+
+	return 0;
+}
+
+void intel_sdvo_set_hotplug(struct drm_output *output, int on)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u8 response[2];
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ACTIVE_HOT_PLUG, NULL, 0);
+	intel_sdvo_read_response(output, &response, 2);
+
+	if (on) {
+		sdvo_priv->hotplug_enabled = 1;
+
+		intel_sdvo_write_cmd(output, SDVO_CMD_GET_HOT_PLUG_SUPPORT, NULL, 0);
+		status = intel_sdvo_read_response(output, &response, 2);
+
+		intel_sdvo_write_cmd(output, SDVO_CMD_SET_ACTIVE_HOT_PLUG, &response, 2);
+	} else {
+		sdvo_priv->hotplug_enabled = 0;
+
+		response[0] = 0;
+		response[1] = 0;
+		intel_sdvo_write_cmd(output, SDVO_CMD_SET_ACTIVE_HOT_PLUG, &response, 2);
+	}
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ACTIVE_HOT_PLUG, NULL, 0);
+	intel_sdvo_read_response(output, &response, 2);
+}
+
+static enum drm_output_status intel_sdvo_detect(struct drm_output *output)
+{
+	u8 response[2];
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ATTACHED_DISPLAYS, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 2);
+
+	DRM_DEBUG("SDVO response %d %d\n", response[0], response[1]);
+	if ((response[0] != 0) || (response[1] != 0))
+		return output_status_connected;
+	else
+		return output_status_disconnected;
+}
+
+static int intel_sdvo_get_modes(struct drm_output *output)
+{
+	/* set the bus switch and get the modes */
+	intel_sdvo_set_control_bus_switch(output, SDVO_CONTROL_BUS_DDC2);
+	intel_ddc_get_modes(output);
+
+	if (list_empty(&output->probed_modes))
+		return 0;
+	return 1;
+#if 0
+	/* Mac mini hack.  On this device, I get DDC through the analog, which
+	 * load-detects as disconnected.  I fail to DDC through the SDVO DDC,
+	 * but it does load-detect as connected.  So, just steal the DDC bits 
+	 * from analog when we fail at finding it the right way.
+	 */
+	/* TODO */
+	return NULL;
+
+	return NULL;
+#endif
+}
+
+static void intel_sdvo_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	if (intel_output->i2c_bus)
+		intel_i2c_destroy(intel_output->i2c_bus);
+
+	if (intel_output) {
+		kfree(intel_output);
+		output->driver_private = NULL;
+	}
+}
+
+static const struct drm_output_funcs intel_sdvo_output_funcs = {
+	.dpms = intel_sdvo_dpms,
+	.save = intel_sdvo_save,
+	.restore = intel_sdvo_restore,
+	.mode_valid = intel_sdvo_mode_valid,
+	.mode_fixup = intel_sdvo_mode_fixup,
+	.prepare = intel_output_prepare,
+	.mode_set = intel_sdvo_mode_set,
+	.commit = intel_output_commit,
+	.detect = intel_sdvo_detect,
+	.get_modes = intel_sdvo_get_modes,
+	.cleanup = intel_sdvo_destroy
+};
+
+void intel_sdvo_init(struct drm_device *dev, int output_device)
+{
+	struct drm_output *output;
+	struct intel_output *intel_output;
+	struct intel_sdvo_priv *sdvo_priv;
+	struct intel_i2c_chan *i2cbus = NULL;
+	int connector_type;
+	u8 ch[0x40];
+	int i;
+	int output_type, output_id;
+
+	output = drm_output_create(dev, &intel_sdvo_output_funcs,
+				   DRM_MODE_OUTPUT_NONE);
+	if (!output)
+		return;
+
+	intel_output = kcalloc(sizeof(struct intel_output)+sizeof(struct intel_sdvo_priv), 1, GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	sdvo_priv = (struct intel_sdvo_priv *)(intel_output + 1);
+	intel_output->type = INTEL_OUTPUT_SDVO;
+	output->driver_private = intel_output;
+	output->interlace_allowed = 0;
+	output->doublescan_allowed = 0;
+
+	/* setup the DDC bus. */
+	if (output_device == SDVOB)
+		i2cbus = intel_i2c_create(dev, GPIOE, "SDVOCTRL_E for SDVOB");
+	else
+		i2cbus = intel_i2c_create(dev, GPIOE, "SDVOCTRL_E for SDVOC");
+
+	if (i2cbus == NULL) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	sdvo_priv->i2c_bus = i2cbus;
+
+	if (output_device == SDVOB) {
+		output_id = 1;
+		sdvo_priv->i2c_bus->slave_addr = 0x38;
+	} else {
+		output_id = 2;
+		sdvo_priv->i2c_bus->slave_addr = 0x39;
+	}
+
+	sdvo_priv->output_device = output_device;
+	sdvo_priv->hotplug_enabled = 0;
+	intel_output->i2c_bus = i2cbus;
+	intel_output->dev_priv = sdvo_priv;
+
+
+	/* Read the regs to test if we can talk to the device */
+	for (i = 0; i < 0x40; i++) {
+		if (!intel_sdvo_read_byte(output, i, &ch[i])) {
+			DRM_DEBUG("No SDVO device found on SDVO%c\n",
+				  output_device == SDVOB ? 'B' : 'C');
+			drm_output_destroy(output);
+			return;
+		}
+	}
+
+	intel_sdvo_get_capabilities(output, &sdvo_priv->caps);
+
+	memset(&sdvo_priv->active_outputs, 0, sizeof(sdvo_priv->active_outputs));
+
+	/* TODO, CVBS, SVID, YPRPB & SCART outputs. */
+	if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_RGB0)
+	{
+		sdvo_priv->active_outputs = SDVO_OUTPUT_RGB0;
+		output->subpixel_order = SubPixelHorizontalRGB;
+		output_type = DRM_MODE_OUTPUT_DAC;
+		connector_type = ConnectorVGA;
+	}
+	else if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_RGB1)
+	{
+		sdvo_priv->active_outputs = SDVO_OUTPUT_RGB1;
+		output->subpixel_order = SubPixelHorizontalRGB;
+		output_type = DRM_MODE_OUTPUT_DAC;
+		connector_type = ConnectorVGA;
+	}
+	else if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS0)
+	{
+		sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS0;
+		output->subpixel_order = SubPixelHorizontalRGB;
+		output_type = DRM_MODE_OUTPUT_TMDS;
+		connector_type = ConnectorDVID;
+	}
+	else if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS1)
+	{
+		sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS1;
+		output->subpixel_order = SubPixelHorizontalRGB;
+		output_type = DRM_MODE_OUTPUT_TMDS;
+		connector_type = ConnectorDVID;
+	}
+	else
+	{
+		unsigned char bytes[2];
+		
+		memcpy (bytes, &sdvo_priv->caps.output_flags, 2);
+		DRM_DEBUG("%s: No active RGB or TMDS outputs (0x%02x%02x)\n",
+			  SDVO_NAME(sdvo_priv),
+			  bytes[0], bytes[1]);
+		drm_output_destroy(output);
+		return;
+	}
+	
+	output->output_type = output_type;
+	output->output_type_id = output_id;
+
+	/* Set the input timing to the screen. Assume always input 0. */
+	intel_sdvo_set_target_input(output, true, false);
+	
+	intel_sdvo_get_input_pixel_clock_range(output,
+					       &sdvo_priv->pixel_clock_min,
+					       &sdvo_priv->pixel_clock_max);
+
+
+	DRM_DEBUG("%s device VID/DID: %02X:%02X.%02X, "
+		  "clock range %dMHz - %dMHz, "
+		  "input 1: %c, input 2: %c, "
+		  "output 1: %c, output 2: %c\n",
+		  SDVO_NAME(sdvo_priv),
+		  sdvo_priv->caps.vendor_id, sdvo_priv->caps.device_id,
+		  sdvo_priv->caps.device_rev_id,
+		  sdvo_priv->pixel_clock_min / 1000,
+		  sdvo_priv->pixel_clock_max / 1000,
+		  (sdvo_priv->caps.sdvo_inputs_mask & 0x1) ? 'Y' : 'N',
+		  (sdvo_priv->caps.sdvo_inputs_mask & 0x2) ? 'Y' : 'N',
+		  /* check currently supported outputs */
+		  sdvo_priv->caps.output_flags & 
+		  	(SDVO_OUTPUT_TMDS0 | SDVO_OUTPUT_RGB0) ? 'Y' : 'N',
+		  sdvo_priv->caps.output_flags & 
+		  	(SDVO_OUTPUT_TMDS1 | SDVO_OUTPUT_RGB1) ? 'Y' : 'N');
+
+	intel_output->ddc_bus = i2cbus;	
+
+	drm_output_attach_property(output, dev->mode_config.connector_type_property, connector_type);
+}
diff --git a/drivers/char/drm/intel_sdvo_regs.h b/drivers/char/drm/intel_sdvo_regs.h
new file mode 100644
index 0000000..a9d1671
--- /dev/null
+++ b/drivers/char/drm/intel_sdvo_regs.h
@@ -0,0 +1,328 @@
+/*
+ * Copyright  2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+/**
+ * @file SDVO command definitions and structures.
+ */
+
+#define SDVO_OUTPUT_FIRST   (0)
+#define SDVO_OUTPUT_TMDS0   (1 << 0)
+#define SDVO_OUTPUT_RGB0    (1 << 1)
+#define SDVO_OUTPUT_CVBS0   (1 << 2)
+#define SDVO_OUTPUT_SVID0   (1 << 3)
+#define SDVO_OUTPUT_YPRPB0  (1 << 4)
+#define SDVO_OUTPUT_SCART0  (1 << 5)
+#define SDVO_OUTPUT_LVDS0   (1 << 6)
+#define SDVO_OUTPUT_TMDS1   (1 << 8)
+#define SDVO_OUTPUT_RGB1    (1 << 9)
+#define SDVO_OUTPUT_CVBS1   (1 << 10)
+#define SDVO_OUTPUT_SVID1   (1 << 11)
+#define SDVO_OUTPUT_YPRPB1  (1 << 12)
+#define SDVO_OUTPUT_SCART1  (1 << 13)
+#define SDVO_OUTPUT_LVDS1   (1 << 14)
+#define SDVO_OUTPUT_LAST    (14)
+
+struct intel_sdvo_caps {
+    u8 vendor_id;
+    u8 device_id;
+    u8 device_rev_id;
+    u8 sdvo_version_major;
+    u8 sdvo_version_minor;
+    unsigned int sdvo_inputs_mask:2;
+    unsigned int smooth_scaling:1;
+    unsigned int sharp_scaling:1;
+    unsigned int up_scaling:1;
+    unsigned int down_scaling:1;
+    unsigned int stall_support:1;
+    unsigned int pad:1;
+    u16 output_flags;
+} __attribute__((packed));
+
+/** This matches the EDID DTD structure, more or less */
+struct intel_sdvo_dtd {
+    struct {
+	u16 clock;		/**< pixel clock, in 10kHz units */
+	u8 h_active;		/**< lower 8 bits (pixels) */
+	u8 h_blank;		/**< lower 8 bits (pixels) */
+	u8 h_high;		/**< upper 4 bits each h_active, h_blank */
+	u8 v_active;		/**< lower 8 bits (lines) */
+	u8 v_blank;		/**< lower 8 bits (lines) */
+	u8 v_high;		/**< upper 4 bits each v_active, v_blank */
+    } part1;
+
+    struct {
+	u8 h_sync_off;	/**< lower 8 bits, from hblank start */
+	u8 h_sync_width;	/**< lower 8 bits (pixels) */
+	/** lower 4 bits each vsync offset, vsync width */
+	u8 v_sync_off_width;
+	/**
+	 * 2 high bits of hsync offset, 2 high bits of hsync width,
+	 * bits 4-5 of vsync offset, and 2 high bits of vsync width.
+	 */
+	u8 sync_off_width_high;
+	u8 dtd_flags;
+	u8 sdvo_flags;
+	/** bits 6-7 of vsync offset at bits 6-7 */
+	u8 v_sync_off_high;
+	u8 reserved;
+    } part2;
+} __attribute__((packed));
+
+struct intel_sdvo_pixel_clock_range {
+    u16 min;			/**< pixel clock, in 10kHz units */
+    u16 max;			/**< pixel clock, in 10kHz units */
+} __attribute__((packed));
+
+struct intel_sdvo_preferred_input_timing_args {
+    u16 clock;
+    u16 width;
+    u16 height;
+} __attribute__((packed));
+
+/* I2C registers for SDVO */
+#define SDVO_I2C_ARG_0				0x07
+#define SDVO_I2C_ARG_1				0x06
+#define SDVO_I2C_ARG_2				0x05
+#define SDVO_I2C_ARG_3				0x04
+#define SDVO_I2C_ARG_4				0x03
+#define SDVO_I2C_ARG_5				0x02
+#define SDVO_I2C_ARG_6				0x01
+#define SDVO_I2C_ARG_7				0x00
+#define SDVO_I2C_OPCODE				0x08
+#define SDVO_I2C_CMD_STATUS			0x09
+#define SDVO_I2C_RETURN_0			0x0a
+#define SDVO_I2C_RETURN_1			0x0b
+#define SDVO_I2C_RETURN_2			0x0c
+#define SDVO_I2C_RETURN_3			0x0d
+#define SDVO_I2C_RETURN_4			0x0e
+#define SDVO_I2C_RETURN_5			0x0f
+#define SDVO_I2C_RETURN_6			0x10
+#define SDVO_I2C_RETURN_7			0x11
+#define SDVO_I2C_VENDOR_BEGIN			0x20
+
+/* Status results */
+#define SDVO_CMD_STATUS_POWER_ON		0x0
+#define SDVO_CMD_STATUS_SUCCESS			0x1
+#define SDVO_CMD_STATUS_NOTSUPP			0x2
+#define SDVO_CMD_STATUS_INVALID_ARG		0x3
+#define SDVO_CMD_STATUS_PENDING			0x4
+#define SDVO_CMD_STATUS_TARGET_NOT_SPECIFIED	0x5
+#define SDVO_CMD_STATUS_SCALING_NOT_SUPP	0x6
+
+/* SDVO commands, argument/result registers */
+
+#define SDVO_CMD_RESET					0x01
+
+/** Returns a struct intel_sdvo_caps */
+#define SDVO_CMD_GET_DEVICE_CAPS			0x02
+
+#define SDVO_CMD_GET_FIRMWARE_REV			0x86
+# define SDVO_DEVICE_FIRMWARE_MINOR			SDVO_I2C_RETURN_0
+# define SDVO_DEVICE_FIRMWARE_MAJOR			SDVO_I2C_RETURN_1
+# define SDVO_DEVICE_FIRMWARE_PATCH			SDVO_I2C_RETURN_2
+
+/**
+ * Reports which inputs are trained (managed to sync).
+ *
+ * Devices must have trained within 2 vsyncs of a mode change.
+ */
+#define SDVO_CMD_GET_TRAINED_INPUTS			0x03
+struct intel_sdvo_get_trained_inputs_response {
+    unsigned int input0_trained:1;
+    unsigned int input1_trained:1;
+    unsigned int pad:6;
+} __attribute__((packed));
+
+/** Returns a struct intel_sdvo_output_flags of active outputs. */
+#define SDVO_CMD_GET_ACTIVE_OUTPUTS			0x04
+
+/**
+ * Sets the current set of active outputs.
+ *
+ * Takes a struct intel_sdvo_output_flags.  Must be preceded by a SET_IN_OUT_MAP
+ * on multi-output devices.
+ */
+#define SDVO_CMD_SET_ACTIVE_OUTPUTS			0x05
+
+/**
+ * Returns the current mapping of SDVO inputs to outputs on the device.
+ *
+ * Returns two struct intel_sdvo_output_flags structures.
+ */
+#define SDVO_CMD_GET_IN_OUT_MAP				0x06
+
+/**
+ * Sets the current mapping of SDVO inputs to outputs on the device.
+ *
+ * Takes two struct i380_sdvo_output_flags structures.
+ */
+#define SDVO_CMD_SET_IN_OUT_MAP				0x07
+
+/**
+ * Returns a struct intel_sdvo_output_flags of attached displays.
+ */
+#define SDVO_CMD_GET_ATTACHED_DISPLAYS			0x0b
+
+/**
+ * Returns a struct intel_sdvo_ouptut_flags of displays supporting hot plugging.
+ */
+#define SDVO_CMD_GET_HOT_PLUG_SUPPORT			0x0c
+
+/**
+ * Takes a struct intel_sdvo_output_flags.
+ */
+#define SDVO_CMD_SET_ACTIVE_HOT_PLUG			0x0d
+
+/**
+ * Returns a struct intel_sdvo_output_flags of displays with hot plug
+ * interrupts enabled.
+ */
+#define SDVO_CMD_GET_ACTIVE_HOT_PLUG			0x0e
+
+#define SDVO_CMD_GET_INTERRUPT_EVENT_SOURCE		0x0f
+struct intel_sdvo_get_interrupt_event_source_response {
+    u16 interrupt_status;
+    unsigned int ambient_light_interrupt:1;
+    unsigned int pad:7;
+} __attribute__((packed));
+
+/**
+ * Selects which input is affected by future input commands.
+ *
+ * Commands affected include SET_INPUT_TIMINGS_PART[12],
+ * GET_INPUT_TIMINGS_PART[12], GET_PREFERRED_INPUT_TIMINGS_PART[12],
+ * GET_INPUT_PIXEL_CLOCK_RANGE, and CREATE_PREFERRED_INPUT_TIMINGS.
+ */
+#define SDVO_CMD_SET_TARGET_INPUT			0x10
+struct intel_sdvo_set_target_input_args {
+    unsigned int target_1:1;
+    unsigned int pad:7;
+} __attribute__((packed));
+
+/**
+ * Takes a struct intel_sdvo_output_flags of which outputs are targetted by
+ * future output commands.
+ *
+ * Affected commands inclue SET_OUTPUT_TIMINGS_PART[12],
+ * GET_OUTPUT_TIMINGS_PART[12], and GET_OUTPUT_PIXEL_CLOCK_RANGE.
+ */
+#define SDVO_CMD_SET_TARGET_OUTPUT			0x11
+
+#define SDVO_CMD_GET_INPUT_TIMINGS_PART1		0x12
+#define SDVO_CMD_GET_INPUT_TIMINGS_PART2		0x13
+#define SDVO_CMD_SET_INPUT_TIMINGS_PART1		0x14
+#define SDVO_CMD_SET_INPUT_TIMINGS_PART2		0x15
+#define SDVO_CMD_SET_OUTPUT_TIMINGS_PART1		0x16
+#define SDVO_CMD_SET_OUTPUT_TIMINGS_PART2		0x17
+#define SDVO_CMD_GET_OUTPUT_TIMINGS_PART1		0x18
+#define SDVO_CMD_GET_OUTPUT_TIMINGS_PART2		0x19
+/* Part 1 */
+# define SDVO_DTD_CLOCK_LOW				SDVO_I2C_ARG_0
+# define SDVO_DTD_CLOCK_HIGH				SDVO_I2C_ARG_1
+# define SDVO_DTD_H_ACTIVE				SDVO_I2C_ARG_2
+# define SDVO_DTD_H_BLANK				SDVO_I2C_ARG_3
+# define SDVO_DTD_H_HIGH				SDVO_I2C_ARG_4
+# define SDVO_DTD_V_ACTIVE				SDVO_I2C_ARG_5
+# define SDVO_DTD_V_BLANK				SDVO_I2C_ARG_6
+# define SDVO_DTD_V_HIGH				SDVO_I2C_ARG_7
+/* Part 2 */
+# define SDVO_DTD_HSYNC_OFF				SDVO_I2C_ARG_0
+# define SDVO_DTD_HSYNC_WIDTH				SDVO_I2C_ARG_1
+# define SDVO_DTD_VSYNC_OFF_WIDTH			SDVO_I2C_ARG_2
+# define SDVO_DTD_SYNC_OFF_WIDTH_HIGH			SDVO_I2C_ARG_3
+# define SDVO_DTD_DTD_FLAGS				SDVO_I2C_ARG_4
+# define SDVO_DTD_DTD_FLAG_INTERLACED				(1 << 7)
+# define SDVO_DTD_DTD_FLAG_STEREO_MASK				(3 << 5)
+# define SDVO_DTD_DTD_FLAG_INPUT_MASK				(3 << 3)
+# define SDVO_DTD_DTD_FLAG_SYNC_MASK				(3 << 1)
+# define SDVO_DTD_SDVO_FLAS				SDVO_I2C_ARG_5
+# define SDVO_DTD_SDVO_FLAG_STALL				(1 << 7)
+# define SDVO_DTD_SDVO_FLAG_CENTERED				(0 << 6)
+# define SDVO_DTD_SDVO_FLAG_UPPER_LEFT				(1 << 6)
+# define SDVO_DTD_SDVO_FLAG_SCALING_MASK			(3 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_NONE			(0 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_SHARP			(1 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_SMOOTH			(2 << 4)
+# define SDVO_DTD_VSYNC_OFF_HIGH			SDVO_I2C_ARG_6
+
+/**
+ * Generates a DTD based on the given width, height, and flags.
+ *
+ * This will be supported by any device supporting scaling or interlaced
+ * modes.
+ */
+#define SDVO_CMD_CREATE_PREFERRED_INPUT_TIMING		0x1a
+# define SDVO_PREFERRED_INPUT_TIMING_CLOCK_LOW		SDVO_I2C_ARG_0
+# define SDVO_PREFERRED_INPUT_TIMING_CLOCK_HIGH		SDVO_I2C_ARG_1
+# define SDVO_PREFERRED_INPUT_TIMING_WIDTH_LOW		SDVO_I2C_ARG_2
+# define SDVO_PREFERRED_INPUT_TIMING_WIDTH_HIGH		SDVO_I2C_ARG_3
+# define SDVO_PREFERRED_INPUT_TIMING_HEIGHT_LOW		SDVO_I2C_ARG_4
+# define SDVO_PREFERRED_INPUT_TIMING_HEIGHT_HIGH	SDVO_I2C_ARG_5
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS		SDVO_I2C_ARG_6
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS_INTERLACED		(1 << 0)
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS_SCALED		(1 << 1)
+
+#define SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1	0x1b
+#define SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2	0x1c
+
+/** Returns a struct intel_sdvo_pixel_clock_range */
+#define SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE		0x1d
+/** Returns a struct intel_sdvo_pixel_clock_range */
+#define SDVO_CMD_GET_OUTPUT_PIXEL_CLOCK_RANGE		0x1e
+
+/** Returns a byte bitfield containing SDVO_CLOCK_RATE_MULT_* flags */
+#define SDVO_CMD_GET_SUPPORTED_CLOCK_RATE_MULTS		0x1f
+
+/** Returns a byte containing a SDVO_CLOCK_RATE_MULT_* flag */
+#define SDVO_CMD_GET_CLOCK_RATE_MULT			0x20
+/** Takes a byte containing a SDVO_CLOCK_RATE_MULT_* flag */
+#define SDVO_CMD_SET_CLOCK_RATE_MULT			0x21
+# define SDVO_CLOCK_RATE_MULT_1X				(1 << 0)
+# define SDVO_CLOCK_RATE_MULT_2X				(1 << 1)
+# define SDVO_CLOCK_RATE_MULT_4X				(1 << 3)
+
+#define SDVO_CMD_GET_SUPPORTED_TV_FORMATS		0x27
+
+#define SDVO_CMD_GET_TV_FORMAT				0x28
+
+#define SDVO_CMD_SET_TV_FORMAT				0x29
+
+#define SDVO_CMD_GET_SUPPORTED_POWER_STATES		0x2a
+#define SDVO_CMD_GET_ENCODER_POWER_STATE		0x2b
+#define SDVO_CMD_SET_ENCODER_POWER_STATE		0x2c
+# define SDVO_ENCODER_STATE_ON					(1 << 0)
+# define SDVO_ENCODER_STATE_STANDBY				(1 << 1)
+# define SDVO_ENCODER_STATE_SUSPEND				(1 << 2)
+# define SDVO_ENCODER_STATE_OFF					(1 << 3)
+
+#define SDVO_CMD_SET_TV_RESOLUTION_SUPPORT		0x93
+
+#define SDVO_CMD_SET_CONTROL_BUS_SWITCH			0x7a
+# define SDVO_CONTROL_BUS_PROM				0x0
+# define SDVO_CONTROL_BUS_DDC1				0x1
+# define SDVO_CONTROL_BUS_DDC2				0x2
+# define SDVO_CONTROL_BUS_DDC3				0x3
+
diff --git a/drivers/char/drm/intel_tv.c b/drivers/char/drm/intel_tv.c
new file mode 100644
index 0000000..82c186e
--- /dev/null
+++ b/drivers/char/drm/intel_tv.c
@@ -0,0 +1,1763 @@
+/*
+ * Copyright  2006 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Eric Anholt <eric@anholt.net>
+ *
+ */
+
+/** @file
+ * Integrated TV-out support for the 915GM and 945GM.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "drm_edid.h"
+#include "intel_drv.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+
+enum tv_type {
+	TV_TYPE_NONE,
+	TV_TYPE_UNKNOWN,
+	TV_TYPE_COMPOSITE,
+	TV_TYPE_SVIDEO,
+	TV_TYPE_COMPONENT
+};
+
+enum tv_margin {
+	TV_MARGIN_LEFT, TV_MARGIN_TOP,
+	TV_MARGIN_RIGHT, TV_MARGIN_BOTTOM
+};
+
+/** Private structure for the integrated TV support */
+struct intel_tv_priv {
+	int type;
+	char *tv_format;
+	int margin[4];
+	u32 save_TV_H_CTL_1;
+	u32 save_TV_H_CTL_2;
+	u32 save_TV_H_CTL_3;
+	u32 save_TV_V_CTL_1;
+	u32 save_TV_V_CTL_2;
+	u32 save_TV_V_CTL_3;
+	u32 save_TV_V_CTL_4;
+	u32 save_TV_V_CTL_5;
+	u32 save_TV_V_CTL_6;
+	u32 save_TV_V_CTL_7;
+	u32 save_TV_SC_CTL_1, save_TV_SC_CTL_2, save_TV_SC_CTL_3;
+
+	u32 save_TV_CSC_Y;
+	u32 save_TV_CSC_Y2;
+	u32 save_TV_CSC_U;
+	u32 save_TV_CSC_U2;
+	u32 save_TV_CSC_V;
+	u32 save_TV_CSC_V2;
+	u32 save_TV_CLR_KNOBS;
+	u32 save_TV_CLR_LEVEL;
+	u32 save_TV_WIN_POS;
+	u32 save_TV_WIN_SIZE;
+	u32 save_TV_FILTER_CTL_1;
+	u32 save_TV_FILTER_CTL_2;
+	u32 save_TV_FILTER_CTL_3;
+
+	u32 save_TV_H_LUMA[60];
+	u32 save_TV_H_CHROMA[60];
+	u32 save_TV_V_LUMA[43];
+	u32 save_TV_V_CHROMA[43];
+
+	u32 save_TV_DAC;
+	u32 save_TV_CTL;
+};
+
+struct video_levels {
+	int blank, black, burst;
+};
+
+struct color_conversion {
+	u16 ry, gy, by, ay;
+	u16 ru, gu, bu, au;
+	u16 rv, gv, bv, av;
+};
+
+static const u32 filter_table[] = {
+	0xB1403000, 0x2E203500, 0x35002E20, 0x3000B140,
+	0x35A0B160, 0x2DC02E80, 0xB1403480, 0xB1603000,
+	0x2EA03640, 0x34002D80, 0x3000B120, 0x36E0B160,
+	0x2D202EF0, 0xB1203380, 0xB1603000, 0x2F303780,
+	0x33002CC0, 0x3000B100, 0x3820B160, 0x2C802F50,
+	0xB10032A0, 0xB1603000, 0x2F9038C0, 0x32202C20,
+	0x3000B0E0, 0x3980B160, 0x2BC02FC0, 0xB0E031C0,
+	0xB1603000, 0x2FF03A20, 0x31602B60, 0xB020B0C0,
+	0x3AE0B160, 0x2B001810, 0xB0C03120, 0xB140B020,
+	0x18283BA0, 0x30C02A80, 0xB020B0A0, 0x3C60B140,
+	0x2A201838, 0xB0A03080, 0xB120B020, 0x18383D20,
+	0x304029C0, 0xB040B080, 0x3DE0B100, 0x29601848,
+	0xB0803000, 0xB100B040, 0x18483EC0, 0xB0402900,
+	0xB040B060, 0x3F80B0C0, 0x28801858, 0xB060B080,
+	0xB0A0B060, 0x18602820, 0xB0A02820, 0x0000B060,
+	0xB1403000, 0x2E203500, 0x35002E20, 0x3000B140,
+	0x35A0B160, 0x2DC02E80, 0xB1403480, 0xB1603000,
+	0x2EA03640, 0x34002D80, 0x3000B120, 0x36E0B160,
+	0x2D202EF0, 0xB1203380, 0xB1603000, 0x2F303780,
+	0x33002CC0, 0x3000B100, 0x3820B160, 0x2C802F50,
+	0xB10032A0, 0xB1603000, 0x2F9038C0, 0x32202C20,
+	0x3000B0E0, 0x3980B160, 0x2BC02FC0, 0xB0E031C0,
+	0xB1603000, 0x2FF03A20, 0x31602B60, 0xB020B0C0,
+	0x3AE0B160, 0x2B001810, 0xB0C03120, 0xB140B020,
+	0x18283BA0, 0x30C02A80, 0xB020B0A0, 0x3C60B140,
+	0x2A201838, 0xB0A03080, 0xB120B020, 0x18383D20,
+	0x304029C0, 0xB040B080, 0x3DE0B100, 0x29601848,
+	0xB0803000, 0xB100B040, 0x18483EC0, 0xB0402900,
+	0xB040B060, 0x3F80B0C0, 0x28801858, 0xB060B080,
+	0xB0A0B060, 0x18602820, 0xB0A02820, 0x0000B060,
+	0x36403000, 0x2D002CC0, 0x30003640, 0x2D0036C0,
+	0x35C02CC0, 0x37403000, 0x2C802D40, 0x30003540,
+	0x2D8037C0, 0x34C02C40, 0x38403000, 0x2BC02E00,
+	0x30003440, 0x2E2038C0, 0x34002B80, 0x39803000,
+	0x2B402E40, 0x30003380, 0x2E603A00, 0x33402B00,
+	0x3A803040, 0x2A802EA0, 0x30403300, 0x2EC03B40,
+	0x32802A40, 0x3C003040, 0x2A002EC0, 0x30803240,
+	0x2EC03C80, 0x320029C0, 0x3D403080, 0x29402F00,
+	0x308031C0, 0x2F203DC0, 0x31802900, 0x3E8030C0,
+	0x28802F40, 0x30C03140, 0x2F203F40, 0x31402840,
+	0x28003100, 0x28002F00, 0x00003100, 0x36403000, 
+	0x2D002CC0, 0x30003640, 0x2D0036C0,
+	0x35C02CC0, 0x37403000, 0x2C802D40, 0x30003540,
+	0x2D8037C0, 0x34C02C40, 0x38403000, 0x2BC02E00,
+	0x30003440, 0x2E2038C0, 0x34002B80, 0x39803000,
+	0x2B402E40, 0x30003380, 0x2E603A00, 0x33402B00,
+	0x3A803040, 0x2A802EA0, 0x30403300, 0x2EC03B40,
+	0x32802A40, 0x3C003040, 0x2A002EC0, 0x30803240,
+	0x2EC03C80, 0x320029C0, 0x3D403080, 0x29402F00,
+	0x308031C0, 0x2F203DC0, 0x31802900, 0x3E8030C0,
+	0x28802F40, 0x30C03140, 0x2F203F40, 0x31402840,
+	0x28003100, 0x28002F00, 0x00003100,
+};
+
+/*
+ * Color conversion values have 3 separate fixed point formats:
+ *
+ * 10 bit fields (ay, au)
+ *   1.9 fixed point (b.bbbbbbbbb)
+ * 11 bit fields (ry, by, ru, gu, gv)
+ *   exp.mantissa (ee.mmmmmmmmm)
+ *   ee = 00 = 10^-1 (0.mmmmmmmmm)
+ *   ee = 01 = 10^-2 (0.0mmmmmmmmm)
+ *   ee = 10 = 10^-3 (0.00mmmmmmmmm)
+ *   ee = 11 = 10^-4 (0.000mmmmmmmmm)
+ * 12 bit fields (gy, rv, bu)
+ *   exp.mantissa (eee.mmmmmmmmm)
+ *   eee = 000 = 10^-1 (0.mmmmmmmmm)
+ *   eee = 001 = 10^-2 (0.0mmmmmmmmm)
+ *   eee = 010 = 10^-3 (0.00mmmmmmmmm)
+ *   eee = 011 = 10^-4 (0.000mmmmmmmmm)
+ *   eee = 100 = reserved
+ *   eee = 101 = reserved
+ *   eee = 110 = reserved
+ *   eee = 111 = 10^0 (m.mmmmmmmm) (only usable for 1.0 representation)
+ *
+ * Saturation and contrast are 8 bits, with their own representation:
+ * 8 bit field (saturation, contrast)
+ *   exp.mantissa (ee.mmmmmm)
+ *   ee = 00 = 10^-1 (0.mmmmmm)
+ *   ee = 01 = 10^0 (m.mmmmm)
+ *   ee = 10 = 10^1 (mm.mmmm)
+ *   ee = 11 = 10^2 (mmm.mmm)
+ *
+ * Simple conversion function:
+ *
+ * static u32
+ * float_to_csc_11(float f)
+ * {
+ *     u32 exp;
+ *     u32 mant;
+ *     u32 ret;
+ * 
+ *     if (f < 0)
+ *         f = -f;
+ * 
+ *     if (f >= 1) {
+ *         exp = 0x7;
+ * 	   mant = 1 << 8;
+ *     } else {
+ *         for (exp = 0; exp < 3 && f < 0.5; exp++)
+ * 	       f *= 2.0;
+ *         mant = (f * (1 << 9) + 0.5);
+ *         if (mant >= (1 << 9))
+ *             mant = (1 << 9) - 1;
+ *     }
+ *     ret = (exp << 9) | mant;
+ *     return ret;
+ * }
+ */
+
+/*
+ * Behold, magic numbers!  If we plant them they might grow a big
+ * s-video cable to the sky... or something.
+ *
+ * Pre-converted to appropriate hex value.
+ */
+
+/*
+ * PAL & NTSC values for composite & s-video connections
+ */
+static const struct color_conversion ntsc_m_csc_composite = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0104,
+	.ru = 0x0733, .gu = 0x052d, .bu = 0x05c7, .au = 0x0f00,
+	.rv = 0x0340, .gv = 0x030c, .bv = 0x06d0, .av = 0x0f00,
+};
+
+static const struct video_levels ntsc_m_levels_composite = {
+	.blank = 225, .black = 267, .burst = 113,
+};
+
+static const struct color_conversion ntsc_m_csc_svideo = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0134,
+	.ru = 0x076a, .gu = 0x0564, .bu = 0x030d, .au = 0x0f00,
+	.rv = 0x037a, .gv = 0x033d, .bv = 0x06f6, .av = 0x0f00,
+};
+
+static const struct video_levels ntsc_m_levels_svideo = {
+	.blank = 266, .black = 316, .burst = 133,
+};
+
+static const struct color_conversion ntsc_j_csc_composite = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0119,
+	.ru = 0x074c, .gu = 0x0546, .bu = 0x05ec, .au = 0x0f00,
+	.rv = 0x035a, .gv = 0x0322, .bv = 0x06e1, .av = 0x0f00,
+};
+
+static const struct video_levels ntsc_j_levels_composite = {
+	.blank = 225, .black = 225, .burst = 113,
+};
+
+static const struct color_conversion ntsc_j_csc_svideo = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x014c,
+	.ru = 0x0788, .gu = 0x0581, .bu = 0x0322, .au = 0x0f00,
+	.rv = 0x0399, .gv = 0x0356, .bv = 0x070a, .av = 0x0f00,
+};
+
+static const struct video_levels ntsc_j_levels_svideo = {
+	.blank = 266, .black = 266, .burst = 133,
+};
+
+static const struct color_conversion pal_csc_composite = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0113,
+	.ru = 0x0745, .gu = 0x053f, .bu = 0x05e1, .au = 0x0f00,
+	.rv = 0x0353, .gv = 0x031c, .bv = 0x06dc, .av = 0x0f00,
+};
+	
+static const struct video_levels pal_levels_composite = {
+	.blank = 237, .black = 237, .burst = 118,
+};
+
+static const struct color_conversion pal_csc_svideo = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0145,
+	.ru = 0x0780, .gu = 0x0579, .bu = 0x031c, .au = 0x0f00,
+	.rv = 0x0390, .gv = 0x034f, .bv = 0x0705, .av = 0x0f00,
+};
+
+static const struct video_levels pal_levels_svideo = {
+	.blank = 280, .black = 280, .burst = 139,
+};
+
+static const struct color_conversion pal_m_csc_composite = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0104,
+	.ru = 0x0733, .gu = 0x052d, .bu = 0x05c7, .au = 0x0f00,
+	.rv = 0x0340, .gv = 0x030c, .bv = 0x06d0, .av = 0x0f00,
+};
+
+static const struct video_levels pal_m_levels_composite = {
+	.blank = 225, .black = 267, .burst = 113,
+};
+
+static const struct color_conversion pal_m_csc_svideo = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0134,
+	.ru = 0x076a, .gu = 0x0564, .bu = 0x030d, .au = 0x0f00,
+	.rv = 0x037a, .gv = 0x033d, .bv = 0x06f6, .av = 0x0f00,
+};
+
+static const struct video_levels pal_m_levels_svideo = {
+	.blank = 266, .black = 316, .burst = 133,
+};
+
+static const struct color_conversion pal_n_csc_composite = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0104,
+	.ru = 0x0733, .gu = 0x052d, .bu = 0x05c7, .au = 0x0f00,
+	.rv = 0x0340, .gv = 0x030c, .bv = 0x06d0, .av = 0x0f00,
+};
+
+static const struct video_levels pal_n_levels_composite = {
+	.blank = 225, .black = 267, .burst = 118,
+};
+
+static const struct color_conversion pal_n_csc_svideo = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0134,
+	.ru = 0x076a, .gu = 0x0564, .bu = 0x030d, .au = 0x0f00,
+	.rv = 0x037a, .gv = 0x033d, .bv = 0x06f6, .av = 0x0f00,
+};
+
+static const struct video_levels pal_n_levels_svideo = {
+	.blank = 266, .black = 316, .burst = 139,
+};
+
+/*
+ * Component connections
+ */
+static const struct color_conversion sdtv_csc_yprpb = {
+	.ry = 0x0332, .gy = 0x012d, .by = 0x07d3, .ay = 0x0146,
+	.ru = 0x0559, .gu = 0x0353, .bu = 0x0100, .au = 0x0f00,
+	.rv = 0x0100, .gv = 0x03ad, .bv = 0x074d, .av = 0x0f00,
+};
+
+static const struct color_conversion sdtv_csc_rgb = {
+	.ry = 0x0000, .gy = 0x0f00, .by = 0x0000, .ay = 0x0166,
+	.ru = 0x0000, .gu = 0x0000, .bu = 0x0f00, .au = 0x0166,
+	.rv = 0x0f00, .gv = 0x0000, .bv = 0x0000, .av = 0x0166,
+};
+
+static const struct color_conversion hdtv_csc_yprpb = {
+	.ry = 0x05b3, .gy = 0x016e, .by = 0x0728, .ay = 0x0146,
+	.ru = 0x07d5, .gu = 0x038b, .bu = 0x0100, .au = 0x0f00,
+	.rv = 0x0100, .gv = 0x03d1, .bv = 0x06bc, .av = 0x0f00,
+};
+
+static const struct color_conversion hdtv_csc_rgb = {
+	.ry = 0x0000, .gy = 0x0f00, .by = 0x0000, .ay = 0x0166,
+	.ru = 0x0000, .gu = 0x0000, .bu = 0x0f00, .au = 0x0166,
+	.rv = 0x0f00, .gv = 0x0000, .bv = 0x0000, .av = 0x0166,
+};
+
+static const struct video_levels component_levels = {
+	.blank = 279, .black = 279, .burst = 0,
+};
+
+
+struct tv_mode {
+	char *name;
+	int clock;
+	int refresh; /* in millihertz (for precision) */
+	u32 oversample;
+	int hsync_end, hblank_start, hblank_end, htotal;
+	bool progressive, trilevel_sync, component_only;
+	int vsync_start_f1, vsync_start_f2, vsync_len;
+	bool veq_ena;
+	int veq_start_f1, veq_start_f2, veq_len;
+	int vi_end_f1, vi_end_f2, nbr_end;
+	bool burst_ena;
+	int hburst_start, hburst_len;
+	int vburst_start_f1, vburst_end_f1;
+	int vburst_start_f2, vburst_end_f2;
+	int vburst_start_f3, vburst_end_f3;
+	int vburst_start_f4, vburst_end_f4;
+	/*
+	 * subcarrier programming
+	 */
+	int dda2_size, dda3_size, dda1_inc, dda2_inc, dda3_inc;
+	u32 sc_reset;
+	bool pal_burst;
+	/*
+	 * blank/black levels
+	 */
+	const struct video_levels *composite_levels, *svideo_levels;
+	const struct color_conversion *composite_color, *svideo_color;
+	const u32 *filter_table;
+	int max_srcw;
+};
+
+
+/*
+ * Sub carrier DDA
+ *
+ *  I think this works as follows:
+ *
+ *  subcarrier freq = pixel_clock * (dda1_inc + dda2_inc / dda2_size) / 4096
+ *
+ * Presumably, when dda3 is added in, it gets to adjust the dda2_inc value
+ *
+ * So,
+ *  dda1_ideal = subcarrier/pixel * 4096
+ *  dda1_inc = floor (dda1_ideal)
+ *  dda2 = dda1_ideal - dda1_inc
+ *
+ *  then pick a ratio for dda2 that gives the closest approximation. If
+ *  you can't get close enough, you can play with dda3 as well. This
+ *  seems likely to happen when dda2 is small as the jumps would be larger
+ *
+ * To invert this,
+ *
+ *  pixel_clock = subcarrier * 4096 / (dda1_inc + dda2_inc / dda2_size)
+ *
+ * The constants below were all computed using a 107.520MHz clock
+ */
+ 
+/**
+ * Register programming values for TV modes.
+ *
+ * These values account for -1s required.
+ */
+
+const static struct tv_mode tv_modes[] = {
+	{
+		.name		= "NTSC-M",
+		.clock		= 107520,	
+		.refresh	= 29970,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+		/* 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */
+
+		.hsync_end	= 64,		    .hblank_end		= 124,
+		.hblank_start	= 836,		    .htotal		= 857,
+
+		.progressive	= false,	    .trilevel_sync = false,
+
+		.vsync_start_f1	= 6,		    .vsync_start_f2	= 7,
+		.vsync_len	= 6,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2	= 1,		    .veq_len		= 18,
+
+		.vi_end_f1	= 20,		    .vi_end_f2		= 21,
+		.nbr_end	= 240,
+
+		.burst_ena	= true,
+		.hburst_start	= 72,		    .hburst_len		= 34,
+		.vburst_start_f1 = 9,		    .vburst_end_f1	= 240,
+		.vburst_start_f2 = 10,		    .vburst_end_f2	= 240,
+		.vburst_start_f3 = 9,		    .vburst_end_f3	= 240, 
+		.vburst_start_f4 = 10,		    .vburst_end_f4	= 240,
+
+		/* desired 3.5800000 actual 3.5800000 clock 107.52 */
+		.dda1_inc	=    136,
+		.dda2_inc	=   7624,	    .dda2_size		=  20013,
+		.dda3_inc	=      0,	    .dda3_size		=      0,
+		.sc_reset	= TV_SC_RESET_EVERY_4,
+		.pal_burst	= false,
+
+		.composite_levels = &ntsc_m_levels_composite,
+		.composite_color = &ntsc_m_csc_composite,
+		.svideo_levels  = &ntsc_m_levels_svideo,
+		.svideo_color = &ntsc_m_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name		= "NTSC-443",
+		.clock		= 107520,	
+		.refresh	= 29970,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+		/* 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 4.43MHz */
+		.hsync_end	= 64,		    .hblank_end		= 124,
+		.hblank_start	= 836,		    .htotal		= 857,
+
+		.progressive	= false,	    .trilevel_sync = false,
+
+		.vsync_start_f1 = 6,		    .vsync_start_f2	= 7,
+		.vsync_len	= 6,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2	= 1,		    .veq_len		= 18,
+
+		.vi_end_f1	= 20,		    .vi_end_f2		= 21,
+		.nbr_end	= 240,
+
+		.burst_ena	= 8,
+		.hburst_start	= 72,		    .hburst_len		= 34,
+		.vburst_start_f1 = 9,		    .vburst_end_f1	= 240,
+		.vburst_start_f2 = 10,		    .vburst_end_f2	= 240,
+		.vburst_start_f3 = 9,		    .vburst_end_f3	= 240, 
+		.vburst_start_f4 = 10,		    .vburst_end_f4	= 240,
+
+		/* desired 4.4336180 actual 4.4336180 clock 107.52 */
+		.dda1_inc       =    168,
+		.dda2_inc       =  18557,       .dda2_size      =  20625,
+		.dda3_inc       =      0,       .dda3_size      =      0,
+		.sc_reset   = TV_SC_RESET_EVERY_8,
+		.pal_burst  = true,
+
+		.composite_levels = &ntsc_m_levels_composite,
+		.composite_color = &ntsc_m_csc_composite,
+		.svideo_levels  = &ntsc_m_levels_svideo,
+		.svideo_color = &ntsc_m_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name		= "NTSC-J",
+		.clock		= 107520,	
+		.refresh	= 29970,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+
+		/* 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */
+		.hsync_end	= 64,		    .hblank_end		= 124,
+		.hblank_start = 836,	    .htotal		= 857,
+
+		.progressive	= false,    .trilevel_sync = false,
+
+		.vsync_start_f1	= 6,	    .vsync_start_f2	= 7,
+		.vsync_len	= 6,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2 = 1,	    .veq_len		= 18,
+
+		.vi_end_f1	= 20,		    .vi_end_f2		= 21,
+		.nbr_end	= 240,
+
+		.burst_ena	= true,
+		.hburst_start	= 72,		    .hburst_len		= 34,
+		.vburst_start_f1 = 9,		    .vburst_end_f1	= 240,
+		.vburst_start_f2 = 10,		    .vburst_end_f2	= 240,
+		.vburst_start_f3 = 9,		    .vburst_end_f3	= 240, 
+		.vburst_start_f4 = 10,		    .vburst_end_f4	= 240,
+
+		/* desired 3.5800000 actual 3.5800000 clock 107.52 */
+		.dda1_inc	=    136,
+		.dda2_inc	=   7624,	    .dda2_size		=  20013,
+		.dda3_inc	=      0,	    .dda3_size		=      0,
+		.sc_reset	= TV_SC_RESET_EVERY_4,
+		.pal_burst	= false,
+
+		.composite_levels = &ntsc_j_levels_composite,
+		.composite_color = &ntsc_j_csc_composite,
+		.svideo_levels  = &ntsc_j_levels_svideo,
+		.svideo_color = &ntsc_j_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name		= "PAL-M",
+		.clock		= 107520,	
+		.refresh	= 29970,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+
+		/* 525 Lines, 60 Fields, 15.734KHz line, Sub-Carrier 3.580MHz */
+		.hsync_end	= 64,		  .hblank_end		= 124,
+		.hblank_start = 836,	  .htotal		= 857,
+
+		.progressive	= false,	    .trilevel_sync = false,
+
+		.vsync_start_f1	= 6,		    .vsync_start_f2	= 7,
+		.vsync_len	= 6,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2	= 1,		    .veq_len		= 18,
+
+		.vi_end_f1	= 20,		    .vi_end_f2		= 21,
+		.nbr_end	= 240,
+
+		.burst_ena	= true,
+		.hburst_start	= 72,		    .hburst_len		= 34,
+		.vburst_start_f1 = 9,		    .vburst_end_f1	= 240,
+		.vburst_start_f2 = 10,		    .vburst_end_f2	= 240,
+		.vburst_start_f3 = 9,		    .vburst_end_f3	= 240, 
+		.vburst_start_f4 = 10,		    .vburst_end_f4	= 240,
+
+		/* desired 3.5800000 actual 3.5800000 clock 107.52 */
+		.dda1_inc	=    136,
+		.dda2_inc	=    7624,	    .dda2_size		=  20013,
+		.dda3_inc	=      0,	    .dda3_size		=      0,
+		.sc_reset	= TV_SC_RESET_EVERY_4,
+		.pal_burst  = false,
+
+		.composite_levels = &pal_m_levels_composite,
+		.composite_color = &pal_m_csc_composite,
+		.svideo_levels  = &pal_m_levels_svideo,
+		.svideo_color = &pal_m_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		/* 625 Lines, 50 Fields, 15.625KHz line, Sub-Carrier 4.434MHz */
+		.name	    = "PAL-N",
+		.clock		= 107520,	
+		.refresh	= 25000,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+
+		.hsync_end	= 64,		    .hblank_end		= 128,
+		.hblank_start = 844,	    .htotal		= 863,
+
+		.progressive  = false,    .trilevel_sync = false,
+
+
+		.vsync_start_f1	= 6,	   .vsync_start_f2	= 7,
+		.vsync_len	= 6,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2	= 1,		    .veq_len		= 18,
+
+		.vi_end_f1	= 24,		    .vi_end_f2		= 25,
+		.nbr_end	= 286,
+
+		.burst_ena	= true,
+		.hburst_start = 73,	    	    .hburst_len		= 34,
+		.vburst_start_f1 = 8,	    .vburst_end_f1	= 285,
+		.vburst_start_f2 = 8,	    .vburst_end_f2	= 286,
+		.vburst_start_f3 = 9,	    .vburst_end_f3	= 286, 
+		.vburst_start_f4 = 9,	    .vburst_end_f4	= 285,
+
+
+		/* desired 4.4336180 actual 4.4336180 clock 107.52 */
+		.dda1_inc       =    168,
+		.dda2_inc       =  18557,       .dda2_size      =  20625,
+		.dda3_inc       =      0,       .dda3_size      =      0,
+		.sc_reset   = TV_SC_RESET_EVERY_8,
+		.pal_burst  = true,
+
+		.composite_levels = &pal_n_levels_composite,
+		.composite_color = &pal_n_csc_composite,
+		.svideo_levels  = &pal_n_levels_svideo,
+		.svideo_color = &pal_n_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		/* 625 Lines, 50 Fields, 15.625KHz line, Sub-Carrier 4.434MHz */
+		.name	    = "PAL",
+		.clock		= 107520,	
+		.refresh	= 25000,
+		.oversample	= TV_OVERSAMPLE_8X,
+		.component_only = 0,
+
+		.hsync_end	= 64,		    .hblank_end		= 128,
+		.hblank_start	= 844,	    .htotal		= 863,
+
+		.progressive	= false,    .trilevel_sync = false,
+
+		.vsync_start_f1	= 5,	    .vsync_start_f2	= 6,
+		.vsync_len	= 5,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 0,
+		.veq_start_f2	= 1,	    .veq_len		= 15,
+
+		.vi_end_f1	= 24,		    .vi_end_f2		= 25,
+		.nbr_end	= 286,
+
+		.burst_ena	= true,
+		.hburst_start	= 73,		    .hburst_len		= 32,
+		.vburst_start_f1 = 8,		    .vburst_end_f1	= 285,
+		.vburst_start_f2 = 8,		    .vburst_end_f2	= 286,
+		.vburst_start_f3 = 9,		    .vburst_end_f3	= 286, 
+		.vburst_start_f4 = 9,		    .vburst_end_f4	= 285,
+
+		/* desired 4.4336180 actual 4.4336180 clock 107.52 */
+		.dda1_inc       =    168,
+		.dda2_inc       =  18557,       .dda2_size      =  20625,
+		.dda3_inc       =      0,       .dda3_size      =      0,
+		.sc_reset   = TV_SC_RESET_EVERY_8,
+		.pal_burst  = true,
+
+		.composite_levels = &pal_levels_composite,
+		.composite_color = &pal_csc_composite,
+		.svideo_levels  = &pal_levels_svideo,
+		.svideo_color = &pal_csc_svideo,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "480p@59.94Hz",
+		.clock 	= 107520,	
+		.refresh	= 59940,
+		.oversample     = TV_OVERSAMPLE_4X,
+		.component_only = 1,
+
+		.hsync_end      = 64,               .hblank_end         = 122,
+		.hblank_start   = 842,              .htotal             = 857,
+
+		.progressive    = true,.trilevel_sync = false,
+
+		.vsync_start_f1 = 12,               .vsync_start_f2     = 12,
+		.vsync_len      = 12,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 44,               .vi_end_f2          = 44,
+		.nbr_end        = 496,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "480p@60Hz",
+		.clock 	= 107520,	
+		.refresh	= 60000,
+		.oversample     = TV_OVERSAMPLE_4X,
+		.component_only = 1,
+
+		.hsync_end      = 64,               .hblank_end         = 122,
+		.hblank_start   = 842,              .htotal             = 856,
+
+		.progressive    = true,.trilevel_sync = false,
+
+		.vsync_start_f1 = 12,               .vsync_start_f2     = 12,
+		.vsync_len      = 12,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 44,               .vi_end_f2          = 44,
+		.nbr_end        = 496,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "576p",
+		.clock 	= 107520,	
+		.refresh	= 50000,
+		.oversample     = TV_OVERSAMPLE_4X,
+		.component_only = 1,
+
+		.hsync_end      = 64,               .hblank_end         = 139,
+		.hblank_start   = 859,              .htotal             = 863,
+
+		.progressive    = true,		.trilevel_sync = false,
+
+		.vsync_start_f1 = 10,               .vsync_start_f2     = 10,
+		.vsync_len      = 10,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 48,               .vi_end_f2          = 48,
+		.nbr_end        = 575,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "720p@60Hz",
+		.clock		= 148800,	
+		.refresh	= 60000,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 80,               .hblank_end         = 300,
+		.hblank_start   = 1580,             .htotal             = 1649,
+
+		.progressive    = true, 	    .trilevel_sync = true,
+
+		.vsync_start_f1 = 10,               .vsync_start_f2     = 10,
+		.vsync_len      = 10,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 29,               .vi_end_f2          = 29,
+		.nbr_end        = 719,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "720p@59.94Hz",
+		.clock		= 148800,	
+		.refresh	= 59940,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 80,               .hblank_end         = 300,
+		.hblank_start   = 1580,             .htotal             = 1651,
+
+		.progressive    = true, 	    .trilevel_sync = true,
+
+		.vsync_start_f1 = 10,               .vsync_start_f2     = 10,
+		.vsync_len      = 10,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 29,               .vi_end_f2          = 29,
+		.nbr_end        = 719,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "720p@50Hz",
+		.clock		= 148800,	
+		.refresh	= 50000,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 80,               .hblank_end         = 300,
+		.hblank_start   = 1580,             .htotal             = 1979,
+
+		.progressive    = true, 	        .trilevel_sync = true,
+
+		.vsync_start_f1 = 10,               .vsync_start_f2     = 10,
+		.vsync_len      = 10,
+
+		.veq_ena        = false,
+
+		.vi_end_f1      = 29,               .vi_end_f2          = 29,
+		.nbr_end        = 719,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+		.max_srcw = 800
+	},
+	{
+		.name       = "1080i@50Hz",
+		.clock		= 148800,	
+		.refresh	= 25000,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 88,               .hblank_end         = 235,
+		.hblank_start   = 2155,             .htotal             = 2639,
+
+		.progressive    = false, 	    .trilevel_sync = true,
+
+		.vsync_start_f1 = 4,              .vsync_start_f2     = 5,
+		.vsync_len      = 10,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 4,
+		.veq_start_f2   = 4,	    .veq_len		= 10,
+
+
+		.vi_end_f1      = 21,           .vi_end_f2          = 22,
+		.nbr_end        = 539,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "1080i@60Hz",
+		.clock		= 148800,	
+		.refresh	= 30000,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 88,               .hblank_end         = 235,
+		.hblank_start   = 2155,             .htotal             = 2199,
+
+		.progressive    = false, 	    .trilevel_sync = true,
+
+		.vsync_start_f1 = 4,               .vsync_start_f2     = 5,
+		.vsync_len      = 10,
+
+		.veq_ena	= true,		    .veq_start_f1    	= 4,
+		.veq_start_f2	= 4,		    .veq_len		= 10,
+
+
+		.vi_end_f1      = 21,               .vi_end_f2          = 22,
+		.nbr_end        = 539,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+	{
+		.name       = "1080i@59.94Hz",
+		.clock		= 148800,	
+		.refresh	= 29970,
+		.oversample     = TV_OVERSAMPLE_2X,
+		.component_only = 1,
+
+		.hsync_end      = 88,               .hblank_end         = 235,
+		.hblank_start   = 2155,             .htotal             = 2200,
+
+		.progressive    = false, 	    .trilevel_sync = true,
+
+		.vsync_start_f1 = 4,            .vsync_start_f2    = 5,
+		.vsync_len      = 10,
+
+		.veq_ena	= true,		    .veq_start_f1	= 4,
+		.veq_start_f2 = 4,	    	    .veq_len = 10,
+
+
+		.vi_end_f1      = 21,           .vi_end_f2         	= 22,
+		.nbr_end        = 539,
+
+		.burst_ena      = false,
+
+		.filter_table = filter_table,
+	},
+};
+
+#define NUM_TV_MODES sizeof(tv_modes) / sizeof (tv_modes[0])
+
+static void
+intel_tv_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	switch(mode) {
+	case DPMSModeOn:
+		I915_WRITE(TV_CTL, I915_READ(TV_CTL) | TV_ENC_ENABLE);
+		break;
+	case DPMSModeStandby:
+	case DPMSModeSuspend:
+	case DPMSModeOff:
+		I915_WRITE(TV_CTL, I915_READ(TV_CTL) & ~TV_ENC_ENABLE);
+		break;
+	}
+}
+
+static void
+intel_tv_save(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	int i;
+
+	tv_priv->save_TV_H_CTL_1 = I915_READ(TV_H_CTL_1);
+	tv_priv->save_TV_H_CTL_2 = I915_READ(TV_H_CTL_2);
+	tv_priv->save_TV_H_CTL_3 = I915_READ(TV_H_CTL_3);
+	tv_priv->save_TV_V_CTL_1 = I915_READ(TV_V_CTL_1);
+	tv_priv->save_TV_V_CTL_2 = I915_READ(TV_V_CTL_2);
+	tv_priv->save_TV_V_CTL_3 = I915_READ(TV_V_CTL_3);
+	tv_priv->save_TV_V_CTL_4 = I915_READ(TV_V_CTL_4);
+	tv_priv->save_TV_V_CTL_5 = I915_READ(TV_V_CTL_5);
+	tv_priv->save_TV_V_CTL_6 = I915_READ(TV_V_CTL_6);
+	tv_priv->save_TV_V_CTL_7 = I915_READ(TV_V_CTL_7);
+	tv_priv->save_TV_SC_CTL_1 = I915_READ(TV_SC_CTL_1);
+	tv_priv->save_TV_SC_CTL_2 = I915_READ(TV_SC_CTL_2);
+	tv_priv->save_TV_SC_CTL_3 = I915_READ(TV_SC_CTL_3);
+
+	tv_priv->save_TV_CSC_Y = I915_READ(TV_CSC_Y);
+	tv_priv->save_TV_CSC_Y2 = I915_READ(TV_CSC_Y2);
+	tv_priv->save_TV_CSC_U = I915_READ(TV_CSC_U);
+	tv_priv->save_TV_CSC_U2 = I915_READ(TV_CSC_U2);
+	tv_priv->save_TV_CSC_V = I915_READ(TV_CSC_V);
+	tv_priv->save_TV_CSC_V2 = I915_READ(TV_CSC_V2);
+	tv_priv->save_TV_CLR_KNOBS = I915_READ(TV_CLR_KNOBS);
+	tv_priv->save_TV_CLR_LEVEL = I915_READ(TV_CLR_LEVEL);
+	tv_priv->save_TV_WIN_POS = I915_READ(TV_WIN_POS);
+	tv_priv->save_TV_WIN_SIZE = I915_READ(TV_WIN_SIZE);
+	tv_priv->save_TV_FILTER_CTL_1 = I915_READ(TV_FILTER_CTL_1);
+	tv_priv->save_TV_FILTER_CTL_2 = I915_READ(TV_FILTER_CTL_2);
+	tv_priv->save_TV_FILTER_CTL_3 = I915_READ(TV_FILTER_CTL_3);
+
+	for (i = 0; i < 60; i++)
+		tv_priv->save_TV_H_LUMA[i] = I915_READ(TV_H_LUMA_0 + (i <<2));
+	for (i = 0; i < 60; i++)
+		tv_priv->save_TV_H_CHROMA[i] = I915_READ(TV_H_CHROMA_0 + (i <<2));
+	for (i = 0; i < 43; i++)
+		tv_priv->save_TV_V_LUMA[i] = I915_READ(TV_V_LUMA_0 + (i <<2));
+	for (i = 0; i < 43; i++)
+		tv_priv->save_TV_V_CHROMA[i] = I915_READ(TV_V_CHROMA_0 + (i <<2));
+
+	tv_priv->save_TV_DAC = I915_READ(TV_DAC);
+	tv_priv->save_TV_CTL = I915_READ(TV_CTL);
+}
+
+static void
+intel_tv_restore(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc;
+	int i;
+
+	/* FIXME: No CRTC? */
+	if (!crtc)
+		return;
+
+	intel_crtc = crtc->driver_private;
+	I915_WRITE(TV_H_CTL_1, tv_priv->save_TV_H_CTL_1);
+	I915_WRITE(TV_H_CTL_2, tv_priv->save_TV_H_CTL_2);
+	I915_WRITE(TV_H_CTL_3, tv_priv->save_TV_H_CTL_3);
+	I915_WRITE(TV_V_CTL_1, tv_priv->save_TV_V_CTL_1);
+	I915_WRITE(TV_V_CTL_2, tv_priv->save_TV_V_CTL_2);
+	I915_WRITE(TV_V_CTL_3, tv_priv->save_TV_V_CTL_3);
+	I915_WRITE(TV_V_CTL_4, tv_priv->save_TV_V_CTL_4);
+	I915_WRITE(TV_V_CTL_5, tv_priv->save_TV_V_CTL_5);
+	I915_WRITE(TV_V_CTL_6, tv_priv->save_TV_V_CTL_6);
+	I915_WRITE(TV_V_CTL_7, tv_priv->save_TV_V_CTL_7);
+	I915_WRITE(TV_SC_CTL_1, tv_priv->save_TV_SC_CTL_1);
+	I915_WRITE(TV_SC_CTL_2, tv_priv->save_TV_SC_CTL_2);
+	I915_WRITE(TV_SC_CTL_3, tv_priv->save_TV_SC_CTL_3);
+
+	I915_WRITE(TV_CSC_Y, tv_priv->save_TV_CSC_Y);
+	I915_WRITE(TV_CSC_Y2, tv_priv->save_TV_CSC_Y2);
+	I915_WRITE(TV_CSC_U, tv_priv->save_TV_CSC_U);
+	I915_WRITE(TV_CSC_U2, tv_priv->save_TV_CSC_U2);
+	I915_WRITE(TV_CSC_V, tv_priv->save_TV_CSC_V);
+	I915_WRITE(TV_CSC_V2, tv_priv->save_TV_CSC_V2);
+	I915_WRITE(TV_CLR_KNOBS, tv_priv->save_TV_CLR_KNOBS);
+	I915_WRITE(TV_CLR_LEVEL, tv_priv->save_TV_CLR_LEVEL);
+
+	{
+		int pipeconf_reg = (intel_crtc->pipe == 0) ?
+			PIPEACONF : PIPEBCONF;
+		int dspcntr_reg = (intel_crtc->plane == 0) ?
+			DSPACNTR : DSPBCNTR;
+		int pipeconf = I915_READ(pipeconf_reg);
+		int dspcntr = I915_READ(dspcntr_reg);
+		int dspbase_reg = (intel_crtc->plane == 0) ?
+			DSPABASE : DSPBBASE;
+		/* Pipe must be off here */
+		I915_WRITE(dspcntr_reg, dspcntr & ~DISPLAY_PLANE_ENABLE);
+		/* Flush the plane changes */
+		I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+
+		if (!IS_I9XX(dev)) {
+			/* Wait for vblank for the disable to take effect */
+			intel_wait_for_vblank(dev);
+		}
+
+		I915_WRITE(pipeconf_reg, pipeconf & ~PIPEACONF_ENABLE);
+		/* Wait for vblank for the disable to take effect. */
+		intel_wait_for_vblank(dev);
+
+		/* Filter ctl must be set before TV_WIN_SIZE */
+		I915_WRITE(TV_FILTER_CTL_1, tv_priv->save_TV_FILTER_CTL_1);
+		I915_WRITE(TV_FILTER_CTL_2, tv_priv->save_TV_FILTER_CTL_2);
+		I915_WRITE(TV_FILTER_CTL_3, tv_priv->save_TV_FILTER_CTL_3);
+		I915_WRITE(TV_WIN_POS, tv_priv->save_TV_WIN_POS);
+		I915_WRITE(TV_WIN_SIZE, tv_priv->save_TV_WIN_SIZE);
+		I915_WRITE(pipeconf_reg, pipeconf);
+		I915_WRITE(dspcntr_reg, dspcntr);
+		/* Flush the plane changes */
+		I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+	}
+
+	for (i = 0; i < 60; i++)
+		I915_WRITE(TV_H_LUMA_0 + (i <<2), tv_priv->save_TV_H_LUMA[i]);
+	for (i = 0; i < 60; i++)
+		I915_WRITE(TV_H_CHROMA_0 + (i <<2), tv_priv->save_TV_H_CHROMA[i]);
+	for (i = 0; i < 43; i++)
+		I915_WRITE(TV_V_LUMA_0 + (i <<2), tv_priv->save_TV_V_LUMA[i]);
+	for (i = 0; i < 43; i++)
+		I915_WRITE(TV_V_CHROMA_0 + (i <<2), tv_priv->save_TV_V_CHROMA[i]);
+
+	I915_WRITE(TV_DAC, tv_priv->save_TV_DAC);
+	I915_WRITE(TV_CTL, tv_priv->save_TV_CTL);
+}
+
+static const struct tv_mode *
+intel_tv_mode_lookup (char *tv_format)
+{
+	int i;
+    
+	for (i = 0; i < sizeof(tv_modes) / sizeof (tv_modes[0]); i++) {
+		const struct tv_mode *tv_mode = &tv_modes[i];
+
+		if (!strcmp(tv_format, tv_mode->name))
+			return tv_mode;
+	}
+	return NULL;
+}
+
+static const struct tv_mode *
+intel_tv_mode_find (struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+
+	return intel_tv_mode_lookup(tv_priv->tv_format);
+}
+
+static enum drm_mode_status
+intel_tv_mode_valid(struct drm_output *output, struct drm_display_mode *mode)
+{
+	const struct tv_mode *tv_mode = intel_tv_mode_find(output);
+
+	/* Ensure TV refresh is close to desired refresh */
+	if (tv_mode && abs(tv_mode->refresh - drm_mode_vrefresh(mode)) < 1)
+		return MODE_OK;
+	return MODE_CLOCK_RANGE;
+}
+
+
+static bool
+intel_tv_mode_fixup(struct drm_output *output, struct drm_display_mode *mode,
+		    struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_mode_config *drm_config = &dev->mode_config;
+	const struct tv_mode *tv_mode = intel_tv_mode_find (output);
+	struct drm_output *other_output;
+
+	if (!tv_mode)
+		return false;
+    
+	/* FIXME: lock output list */
+	list_for_each_entry(other_output, &drm_config->output_list, head) {
+		if (other_output != output &&
+		    other_output->crtc == output->crtc)
+			return false;
+	}
+
+	adjusted_mode->clock = tv_mode->clock;
+	return true;
+}
+
+static void
+intel_tv_mode_set(struct drm_output *output, struct drm_display_mode *mode,
+		  struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	const struct tv_mode *tv_mode = intel_tv_mode_find(output);
+	u32 tv_ctl;
+	u32 hctl1, hctl2, hctl3;
+	u32 vctl1, vctl2, vctl3, vctl4, vctl5, vctl6, vctl7;
+	u32 scctl1, scctl2, scctl3;
+	int i, j;
+	const struct video_levels *video_levels;
+	const struct color_conversion *color_conversion;
+	bool burst_ena;
+    
+	if (!tv_mode)
+		return;	/* can't happen (mode_prepare prevents this) */
+    
+	tv_ctl = 0;
+
+	switch (tv_priv->type) {
+	default:
+	case TV_TYPE_UNKNOWN:
+	case TV_TYPE_COMPOSITE:
+		tv_ctl |= TV_ENC_OUTPUT_COMPOSITE;
+		video_levels = tv_mode->composite_levels;
+		color_conversion = tv_mode->composite_color;
+		burst_ena = tv_mode->burst_ena;
+		break;
+	case TV_TYPE_COMPONENT:
+		tv_ctl |= TV_ENC_OUTPUT_COMPONENT;
+		video_levels = &component_levels;
+		if (tv_mode->burst_ena)
+			color_conversion = &sdtv_csc_yprpb;
+		else
+			color_conversion = &hdtv_csc_yprpb;
+		burst_ena = false;
+		break;
+	case TV_TYPE_SVIDEO:
+		tv_ctl |= TV_ENC_OUTPUT_SVIDEO;
+		video_levels = tv_mode->svideo_levels;
+		color_conversion = tv_mode->svideo_color;
+		burst_ena = tv_mode->burst_ena;
+		break;
+	}
+	hctl1 = (tv_mode->hsync_end << TV_HSYNC_END_SHIFT) |
+		(tv_mode->htotal << TV_HTOTAL_SHIFT);
+
+	hctl2 = (tv_mode->hburst_start << 16) |
+		(tv_mode->hburst_len << TV_HBURST_LEN_SHIFT);
+
+	if (burst_ena)
+		hctl2 |= TV_BURST_ENA;
+
+	hctl3 = (tv_mode->hblank_start << TV_HBLANK_START_SHIFT) |
+		(tv_mode->hblank_end << TV_HBLANK_END_SHIFT);
+
+	vctl1 = (tv_mode->nbr_end << TV_NBR_END_SHIFT) |
+		(tv_mode->vi_end_f1 << TV_VI_END_F1_SHIFT) |
+		(tv_mode->vi_end_f2 << TV_VI_END_F2_SHIFT);
+
+	vctl2 = (tv_mode->vsync_len << TV_VSYNC_LEN_SHIFT) |
+		(tv_mode->vsync_start_f1 << TV_VSYNC_START_F1_SHIFT) |
+		(tv_mode->vsync_start_f2 << TV_VSYNC_START_F2_SHIFT);
+
+	vctl3 = (tv_mode->veq_len << TV_VEQ_LEN_SHIFT) |
+		(tv_mode->veq_start_f1 << TV_VEQ_START_F1_SHIFT) |
+		(tv_mode->veq_start_f2 << TV_VEQ_START_F2_SHIFT);
+
+	if (tv_mode->veq_ena)
+		vctl3 |= TV_EQUAL_ENA;
+
+	vctl4 = (tv_mode->vburst_start_f1 << TV_VBURST_START_F1_SHIFT) |
+		(tv_mode->vburst_end_f1 << TV_VBURST_END_F1_SHIFT);
+
+	vctl5 = (tv_mode->vburst_start_f2 << TV_VBURST_START_F2_SHIFT) |
+		(tv_mode->vburst_end_f2 << TV_VBURST_END_F2_SHIFT);
+
+	vctl6 = (tv_mode->vburst_start_f3 << TV_VBURST_START_F3_SHIFT) |
+		(tv_mode->vburst_end_f3 << TV_VBURST_END_F3_SHIFT);
+
+	vctl7 = (tv_mode->vburst_start_f4 << TV_VBURST_START_F4_SHIFT) |
+		(tv_mode->vburst_end_f4 << TV_VBURST_END_F4_SHIFT);
+
+	if (intel_crtc->pipe == 1)
+		tv_ctl |= TV_ENC_PIPEB_SELECT;
+	tv_ctl |= tv_mode->oversample;
+
+	if (tv_mode->progressive)
+		tv_ctl |= TV_PROGRESSIVE;
+	if (tv_mode->trilevel_sync)
+		tv_ctl |= TV_TRILEVEL_SYNC;
+	if (tv_mode->pal_burst)
+		tv_ctl |= TV_PAL_BURST;
+	scctl1 = 0;
+	if (tv_mode->dda1_inc)
+		scctl1 |= TV_SC_DDA1_EN;
+
+	if (tv_mode->dda2_inc)
+		scctl1 |= TV_SC_DDA2_EN;
+
+	if (tv_mode->dda3_inc)
+		scctl1 |= TV_SC_DDA3_EN;
+
+	scctl1 |= tv_mode->sc_reset;
+	scctl1 |= video_levels->burst << TV_BURST_LEVEL_SHIFT;
+	scctl1 |= tv_mode->dda1_inc << TV_SCDDA1_INC_SHIFT;
+
+	scctl2 = tv_mode->dda2_size << TV_SCDDA2_SIZE_SHIFT |
+		tv_mode->dda2_inc << TV_SCDDA2_INC_SHIFT;
+
+	scctl3 = tv_mode->dda3_size << TV_SCDDA3_SIZE_SHIFT |
+		tv_mode->dda3_inc << TV_SCDDA3_INC_SHIFT;
+
+	/* Enable two fixes for the chips that need them. */
+	if (dev->pci_device < 0x2772)
+		tv_ctl |= TV_ENC_C0_FIX | TV_ENC_SDP_FIX;
+
+	I915_WRITE(TV_H_CTL_1, hctl1);
+	I915_WRITE(TV_H_CTL_2, hctl2);
+	I915_WRITE(TV_H_CTL_3, hctl3);
+	I915_WRITE(TV_V_CTL_1, vctl1);
+	I915_WRITE(TV_V_CTL_2, vctl2);
+	I915_WRITE(TV_V_CTL_3, vctl3);
+	I915_WRITE(TV_V_CTL_4, vctl4);
+	I915_WRITE(TV_V_CTL_5, vctl5);
+	I915_WRITE(TV_V_CTL_6, vctl6);
+	I915_WRITE(TV_V_CTL_7, vctl7);
+	I915_WRITE(TV_SC_CTL_1, scctl1);
+	I915_WRITE(TV_SC_CTL_2, scctl2);
+	I915_WRITE(TV_SC_CTL_3, scctl3);
+
+	I915_WRITE(TV_CSC_Y, (color_conversion->ry << 16) |
+		   color_conversion->gy);
+	I915_WRITE(TV_CSC_Y2,(color_conversion->by << 16) |
+		   color_conversion->ay);
+	I915_WRITE(TV_CSC_U, (color_conversion->ru << 16) |
+		   color_conversion->gu);
+	I915_WRITE(TV_CSC_U2, (color_conversion->bu << 16) |
+		   color_conversion->au);
+	I915_WRITE(TV_CSC_V, (color_conversion->rv << 16) |
+		   color_conversion->gv);
+	I915_WRITE(TV_CSC_V2, (color_conversion->bv << 16) |
+		   color_conversion->av);
+
+	I915_WRITE(TV_CLR_KNOBS, 0x00606000);
+	I915_WRITE(TV_CLR_LEVEL, ((video_levels->black << TV_BLACK_LEVEL_SHIFT) |
+			      (video_levels->blank << TV_BLANK_LEVEL_SHIFT)));
+	{
+		int pipeconf_reg = (intel_crtc->pipe == 0) ?
+			PIPEACONF : PIPEBCONF;
+		int dspcntr_reg = (intel_crtc->plane == 0) ?
+			DSPACNTR : DSPBCNTR;
+		int pipeconf = I915_READ(pipeconf_reg);
+		int dspcntr = I915_READ(dspcntr_reg);
+		int dspbase_reg = (intel_crtc->plane == 0) ?
+			DSPABASE : DSPBBASE;
+		int xpos = 0x0, ypos = 0x0;
+		unsigned int xsize, ysize;
+		/* Pipe must be off here */
+		I915_WRITE(dspcntr_reg, dspcntr & ~DISPLAY_PLANE_ENABLE);
+		/* Flush the plane changes */
+		I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+
+		/* Wait for vblank for the disable to take effect */
+		if (!IS_I9XX(dev))
+			intel_wait_for_vblank(dev);
+
+		I915_WRITE(pipeconf_reg, pipeconf & ~PIPEACONF_ENABLE);
+		/* Wait for vblank for the disable to take effect. */
+		intel_wait_for_vblank(dev);
+
+		/* Filter ctl must be set before TV_WIN_SIZE */
+		I915_WRITE(TV_FILTER_CTL_1, TV_AUTO_SCALE); 
+		xsize = tv_mode->hblank_start - tv_mode->hblank_end;
+		if (tv_mode->progressive)
+			ysize = tv_mode->nbr_end + 1;
+		else
+			ysize = 2*tv_mode->nbr_end + 1;
+
+		xpos += tv_priv->margin[TV_MARGIN_LEFT];
+		ypos += tv_priv->margin[TV_MARGIN_TOP];
+		xsize -= (tv_priv->margin[TV_MARGIN_LEFT] + 
+			  tv_priv->margin[TV_MARGIN_RIGHT]);
+		ysize -= (tv_priv->margin[TV_MARGIN_TOP] + 
+			  tv_priv->margin[TV_MARGIN_BOTTOM]);
+		I915_WRITE(TV_WIN_POS, (xpos<<16)|ypos);
+		I915_WRITE(TV_WIN_SIZE, (xsize<<16)|ysize);
+
+		I915_WRITE(pipeconf_reg, pipeconf);
+		I915_WRITE(dspcntr_reg, dspcntr);
+		/* Flush the plane changes */
+		I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+	} 	
+
+	j = 0;
+	for (i = 0; i < 60; i++)
+		I915_WRITE(TV_H_LUMA_0 + (i<<2), tv_mode->filter_table[j++]);
+	for (i = 0; i < 60; i++)
+		I915_WRITE(TV_H_CHROMA_0 + (i<<2), tv_mode->filter_table[j++]);
+	for (i = 0; i < 43; i++)
+		I915_WRITE(TV_V_LUMA_0 + (i<<2), tv_mode->filter_table[j++]);
+	for (i = 0; i < 43; i++)
+		I915_WRITE(TV_V_CHROMA_0 + (i<<2), tv_mode->filter_table[j++]);
+	I915_WRITE(TV_DAC, 0);
+	I915_WRITE(TV_CTL, tv_ctl);
+}
+
+static const struct drm_display_mode reported_modes[] = {
+	{
+		.name = "NTSC 480i",
+		.clock = 107520,
+		.hdisplay = 1280,
+		.hsync_start = 1368,
+		.hsync_end = 1496,
+		.htotal = 1712,
+
+		.vdisplay = 1024,
+		.vsync_start = 1027,
+		.vsync_end = 1034,
+		.vtotal = 1104,
+		.type = DRM_MODE_TYPE_DRIVER,
+	},
+};
+
+/**
+ * Detects TV presence by checking for load.
+ *
+ * Requires that the current pipe's DPLL is active.
+
+ * \return true if TV is connected.
+ * \return false if TV is disconnected.
+ */
+static int
+intel_tv_detect_type (struct drm_crtc *crtc, struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	u32 tv_ctl, save_tv_ctl;
+	u32 tv_dac, save_tv_dac;
+	int type = TV_TYPE_UNKNOWN;
+
+	tv_dac = I915_READ(TV_DAC);
+	/*
+	 * Detect TV by polling)
+	 */
+	if (intel_output->load_detect_temp) {
+		/* TV not currently running, prod it with destructive detect */
+		save_tv_dac = tv_dac;
+		tv_ctl = I915_READ(TV_CTL);
+		save_tv_ctl = tv_ctl;
+		tv_ctl &= ~TV_ENC_ENABLE;
+		tv_ctl &= ~TV_TEST_MODE_MASK;
+		tv_ctl |= TV_TEST_MODE_MONITOR_DETECT;
+		tv_dac &= ~TVDAC_SENSE_MASK;
+		tv_dac |= (TVDAC_STATE_CHG_EN |
+			   TVDAC_A_SENSE_CTL |
+			   TVDAC_B_SENSE_CTL |
+			   TVDAC_C_SENSE_CTL |
+			   DAC_CTL_OVERRIDE |
+			   DAC_A_0_7_V |
+			   DAC_B_0_7_V |
+			   DAC_C_0_7_V);
+		I915_WRITE(TV_CTL, tv_ctl);
+		I915_WRITE(TV_DAC, tv_dac);
+		intel_wait_for_vblank(dev);
+		tv_dac = I915_READ(TV_DAC);
+		I915_WRITE(TV_DAC, save_tv_dac);
+		I915_WRITE(TV_CTL, save_tv_ctl);
+	}
+	/*
+	 *  A B C
+	 *  0 1 1 Composite
+	 *  1 0 X svideo
+	 *  0 0 0 Component
+	 */
+	if ((tv_dac & TVDAC_SENSE_MASK) == (TVDAC_B_SENSE | TVDAC_C_SENSE)) {
+		DRM_DEBUG("Detected Composite TV connection\n");
+		type = TV_TYPE_COMPOSITE;
+	} else if ((tv_dac & (TVDAC_A_SENSE|TVDAC_B_SENSE)) == TVDAC_A_SENSE) {
+		DRM_DEBUG("Detected S-Video TV connection\n");
+		type = TV_TYPE_SVIDEO;
+	} else if ((tv_dac & TVDAC_SENSE_MASK) == 0) {
+		DRM_DEBUG("Detected Component TV connection\n");
+		type = TV_TYPE_COMPONENT;
+	} else {
+		DRM_DEBUG("No TV connection detected\n");
+		type = TV_TYPE_NONE;
+	}
+
+	return type;
+}
+
+static int
+intel_tv_format_configure_property (struct drm_output *output);
+
+/**
+ * Detect the TV connection.
+ *
+ * Currently this always returns OUTPUT_STATUS_UNKNOWN, as we need to be sure
+ * we have a pipe programmed in order to probe the TV.
+ */
+static enum drm_output_status
+intel_tv_detect(struct drm_output *output)
+{
+	struct drm_crtc *crtc;
+	struct drm_display_mode mode;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	int dpms_mode;
+	int type = tv_priv->type;
+
+	mode = reported_modes[0];
+	drm_mode_set_crtcinfo(&mode, CRTC_INTERLACE_HALVE_V);
+#if 0
+	/* FIXME: pipe allocation for load detection */
+	crtc = i830GetLoadDetectPipe (output, &mode, &dpms_mode);
+	if (crtc) {
+		type = intel_tv_detect_type(crtc, output);
+		i830ReleaseLoadDetectPipe (output, dpms_mode);
+	}
+#endif
+	if (type != tv_priv->type) {
+		tv_priv->type = type;
+		intel_tv_format_configure_property (output);
+	}
+	
+	switch (type) {
+	case TV_TYPE_NONE:
+		return output_status_disconnected;
+	case TV_TYPE_UNKNOWN:
+		return output_status_unknown;
+	default:
+		return output_status_connected;
+	}
+}
+
+static struct input_res {
+	char *name;
+	int w, h;	
+} input_res_table[] = 
+{
+	{"640x480", 640, 480},
+	{"800x600", 800, 600},
+	{"1024x768", 1024, 768},
+	{"1280x1024", 1280, 1024},
+	{"848x480", 848, 480},
+	{"1280x720", 1280, 720},
+	{"1920x1080", 1920, 1080},
+};
+
+/**
+ * Stub get_modes function.
+ *
+ * This should probably return a set of fixed modes, unless we can figure out
+ * how to probe modes off of TV connections.
+ */
+
+static int
+intel_tv_get_modes(struct drm_output *output)
+{
+	struct drm_display_mode *mode_ptr;
+	const struct tv_mode *tv_mode = intel_tv_mode_find(output);
+	int j;
+
+	for (j = 0; j < sizeof(input_res_table) / sizeof(input_res_table[0]);
+	     j++) {
+		struct input_res *input = &input_res_table[j];
+		unsigned int hactive_s = input->w;
+		unsigned int vactive_s = input->h;
+	
+		if (tv_mode->max_srcw && input->w > tv_mode->max_srcw)
+			continue;
+
+		if (input->w > 1024 && (!tv_mode->progressive 
+					&& !tv_mode->component_only))
+			continue;
+
+		mode_ptr = drm_calloc(1, sizeof(struct drm_display_mode),
+				      DRM_MEM_DRIVER);
+		strncpy(mode_ptr->name, input->name, DRM_DISPLAY_MODE_LEN);
+
+		mode_ptr->hdisplay = hactive_s;
+		mode_ptr->hsync_start = hactive_s + 1;
+		mode_ptr->hsync_end = hactive_s + 64;
+		if (mode_ptr->hsync_end <= mode_ptr->hsync_start)
+			mode_ptr->hsync_end = mode_ptr->hsync_start + 1;
+		mode_ptr->htotal = hactive_s + 96;
+
+		mode_ptr->vdisplay = vactive_s;
+		mode_ptr->vsync_start = vactive_s + 1;
+		mode_ptr->vsync_end = vactive_s + 32;
+		if (mode_ptr->vsync_end <= mode_ptr->vsync_start)
+			mode_ptr->vsync_end = mode_ptr->vsync_start  + 1;
+		mode_ptr->vtotal = vactive_s + 33;
+
+		mode_ptr->clock = (int) (tv_mode->refresh * 
+					 mode_ptr->vtotal * 
+					 mode_ptr->htotal / 1000) / 1000;
+	
+		mode_ptr->type = DRM_MODE_TYPE_DRIVER;
+		drm_mode_probed_add(output, mode_ptr);
+	} 
+
+	return 0;
+}
+
+static void
+intel_tv_destroy (struct drm_output *output)
+{
+	if (output->driver_private)
+		drm_free(output->driver_private, sizeof(struct intel_tv_priv),
+			 DRM_MEM_DRIVER);
+}
+
+static bool
+intel_tv_format_set_property(struct drm_output *output,
+			     struct drm_property *prop, uint64_t val)
+{
+#if 0
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	const struct tv_mode *tv_mode =
+		intel_tv_mode_lookup(tv_priv->tv_format);
+	int			    err;
+
+	if (!tv_mode)
+		tv_mode = &tv_modes[0];
+	err = RRChangeOutputProperty (output->randr_output, tv_format_atom,
+				      XA_ATOM, 32, PropModeReplace, 1,
+				      &tv_format_name_atoms[tv_mode - tv_modes],
+				      false, true);
+	return err == Success;
+#endif
+	return 0;
+}
+
+    
+/**
+ * Configure the TV_FORMAT property to list only supported formats
+ *
+ * Unless the connector is component, list only the formats supported by
+ * svideo and composite
+ */
+
+static int
+intel_tv_format_configure_property(struct drm_output *output)
+{
+#if 0
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	Atom		    current_atoms[NUM_TV_MODES];
+	int			    num_atoms = 0;
+	int			    i;
+    
+	if (!output->randr_output)
+		return Success;
+
+	for (i = 0; i < NUM_TV_MODES; i++)
+		if (!tv_modes[i].component_only ||
+		    tv_priv->type == TV_TYPE_COMPONENT)
+			current_atoms[num_atoms++] = tv_format_name_atoms[i];
+    
+	return RRConfigureOutputProperty(output->randr_output, tv_format_atom,
+					 true, false, false, 
+					 num_atoms, (INT32 *) current_atoms);
+#endif
+	return 0;
+}
+
+static void
+intel_tv_create_resources(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_tv_priv *tv_priv = intel_output->dev_priv;
+	int i, err;
+
+#if 0
+	/* Set up the tv_format property, which takes effect on mode set
+	 * and accepts strings that match exactly
+	 */
+	tv_format_atom = MakeAtom(TV_FORMAT_NAME, sizeof(TV_FORMAT_NAME) - 1,
+				  true);
+
+	for (i = 0; i < NUM_TV_MODES; i++)
+		tv_format_name_atoms[i] = MakeAtom (tv_modes[i].name,
+						    strlen (tv_modes[i].name),
+						    true);
+
+	err = intel_tv_format_configure_property (output);
+
+	if (err != 0) {
+		xf86DrvMsg(dev->scrnIndex, X_ERROR,
+			   "RRConfigureOutputProperty error, %d\n", err);
+	}
+
+	/* Set the current value of the tv_format property */
+	if (!intel_tv_format_set_property (output))
+		xf86DrvMsg(dev->scrnIndex, X_ERROR,
+			   "RRChangeOutputProperty error, %d\n", err);
+
+	for (i = 0; i < 4; i++)
+	{
+		INT32	range[2];
+		margin_atoms[i] = MakeAtom(margin_names[i], strlen (margin_names[i]),
+					   true);
+
+		range[0] = 0;
+		range[1] = 100;
+		err = RRConfigureOutputProperty(output->randr_output, margin_atoms[i],
+						true, true, false, 2, range);
+    
+		if (err != 0)
+			xf86DrvMsg(dev->scrnIndex, X_ERROR,
+				   "RRConfigureOutputProperty error, %d\n", err);
+
+		err = RRChangeOutputProperty(output->randr_output, margin_atoms[i],
+					     XA_INTEGER, 32, PropModeReplace,
+					     1, &tv_priv->margin[i],
+					     false, true);
+		if (err != 0)
+			xf86DrvMsg(dev->scrnIndex, X_ERROR,
+				   "RRChangeOutputProperty error, %d\n", err);
+	}
+#endif
+}
+
+static bool
+intel_tv_set_property(struct drm_output *output, struct drm_property *property,
+		      uint64_t val)
+{
+	struct drm_device *dev = output->dev;
+	int ret = 0;
+    
+	if (property == dev->mode_config.tv_left_margin_property ||
+	    property == dev->mode_config.tv_right_margin_property ||
+	    property == dev->mode_config.tv_top_margin_property ||
+	    property == dev->mode_config.tv_bottom_margin_property) {
+		ret = drm_output_property_set_value(output, property, val);
+	} else {
+		/* TV mode handling here */
+	}
+
+	return ret;
+}
+
+static const struct drm_output_funcs intel_tv_output_funcs = {
+	.dpms = intel_tv_dpms,
+	.save = intel_tv_save,
+	.restore = intel_tv_restore,
+	.mode_valid = intel_tv_mode_valid,
+	.mode_fixup = intel_tv_mode_fixup,
+	.prepare = intel_output_prepare,
+	.mode_set = intel_tv_mode_set,
+	.commit = intel_output_commit,
+	.detect = intel_tv_detect,
+	.get_modes = intel_tv_get_modes,
+	.cleanup = intel_tv_destroy,
+	.set_property = intel_tv_set_property,
+};
+
+void
+intel_tv_init(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_output *output;
+	struct intel_output *intel_output;
+	struct intel_tv_priv *tv_priv;
+	u32 tv_dac_on, tv_dac_off, save_tv_dac;
+
+	/* FIXME: better TV detection and/or quirks */
+#if 0
+	if (tv_priv->quirk_flag & QUIRK_IGNORE_TV)
+		return;
+#endif
+	if ((I915_READ(TV_CTL) & TV_FUSE_STATE_MASK) == TV_FUSE_STATE_DISABLED)
+		return;
+
+	/*
+	 * Sanity check the TV output by checking to see if the
+	 * DAC register holds a value
+	 */
+	save_tv_dac = I915_READ(TV_DAC);
+
+	I915_WRITE(TV_DAC, save_tv_dac | TVDAC_STATE_CHG_EN);
+	tv_dac_on = I915_READ(TV_DAC);
+
+	I915_WRITE(TV_DAC, save_tv_dac & ~TVDAC_STATE_CHG_EN);
+	tv_dac_off = I915_READ(TV_DAC);
+
+	I915_WRITE(TV_DAC, save_tv_dac);
+
+	/*
+	 * If the register does not hold the state change enable
+	 * bit, (either as a 0 or a 1), assume it doesn't really
+	 * exist
+	 */
+	if ((tv_dac_on & TVDAC_STATE_CHG_EN) == 0 || 
+	    (tv_dac_off & TVDAC_STATE_CHG_EN) != 0)
+		return;
+
+	output = drm_output_create(dev, &intel_tv_output_funcs,
+				   DRM_MODE_OUTPUT_TVDAC);
+
+	if (!output)
+		return;
+
+	intel_output = drm_calloc(1, sizeof(struct intel_output) +
+				  sizeof(struct intel_tv_priv), DRM_MEM_DRIVER);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	tv_priv = (struct intel_tv_priv *)(intel_output + 1);
+	intel_output->type = INTEL_OUTPUT_TVOUT;
+	output->possible_crtcs = ((1 << 0) | (1 << 1));
+	output->possible_clones = (1 << INTEL_OUTPUT_TVOUT);
+	intel_output->dev_priv = tv_priv;
+	tv_priv->type = TV_TYPE_UNKNOWN;
+
+	tv_priv->tv_format = NULL;
+    
+	/* BIOS margin values */
+	tv_priv->margin[TV_MARGIN_LEFT] = 54;
+	tv_priv->margin[TV_MARGIN_TOP] = 36;
+	tv_priv->margin[TV_MARGIN_RIGHT] = 46;
+	tv_priv->margin[TV_MARGIN_BOTTOM] = 37;
+    
+	if (!tv_priv->tv_format)
+		tv_priv->tv_format = kstrdup(tv_modes[0].name, GFP_KERNEL);
+    
+	output->driver_private = intel_output;
+	output->interlace_allowed = false;
+	output->doublescan_allowed = false;
+}
diff --git a/drivers/char/drm/nouveau_bo.c b/drivers/char/drm/nouveau_bo.c
new file mode 100644
index 0000000..ab3b23a
--- /dev/null
+++ b/drivers/char/drm/nouveau_bo.c
@@ -0,0 +1,296 @@
+/*
+ * Copyright 2007 Dave Airlied
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+/*
+ * Authors: Dave Airlied <airlied@linux.ie>
+ *	    Ben Skeggs   <darktama@iinet.net.au>
+ *	    Jeremy Kolb  <jkolb@brandeis.edu>
+ */
+
+#include "drmP.h"
+#include "nouveau_drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_dma.h"
+
+static struct drm_ttm_backend *
+nouveau_bo_create_ttm_backend_entry(struct drm_device * dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	switch (dev_priv->gart_info.type) {
+	case NOUVEAU_GART_AGP:
+		return drm_agp_init_ttm(dev);
+	case NOUVEAU_GART_SGDMA:
+		return nouveau_sgdma_init_ttm(dev);
+	default:
+		DRM_ERROR("Unknown GART type %d\n", dev_priv->gart_info.type);
+		break;
+	}
+
+	return NULL;
+}
+
+static int
+nouveau_bo_fence_type(struct drm_buffer_object *bo,
+		      uint32_t *fclass, uint32_t *type)
+{
+	/* When we get called, *fclass is set to the requested fence class */
+
+	if (bo->mem.proposed_flags & (DRM_BO_FLAG_READ | DRM_BO_FLAG_WRITE))
+		*type = 3;
+	else
+		*type = 1;
+	return 0;
+
+}
+
+static int
+nouveau_bo_invalidate_caches(struct drm_device *dev, uint64_t buffer_flags)
+{
+	/* We'll do this from user space. */
+	return 0;
+}
+
+static int
+nouveau_bo_init_mem_type(struct drm_device *dev, uint32_t type,
+			 struct drm_mem_type_manager *man)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	switch (type) {
+	case DRM_BO_MEM_LOCAL:
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+			     _DRM_FLAG_MEMTYPE_CACHED;
+		man->drm_bus_maptype = 0;
+		break;
+	case DRM_BO_MEM_VRAM:
+		man->flags = _DRM_FLAG_MEMTYPE_FIXED |
+			     _DRM_FLAG_MEMTYPE_MAPPABLE |
+			     _DRM_FLAG_NEEDS_IOREMAP;
+		man->io_addr = NULL;
+		man->drm_bus_maptype = _DRM_FRAME_BUFFER;
+		man->io_offset = drm_get_resource_start(dev, 1);
+		man->io_size = drm_get_resource_len(dev, 1);
+		if (man->io_size > nouveau_mem_fb_amount(dev))
+			man->io_size = nouveau_mem_fb_amount(dev);
+		break;
+	case DRM_BO_MEM_PRIV0:
+		/* Unmappable VRAM */
+		man->flags = _DRM_FLAG_MEMTYPE_CMA;
+		man->drm_bus_maptype = 0;
+		break;
+	case DRM_BO_MEM_TT:
+		switch (dev_priv->gart_info.type) {
+		case NOUVEAU_GART_AGP:
+			man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+				     _DRM_FLAG_MEMTYPE_CSELECT |
+				     _DRM_FLAG_NEEDS_IOREMAP;
+			man->drm_bus_maptype = _DRM_AGP;
+			break;
+		case NOUVEAU_GART_SGDMA:
+			man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+				     _DRM_FLAG_MEMTYPE_CSELECT |
+				     _DRM_FLAG_MEMTYPE_CMA;
+			man->drm_bus_maptype = _DRM_SCATTER_GATHER;
+			break;
+		default:
+			DRM_ERROR("Unknown GART type: %d\n",
+				  dev_priv->gart_info.type);
+			return -EINVAL;
+		}
+
+		man->io_offset  = dev_priv->gart_info.aper_base;
+		man->io_size    = dev_priv->gart_info.aper_size;
+		man->io_addr   = NULL;
+		break;
+	default:
+		DRM_ERROR("Unsupported memory type %u\n", (unsigned)type);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static uint64_t
+nouveau_bo_evict_flags(struct drm_buffer_object *bo)
+{
+	switch (bo->mem.mem_type) {
+	case DRM_BO_MEM_LOCAL:
+	case DRM_BO_MEM_TT:
+		return DRM_BO_FLAG_MEM_LOCAL;
+	default:
+		return DRM_BO_FLAG_MEM_TT | DRM_BO_FLAG_CACHED;
+	}
+	return 0;
+}
+
+
+/* GPU-assisted copy using NV_MEMORY_TO_MEMORY_FORMAT, can access
+ * DRM_BO_MEM_{VRAM,PRIV0,TT} directly.
+ */
+static int
+nouveau_bo_move_m2mf(struct drm_buffer_object *bo, int evict, int no_wait,
+		     struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_drm_channel *dchan = &dev_priv->channel;
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	uint32_t srch, dsth, page_count;
+
+	/* Can happen during init/takedown */
+	if (!dchan->chan)
+		return -EINVAL;
+
+	srch = old_mem->mem_type == DRM_BO_MEM_TT ? NvDmaTT : NvDmaFB;
+	dsth = new_mem->mem_type == DRM_BO_MEM_TT ? NvDmaTT : NvDmaFB;
+	if (srch != dchan->m2mf_dma_source || dsth != dchan->m2mf_dma_destin) {
+		dchan->m2mf_dma_source = srch;
+		dchan->m2mf_dma_destin = dsth;
+
+		BEGIN_RING(NvSubM2MF,
+			   NV_MEMORY_TO_MEMORY_FORMAT_SET_DMA_SOURCE, 2);
+		OUT_RING  (dchan->m2mf_dma_source);
+		OUT_RING  (dchan->m2mf_dma_destin);
+	}
+
+	page_count = new_mem->num_pages;
+	while (page_count) {
+		int line_count = (page_count > 2047) ? 2047 : page_count;
+
+		BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);
+		OUT_RING  (old_mem->mm_node->start << PAGE_SHIFT);
+		OUT_RING  (new_mem->mm_node->start << PAGE_SHIFT);
+		OUT_RING  (PAGE_SIZE); /* src_pitch */
+		OUT_RING  (PAGE_SIZE); /* dst_pitch */
+		OUT_RING  (PAGE_SIZE); /* line_length */
+		OUT_RING  (line_count);
+		OUT_RING  ((1<<8)|(1<<0));
+		OUT_RING  (0);
+		BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);
+		OUT_RING  (0);
+
+		page_count -= line_count;
+	}
+
+	return drm_bo_move_accel_cleanup(bo, evict, no_wait, dchan->chan->id,
+					 DRM_FENCE_TYPE_EXE, 0, new_mem);
+}
+
+/* Flip pages into the GART and move if we can. */
+static int
+nouveau_bo_move_flipd(struct drm_buffer_object *bo, int evict, int no_wait,
+		      struct drm_bo_mem_reg *new_mem)
+{
+        struct drm_device *dev = bo->dev;
+        struct drm_bo_mem_reg tmp_mem;
+        int ret;
+
+        tmp_mem = *new_mem;
+        tmp_mem.mm_node = NULL;
+        tmp_mem.proposed_flags = (DRM_BO_FLAG_MEM_TT |
+				  DRM_BO_FLAG_CACHED |
+				  DRM_BO_FLAG_FORCE_CACHING);
+
+        ret = drm_bo_mem_space(bo, &tmp_mem, no_wait);
+        if (ret)
+                return ret;
+
+        ret = drm_ttm_bind(bo->ttm, &tmp_mem);
+        if (ret)
+                goto out_cleanup;
+
+        ret = nouveau_bo_move_m2mf(bo, 1, no_wait, &tmp_mem);
+        if (ret)
+                goto out_cleanup;
+
+        ret = drm_bo_move_ttm(bo, evict, no_wait, new_mem);
+
+out_cleanup:
+        if (tmp_mem.mm_node) {
+                mutex_lock(&dev->struct_mutex);
+                if (tmp_mem.mm_node != bo->pinned_node)
+                        drm_mm_put_block(tmp_mem.mm_node);
+                tmp_mem.mm_node = NULL;
+                mutex_unlock(&dev->struct_mutex);
+        }
+
+        return ret;
+}
+
+static int
+nouveau_bo_move(struct drm_buffer_object *bo, int evict, int no_wait,
+		struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+
+	if (new_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		if (old_mem->mem_type == DRM_BO_MEM_LOCAL)
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+		if (nouveau_bo_move_flipd(bo, evict, no_wait, new_mem))
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	}
+	else
+	if (old_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		if (1 /*nouveau_bo_move_flips(bo, evict, no_wait, new_mem)*/)
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	}
+	else {
+		if (nouveau_bo_move_m2mf(bo, evict, no_wait, new_mem))
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	}
+
+	return 0;
+}
+
+static void
+nouveau_bo_flush_ttm(struct drm_ttm *ttm)
+{
+}
+
+static uint32_t nouveau_mem_prios[]  = {
+	DRM_BO_MEM_PRIV0,
+	DRM_BO_MEM_VRAM,
+	DRM_BO_MEM_TT,
+	DRM_BO_MEM_LOCAL
+};
+static uint32_t nouveau_busy_prios[] = {
+	DRM_BO_MEM_TT,
+	DRM_BO_MEM_PRIV0,
+	DRM_BO_MEM_VRAM,
+	DRM_BO_MEM_LOCAL
+};
+
+struct drm_bo_driver nouveau_bo_driver = {
+	.mem_type_prio = nouveau_mem_prios,
+	.mem_busy_prio = nouveau_busy_prios,
+	.num_mem_type_prio = sizeof(nouveau_mem_prios)/sizeof(uint32_t),
+	.num_mem_busy_prio = sizeof(nouveau_busy_prios)/sizeof(uint32_t),
+	.create_ttm_backend_entry = nouveau_bo_create_ttm_backend_entry,
+	.fence_type = nouveau_bo_fence_type,
+	.invalidate_caches = nouveau_bo_invalidate_caches,
+	.init_mem_type = nouveau_bo_init_mem_type,
+	.evict_flags = nouveau_bo_evict_flags,
+	.move = nouveau_bo_move,
+	.ttm_cache_flush= nouveau_bo_flush_ttm,
+	.command_stream_barrier = NULL
+};
diff --git a/drivers/char/drm/nouveau_dma.c b/drivers/char/drm/nouveau_dma.c
new file mode 100644
index 0000000..e519dc4
--- /dev/null
+++ b/drivers/char/drm/nouveau_dma.c
@@ -0,0 +1,172 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_dma.h"
+
+int
+nouveau_dma_channel_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_drm_channel *dchan = &dev_priv->channel;
+	struct nouveau_gpuobj *gpuobj = NULL;
+	struct mem_block *pushbuf;
+	int grclass, ret, i;
+
+	DRM_DEBUG("\n");
+
+	pushbuf = nouveau_mem_alloc(dev, 0, 0x8000,
+				    NOUVEAU_MEM_FB | NOUVEAU_MEM_MAPPED,
+				    (struct drm_file *)-2);
+	if (!pushbuf) {
+		DRM_ERROR("Failed to allocate DMA push buffer\n");
+		return -ENOMEM;
+	}
+
+	/* Allocate channel */
+	ret = nouveau_fifo_alloc(dev, &dchan->chan, (struct drm_file *)-2,
+				 pushbuf, NvDmaFB, NvDmaTT);
+	if (ret) {
+		DRM_ERROR("Error allocating GPU channel: %d\n", ret);
+		return ret;
+	}
+	DRM_DEBUG("Using FIFO channel %d\n", dchan->chan->id);
+
+	/* Map push buffer */
+	drm_core_ioremap(dchan->chan->pushbuf_mem->map, dev);
+	if (!dchan->chan->pushbuf_mem->map->handle) {
+		DRM_ERROR("Failed to ioremap push buffer\n");
+		return -EINVAL;
+	}
+	dchan->pushbuf = (void*)dchan->chan->pushbuf_mem->map->handle;
+
+	/* Initialise DMA vars */
+	dchan->max  = (dchan->chan->pushbuf_mem->size >> 2) - 2;
+	dchan->put  = dchan->chan->pushbuf_base >> 2;
+	dchan->cur  = dchan->put;
+	dchan->free = dchan->max - dchan->cur;
+
+	/* Insert NOPS for NOUVEAU_DMA_SKIPS */
+	dchan->free -= NOUVEAU_DMA_SKIPS;
+	dchan->push_free = NOUVEAU_DMA_SKIPS;
+	for (i=0; i < NOUVEAU_DMA_SKIPS; i++)
+		OUT_RING(0);
+
+	/* NV_MEMORY_TO_MEMORY_FORMAT requires a notifier */
+	if ((ret = nouveau_notifier_alloc(dchan->chan, NvNotify0, 1,
+					  &dchan->notify0_offset))) {
+		DRM_ERROR("Error allocating NvNotify0: %d\n", ret);
+		return ret;
+	}
+
+	/* We use NV_MEMORY_TO_MEMORY_FORMAT for buffer moves */
+	if (dev_priv->card_type < NV_50) grclass = NV_MEMORY_TO_MEMORY_FORMAT;
+	else                             grclass = NV50_MEMORY_TO_MEMORY_FORMAT;
+	if ((ret = nouveau_gpuobj_gr_new(dchan->chan, grclass, &gpuobj))) {
+		DRM_ERROR("Error creating NvM2MF: %d\n", ret);
+		return ret;
+	}
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, dchan->chan, NvM2MF,
+					  gpuobj, NULL))) {
+		DRM_ERROR("Error referencing NvM2MF: %d\n", ret);
+		return ret;
+	}
+	dchan->m2mf_dma_source = NvDmaFB;
+	dchan->m2mf_dma_destin = NvDmaFB;
+
+	BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_NAME, 1);
+	OUT_RING  (NvM2MF);
+	BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_SET_DMA_NOTIFY, 1);
+	OUT_RING  (NvNotify0);
+	BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_SET_DMA_SOURCE, 2);
+	OUT_RING  (dchan->m2mf_dma_source);
+	OUT_RING  (dchan->m2mf_dma_destin);
+	FIRE_RING();
+
+	return 0;
+}
+
+void
+nouveau_dma_channel_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_drm_channel *dchan = &dev_priv->channel;
+
+	DRM_DEBUG("\n");
+
+	if (dchan->chan) {
+		nouveau_fifo_free(dchan->chan);
+		dchan->chan = NULL;
+	}
+}
+
+#define READ_GET() ((NV_READ(dchan->chan->get) -                               \
+		    dchan->chan->pushbuf_base) >> 2)
+#define WRITE_PUT(val) do {                                                    \
+	NV_WRITE(dchan->chan->put,                                             \
+		 ((val) << 2) + dchan->chan->pushbuf_base);                    \
+} while(0)
+
+int
+nouveau_dma_wait(struct drm_device *dev, int size)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_drm_channel *dchan = &dev_priv->channel;
+	uint32_t get;
+
+	while (dchan->free < size) {
+		get = READ_GET();
+
+		if (dchan->put >= get) {
+			dchan->free = dchan->max - dchan->cur;
+
+			if (dchan->free < size) {
+				dchan->push_free = 1;
+				OUT_RING(0x20000000|dchan->chan->pushbuf_base);
+				if (get <= NOUVEAU_DMA_SKIPS) {
+					/*corner case - will be idle*/
+					if (dchan->put <= NOUVEAU_DMA_SKIPS)
+						WRITE_PUT(NOUVEAU_DMA_SKIPS + 1);
+
+					do {
+						get = READ_GET();
+					} while (get <= NOUVEAU_DMA_SKIPS);
+				}
+
+				WRITE_PUT(NOUVEAU_DMA_SKIPS);
+				dchan->cur  = dchan->put = NOUVEAU_DMA_SKIPS;
+				dchan->free = get - (NOUVEAU_DMA_SKIPS + 1);
+			}
+		} else {
+			dchan->free = get - dchan->cur - 1;
+		}
+	}
+
+	return 0;
+}
diff --git a/drivers/char/drm/nouveau_dma.h b/drivers/char/drm/nouveau_dma.h
new file mode 100644
index 0000000..ce3c58c
--- /dev/null
+++ b/drivers/char/drm/nouveau_dma.h
@@ -0,0 +1,96 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef __NOUVEAU_DMA_H__
+#define __NOUVEAU_DMA_H__
+
+typedef enum {
+	NvSubM2MF	= 0,
+} nouveau_subchannel_id_t;
+
+typedef enum {
+	NvM2MF		= 0x80039001,
+	NvDmaFB		= 0x8003d001,
+	NvDmaTT		= 0x8003d002,
+	NvNotify0       = 0x8003d003
+} nouveau_object_handle_t;
+
+#define NV_MEMORY_TO_MEMORY_FORMAT                                    0x00000039
+#define NV_MEMORY_TO_MEMORY_FORMAT_NAME                               0x00000000
+#define NV_MEMORY_TO_MEMORY_FORMAT_SET_REF                            0x00000050
+#define NV_MEMORY_TO_MEMORY_FORMAT_NOP                                0x00000100
+#define NV_MEMORY_TO_MEMORY_FORMAT_NOTIFY                             0x00000104
+#define NV_MEMORY_TO_MEMORY_FORMAT_NOTIFY_STYLE_WRITE                 0x00000000
+#define NV_MEMORY_TO_MEMORY_FORMAT_NOTIFY_STYLE_WRITE_LE_AWAKEN       0x00000001
+#define NV_MEMORY_TO_MEMORY_FORMAT_SET_DMA_NOTIFY                     0x00000180
+#define NV_MEMORY_TO_MEMORY_FORMAT_SET_DMA_SOURCE                     0x00000184
+#define NV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN                          0x0000030c
+
+#define NV50_MEMORY_TO_MEMORY_FORMAT                                  0x00005039
+#define NV50_MEMORY_TO_MEMORY_FORMAT_UNK200                           0x00000200
+#define NV50_MEMORY_TO_MEMORY_FORMAT_UNK21C                           0x0000021c
+#define NV50_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN_HIGH                   0x00000238
+#define NV50_MEMORY_TO_MEMORY_FORMAT_OFFSET_OUT_HIGH                  0x0000023c
+
+#define BEGIN_RING(subc, mthd, cnt) do {                                       \
+	int push_size = (cnt) + 1;                                             \
+	if (dchan->push_free) {                                                \
+		DRM_ERROR("prior packet incomplete: %d\n", dchan->push_free);  \
+		break;                                                         \
+	}                                                                      \
+	if (dchan->free < push_size) {                                         \
+		if (nouveau_dma_wait(dev, push_size)) {                        \
+			DRM_ERROR("FIFO timeout\n");                           \
+			break;                                                 \
+		}                                                              \
+	}                                                                      \
+	dchan->free -= push_size;                                              \
+	dchan->push_free = push_size;                                          \
+	OUT_RING(((cnt)<<18) | ((subc)<<15) | mthd);                           \
+} while(0)
+
+#define OUT_RING(data) do {                                                    \
+	if (dchan->push_free == 0) {                                           \
+		DRM_ERROR("no space left in packet\n");                        \
+		break;                                                         \
+	}                                                                      \
+	dchan->pushbuf[dchan->cur++] = (data);                                 \
+	dchan->push_free--;                                                    \
+} while(0)
+
+#define FIRE_RING() do {                                                       \
+	if (dchan->push_free) {                                                \
+		DRM_ERROR("packet incomplete: %d\n", dchan->push_free);        \
+		break;                                                         \
+	}                                                                      \
+	if (dchan->cur != dchan->put) {                                        \
+		DRM_MEMORYBARRIER();                                           \
+		dchan->put = dchan->cur;                                       \
+		NV_WRITE(dchan->chan->put, dchan->put << 2);                   \
+	}                                                                      \
+} while(0)
+
+#endif
diff --git a/drivers/char/drm/nouveau_drm.h b/drivers/char/drm/nouveau_drm.h
new file mode 100644
index 0000000..cf76205
--- /dev/null
+++ b/drivers/char/drm/nouveau_drm.h
@@ -0,0 +1,170 @@
+/*
+ * Copyright 2005 Stephane Marchesin.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __NOUVEAU_DRM_H__
+#define __NOUVEAU_DRM_H__
+
+#define NOUVEAU_DRM_HEADER_PATCHLEVEL 10
+
+struct drm_nouveau_channel_alloc {
+	uint32_t     fb_ctxdma_handle;
+	uint32_t     tt_ctxdma_handle;
+
+	int          channel;
+	uint32_t     put_base;
+	/* FIFO control regs */
+	drm_handle_t ctrl;
+	int          ctrl_size;
+	/* DMA command buffer */
+	drm_handle_t cmdbuf;
+	int          cmdbuf_size;
+	/* Notifier memory */
+	drm_handle_t notifier;
+	int          notifier_size;
+};
+
+struct drm_nouveau_channel_free {
+	int channel;
+};
+
+struct drm_nouveau_grobj_alloc {
+	int      channel;
+	uint32_t handle;
+	int      class;
+};
+
+#define NOUVEAU_MEM_ACCESS_RO	1
+#define NOUVEAU_MEM_ACCESS_WO	2
+#define NOUVEAU_MEM_ACCESS_RW	3
+struct drm_nouveau_notifierobj_alloc {
+	int      channel;
+	uint32_t handle;
+	int      count;
+
+	uint32_t offset;
+};
+
+struct drm_nouveau_gpuobj_free {
+	int      channel;
+	uint32_t handle;
+};
+
+/* This is needed to avoid a race condition.
+ * Otherwise you may be writing in the fetch area.
+ * Is this large enough, as it's only 32 bytes, and the maximum fetch size is 256 bytes?
+ */
+#define NOUVEAU_DMA_SKIPS 8
+
+#define NOUVEAU_MEM_FB			0x00000001
+#define NOUVEAU_MEM_AGP			0x00000002
+#define NOUVEAU_MEM_FB_ACCEPTABLE	0x00000004
+#define NOUVEAU_MEM_AGP_ACCEPTABLE	0x00000008
+#define NOUVEAU_MEM_PCI			0x00000010
+#define NOUVEAU_MEM_PCI_ACCEPTABLE	0x00000020
+#define NOUVEAU_MEM_PINNED		0x00000040
+#define NOUVEAU_MEM_USER_BACKED		0x00000080
+#define NOUVEAU_MEM_MAPPED		0x00000100
+#define NOUVEAU_MEM_INSTANCE		0x00000200 /* internal */
+#define NOUVEAU_MEM_NOTIFIER            0x00000400 /* internal */
+#define NOUVEAU_MEM_NOVM		0x00000800 /* internal */
+#define NOUVEAU_MEM_INTERNAL (NOUVEAU_MEM_INSTANCE | \
+			      NOUVEAU_MEM_NOTIFIER | \
+			      NOUVEAU_MEM_NOVM)
+
+struct drm_nouveau_mem_alloc {
+	int flags;
+	int alignment;
+	uint64_t size;	// in bytes
+	uint64_t offset;
+	drm_handle_t map_handle;
+};
+
+struct drm_nouveau_mem_free {
+	uint64_t offset;
+	int flags;
+};
+
+/* FIXME : maybe unify {GET,SET}PARAMs */
+#define NOUVEAU_GETPARAM_PCI_VENDOR      3
+#define NOUVEAU_GETPARAM_PCI_DEVICE      4
+#define NOUVEAU_GETPARAM_BUS_TYPE        5
+#define NOUVEAU_GETPARAM_FB_PHYSICAL     6
+#define NOUVEAU_GETPARAM_AGP_PHYSICAL    7
+#define NOUVEAU_GETPARAM_FB_SIZE         8
+#define NOUVEAU_GETPARAM_AGP_SIZE        9
+#define NOUVEAU_GETPARAM_PCI_PHYSICAL    10
+#define NOUVEAU_GETPARAM_CHIPSET_ID      11
+struct drm_nouveau_getparam {
+	uint64_t param;
+	uint64_t value;
+};
+
+#define NOUVEAU_SETPARAM_CMDBUF_LOCATION 1
+#define NOUVEAU_SETPARAM_CMDBUF_SIZE     2
+struct drm_nouveau_setparam {
+	uint64_t param;
+	uint64_t value;
+};
+
+enum nouveau_card_type {
+	NV_UNKNOWN =0,
+	NV_04      =4,
+	NV_05      =5,
+	NV_10      =10,
+	NV_11      =11,
+	NV_17      =17,
+	NV_20      =20,
+	NV_30      =30,
+	NV_40      =40,
+	NV_44      =44,
+	NV_50      =50,
+	NV_LAST    =0xffff,
+};
+
+enum nouveau_bus_type {
+	NV_AGP     =0,
+	NV_PCI     =1,
+	NV_PCIE    =2,
+};
+
+#define NOUVEAU_MAX_SAREA_CLIPRECTS 16
+
+struct drm_nouveau_sarea {
+	/* the cliprects */
+	struct drm_clip_rect boxes[NOUVEAU_MAX_SAREA_CLIPRECTS];
+	unsigned int nbox;
+};
+
+#define DRM_NOUVEAU_CARD_INIT          0x00
+#define DRM_NOUVEAU_GETPARAM           0x01
+#define DRM_NOUVEAU_SETPARAM           0x02
+#define DRM_NOUVEAU_CHANNEL_ALLOC      0x03
+#define DRM_NOUVEAU_CHANNEL_FREE       0x04
+#define DRM_NOUVEAU_GROBJ_ALLOC        0x05
+#define DRM_NOUVEAU_NOTIFIEROBJ_ALLOC  0x06
+#define DRM_NOUVEAU_GPUOBJ_FREE        0x07
+#define DRM_NOUVEAU_MEM_ALLOC          0x08
+#define DRM_NOUVEAU_MEM_FREE           0x09
+
+#endif /* __NOUVEAU_DRM_H__ */
diff --git a/drivers/char/drm/nouveau_drv.c b/drivers/char/drm/nouveau_drv.c
new file mode 100644
index 0000000..d26cde4
--- /dev/null
+++ b/drivers/char/drm/nouveau_drv.c
@@ -0,0 +1,108 @@
+/*
+ * Copyright 2005 Stephane Marchesin.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+#include "drm_pciids.h"
+
+static struct pci_device_id pciidlist[] = {
+	{
+		PCI_DEVICE(PCI_VENDOR_ID_NVIDIA, PCI_ANY_ID),
+		.class = PCI_BASE_CLASS_DISPLAY << 16,
+		.class_mask  = 0xff << 16,
+	},
+	{
+		PCI_DEVICE(PCI_VENDOR_ID_NVIDIA_SGS, PCI_ANY_ID),
+		.class = PCI_BASE_CLASS_DISPLAY << 16,
+		.class_mask  = 0xff << 16,
+	}
+};
+
+extern struct drm_ioctl_desc nouveau_ioctls[];
+extern int nouveau_max_ioctl;
+
+static struct drm_driver driver = {
+	.driver_features =
+		DRIVER_USE_AGP | DRIVER_PCI_DMA | DRIVER_SG |
+		DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED,
+	.load = nouveau_load,
+	.firstopen = nouveau_firstopen,
+	.lastclose = nouveau_lastclose,
+	.unload = nouveau_unload,
+	.preclose = nouveau_preclose,
+	.irq_preinstall = nouveau_irq_preinstall,
+	.irq_postinstall = nouveau_irq_postinstall,
+	.irq_uninstall = nouveau_irq_uninstall,
+	.irq_handler = nouveau_irq_handler,
+	.reclaim_buffers = drm_core_reclaim_buffers,
+	.get_map_ofs = drm_core_get_map_ofs,
+	.get_reg_ofs = drm_core_get_reg_ofs,
+	.ioctls = nouveau_ioctls,
+	.fops = {
+		.owner = THIS_MODULE,
+		.open = drm_open,
+		.release = drm_release,
+		.ioctl = drm_ioctl,
+		.mmap = drm_mmap,
+		.poll = drm_poll,
+		.fasync = drm_fasync,
+#ifdef CONFIG_COMPAT
+		.compat_ioctl = nouveau_compat_ioctl,
+#endif
+	},
+	.pci_driver = {
+		.name = DRIVER_NAME,
+		.id_table = pciidlist,
+	},
+
+	.bo_driver = &nouveau_bo_driver,
+	.fence_driver = &nouveau_fence_driver,
+
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = DRIVER_MAJOR,
+	.minor = DRIVER_MINOR,
+	.patchlevel = DRIVER_PATCHLEVEL,
+};
+
+static int __init nouveau_init(void)
+{
+	driver.num_ioctls = nouveau_max_ioctl;
+	return drm_init(&driver);
+}
+
+static void __exit nouveau_exit(void)
+{
+	drm_exit(&driver);
+}
+
+module_init(nouveau_init);
+module_exit(nouveau_exit);
+
+MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_LICENSE("GPL and additional rights");
diff --git a/drivers/char/drm/nouveau_drv.h b/drivers/char/drm/nouveau_drv.h
new file mode 100644
index 0000000..a51e552
--- /dev/null
+++ b/drivers/char/drm/nouveau_drv.h
@@ -0,0 +1,606 @@
+/*
+ * Copyright 2005 Stephane Marchesin.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __NOUVEAU_DRV_H__
+#define __NOUVEAU_DRV_H__
+
+#define DRIVER_AUTHOR		"Stephane Marchesin"
+#define DRIVER_EMAIL		"dri-devel@lists.sourceforge.net"
+
+#define DRIVER_NAME		"nouveau"
+#define DRIVER_DESC		"nVidia Riva/TNT/GeForce"
+#define DRIVER_DATE		"20060213"
+
+#define DRIVER_MAJOR		0
+#define DRIVER_MINOR		0
+#define DRIVER_PATCHLEVEL	10
+
+#define NOUVEAU_FAMILY   0x0000FFFF
+#define NOUVEAU_FLAGS    0xFFFF0000
+
+#include "nouveau_drm.h"
+#include "nouveau_reg.h"
+
+struct mem_block {
+	struct mem_block *next;
+	struct mem_block *prev;
+	uint64_t start;
+	uint64_t size;
+	struct drm_file *file_priv; /* NULL: free, -1: heap, other: real files */
+	int flags;
+	drm_local_map_t *map;
+	drm_handle_t map_handle;
+};
+
+enum nouveau_flags {
+	NV_NFORCE   =0x10000000,
+	NV_NFORCE2  =0x20000000
+};
+
+#define NVOBJ_ENGINE_SW		0
+#define NVOBJ_ENGINE_GR		1
+#define NVOBJ_ENGINE_INT	0xdeadbeef
+
+#define NVOBJ_FLAG_ALLOW_NO_REFS	(1 << 0)
+#define NVOBJ_FLAG_ZERO_ALLOC		(1 << 1)
+#define NVOBJ_FLAG_ZERO_FREE		(1 << 2)
+#define NVOBJ_FLAG_FAKE			(1 << 3)
+struct nouveau_gpuobj {
+	struct list_head list;
+
+	int im_channel;
+	struct mem_block *im_pramin;
+	struct mem_block *im_backing;
+	int im_bound;
+
+	uint32_t flags;
+	int refcount;
+
+	uint32_t engine;
+	uint32_t class;
+
+	void (*dtor)(struct drm_device *, struct nouveau_gpuobj *);
+	void *priv;
+};
+
+struct nouveau_gpuobj_ref {
+	struct list_head list;
+
+	struct nouveau_gpuobj *gpuobj;
+	uint32_t instance;
+
+	int channel;
+	int handle;
+};
+
+struct nouveau_channel
+{
+	struct drm_device *dev;
+	int id;
+
+	/* owner of this fifo */
+	struct drm_file *file_priv;
+	/* mapping of the fifo itself */
+	drm_local_map_t *map;
+	/* mapping of the regs controling the fifo */
+	drm_local_map_t *regs;
+
+	/* Fencing */
+	uint32_t next_sequence;
+
+	/* DMA push buffer */
+	struct nouveau_gpuobj_ref *pushbuf;
+	struct mem_block          *pushbuf_mem;
+	uint32_t                   pushbuf_base;
+
+	/* FIFO user control regs */
+	uint32_t user, user_size;
+	uint32_t put;
+	uint32_t get;
+	uint32_t ref_cnt;
+
+	/* Notifier memory */
+	struct mem_block *notifier_block;
+	struct mem_block *notifier_heap;
+	drm_local_map_t  *notifier_map;
+
+	/* PFIFO context */
+	struct nouveau_gpuobj_ref *ramfc;
+
+	/* PGRAPH context */
+	/* XXX may be merge 2 pointers as private data ??? */
+	struct nouveau_gpuobj_ref *ramin_grctx;
+	void *pgraph_ctx;
+
+	/* NV50 VM */
+	struct nouveau_gpuobj     *vm_pd;
+	struct nouveau_gpuobj_ref *vm_gart_pt;
+	struct nouveau_gpuobj_ref *vm_vram_pt;
+
+	/* Objects */
+	struct nouveau_gpuobj_ref *ramin; /* Private instmem */
+	struct mem_block          *ramin_heap; /* Private PRAMIN heap */
+	struct nouveau_gpuobj_ref *ramht; /* Hash table */
+	struct list_head           ramht_refs; /* Objects referenced by RAMHT */
+};
+
+struct nouveau_drm_channel {
+	struct nouveau_channel *chan;
+
+	/* DMA state */
+	int max, put, cur, free;
+	int push_free;
+	volatile uint32_t *pushbuf;
+
+	/* Notifiers */
+	uint32_t notify0_offset;
+
+	/* Buffer moves */
+	uint32_t m2mf_dma_source;
+	uint32_t m2mf_dma_destin;
+};
+
+struct nouveau_config {
+	struct {
+		int location;
+		int size;
+	} cmdbuf;
+};
+
+struct nouveau_instmem_engine {
+	void	*priv;
+
+	int	(*init)(struct drm_device *dev);
+	void	(*takedown)(struct drm_device *dev);
+
+	int	(*populate)(struct drm_device *, struct nouveau_gpuobj *,
+			    uint32_t *size);
+	void	(*clear)(struct drm_device *, struct nouveau_gpuobj *);
+	int	(*bind)(struct drm_device *, struct nouveau_gpuobj *);
+	int	(*unbind)(struct drm_device *, struct nouveau_gpuobj *);
+};
+
+struct nouveau_mc_engine {
+	int  (*init)(struct drm_device *dev);
+	void (*takedown)(struct drm_device *dev);
+};
+
+struct nouveau_timer_engine {
+	int      (*init)(struct drm_device *dev);
+	void     (*takedown)(struct drm_device *dev);
+	uint64_t (*read)(struct drm_device *dev);
+};
+
+struct nouveau_fb_engine {
+	int  (*init)(struct drm_device *dev);
+	void (*takedown)(struct drm_device *dev);
+};
+
+struct nouveau_fifo_engine {
+	void *priv;
+
+	int  channels;
+
+	int  (*init)(struct drm_device *);
+	void (*takedown)(struct drm_device *);
+
+	int  (*channel_id)(struct drm_device *);
+
+	int  (*create_context)(struct nouveau_channel *);
+	void (*destroy_context)(struct nouveau_channel *);
+	int  (*load_context)(struct nouveau_channel *);
+	int  (*save_context)(struct nouveau_channel *);
+};
+
+struct nouveau_pgraph_engine {
+	int  (*init)(struct drm_device *);
+	void (*takedown)(struct drm_device *);
+
+	int  (*create_context)(struct nouveau_channel *);
+	void (*destroy_context)(struct nouveau_channel *);
+	int  (*load_context)(struct nouveau_channel *);
+	int  (*save_context)(struct nouveau_channel *);
+};
+
+struct nouveau_engine {
+	struct nouveau_instmem_engine instmem;
+	struct nouveau_mc_engine      mc;
+	struct nouveau_timer_engine   timer;
+	struct nouveau_fb_engine      fb;
+	struct nouveau_pgraph_engine  graph;
+	struct nouveau_fifo_engine    fifo;
+};
+
+#define NOUVEAU_MAX_CHANNEL_NR 128
+struct drm_nouveau_private {
+	enum {
+		NOUVEAU_CARD_INIT_DOWN,
+		NOUVEAU_CARD_INIT_DONE,
+		NOUVEAU_CARD_INIT_FAILED
+	} init_state;
+
+	int ttm;
+
+	/* the card type, takes NV_* as values */
+	int card_type;
+	/* exact chipset, derived from NV_PMC_BOOT_0 */
+	int chipset;
+	int flags;
+
+	drm_local_map_t *mmio;
+	drm_local_map_t *fb;
+	drm_local_map_t *ramin; /* NV40 onwards */
+
+	int fifo_alloc_count;
+	struct nouveau_channel *fifos[NOUVEAU_MAX_CHANNEL_NR];
+
+	struct nouveau_engine Engine;
+	struct nouveau_drm_channel channel;
+
+	/* RAMIN configuration, RAMFC, RAMHT and RAMRO offsets */
+	struct nouveau_gpuobj *ramht;
+	uint32_t ramin_rsvd_vram;
+	uint32_t ramht_offset;
+	uint32_t ramht_size;
+	uint32_t ramht_bits;
+	uint32_t ramfc_offset;
+	uint32_t ramfc_size;
+	uint32_t ramro_offset;
+	uint32_t ramro_size;
+
+	/* base physical adresses */
+	uint64_t fb_phys;
+	uint64_t fb_available_size;
+
+	struct {
+		enum {
+			NOUVEAU_GART_NONE = 0,
+			NOUVEAU_GART_AGP,
+			NOUVEAU_GART_SGDMA
+		} type;
+		uint64_t aper_base;
+		uint64_t aper_size;
+
+		struct nouveau_gpuobj *sg_ctxdma;
+		struct page *sg_dummy_page;
+		dma_addr_t sg_dummy_bus;
+
+		/* nottm hack */
+		struct drm_ttm_backend *sg_be;
+		unsigned long sg_handle;
+	} gart_info;
+
+	/* G8x global VRAM page table */
+	struct nouveau_gpuobj *vm_vram_pt;
+
+	/* the mtrr covering the FB */
+	int fb_mtrr;
+
+	struct mem_block *agp_heap;
+	struct mem_block *fb_heap;
+	struct mem_block *fb_nomap_heap;
+	struct mem_block *ramin_heap;
+	struct mem_block *pci_heap;
+
+        /* context table pointed to be NV_PGRAPH_CHANNEL_CTX_TABLE (0x400780) */
+        uint32_t ctx_table_size;
+	struct nouveau_gpuobj_ref *ctx_table;
+
+	struct nouveau_config config;
+
+	struct list_head gpuobj_list;
+};
+
+#define NOUVEAU_CHECK_INITIALISED_WITH_RETURN do {         \
+	struct drm_nouveau_private *nv = dev->dev_private; \
+	if (nv->init_state != NOUVEAU_CARD_INIT_DONE) {    \
+		DRM_ERROR("called without init\n");        \
+		return -EINVAL;                            \
+	}                                                  \
+} while(0)
+
+#define NOUVEAU_GET_USER_CHANNEL_WITH_RETURN(id,cl,ch) do {  \
+	struct drm_nouveau_private *nv = dev->dev_private;   \
+	if (!nouveau_fifo_owner(dev, (cl), (id))) {          \
+		DRM_ERROR("pid %d doesn't own channel %d\n", \
+			  DRM_CURRENTPID, (id));             \
+		return -EPERM;                               \
+	}                                                    \
+	(ch) = nv->fifos[(id)];                              \
+} while(0)
+
+/* nouveau_state.c */
+extern void nouveau_preclose(struct drm_device *dev, struct drm_file *);
+extern int  nouveau_load(struct drm_device *, unsigned long flags);
+extern int  nouveau_firstopen(struct drm_device *);
+extern void nouveau_lastclose(struct drm_device *);
+extern int  nouveau_unload(struct drm_device *);
+extern int  nouveau_ioctl_getparam(struct drm_device *, void *data,
+				   struct drm_file *);
+extern int  nouveau_ioctl_setparam(struct drm_device *, void *data,
+				   struct drm_file *);
+extern void nouveau_wait_for_idle(struct drm_device *);
+extern int  nouveau_card_init(struct drm_device *);
+extern int  nouveau_ioctl_card_init(struct drm_device *, void *data,
+				    struct drm_file *);
+
+/* nouveau_mem.c */
+extern int  nouveau_mem_init_heap(struct mem_block **, uint64_t start,
+				 uint64_t size);
+extern struct mem_block *nouveau_mem_alloc_block(struct mem_block *,
+						 uint64_t size, int align2,
+						 struct drm_file *);
+extern void nouveau_mem_takedown(struct mem_block **heap);
+extern void nouveau_mem_free_block(struct mem_block *);
+extern uint64_t nouveau_mem_fb_amount(struct drm_device *);
+extern void nouveau_mem_release(struct drm_file *, struct mem_block *heap);
+extern int  nouveau_ioctl_mem_alloc(struct drm_device *, void *data,
+				    struct drm_file *);
+extern int  nouveau_ioctl_mem_free(struct drm_device *, void *data,
+				   struct drm_file *);
+extern struct mem_block* nouveau_mem_alloc(struct drm_device *,
+					   int alignment, uint64_t size,
+					   int flags, struct drm_file *);
+extern void nouveau_mem_free(struct drm_device *dev, struct mem_block*);
+extern int  nouveau_mem_init(struct drm_device *);
+extern int  nouveau_mem_init_ttm(struct drm_device *);
+extern void nouveau_mem_close(struct drm_device *);
+
+/* nouveau_notifier.c */
+extern int  nouveau_notifier_init_channel(struct nouveau_channel *);
+extern void nouveau_notifier_takedown_channel(struct nouveau_channel *);
+extern int  nouveau_notifier_alloc(struct nouveau_channel *, uint32_t handle,
+				   int cout, uint32_t *offset);
+extern int  nouveau_ioctl_notifier_alloc(struct drm_device *, void *data,
+					 struct drm_file *);
+extern int  nouveau_ioctl_notifier_free(struct drm_device *, void *data,
+					struct drm_file *);
+
+/* nouveau_fifo.c */
+extern int  nouveau_fifo_init(struct drm_device *);
+extern int  nouveau_fifo_ctx_size(struct drm_device *);
+extern void nouveau_fifo_cleanup(struct drm_device *, struct drm_file *);
+extern int  nouveau_fifo_owner(struct drm_device *, struct drm_file *,
+			       int channel);
+extern int  nouveau_fifo_alloc(struct drm_device *dev,
+			       struct nouveau_channel **chan,
+			       struct drm_file *file_priv,
+			       struct mem_block *pushbuf,
+			       uint32_t fb_ctxdma, uint32_t tt_ctxdma);
+extern void nouveau_fifo_free(struct nouveau_channel *);
+
+/* nouveau_object.c */
+extern int  nouveau_gpuobj_early_init(struct drm_device *);
+extern int  nouveau_gpuobj_init(struct drm_device *);
+extern void nouveau_gpuobj_takedown(struct drm_device *);
+extern void nouveau_gpuobj_late_takedown(struct drm_device *);
+extern int nouveau_gpuobj_channel_init(struct nouveau_channel *,
+				       uint32_t vram_h, uint32_t tt_h);
+extern void nouveau_gpuobj_channel_takedown(struct nouveau_channel *);
+extern int nouveau_gpuobj_new(struct drm_device *, struct nouveau_channel *,
+			      int size, int align, uint32_t flags,
+			      struct nouveau_gpuobj **);
+extern int nouveau_gpuobj_del(struct drm_device *, struct nouveau_gpuobj **);
+extern int nouveau_gpuobj_ref_add(struct drm_device *, struct nouveau_channel *,
+				  uint32_t handle, struct nouveau_gpuobj *,
+				  struct nouveau_gpuobj_ref **);
+extern int nouveau_gpuobj_ref_del(struct drm_device *,
+				  struct nouveau_gpuobj_ref **);
+extern int nouveau_gpuobj_ref_find(struct nouveau_channel *, uint32_t handle,
+				   struct nouveau_gpuobj_ref **ref_ret);
+extern int nouveau_gpuobj_new_ref(struct drm_device *,
+				  struct nouveau_channel *alloc_chan,
+				  struct nouveau_channel *ref_chan,
+				  uint32_t handle, int size, int align,
+				  uint32_t flags, struct nouveau_gpuobj_ref **);
+extern int nouveau_gpuobj_new_fake(struct drm_device *,
+				   uint32_t p_offset, uint32_t b_offset,
+				   uint32_t size, uint32_t flags,
+				   struct nouveau_gpuobj **,
+				   struct nouveau_gpuobj_ref**);
+extern int nouveau_gpuobj_dma_new(struct nouveau_channel *, int class,
+				  uint64_t offset, uint64_t size, int access,
+				  int target, struct nouveau_gpuobj **);
+extern int nouveau_gpuobj_gart_dma_new(struct nouveau_channel *,
+				       uint64_t offset, uint64_t size,
+				       int access, struct nouveau_gpuobj **,
+				       uint32_t *o_ret);
+extern int nouveau_gpuobj_gr_new(struct nouveau_channel *, int class,
+				 struct nouveau_gpuobj **);
+extern int nouveau_ioctl_grobj_alloc(struct drm_device *, void *data,
+				     struct drm_file *);
+extern int nouveau_ioctl_gpuobj_free(struct drm_device *, void *data,
+				     struct drm_file *);
+
+/* nouveau_irq.c */
+extern irqreturn_t nouveau_irq_handler(DRM_IRQ_ARGS);
+extern void        nouveau_irq_preinstall(struct drm_device *);
+extern int         nouveau_irq_postinstall(struct drm_device *);
+extern void        nouveau_irq_uninstall(struct drm_device *);
+
+/* nouveau_sgdma.c */
+extern int nouveau_sgdma_init(struct drm_device *);
+extern void nouveau_sgdma_takedown(struct drm_device *);
+extern int nouveau_sgdma_get_page(struct drm_device *, uint32_t offset,
+				  uint32_t *page);
+extern struct drm_ttm_backend *nouveau_sgdma_init_ttm(struct drm_device *);
+extern int nouveau_sgdma_nottm_hack_init(struct drm_device *);
+extern void nouveau_sgdma_nottm_hack_takedown(struct drm_device *);
+
+/* nouveau_dma.c */
+extern int  nouveau_dma_channel_init(struct drm_device *);
+extern void nouveau_dma_channel_takedown(struct drm_device *);
+extern int  nouveau_dma_wait(struct drm_device *, int size);
+
+/* nv04_fb.c */
+extern int  nv04_fb_init(struct drm_device *);
+extern void nv04_fb_takedown(struct drm_device *);
+
+/* nv10_fb.c */
+extern int  nv10_fb_init(struct drm_device *);
+extern void nv10_fb_takedown(struct drm_device *);
+
+/* nv40_fb.c */
+extern int  nv40_fb_init(struct drm_device *);
+extern void nv40_fb_takedown(struct drm_device *);
+
+/* nv04_fifo.c */
+extern int  nv04_fifo_channel_id(struct drm_device *);
+extern int  nv04_fifo_create_context(struct nouveau_channel *);
+extern void nv04_fifo_destroy_context(struct nouveau_channel *);
+extern int  nv04_fifo_load_context(struct nouveau_channel *);
+extern int  nv04_fifo_save_context(struct nouveau_channel *);
+
+/* nv10_fifo.c */
+extern int  nv10_fifo_channel_id(struct drm_device *);
+extern int  nv10_fifo_create_context(struct nouveau_channel *);
+extern void nv10_fifo_destroy_context(struct nouveau_channel *);
+extern int  nv10_fifo_load_context(struct nouveau_channel *);
+extern int  nv10_fifo_save_context(struct nouveau_channel *);
+
+/* nv40_fifo.c */
+extern int  nv40_fifo_init(struct drm_device *);
+extern int  nv40_fifo_create_context(struct nouveau_channel *);
+extern void nv40_fifo_destroy_context(struct nouveau_channel *);
+extern int  nv40_fifo_load_context(struct nouveau_channel *);
+extern int  nv40_fifo_save_context(struct nouveau_channel *);
+
+/* nv50_fifo.c */
+extern int  nv50_fifo_init(struct drm_device *);
+extern void nv50_fifo_takedown(struct drm_device *);
+extern int  nv50_fifo_channel_id(struct drm_device *);
+extern int  nv50_fifo_create_context(struct nouveau_channel *);
+extern void nv50_fifo_destroy_context(struct nouveau_channel *);
+extern int  nv50_fifo_load_context(struct nouveau_channel *);
+extern int  nv50_fifo_save_context(struct nouveau_channel *);
+
+/* nv04_graph.c */
+extern void nouveau_nv04_context_switch(struct drm_device *);
+extern int  nv04_graph_init(struct drm_device *);
+extern void nv04_graph_takedown(struct drm_device *);
+extern int  nv04_graph_create_context(struct nouveau_channel *);
+extern void nv04_graph_destroy_context(struct nouveau_channel *);
+extern int  nv04_graph_load_context(struct nouveau_channel *);
+extern int  nv04_graph_save_context(struct nouveau_channel *);
+
+/* nv10_graph.c */
+extern void nouveau_nv10_context_switch(struct drm_device *);
+extern int  nv10_graph_init(struct drm_device *);
+extern void nv10_graph_takedown(struct drm_device *);
+extern int  nv10_graph_create_context(struct nouveau_channel *);
+extern void nv10_graph_destroy_context(struct nouveau_channel *);
+extern int  nv10_graph_load_context(struct nouveau_channel *);
+extern int  nv10_graph_save_context(struct nouveau_channel *);
+
+/* nv20_graph.c */
+extern int  nv20_graph_create_context(struct nouveau_channel *);
+extern void nv20_graph_destroy_context(struct nouveau_channel *);
+extern int  nv20_graph_load_context(struct nouveau_channel *);
+extern int  nv20_graph_save_context(struct nouveau_channel *);
+extern int  nv20_graph_init(struct drm_device *);
+extern void nv20_graph_takedown(struct drm_device *);
+extern int  nv30_graph_init(struct drm_device *);
+
+/* nv40_graph.c */
+extern int  nv40_graph_init(struct drm_device *);
+extern void nv40_graph_takedown(struct drm_device *);
+extern int  nv40_graph_create_context(struct nouveau_channel *);
+extern void nv40_graph_destroy_context(struct nouveau_channel *);
+extern int  nv40_graph_load_context(struct nouveau_channel *);
+extern int  nv40_graph_save_context(struct nouveau_channel *);
+
+/* nv50_graph.c */
+extern int  nv50_graph_init(struct drm_device *);
+extern void nv50_graph_takedown(struct drm_device *);
+extern int  nv50_graph_create_context(struct nouveau_channel *);
+extern void nv50_graph_destroy_context(struct nouveau_channel *);
+extern int  nv50_graph_load_context(struct nouveau_channel *);
+extern int  nv50_graph_save_context(struct nouveau_channel *);
+
+/* nv04_instmem.c */
+extern int  nv04_instmem_init(struct drm_device *);
+extern void nv04_instmem_takedown(struct drm_device *);
+extern int  nv04_instmem_populate(struct drm_device *, struct nouveau_gpuobj *,
+				  uint32_t *size);
+extern void nv04_instmem_clear(struct drm_device *, struct nouveau_gpuobj *);
+extern int  nv04_instmem_bind(struct drm_device *, struct nouveau_gpuobj *);
+extern int  nv04_instmem_unbind(struct drm_device *, struct nouveau_gpuobj *);
+
+/* nv50_instmem.c */
+extern int  nv50_instmem_init(struct drm_device *);
+extern void nv50_instmem_takedown(struct drm_device *);
+extern int  nv50_instmem_populate(struct drm_device *, struct nouveau_gpuobj *,
+				  uint32_t *size);
+extern void nv50_instmem_clear(struct drm_device *, struct nouveau_gpuobj *);
+extern int  nv50_instmem_bind(struct drm_device *, struct nouveau_gpuobj *);
+extern int  nv50_instmem_unbind(struct drm_device *, struct nouveau_gpuobj *);
+
+/* nv04_mc.c */
+extern int  nv04_mc_init(struct drm_device *);
+extern void nv04_mc_takedown(struct drm_device *);
+
+/* nv40_mc.c */
+extern int  nv40_mc_init(struct drm_device *);
+extern void nv40_mc_takedown(struct drm_device *);
+
+/* nv50_mc.c */
+extern int  nv50_mc_init(struct drm_device *);
+extern void nv50_mc_takedown(struct drm_device *);
+
+/* nv04_timer.c */
+extern int  nv04_timer_init(struct drm_device *);
+extern uint64_t nv04_timer_read(struct drm_device *);
+extern void nv04_timer_takedown(struct drm_device *);
+
+extern long nouveau_compat_ioctl(struct file *file, unsigned int cmd,
+				 unsigned long arg);
+
+/* nouveau_buffer.c */
+extern struct drm_bo_driver nouveau_bo_driver;
+
+/* nouveau_fence.c */
+extern struct drm_fence_driver nouveau_fence_driver;
+extern void nouveau_fence_handler(struct drm_device *dev, int channel);
+
+#if defined(__powerpc__)
+#define NV_READ(reg)        in_be32((void __iomem *)(dev_priv->mmio)->handle + (reg) )
+#define NV_WRITE(reg,val)   out_be32((void __iomem *)(dev_priv->mmio)->handle + (reg) , (val) )
+#else
+#define NV_READ(reg)        DRM_READ32(  dev_priv->mmio, (reg) )
+#define NV_WRITE(reg,val)   DRM_WRITE32( dev_priv->mmio, (reg), (val) )
+#endif
+
+/* PRAMIN access */
+#if defined(__powerpc__)
+#define NV_RI32(o) in_be32((void __iomem *)(dev_priv->ramin)->handle+(o))
+#define NV_WI32(o,v) out_be32((void __iomem*)(dev_priv->ramin)->handle+(o), (v))
+#else
+#define NV_RI32(o) DRM_READ32(dev_priv->ramin, (o))
+#define NV_WI32(o,v) DRM_WRITE32(dev_priv->ramin, (o), (v))
+#endif
+
+#define INSTANCE_RD(o,i) NV_RI32((o)->im_pramin->start + ((i)<<2))
+#define INSTANCE_WR(o,i,v) NV_WI32((o)->im_pramin->start + ((i)<<2), (v))
+
+#endif /* __NOUVEAU_DRV_H__ */
diff --git a/drivers/char/drm/nouveau_fence.c b/drivers/char/drm/nouveau_fence.c
new file mode 100644
index 0000000..4ad51ae
--- /dev/null
+++ b/drivers/char/drm/nouveau_fence.c
@@ -0,0 +1,119 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_dma.h"
+
+static int
+nouveau_fence_has_irq(struct drm_device *dev, uint32_t class, uint32_t flags)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("class=%d, flags=0x%08x\n", class, flags);
+
+	/* DRM's channel always uses IRQs to signal fences */
+	if (class == dev_priv->channel.chan->id)
+		return 1;
+
+	/* Other channels don't use IRQs at all yet */
+	return 0;
+}
+
+static int
+nouveau_fence_emit(struct drm_device *dev, uint32_t class, uint32_t flags,
+		   uint32_t *breadcrumb, uint32_t *native_type)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_channel *chan = dev_priv->fifos[class];
+	struct nouveau_drm_channel *dchan = &dev_priv->channel;
+
+	DRM_DEBUG("class=%d, flags=0x%08x\n", class, flags);
+
+	/* We can't emit fences on client channels, update sequence number
+	 * and userspace will emit the fence
+	 */
+	*breadcrumb  = ++chan->next_sequence;
+	*native_type = DRM_FENCE_TYPE_EXE;
+	if (chan != dchan->chan) {
+		DRM_DEBUG("user fence 0x%08x\n", *breadcrumb);
+		return 0;
+	}
+
+	DRM_DEBUG("emit 0x%08x\n", *breadcrumb);
+	BEGIN_RING(NvSubM2MF, NV_MEMORY_TO_MEMORY_FORMAT_SET_REF, 1);
+	OUT_RING  (*breadcrumb);
+	BEGIN_RING(NvSubM2MF, 0x0150, 1);
+	OUT_RING  (0);
+	FIRE_RING ();
+
+	return 0;
+}
+
+static void
+nouveau_fence_poll(struct drm_device *dev, uint32_t class, uint32_t waiting_types)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_fence_class_manager *fc = &dev->fm.fence_class[class];
+	struct nouveau_channel *chan = dev_priv->fifos[class];
+
+	DRM_DEBUG("class=%d\n", class);
+	DRM_DEBUG("pending: 0x%08x 0x%08x\n", waiting_types, fc->waiting_types);
+
+	if (waiting_types & DRM_FENCE_TYPE_EXE) {
+		uint32_t sequence = NV_READ(chan->ref_cnt);
+
+		DRM_DEBUG("got 0x%08x\n", sequence);
+		drm_fence_handler(dev, class, sequence, waiting_types, 0);
+	}
+}
+
+void
+nouveau_fence_handler(struct drm_device *dev, int channel)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[channel];
+
+	DRM_DEBUG("class=%d\n", channel);
+
+	write_lock(&fm->lock);
+	nouveau_fence_poll(dev, channel, fc->waiting_types);
+	write_unlock(&fm->lock);
+}
+
+struct drm_fence_driver nouveau_fence_driver = {
+	.num_classes	= 8,
+	.wrap_diff	= (1 << 30),
+	.flush_diff	= (1 << 29),
+	.sequence_mask	= 0xffffffffU,
+	.has_irq	= nouveau_fence_has_irq,
+	.emit		= nouveau_fence_emit,
+	.flush          = NULL,
+	.poll           = nouveau_fence_poll,
+	.needed_flush   = NULL,
+	.wait           = NULL
+};
diff --git a/drivers/char/drm/nouveau_fifo.c b/drivers/char/drm/nouveau_fifo.c
new file mode 100644
index 0000000..d8fda27
--- /dev/null
+++ b/drivers/char/drm/nouveau_fifo.c
@@ -0,0 +1,598 @@
+/*
+ * Copyright 2005-2006 Stephane Marchesin
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+
+/* returns the size of fifo context */
+int nouveau_fifo_ctx_size(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+
+	if (dev_priv->card_type >= NV_40)
+		return 128;
+	else if (dev_priv->card_type >= NV_17)
+		return 64;
+	else
+		return 32;
+}
+
+/***********************************
+ * functions doing the actual work
+ ***********************************/
+
+static int nouveau_fifo_instmem_configure(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV03_PFIFO_RAMHT,
+			(0x03 << 24) /* search 128 */ |
+			((dev_priv->ramht_bits - 9) << 16) |
+			(dev_priv->ramht_offset >> 8)
+			);
+
+	NV_WRITE(NV03_PFIFO_RAMRO, dev_priv->ramro_offset>>8);
+
+	switch(dev_priv->card_type)
+	{
+		case NV_40:
+			switch (dev_priv->chipset) {
+			case 0x47:
+			case 0x49:
+			case 0x4b:
+				NV_WRITE(0x2230, 1);
+				break;
+			default:
+				break;
+			}
+			NV_WRITE(NV40_PFIFO_RAMFC, 0x30002);
+			break;
+		case NV_44:
+			NV_WRITE(NV40_PFIFO_RAMFC, ((nouveau_mem_fb_amount(dev)-512*1024+dev_priv->ramfc_offset)>>16) |
+					(2 << 16));
+			break;
+		case NV_30:
+		case NV_20:
+		case NV_17:
+			NV_WRITE(NV03_PFIFO_RAMFC, (dev_priv->ramfc_offset>>8) |
+					(1 << 16) /* 64 Bytes entry*/);
+			/* XXX nvidia blob set bit 18, 21,23 for nv20 & nv30 */
+			break;
+		case NV_11:
+		case NV_10:
+		case NV_04:
+			NV_WRITE(NV03_PFIFO_RAMFC, dev_priv->ramfc_offset>>8);
+			break;
+	}
+
+	return 0;
+}
+
+int nouveau_fifo_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PFIFO);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PFIFO);
+
+	/* Enable PFIFO error reporting */
+	NV_WRITE(NV03_PFIFO_INTR_0, 0xFFFFFFFF);
+	NV_WRITE(NV03_PFIFO_INTR_EN_0, 0xFFFFFFFF);
+
+	NV_WRITE(NV03_PFIFO_CACHES, 0x00000000);
+
+	ret = nouveau_fifo_instmem_configure(dev);
+	if (ret) {
+		DRM_ERROR("Failed to configure instance memory\n");
+		return ret;
+	}
+
+	/* FIXME remove all the stuff that's done in nouveau_fifo_alloc */
+
+	DRM_DEBUG("Setting defaults for remaining PFIFO regs\n");
+
+	/* All channels into PIO mode */
+	NV_WRITE(NV04_PFIFO_MODE, 0x00000000);
+
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000000);
+	/* Channel 0 active, PIO mode */
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH1, 0x00000000);
+	/* PUT and GET to 0 */
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUT, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_GET, 0x00000000);
+	/* No cmdbuf object */
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_INSTANCE, 0x00000000);
+	NV_WRITE(NV03_PFIFO_CACHE0_PUSH0, 0x00000000);
+	NV_WRITE(NV03_PFIFO_CACHE0_PULL0, 0x00000000);
+	NV_WRITE(NV04_PFIFO_SIZE, 0x0000FFFF);
+	NV_WRITE(NV04_PFIFO_CACHE1_HASH, 0x0000FFFF);
+	NV_WRITE(NV04_PFIFO_CACHE0_PULL1, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_CTL, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_STATE, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_ENGINE, 0x00000000);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_FETCH, NV_PFIFO_CACHE1_DMA_FETCH_TRIG_112_BYTES |
+				      NV_PFIFO_CACHE1_DMA_FETCH_SIZE_128_BYTES |
+				      NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_4 |
+#ifdef __BIG_ENDIAN
+				      NV_PFIFO_CACHE1_BIG_ENDIAN |
+#endif
+				      0x00000000);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUSH, 0x00000001);
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL1, 0x00000001);
+
+	/* FIXME on NV04 */
+	if (dev_priv->card_type >= NV_10) {
+		NV_WRITE(NV10_PGRAPH_CTX_USER, 0x0);
+		NV_WRITE(NV04_PFIFO_DELAY_0, 0xff /* retrycount*/ );
+		if (dev_priv->card_type >= NV_40)
+			NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x00002001);
+		else
+			NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10110000);
+	} else {
+		NV_WRITE(NV04_PGRAPH_CTX_USER, 0x0);
+		NV_WRITE(NV04_PFIFO_DELAY_0, 0xff /* retrycount*/ );
+		NV_WRITE(NV04_PGRAPH_CTX_CONTROL, 0x10110000);
+	}
+
+	NV_WRITE(NV04_PFIFO_DMA_TIMESLICE, 0x001fffff);
+	NV_WRITE(NV03_PFIFO_CACHES, 0x00000001);
+	return 0;
+}
+
+static int
+nouveau_fifo_pushbuf_ctxdma_init(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct mem_block *pb = chan->pushbuf_mem;
+	struct nouveau_gpuobj *pushbuf = NULL;
+	int ret;
+
+	if (pb->flags & NOUVEAU_MEM_AGP) {
+		ret = nouveau_gpuobj_gart_dma_new(chan, pb->start, pb->size,
+						  NV_DMA_ACCESS_RO,
+						  &pushbuf,
+						  &chan->pushbuf_base);
+	} else
+	if (pb->flags & NOUVEAU_MEM_PCI) {
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     pb->start, pb->size,
+					     NV_DMA_ACCESS_RO,
+					     NV_DMA_TARGET_PCI_NONLINEAR,
+					     &pushbuf);
+		chan->pushbuf_base = 0;
+	} else if (dev_priv->card_type != NV_04) {
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     pb->start, pb->size,
+					     NV_DMA_ACCESS_RO,
+					     NV_DMA_TARGET_VIDMEM, &pushbuf);
+		chan->pushbuf_base = 0;
+	} else {
+		/* NV04 cmdbuf hack, from original ddx.. not sure of it's
+		 * exact reason for existing :)  PCI access to cmdbuf in
+		 * VRAM.
+		 */
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     pb->start +
+					       drm_get_resource_start(dev, 1),
+					     pb->size, NV_DMA_ACCESS_RO,
+					     NV_DMA_TARGET_PCI, &pushbuf);
+		chan->pushbuf_base = 0;
+	}
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, chan, 0, pushbuf,
+					  &chan->pushbuf))) {
+		DRM_ERROR("Error referencing push buffer ctxdma: %d\n", ret);
+		if (pushbuf != dev_priv->gart_info.sg_ctxdma)
+			nouveau_gpuobj_del(dev, &pushbuf);
+		return ret;
+	}
+
+	return 0;
+}
+
+static struct mem_block *
+nouveau_fifo_user_pushbuf_alloc(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_config *config = &dev_priv->config;
+	struct mem_block *pb;
+	int pb_min_size = max(NV03_FIFO_SIZE,PAGE_SIZE);
+
+	/* Defaults for unconfigured values */
+	if (!config->cmdbuf.location)
+		config->cmdbuf.location = NOUVEAU_MEM_FB;
+	if (!config->cmdbuf.size || config->cmdbuf.size < pb_min_size)
+		config->cmdbuf.size = pb_min_size;
+
+	pb = nouveau_mem_alloc(dev, 0, config->cmdbuf.size,
+			       config->cmdbuf.location | NOUVEAU_MEM_MAPPED,
+			       (struct drm_file *)-2);
+	if (!pb)
+		DRM_ERROR("Couldn't allocate DMA push buffer.\n");
+
+	return pb;
+}
+
+/* allocates and initializes a fifo for user space consumption */
+int
+nouveau_fifo_alloc(struct drm_device *dev, struct nouveau_channel **chan_ret,
+		   struct drm_file *file_priv, struct mem_block *pushbuf,
+		   uint32_t vram_handle, uint32_t tt_handle)
+{
+	int ret;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct nouveau_channel *chan;
+	int channel;
+
+	/*
+	 * Alright, here is the full story
+	 * Nvidia cards have multiple hw fifo contexts (praise them for that,
+	 * no complicated crash-prone context switches)
+	 * We allocate a new context for each app and let it write to it directly
+	 * (woo, full userspace command submission !)
+	 * When there are no more contexts, you lost
+	 */
+	for (channel = 0; channel < engine->fifo.channels; channel++) {
+		if (dev_priv->fifos[channel] == NULL)
+			break;
+	}
+
+	/* no more fifos. you lost. */
+	if (channel == engine->fifo.channels)
+		return -EINVAL;
+
+	dev_priv->fifos[channel] = drm_calloc(1, sizeof(struct nouveau_channel),
+					      DRM_MEM_DRIVER);
+	if (!dev_priv->fifos[channel])
+		return -ENOMEM;
+	dev_priv->fifo_alloc_count++;
+	chan = dev_priv->fifos[channel];
+	chan->dev = dev;
+	chan->id = channel;
+	chan->file_priv = file_priv;
+	chan->pushbuf_mem = pushbuf;
+
+	DRM_INFO("Allocating FIFO number %d\n", channel);
+
+	/* Locate channel's user control regs */
+	if (dev_priv->card_type < NV_40) {
+		chan->user = NV03_USER(channel);
+		chan->user_size = NV03_USER_SIZE;
+		chan->put = NV03_USER_DMA_PUT(channel);
+		chan->get = NV03_USER_DMA_GET(channel);
+		chan->ref_cnt = NV03_USER_REF_CNT(channel);
+	} else
+	if (dev_priv->card_type < NV_50) {
+		chan->user = NV40_USER(channel);
+		chan->user_size = NV40_USER_SIZE;
+		chan->put = NV40_USER_DMA_PUT(channel);
+		chan->get = NV40_USER_DMA_GET(channel);
+		chan->ref_cnt = NV40_USER_REF_CNT(channel);
+	} else {
+		chan->user = NV50_USER(channel);
+		chan->user_size = NV50_USER_SIZE;
+		chan->put = NV50_USER_DMA_PUT(channel);
+		chan->get = NV50_USER_DMA_GET(channel);
+		chan->ref_cnt = NV50_USER_REF_CNT(channel);
+	}
+
+	/* Allocate space for per-channel fixed notifier memory */
+	ret = nouveau_notifier_init_channel(chan);
+	if (ret) {
+		nouveau_fifo_free(chan);
+		return ret;
+	}
+
+	/* Setup channel's default objects */
+	ret = nouveau_gpuobj_channel_init(chan, vram_handle, tt_handle);
+	if (ret) {
+		nouveau_fifo_free(chan);
+		return ret;
+	}
+
+	/* Create a dma object for the push buffer */
+	ret = nouveau_fifo_pushbuf_ctxdma_init(chan);
+	if (ret) {
+		nouveau_fifo_free(chan);
+		return ret;
+	}
+
+	nouveau_wait_for_idle(dev);
+
+	/* disable the fifo caches */
+	NV_WRITE(NV03_PFIFO_CACHES, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUSH, NV_READ(NV04_PFIFO_CACHE1_DMA_PUSH)&(~0x1));
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000000);
+
+	/* Create a graphics context for new channel */
+	ret = engine->graph.create_context(chan);
+	if (ret) {
+		nouveau_fifo_free(chan);
+		return ret;
+	}
+
+	/* Construct inital RAMFC for new channel */
+	ret = engine->fifo.create_context(chan);
+	if (ret) {
+		nouveau_fifo_free(chan);
+		return ret;
+	}
+
+	/* setup channel's default get/put values
+	 * XXX: quite possibly extremely pointless..
+	 */
+	NV_WRITE(chan->get, chan->pushbuf_base);
+	NV_WRITE(chan->put, chan->pushbuf_base);
+
+	/* If this is the first channel, setup PFIFO ourselves.  For any
+	 * other case, the GPU will handle this when it switches contexts.
+	 */
+	if (dev_priv->fifo_alloc_count == 1) {
+		ret = engine->fifo.load_context(chan);
+		if (ret) {
+			nouveau_fifo_free(chan);
+			return ret;
+		}
+
+		ret = engine->graph.load_context(chan);
+		if (ret) {
+			nouveau_fifo_free(chan);
+			return ret;
+		}
+	}
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUSH,
+		 NV_READ(NV04_PFIFO_CACHE1_DMA_PUSH) | 1);
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL1, 0x00000001);
+
+	/* reenable the fifo caches */
+	NV_WRITE(NV03_PFIFO_CACHES, 1);
+
+	DRM_INFO("%s: initialised FIFO %d\n", __func__, channel);
+	*chan_ret = chan;
+	return 0;
+}
+
+static int
+nouveau_channel_idle(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	uint32_t caches;
+	int idle;
+
+	caches = NV_READ(NV03_PFIFO_CACHES);
+	NV_WRITE(NV03_PFIFO_CACHES, caches & ~1);
+
+	if (engine->fifo.channel_id(dev) != chan->id) {
+		struct nouveau_gpuobj *ramfc = chan->ramfc->gpuobj;
+
+		if (INSTANCE_RD(ramfc, 0) != INSTANCE_RD(ramfc, 1))
+			idle = 0;
+		else
+			idle = 1;
+	} else {
+		idle = (NV_READ(NV04_PFIFO_CACHE1_DMA_GET) ==
+			NV_READ(NV04_PFIFO_CACHE1_DMA_PUT));
+	}
+
+	NV_WRITE(NV03_PFIFO_CACHES, caches);
+	return idle;
+}
+
+/* stops a fifo */
+void nouveau_fifo_free(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	uint64_t t_start;
+
+	DRM_INFO("%s: freeing fifo %d\n", __func__, chan->id);
+
+	/* Give the channel a chance to idle, wait 2s (hopefully) */
+	t_start = engine->timer.read(dev);
+	while (!nouveau_channel_idle(chan)) {
+		if (engine->timer.read(dev) - t_start > 2000000000ULL) {
+			DRM_ERROR("Failed to idle channel %d before destroy."
+				  "Prepare for strangeness..\n", chan->id);
+			break;
+		}
+	}
+
+	/*XXX: Maybe should wait for PGRAPH to finish with the stuff it fetched
+	 *     from CACHE1 too?
+	 */
+
+	/* disable the fifo caches */
+	NV_WRITE(NV03_PFIFO_CACHES, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUSH, NV_READ(NV04_PFIFO_CACHE1_DMA_PUSH)&(~0x1));
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000000);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000000);
+
+	// FIXME XXX needs more code
+
+	engine->fifo.destroy_context(chan);
+
+	/* Cleanup PGRAPH state */
+	engine->graph.destroy_context(chan);
+
+	/* reenable the fifo caches */
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUSH,
+		 NV_READ(NV04_PFIFO_CACHE1_DMA_PUSH) | 1);
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH0, 0x00000001);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x00000001);
+	NV_WRITE(NV03_PFIFO_CACHES, 0x00000001);
+
+	/* Deallocate push buffer */
+	nouveau_gpuobj_ref_del(dev, &chan->pushbuf);
+	if (chan->pushbuf_mem) {
+		nouveau_mem_free(dev, chan->pushbuf_mem);
+		chan->pushbuf_mem = NULL;
+	}
+
+	/* Destroy objects belonging to the channel */
+	nouveau_gpuobj_channel_takedown(chan);
+
+	nouveau_notifier_takedown_channel(chan);
+
+	dev_priv->fifos[chan->id] = NULL;
+	dev_priv->fifo_alloc_count--;
+	drm_free(chan, sizeof(*chan), DRM_MEM_DRIVER);
+}
+
+/* cleanups all the fifos from file_priv */
+void nouveau_fifo_cleanup(struct drm_device *dev, struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	int i;
+
+	DRM_DEBUG("clearing FIFO enables from file_priv\n");
+	for(i = 0; i < engine->fifo.channels; i++) {
+		struct nouveau_channel *chan = dev_priv->fifos[i];
+
+		if (chan && chan->file_priv == file_priv)
+			nouveau_fifo_free(chan);
+	}
+}
+
+int
+nouveau_fifo_owner(struct drm_device *dev, struct drm_file *file_priv,
+		   int channel)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+
+	if (channel >= engine->fifo.channels)
+		return 0;
+	if (dev_priv->fifos[channel] == NULL)
+		return 0;
+	return (dev_priv->fifos[channel]->file_priv == file_priv);
+}
+
+/***********************************
+ * ioctls wrapping the functions
+ ***********************************/
+
+static int nouveau_ioctl_fifo_alloc(struct drm_device *dev, void *data,
+				    struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_nouveau_channel_alloc *init = data;
+	struct drm_map_list *entry;
+	struct nouveau_channel *chan;
+	struct mem_block *pushbuf;
+	int res;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+
+	if (init->fb_ctxdma_handle == ~0 || init->tt_ctxdma_handle == ~0)
+		return -EINVAL;
+
+	pushbuf = nouveau_fifo_user_pushbuf_alloc(dev);
+	if (!pushbuf)
+		return -ENOMEM;
+
+	res = nouveau_fifo_alloc(dev, &chan, file_priv, pushbuf,
+				 init->fb_ctxdma_handle,
+				 init->tt_ctxdma_handle);
+	if (res)
+		return res;
+	init->channel  = chan->id;
+	init->put_base = chan->pushbuf_base;
+
+	/* make the fifo available to user space */
+	/* first, the fifo control regs */
+	init->ctrl = dev_priv->mmio->offset + chan->user;
+	init->ctrl_size = chan->user_size;
+	res = drm_addmap(dev, init->ctrl, init->ctrl_size, _DRM_REGISTERS,
+			 0, &chan->regs);
+	if (res != 0)
+		return res;
+
+	entry = drm_find_matching_map(dev, chan->regs);
+	if (!entry)
+		return -EINVAL;
+	init->ctrl = entry->user_token;
+
+	/* pass back FIFO map info to the caller */
+	init->cmdbuf      = chan->pushbuf_mem->map_handle;
+	init->cmdbuf_size = chan->pushbuf_mem->size;
+
+	/* and the notifier block */
+	init->notifier      = chan->notifier_block->map_handle;
+	init->notifier_size = chan->notifier_block->size;
+
+	return 0;
+}
+
+static int nouveau_ioctl_fifo_free(struct drm_device *dev, void *data,
+				   struct drm_file *file_priv)
+{
+	struct drm_nouveau_channel_free *cfree = data;
+	struct nouveau_channel *chan;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+	NOUVEAU_GET_USER_CHANNEL_WITH_RETURN(cfree->channel, file_priv, chan);
+
+	nouveau_fifo_free(chan);
+	return 0;
+}
+
+/***********************************
+ * finally, the ioctl table
+ ***********************************/
+
+struct drm_ioctl_desc nouveau_ioctls[] = {
+	DRM_IOCTL_DEF(DRM_NOUVEAU_CARD_INIT, nouveau_ioctl_card_init, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_GETPARAM, nouveau_ioctl_getparam, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_SETPARAM, nouveau_ioctl_setparam, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_CHANNEL_ALLOC, nouveau_ioctl_fifo_alloc, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_CHANNEL_FREE, nouveau_ioctl_fifo_free, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_GROBJ_ALLOC, nouveau_ioctl_grobj_alloc, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_NOTIFIEROBJ_ALLOC, nouveau_ioctl_notifier_alloc, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_GPUOBJ_FREE, nouveau_ioctl_gpuobj_free, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_MEM_ALLOC, nouveau_ioctl_mem_alloc, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_NOUVEAU_MEM_FREE, nouveau_ioctl_mem_free, DRM_AUTH),
+};
+
+int nouveau_max_ioctl = DRM_ARRAY_SIZE(nouveau_ioctls);
diff --git a/drivers/char/drm/nouveau_ioc32.c b/drivers/char/drm/nouveau_ioc32.c
new file mode 100644
index 0000000..4f53a50
--- /dev/null
+++ b/drivers/char/drm/nouveau_ioc32.c
@@ -0,0 +1,68 @@
+/**
+ * \file mga_ioc32.c
+ *
+ * 32-bit ioctl compatibility routines for the MGA DRM.
+ *
+ * \author Dave Airlie <airlied@linux.ie> with code from patches by Egbert Eich
+ *
+ *
+ * Copyright (C) Paul Mackerras 2005
+ * Copyright (C) Egbert Eich 2003,2004
+ * Copyright (C) Dave Airlie 2005
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+ * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#include <linux/compat.h>
+
+#include "drmP.h"
+#include "drm.h"
+
+#include "nouveau_drm.h"
+
+/**
+ * Called whenever a 32-bit process running under a 64-bit kernel
+ * performs an ioctl on /dev/dri/card<n>.
+ *
+ * \param filp file pointer.
+ * \param cmd command.
+ * \param arg user argument.
+ * \return zero on success or negative number on failure.
+ */
+long nouveau_compat_ioctl(struct file *filp, unsigned int cmd,
+			 unsigned long arg)
+{
+	unsigned int nr = DRM_IOCTL_NR(cmd);
+	drm_ioctl_compat_t *fn = NULL;
+	int ret;
+
+	if (nr < DRM_COMMAND_BASE)
+		return drm_compat_ioctl(filp, cmd, arg);
+
+	lock_kernel();	  /* XXX for now */
+	if (fn != NULL)
+		ret = (*fn)(filp, cmd, arg);
+	else
+		ret = drm_ioctl(filp->f_dentry->d_inode, filp, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
diff --git a/drivers/char/drm/nouveau_irq.c b/drivers/char/drm/nouveau_irq.c
new file mode 100644
index 0000000..ec158d8
--- /dev/null
+++ b/drivers/char/drm/nouveau_irq.c
@@ -0,0 +1,511 @@
+/*
+ * Copyright (C) 2006 Ben Skeggs.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/*
+ * Authors:
+ *   Ben Skeggs <darktama@iinet.net.au>
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_reg.h"
+#include "nouveau_swmthd.h"
+
+void
+nouveau_irq_preinstall(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* Master disable */
+	NV_WRITE(NV03_PMC_INTR_EN_0, 0);
+}
+
+int
+nouveau_irq_postinstall(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* Master enable */
+	NV_WRITE(NV03_PMC_INTR_EN_0, NV_PMC_INTR_EN_0_MASTER_ENABLE);
+
+	return 0;
+}
+
+void
+nouveau_irq_uninstall(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* Master disable */
+	NV_WRITE(NV03_PMC_INTR_EN_0, 0);
+}
+
+static void
+nouveau_fifo_irq_handler(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	uint32_t status;
+
+	while ((status = NV_READ(NV03_PFIFO_INTR_0))) {
+		uint32_t chid, get;
+
+		NV_WRITE(NV03_PFIFO_CACHES, 0);
+
+		chid = engine->fifo.channel_id(dev);
+		get  = NV_READ(NV03_PFIFO_CACHE1_GET);
+
+		if (status & NV_PFIFO_INTR_CACHE_ERROR) {
+			uint32_t mthd, data;
+			int ptr;
+
+			ptr = get >> 2;
+			if (dev_priv->card_type < NV_40) {
+				mthd = NV_READ(NV04_PFIFO_CACHE1_METHOD(ptr));
+				data = NV_READ(NV04_PFIFO_CACHE1_DATA(ptr));
+			} else {
+				mthd = NV_READ(NV40_PFIFO_CACHE1_METHOD(ptr));
+				data = NV_READ(NV40_PFIFO_CACHE1_DATA(ptr));
+			}
+
+			DRM_INFO("PFIFO_CACHE_ERROR - "
+				 "Ch %d/%d Mthd 0x%04x Data 0x%08x\n",
+				 chid, (mthd >> 13) & 7, mthd & 0x1ffc, data);
+
+			NV_WRITE(NV03_PFIFO_CACHE1_GET, get + 4);
+			NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 1);
+
+			status &= ~NV_PFIFO_INTR_CACHE_ERROR;
+			NV_WRITE(NV03_PFIFO_INTR_0, NV_PFIFO_INTR_CACHE_ERROR);
+		}
+
+		if (status & NV_PFIFO_INTR_DMA_PUSHER) {
+			DRM_INFO("PFIFO_DMA_PUSHER - Ch %d\n", chid);
+
+			status &= ~NV_PFIFO_INTR_DMA_PUSHER;
+			NV_WRITE(NV03_PFIFO_INTR_0, NV_PFIFO_INTR_DMA_PUSHER);
+
+			NV_WRITE(NV04_PFIFO_CACHE1_DMA_STATE, 0x00000000);
+			if (NV_READ(NV04_PFIFO_CACHE1_DMA_PUT) != get)
+				NV_WRITE(NV04_PFIFO_CACHE1_DMA_GET, get + 4);
+		}
+
+		if (status) {
+			DRM_INFO("Unhandled PFIFO_INTR - 0x%08x\n", status);
+			NV_WRITE(NV03_PFIFO_INTR_0, status);
+		}
+
+		NV_WRITE(NV03_PFIFO_CACHES, 1);
+	}
+
+	NV_WRITE(NV03_PMC_INTR_0, NV_PMC_INTR_0_PFIFO_PENDING);
+}
+
+struct nouveau_bitfield_names {
+	uint32_t mask;
+	const char * name;
+};
+
+static struct nouveau_bitfield_names nouveau_nstatus_names[] =
+{
+	{ NV04_PGRAPH_NSTATUS_STATE_IN_USE,       "STATE_IN_USE" },
+	{ NV04_PGRAPH_NSTATUS_INVALID_STATE,      "INVALID_STATE" },
+	{ NV04_PGRAPH_NSTATUS_BAD_ARGUMENT,       "BAD_ARGUMENT" },
+	{ NV04_PGRAPH_NSTATUS_PROTECTION_FAULT,   "PROTECTION_FAULT" }
+};
+
+static struct nouveau_bitfield_names nouveau_nstatus_names_nv10[] =
+{
+	{ NV10_PGRAPH_NSTATUS_STATE_IN_USE,       "STATE_IN_USE" },
+	{ NV10_PGRAPH_NSTATUS_INVALID_STATE,      "INVALID_STATE" },
+	{ NV10_PGRAPH_NSTATUS_BAD_ARGUMENT,       "BAD_ARGUMENT" },
+	{ NV10_PGRAPH_NSTATUS_PROTECTION_FAULT,   "PROTECTION_FAULT" }
+};
+
+static struct nouveau_bitfield_names nouveau_nsource_names[] =
+{
+	{ NV03_PGRAPH_NSOURCE_NOTIFICATION,       "NOTIFICATION" },
+	{ NV03_PGRAPH_NSOURCE_DATA_ERROR,         "DATA_ERROR" },
+	{ NV03_PGRAPH_NSOURCE_PROTECTION_ERROR,   "PROTECTION_ERROR" },
+	{ NV03_PGRAPH_NSOURCE_RANGE_EXCEPTION,    "RANGE_EXCEPTION" },
+	{ NV03_PGRAPH_NSOURCE_LIMIT_COLOR,        "LIMIT_COLOR" },
+	{ NV03_PGRAPH_NSOURCE_LIMIT_ZETA,         "LIMIT_ZETA" },
+	{ NV03_PGRAPH_NSOURCE_ILLEGAL_MTHD,       "ILLEGAL_MTHD" },
+	{ NV03_PGRAPH_NSOURCE_DMA_R_PROTECTION,   "DMA_R_PROTECTION" },
+	{ NV03_PGRAPH_NSOURCE_DMA_W_PROTECTION,   "DMA_W_PROTECTION" },
+	{ NV03_PGRAPH_NSOURCE_FORMAT_EXCEPTION,   "FORMAT_EXCEPTION" },
+	{ NV03_PGRAPH_NSOURCE_PATCH_EXCEPTION,    "PATCH_EXCEPTION" },
+	{ NV03_PGRAPH_NSOURCE_STATE_INVALID,      "STATE_INVALID" },
+	{ NV03_PGRAPH_NSOURCE_DOUBLE_NOTIFY,      "DOUBLE_NOTIFY" },
+	{ NV03_PGRAPH_NSOURCE_NOTIFY_IN_USE,      "NOTIFY_IN_USE" },
+	{ NV03_PGRAPH_NSOURCE_METHOD_CNT,         "METHOD_CNT" },
+	{ NV03_PGRAPH_NSOURCE_BFR_NOTIFICATION,   "BFR_NOTIFICATION" },
+	{ NV03_PGRAPH_NSOURCE_DMA_VTX_PROTECTION, "DMA_VTX_PROTECTION" },
+	{ NV03_PGRAPH_NSOURCE_DMA_WIDTH_A,        "DMA_WIDTH_A" },
+	{ NV03_PGRAPH_NSOURCE_DMA_WIDTH_B,        "DMA_WIDTH_B" },
+};
+
+static void
+nouveau_print_bitfield_names(uint32_t value,
+                             const struct nouveau_bitfield_names *namelist,
+                             const int namelist_len)
+{
+	int i;
+	for(i=0; i<namelist_len; ++i) {
+		uint32_t mask = namelist[i].mask;
+		if(value & mask) {
+			printk(" %s", namelist[i].name);
+			value &= ~mask;
+		}
+	}
+	if(value)
+		printk(" (unknown bits 0x%08x)", value);
+}
+
+static int
+nouveau_graph_trapped_channel(struct drm_device *dev, int *channel_ret)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	int channel;
+
+	if (dev_priv->card_type < NV_10) {
+		channel = (NV_READ(NV04_PGRAPH_TRAPPED_ADDR) >> 24) & 0xf;
+	} else if (dev_priv->card_type < NV_40) {
+		channel = (NV_READ(NV04_PGRAPH_TRAPPED_ADDR) >> 20) & 0x1f;
+	} else
+	if (dev_priv->card_type < NV_50) {
+		uint32_t cur_grctx = (NV_READ(0x40032C) & 0xfffff) << 4;
+
+		/* 0x400704 *sometimes* contains a sensible channel ID, but
+		 * mostly not.. for now lookup which channel owns the active
+		 * PGRAPH context.  Probably a better way, but this'll do
+		 * for now.
+		 */
+		for (channel = 0; channel < 32; channel++) {
+			if (dev_priv->fifos[channel] == NULL)
+				continue;
+			if (cur_grctx ==
+			    dev_priv->fifos[channel]->ramin_grctx->instance)
+				break;
+		}
+		if (channel == 32) {
+			DRM_ERROR("AIII, unable to determine active channel "
+				  "from PGRAPH context 0x%08x\n", cur_grctx);
+			return -EINVAL;
+		}
+	} else {
+		uint32_t cur_grctx = (NV_READ(0x40032C) & 0xfffff) << 12;
+
+		for (channel = 0; channel < 128; channel++) {
+			if (dev_priv->fifos[channel] == NULL)
+				continue;
+			if (cur_grctx ==
+			    dev_priv->fifos[channel]->ramin_grctx->instance)
+				break;
+		}
+		if (channel == 128) {
+			DRM_ERROR("AIII, unable to determine active channel "
+				  "from PGRAPH context 0x%08x\n", cur_grctx);
+			return -EINVAL;
+		}
+	}
+
+	if (channel > engine->fifo.channels || !dev_priv->fifos[channel]) {
+		DRM_ERROR("AIII, invalid/inactive channel id %d\n", channel);
+		return -EINVAL;
+	}
+
+	*channel_ret = channel;
+	return 0;
+}
+
+struct nouveau_pgraph_trap {
+	int channel;
+	int class;
+	int subc, mthd, size;
+	uint32_t data, data2;
+};
+
+static void
+nouveau_graph_trap_info(struct drm_device *dev,
+			struct nouveau_pgraph_trap *trap)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t address;
+
+	if (nouveau_graph_trapped_channel(dev, &trap->channel))
+		trap->channel = -1;
+	address = NV_READ(NV04_PGRAPH_TRAPPED_ADDR);
+
+	trap->mthd = address & 0x1FFC;
+	trap->data = NV_READ(NV04_PGRAPH_TRAPPED_DATA);
+	if (dev_priv->card_type < NV_10) {
+		trap->subc  = (address >> 13) & 0x7;
+	} else {
+		trap->subc  = (address >> 16) & 0x7;
+		trap->data2 = NV_READ(NV10_PGRAPH_TRAPPED_DATA_HIGH);
+	}
+
+	if (dev_priv->card_type < NV_10) {
+		trap->class = NV_READ(0x400180 + trap->subc*4) & 0xFF;
+	} else if (dev_priv->card_type < NV_40) {
+		trap->class = NV_READ(0x400160 + trap->subc*4) & 0xFFF;
+	} else if (dev_priv->card_type < NV_50) {
+		trap->class = NV_READ(0x400160 + trap->subc*4) & 0xFFFF;
+	} else {
+		trap->class = NV_READ(0x400814);
+	}
+}
+
+static void
+nouveau_graph_dump_trap_info(struct drm_device *dev, const char *id,
+			     struct nouveau_pgraph_trap *trap)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t nsource, nstatus;
+
+	nsource = NV_READ(NV03_PGRAPH_NSOURCE);
+	nstatus = NV_READ(NV03_PGRAPH_NSTATUS);
+
+	DRM_INFO("%s - nSource:", id);
+	nouveau_print_bitfield_names(nsource, nouveau_nsource_names,
+	                             ARRAY_SIZE(nouveau_nsource_names));
+	printk(", nStatus:");
+	if (dev_priv->card_type < NV_10)
+		nouveau_print_bitfield_names(nstatus, nouveau_nstatus_names,
+	                             ARRAY_SIZE(nouveau_nstatus_names));
+	else
+		nouveau_print_bitfield_names(nstatus, nouveau_nstatus_names_nv10,
+	                             ARRAY_SIZE(nouveau_nstatus_names_nv10));
+	printk("\n");
+
+	DRM_INFO("%s - Ch %d/%d Class 0x%04x Mthd 0x%04x Data 0x%08x:0x%08x\n",
+		 id, trap->channel, trap->subc, trap->class, trap->mthd,
+		 trap->data2, trap->data);
+}
+
+static inline void
+nouveau_pgraph_intr_notify(struct drm_device *dev, uint32_t nsource)
+{
+	struct nouveau_pgraph_trap trap;
+	int unhandled = 0;
+
+	nouveau_graph_trap_info(dev, &trap);
+
+	if (nsource & NV03_PGRAPH_NSOURCE_ILLEGAL_MTHD) {
+		/* NV4 (nvidia TNT 1) reports software methods with
+		 * PGRAPH NOTIFY ILLEGAL_MTHD
+		 */
+		DRM_DEBUG("Got NV04 software method method %x for class %#x\n",
+			  trap.mthd, trap.class);
+
+		if (nouveau_sw_method_execute(dev, trap.class, trap.mthd)) {
+			DRM_ERROR("Unable to execute NV04 software method %x "
+				  "for object class %x. Please report.\n",
+				  trap.mthd, trap.class);
+			unhandled = 1;
+		}
+	} else {
+		unhandled = 1;
+	}
+
+	if (unhandled)
+		nouveau_graph_dump_trap_info(dev, "PGRAPH_NOTIFY", &trap);
+}
+
+static inline void
+nouveau_pgraph_intr_error(struct drm_device *dev, uint32_t nsource)
+{
+	struct nouveau_pgraph_trap trap;
+	int unhandled = 0;
+
+	nouveau_graph_trap_info(dev, &trap);
+
+	if (nsource & NV03_PGRAPH_NSOURCE_ILLEGAL_MTHD) {
+		if (trap.channel >= 0 && trap.mthd == 0x0150) {
+			nouveau_fence_handler(dev, trap.channel);
+		} else
+		if (nouveau_sw_method_execute(dev, trap.class, trap.mthd)) {
+			unhandled = 1;
+		}
+	} else {
+		unhandled = 1;
+	}
+
+	if (unhandled)
+		nouveau_graph_dump_trap_info(dev, "PGRAPH_ERROR", &trap);
+}
+
+static inline void
+nouveau_pgraph_intr_context_switch(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	uint32_t chid;
+
+	chid = engine->fifo.channel_id(dev);
+	DRM_DEBUG("PGRAPH context switch interrupt channel %x\n", chid);
+
+	switch(dev_priv->card_type) {
+	case NV_04:
+	case NV_05:
+		nouveau_nv04_context_switch(dev);
+		break;
+	case NV_10:
+	case NV_11:
+	case NV_17:
+		nouveau_nv10_context_switch(dev);
+		break;
+	default:
+		DRM_ERROR("Context switch not implemented\n");
+		break;
+	}
+}
+
+static void
+nouveau_pgraph_irq_handler(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t status;
+
+	while ((status = NV_READ(NV03_PGRAPH_INTR))) {
+		uint32_t nsource = NV_READ(NV03_PGRAPH_NSOURCE);
+
+		if (status & NV_PGRAPH_INTR_NOTIFY) {
+			nouveau_pgraph_intr_notify(dev, nsource);
+
+			status &= ~NV_PGRAPH_INTR_NOTIFY;
+			NV_WRITE(NV03_PGRAPH_INTR, NV_PGRAPH_INTR_NOTIFY);
+		}
+
+		if (status & NV_PGRAPH_INTR_ERROR) {
+			nouveau_pgraph_intr_error(dev, nsource);
+
+			status &= ~NV_PGRAPH_INTR_ERROR;
+			NV_WRITE(NV03_PGRAPH_INTR, NV_PGRAPH_INTR_ERROR);
+		}
+
+		if (status & NV_PGRAPH_INTR_CONTEXT_SWITCH) {
+			nouveau_pgraph_intr_context_switch(dev);
+
+			status &= ~NV_PGRAPH_INTR_CONTEXT_SWITCH;
+			NV_WRITE(NV03_PGRAPH_INTR,
+				 NV_PGRAPH_INTR_CONTEXT_SWITCH);
+		}
+
+		if (status) {
+			DRM_INFO("Unhandled PGRAPH_INTR - 0x%08x\n", status);
+			NV_WRITE(NV03_PGRAPH_INTR, status);
+		}
+
+		if ((NV_READ(NV04_PGRAPH_FIFO) & (1 << 0)) == 0)
+			NV_WRITE(NV04_PGRAPH_FIFO, 1);
+	}
+
+	NV_WRITE(NV03_PMC_INTR_0, NV_PMC_INTR_0_PGRAPH_PENDING);
+}
+
+static void
+nouveau_crtc_irq_handler(struct drm_device *dev, int crtc)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	if (crtc&1) {
+		NV_WRITE(NV_CRTC0_INTSTAT, NV_CRTC_INTR_VBLANK);
+	}
+
+	if (crtc&2) {
+		NV_WRITE(NV_CRTC1_INTSTAT, NV_CRTC_INTR_VBLANK);
+	}
+}
+
+static void
+nouveau_nv50_display_irq_handler(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t val = NV_READ(NV50_DISPLAY_SUPERVISOR);
+
+	DRM_INFO("NV50_DISPLAY_INTR - 0x%08X\n", val);
+
+	NV_WRITE(NV50_DISPLAY_SUPERVISOR, val);
+}
+
+static void
+nouveau_nv50_i2c_irq_handler(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_INFO("NV50_I2C_INTR - 0x%08X\n", NV_READ(NV50_I2C_CONTROLLER));
+
+	/* This seems to be the way to acknowledge an interrupt. */
+	NV_WRITE(NV50_I2C_CONTROLLER, 0x7FFF7FFF);
+}
+
+irqreturn_t
+nouveau_irq_handler(DRM_IRQ_ARGS)
+{
+	struct drm_device *dev = (struct drm_device*)arg;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t status;
+
+	status = NV_READ(NV03_PMC_INTR_0);
+	if (!status)
+		return IRQ_NONE;
+
+	if (status & NV_PMC_INTR_0_PFIFO_PENDING) {
+		nouveau_fifo_irq_handler(dev);
+		status &= ~NV_PMC_INTR_0_PFIFO_PENDING;
+	}
+
+	if (status & NV_PMC_INTR_0_PGRAPH_PENDING) {
+		nouveau_pgraph_irq_handler(dev);
+		status &= ~NV_PMC_INTR_0_PGRAPH_PENDING;
+	}
+
+	if (status & NV_PMC_INTR_0_CRTCn_PENDING) {
+		nouveau_crtc_irq_handler(dev, (status>>24)&3);
+		status &= ~NV_PMC_INTR_0_CRTCn_PENDING;
+	}
+
+	if (status & NV_PMC_INTR_0_NV50_DISPLAY_PENDING) {
+		nouveau_nv50_display_irq_handler(dev);
+		status &= ~NV_PMC_INTR_0_NV50_DISPLAY_PENDING;
+	}
+
+	if (status & NV_PMC_INTR_0_NV50_I2C_PENDING) {
+		nouveau_nv50_i2c_irq_handler(dev);
+		status &= ~NV_PMC_INTR_0_NV50_I2C_PENDING;
+	}
+
+	if (status)
+		DRM_ERROR("Unhandled PMC INTR status bits 0x%08x\n", status);
+
+	return IRQ_HANDLED;
+}
diff --git a/drivers/char/drm/nouveau_mem.c b/drivers/char/drm/nouveau_mem.c
new file mode 100644
index 0000000..c861267
--- /dev/null
+++ b/drivers/char/drm/nouveau_mem.c
@@ -0,0 +1,780 @@
+/*
+ * Copyright (C) The Weather Channel, Inc.  2002.  All Rights Reserved.
+ * Copyright 2005 Stephane Marchesin
+ *
+ * The Weather Channel (TM) funded Tungsten Graphics to develop the
+ * initial release of the Radeon 8500 driver under the XFree86 license.
+ * This notice must be preserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Keith Whitwell <keith@tungstengraphics.com>
+ */
+
+
+#include "drmP.h"
+#include "drm.h"
+#include "drm_sarea.h"
+#include "nouveau_drv.h"
+
+static struct mem_block *split_block(struct mem_block *p, uint64_t start, uint64_t size,
+		struct drm_file *file_priv)
+{
+	/* Maybe cut off the start of an existing block */
+	if (start > p->start) {
+		struct mem_block *newblock =
+			drm_alloc(sizeof(*newblock), DRM_MEM_BUFS);
+		if (!newblock)
+			goto out;
+		newblock->start = start;
+		newblock->size = p->size - (start - p->start);
+		newblock->file_priv = NULL;
+		newblock->next = p->next;
+		newblock->prev = p;
+		p->next->prev = newblock;
+		p->next = newblock;
+		p->size -= newblock->size;
+		p = newblock;
+	}
+
+	/* Maybe cut off the end of an existing block */
+	if (size < p->size) {
+		struct mem_block *newblock =
+			drm_alloc(sizeof(*newblock), DRM_MEM_BUFS);
+		if (!newblock)
+			goto out;
+		newblock->start = start + size;
+		newblock->size = p->size - size;
+		newblock->file_priv = NULL;
+		newblock->next = p->next;
+		newblock->prev = p;
+		p->next->prev = newblock;
+		p->next = newblock;
+		p->size = size;
+	}
+
+out:
+	/* Our block is in the middle */
+	p->file_priv = file_priv;
+	return p;
+}
+
+struct mem_block *nouveau_mem_alloc_block(struct mem_block *heap,
+					  uint64_t size,
+					  int align2,
+					  struct drm_file *file_priv)
+{
+	struct mem_block *p;
+	uint64_t mask = (1 << align2) - 1;
+
+	if (!heap)
+		return NULL;
+
+	list_for_each(p, heap) {
+		uint64_t start = (p->start + mask) & ~mask;
+		if (p->file_priv == 0 && start + size <= p->start + p->size)
+			return split_block(p, start, size, file_priv);
+	}
+
+	return NULL;
+}
+
+static struct mem_block *find_block(struct mem_block *heap, uint64_t start)
+{
+	struct mem_block *p;
+
+	list_for_each(p, heap)
+		if (p->start == start)
+			return p;
+
+	return NULL;
+}
+
+void nouveau_mem_free_block(struct mem_block *p)
+{
+	p->file_priv = NULL;
+
+	/* Assumes a single contiguous range.  Needs a special file_priv in
+	 * 'heap' to stop it being subsumed.
+	 */
+	if (p->next->file_priv == 0) {
+		struct mem_block *q = p->next;
+		p->size += q->size;
+		p->next = q->next;
+		p->next->prev = p;
+		drm_free(q, sizeof(*q), DRM_MEM_BUFS);
+	}
+
+	if (p->prev->file_priv == 0) {
+		struct mem_block *q = p->prev;
+		q->size += p->size;
+		q->next = p->next;
+		q->next->prev = q;
+		drm_free(p, sizeof(*q), DRM_MEM_BUFS);
+	}
+}
+
+/* Initialize.  How to check for an uninitialized heap?
+ */
+int nouveau_mem_init_heap(struct mem_block **heap, uint64_t start,
+			  uint64_t size)
+{
+	struct mem_block *blocks = drm_alloc(sizeof(*blocks), DRM_MEM_BUFS);
+
+	if (!blocks)
+		return -ENOMEM;
+
+	*heap = drm_alloc(sizeof(**heap), DRM_MEM_BUFS);
+	if (!*heap) {
+		drm_free(blocks, sizeof(*blocks), DRM_MEM_BUFS);
+		return -ENOMEM;
+	}
+
+	blocks->start = start;
+	blocks->size = size;
+	blocks->file_priv = NULL;
+	blocks->next = blocks->prev = *heap;
+
+	memset(*heap, 0, sizeof(**heap));
+	(*heap)->file_priv = (struct drm_file *) - 1;
+	(*heap)->next = (*heap)->prev = blocks;
+	return 0;
+}
+
+/*
+ * Free all blocks associated with the releasing file_priv
+ */
+void nouveau_mem_release(struct drm_file *file_priv, struct mem_block *heap)
+{
+	struct mem_block *p;
+
+	if (!heap || !heap->next)
+		return;
+
+	list_for_each(p, heap) {
+		if (p->file_priv == file_priv)
+			p->file_priv = NULL;
+	}
+
+	/* Assumes a single contiguous range.  Needs a special file_priv in
+	 * 'heap' to stop it being subsumed.
+	 */
+	list_for_each(p, heap) {
+		while ((p->file_priv == 0) && (p->next->file_priv == 0) &&
+		       (p->next!=heap)) {
+			struct mem_block *q = p->next;
+			p->size += q->size;
+			p->next = q->next;
+			p->next->prev = p;
+			drm_free(q, sizeof(*q), DRM_MEM_DRIVER);
+		}
+	}
+}
+
+/*
+ * Cleanup everything
+ */
+void nouveau_mem_takedown(struct mem_block **heap)
+{
+	struct mem_block *p;
+
+	if (!*heap)
+		return;
+
+	for (p = (*heap)->next; p != *heap;) {
+		struct mem_block *q = p;
+		p = p->next;
+		drm_free(q, sizeof(*q), DRM_MEM_DRIVER);
+	}
+
+	drm_free(*heap, sizeof(**heap), DRM_MEM_DRIVER);
+	*heap = NULL;
+}
+
+void nouveau_mem_close(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	nouveau_mem_takedown(&dev_priv->agp_heap);
+	nouveau_mem_takedown(&dev_priv->fb_heap);
+	if (dev_priv->pci_heap)
+		nouveau_mem_takedown(&dev_priv->pci_heap);
+}
+
+/*XXX won't work on BSD because of pci_read_config_dword */
+static uint32_t
+nouveau_mem_fb_amount_igp(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct pci_dev *bridge;
+	uint32_t mem;
+
+	bridge = pci_get_bus_and_slot(0, PCI_DEVFN(0,1));
+	if (!bridge) {
+		DRM_ERROR("no bridge device\n");
+		return 0;
+	}
+
+	if (dev_priv->flags&NV_NFORCE) {
+		pci_read_config_dword(bridge, 0x7C, &mem);
+		return (uint64_t)(((mem >> 6) & 31) + 1)*1024*1024;
+	} else
+	if(dev_priv->flags&NV_NFORCE2) {
+		pci_read_config_dword(bridge, 0x84, &mem);
+		return (uint64_t)(((mem >> 4) & 127) + 1)*1024*1024;
+	}
+
+	DRM_ERROR("impossible!\n");
+
+	return 0;
+}
+
+/* returns the amount of FB ram in bytes */
+uint64_t nouveau_mem_fb_amount(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+	switch(dev_priv->card_type)
+	{
+		case NV_04:
+		case NV_05:
+			if (NV_READ(NV03_BOOT_0) & 0x00000100) {
+				return (((NV_READ(NV03_BOOT_0) >> 12) & 0xf)*2+2)*1024*1024;
+			} else
+			switch(NV_READ(NV03_BOOT_0)&NV03_BOOT_0_RAM_AMOUNT)
+			{
+				case NV04_BOOT_0_RAM_AMOUNT_32MB:
+					return 32*1024*1024;
+				case NV04_BOOT_0_RAM_AMOUNT_16MB:
+					return 16*1024*1024;
+				case NV04_BOOT_0_RAM_AMOUNT_8MB:
+					return 8*1024*1024;
+				case NV04_BOOT_0_RAM_AMOUNT_4MB:
+					return 4*1024*1024;
+			}
+			break;
+		case NV_10:
+		case NV_11:
+		case NV_17:
+		case NV_20:
+		case NV_30:
+		case NV_40:
+		case NV_44:
+		case NV_50:
+		default:
+			if (dev_priv->flags & (NV_NFORCE | NV_NFORCE2)) {
+				return nouveau_mem_fb_amount_igp(dev);
+			} else {
+				uint64_t mem;
+
+				mem = (NV_READ(NV04_FIFO_DATA) &
+				       NV10_FIFO_DATA_RAM_AMOUNT_MB_MASK) >>
+				      NV10_FIFO_DATA_RAM_AMOUNT_MB_SHIFT;
+				return mem*1024*1024;
+			}
+			break;
+	}
+
+	DRM_ERROR("Unable to detect video ram size. Please report your setup to " DRIVER_EMAIL "\n");
+	return 0;
+}
+
+static void nouveau_mem_reset_agp(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t saved_pci_nv_1, saved_pci_nv_19, pmc_enable;
+
+	saved_pci_nv_1 = NV_READ(NV04_PBUS_PCI_NV_1);
+	saved_pci_nv_19 = NV_READ(NV04_PBUS_PCI_NV_19);
+
+	/* clear busmaster bit */
+	NV_WRITE(NV04_PBUS_PCI_NV_1, saved_pci_nv_1 & ~0x4);
+	/* clear SBA and AGP bits */
+	NV_WRITE(NV04_PBUS_PCI_NV_19, saved_pci_nv_19 & 0xfffff0ff);
+
+	/* power cycle pgraph, if enabled */
+	pmc_enable = NV_READ(NV03_PMC_ENABLE);
+	if (pmc_enable & NV_PMC_ENABLE_PGRAPH) {
+		NV_WRITE(NV03_PMC_ENABLE, pmc_enable & ~NV_PMC_ENABLE_PGRAPH);
+		NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+				NV_PMC_ENABLE_PGRAPH);
+	}
+
+	/* and restore (gives effect of resetting AGP) */
+	NV_WRITE(NV04_PBUS_PCI_NV_19, saved_pci_nv_19);
+	NV_WRITE(NV04_PBUS_PCI_NV_1, saved_pci_nv_1);
+}
+
+static int
+nouveau_mem_init_agp(struct drm_device *dev, int ttm)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_agp_info info;
+	struct drm_agp_mode mode;
+	int ret;
+
+	nouveau_mem_reset_agp(dev);
+
+	ret = drm_agp_acquire(dev);
+	if (ret) {
+		DRM_ERROR("Unable to acquire AGP: %d\n", ret);
+		return ret;
+	}
+
+	ret = drm_agp_info(dev, &info);
+	if (ret) {
+		DRM_ERROR("Unable to get AGP info: %d\n", ret);
+		return ret;
+	}
+
+	/* see agp.h for the AGPSTAT_* modes available */
+	mode.mode = info.mode;
+	ret = drm_agp_enable(dev, mode);
+	if (ret) {
+		DRM_ERROR("Unable to enable AGP: %d\n", ret);
+		return ret;
+	}
+
+	if (!ttm) {
+		struct drm_agp_buffer agp_req;
+		struct drm_agp_binding bind_req;
+
+		agp_req.size = info.aperture_size;
+		agp_req.type = 0;
+		ret = drm_agp_alloc(dev, &agp_req);
+		if (ret) {
+			DRM_ERROR("Unable to alloc AGP: %d\n", ret);
+				return ret;
+		}
+
+		bind_req.handle = agp_req.handle;
+		bind_req.offset = 0;
+		ret = drm_agp_bind(dev, &bind_req);
+		if (ret) {
+			DRM_ERROR("Unable to bind AGP: %d\n", ret);
+			return ret;
+		}
+	}
+
+	dev_priv->gart_info.type	= NOUVEAU_GART_AGP;
+	dev_priv->gart_info.aper_base	= info.aperture_base;
+	dev_priv->gart_info.aper_size	= info.aperture_size;
+	return 0;
+}
+
+#define HACK_OLD_MM
+int
+nouveau_mem_init_ttm(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t vram_size, bar1_size;
+	int ret;
+
+	dev_priv->agp_heap = dev_priv->pci_heap = dev_priv->fb_heap = NULL;
+	dev_priv->fb_phys = drm_get_resource_start(dev,1);
+	dev_priv->gart_info.type = NOUVEAU_GART_NONE;
+
+	drm_bo_driver_init(dev);
+
+	/* non-mappable vram */
+	dev_priv->fb_available_size = nouveau_mem_fb_amount(dev);
+	dev_priv->fb_available_size -= dev_priv->ramin_rsvd_vram;
+	vram_size = dev_priv->fb_available_size >> PAGE_SHIFT;
+	bar1_size = drm_get_resource_len(dev, 1) >> PAGE_SHIFT;
+	if (bar1_size < vram_size) {
+		if ((ret = drm_bo_init_mm(dev, DRM_BO_MEM_PRIV0,
+					  bar1_size, vram_size - bar1_size, 1))) {
+			DRM_ERROR("Failed PRIV0 mm init: %d\n", ret);
+			return ret;
+		}
+		vram_size = bar1_size;
+	}
+
+	/* mappable vram */
+#ifdef HACK_OLD_MM
+	vram_size /= 4;
+#endif
+	if ((ret = drm_bo_init_mm(dev, DRM_BO_MEM_VRAM, 0, vram_size, 1))) {
+		DRM_ERROR("Failed VRAM mm init: %d\n", ret);
+		return ret;
+	}
+
+	/* GART */
+#if !defined(__powerpc__) && !defined(__ia64__)
+	if (drm_device_is_agp(dev) && dev->agp) {
+		if ((ret = nouveau_mem_init_agp(dev, 1)))
+			DRM_ERROR("Error initialising AGP: %d\n", ret);
+	}
+#endif
+
+	if (dev_priv->gart_info.type == NOUVEAU_GART_NONE) {
+		if ((ret = nouveau_sgdma_init(dev)))
+			DRM_ERROR("Error initialising PCI SGDMA: %d\n", ret);
+	}
+
+	if ((ret = drm_bo_init_mm(dev, DRM_BO_MEM_TT, 0,
+				  dev_priv->gart_info.aper_size >>
+				  PAGE_SHIFT, 1))) {
+		DRM_ERROR("Failed TT mm init: %d\n", ret);
+		return ret;
+	}
+
+#ifdef HACK_OLD_MM
+	vram_size <<= PAGE_SHIFT;
+	DRM_INFO("Old MM using %dKiB VRAM\n", (vram_size * 3) >> 10);
+	if (nouveau_mem_init_heap(&dev_priv->fb_heap, vram_size, vram_size * 3))
+		return -ENOMEM;
+#endif
+
+	return 0;
+}
+
+int nouveau_mem_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t fb_size;
+	int ret = 0;
+
+	dev_priv->agp_heap = dev_priv->pci_heap = dev_priv->fb_heap = NULL;
+	dev_priv->fb_phys = 0;
+	dev_priv->gart_info.type = NOUVEAU_GART_NONE;
+
+	/* setup a mtrr over the FB */
+	dev_priv->fb_mtrr = drm_mtrr_add(drm_get_resource_start(dev, 1),
+					 nouveau_mem_fb_amount(dev),
+					 DRM_MTRR_WC);
+
+	/* Init FB */
+	dev_priv->fb_phys=drm_get_resource_start(dev,1);
+	fb_size = nouveau_mem_fb_amount(dev);
+	/* On G80, limit VRAM to 512MiB temporarily due to limits in how
+	 * we handle VRAM page tables.
+	 */
+	if (dev_priv->card_type >= NV_50 && fb_size > (512 * 1024 * 1024))
+		fb_size = (512 * 1024 * 1024);
+	/* On at least NV40, RAMIN is actually at the end of vram.
+	 * We don't want to allocate this... */
+	if (dev_priv->card_type >= NV_40)
+		fb_size -= dev_priv->ramin_rsvd_vram;
+	dev_priv->fb_available_size = fb_size;
+	DRM_DEBUG("Available VRAM: %dKiB\n", fb_size>>10);
+
+	if (fb_size>256*1024*1024) {
+		/* On cards with > 256Mb, you can't map everything.
+		 * So we create a second FB heap for that type of memory */
+		if (nouveau_mem_init_heap(&dev_priv->fb_heap,
+					  0, 256*1024*1024))
+			return -ENOMEM;
+		if (nouveau_mem_init_heap(&dev_priv->fb_nomap_heap,
+					  256*1024*1024, fb_size-256*1024*1024))
+			return -ENOMEM;
+	} else {
+		if (nouveau_mem_init_heap(&dev_priv->fb_heap, 0, fb_size))
+			return -ENOMEM;
+		dev_priv->fb_nomap_heap=NULL;
+	}
+
+#if !defined(__powerpc__) && !defined(__ia64__)
+	/* Init AGP / NV50 PCIEGART */
+	if (drm_device_is_agp(dev) && dev->agp) {
+		if ((ret = nouveau_mem_init_agp(dev, 0)))
+			DRM_ERROR("Error initialising AGP: %d\n", ret);
+	}
+#endif
+
+	/*Note: this is *not* just NV50 code, but only used on NV50 for now */
+	if (dev_priv->gart_info.type == NOUVEAU_GART_NONE &&
+	    dev_priv->card_type >= NV_50) {
+		ret = nouveau_sgdma_init(dev);
+		if (!ret) {
+			ret = nouveau_sgdma_nottm_hack_init(dev);
+			if (ret)
+				nouveau_sgdma_takedown(dev);
+		}
+
+		if (ret)
+			DRM_ERROR("Error initialising SG DMA: %d\n", ret);
+	}
+
+	if (dev_priv->gart_info.type != NOUVEAU_GART_NONE) {
+		if (nouveau_mem_init_heap(&dev_priv->agp_heap,
+					  0, dev_priv->gart_info.aper_size)) {
+			if (dev_priv->gart_info.type == NOUVEAU_GART_SGDMA) {
+				nouveau_sgdma_nottm_hack_takedown(dev);
+				nouveau_sgdma_takedown(dev);
+			}
+		}
+	}
+
+	/* NV04-NV40 PCIEGART */
+	if (!dev_priv->agp_heap && dev_priv->card_type < NV_50) {
+		struct drm_scatter_gather sgreq;
+
+		DRM_DEBUG("Allocating sg memory for PCI DMA\n");
+		sgreq.size = 16 << 20; //16MB of PCI scatter-gather zone
+
+		if (drm_sg_alloc(dev, &sgreq)) {
+			DRM_ERROR("Unable to allocate %ldMB of scatter-gather"
+				  " pages for PCI DMA!",sgreq.size>>20);
+		} else {
+			if (nouveau_mem_init_heap(&dev_priv->pci_heap, 0,
+						  dev->sg->pages * PAGE_SIZE)) {
+				DRM_ERROR("Unable to initialize pci_heap!");
+			}
+		}
+	}
+
+	/* G8x: Allocate shared page table to map real VRAM pages into */
+	if (dev_priv->card_type >= NV_50) {
+		unsigned size = ((512 * 1024 * 1024) / 65536) * 8;
+
+		ret = nouveau_gpuobj_new(dev, NULL, size, 0,
+					 NVOBJ_FLAG_ZERO_ALLOC |
+					 NVOBJ_FLAG_ALLOW_NO_REFS,
+					 &dev_priv->vm_vram_pt);
+		if (ret) {
+			DRM_ERROR("Error creating VRAM page table: %d\n", ret);
+			return ret;
+		}
+	}
+
+
+	return 0;
+}
+
+struct mem_block* nouveau_mem_alloc(struct drm_device *dev, int alignment,
+				    uint64_t size, int flags,
+				    struct drm_file *file_priv)
+{
+	struct mem_block *block;
+	int type;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/*
+	 * Make things easier on ourselves: all allocations are page-aligned.
+	 * We need that to map allocated regions into the user space
+	 */
+	if (alignment < PAGE_SHIFT)
+		alignment = PAGE_SHIFT;
+
+	/* Align allocation sizes to 64KiB blocks on G8x.  We use a 64KiB
+	 * page size in the GPU VM.
+	 */
+	if (flags & NOUVEAU_MEM_FB && dev_priv->card_type >= NV_50)
+		size = (size + (64 * 1024)) & ~((64 * 1024) - 1);
+
+	/*
+	 * Warn about 0 sized allocations, but let it go through. It'll return 1 page
+	 */
+	if (size == 0)
+		DRM_INFO("warning : 0 byte allocation\n");
+
+	/*
+	 * Keep alloc size a multiple of the page size to keep drm_addmap() happy
+	 */
+	if (size & (~PAGE_MASK))
+		size = ((size/PAGE_SIZE) + 1) * PAGE_SIZE;
+
+
+#define NOUVEAU_MEM_ALLOC_AGP {\
+		type=NOUVEAU_MEM_AGP;\
+                block = nouveau_mem_alloc_block(dev_priv->agp_heap, size,\
+                                                alignment, file_priv); \
+                if (block) goto alloc_ok;\
+	        }
+
+#define NOUVEAU_MEM_ALLOC_PCI {\
+                type = NOUVEAU_MEM_PCI;\
+                block = nouveau_mem_alloc_block(dev_priv->pci_heap, size, \
+						alignment, file_priv); \
+                if ( block ) goto alloc_ok;\
+	        }
+
+#define NOUVEAU_MEM_ALLOC_FB {\
+                type=NOUVEAU_MEM_FB;\
+                if (!(flags&NOUVEAU_MEM_MAPPED)) {\
+                        block = nouveau_mem_alloc_block(dev_priv->fb_nomap_heap,\
+                                                        size, alignment, \
+							file_priv); \
+                        if (block) goto alloc_ok;\
+                }\
+                block = nouveau_mem_alloc_block(dev_priv->fb_heap, size,\
+                                                alignment, file_priv);\
+                if (block) goto alloc_ok;\
+	        }
+
+
+	if (flags&NOUVEAU_MEM_FB) NOUVEAU_MEM_ALLOC_FB
+	if (flags&NOUVEAU_MEM_AGP) NOUVEAU_MEM_ALLOC_AGP
+	if (flags&NOUVEAU_MEM_PCI) NOUVEAU_MEM_ALLOC_PCI
+	if (flags&NOUVEAU_MEM_FB_ACCEPTABLE) NOUVEAU_MEM_ALLOC_FB
+	if (flags&NOUVEAU_MEM_AGP_ACCEPTABLE) NOUVEAU_MEM_ALLOC_AGP
+	if (flags&NOUVEAU_MEM_PCI_ACCEPTABLE) NOUVEAU_MEM_ALLOC_PCI
+
+
+	return NULL;
+
+alloc_ok:
+	block->flags=type;
+
+	/* On G8x, map memory into VM */
+	if (block->flags & NOUVEAU_MEM_FB && dev_priv->card_type >= NV_50 &&
+	    !(flags & NOUVEAU_MEM_NOVM)) {
+		struct nouveau_gpuobj *pt = dev_priv->vm_vram_pt;
+		unsigned offset = block->start;
+		unsigned count = block->size / 65536;
+
+		if (!pt) {
+			DRM_ERROR("vm alloc without vm pt\n");
+			nouveau_mem_free_block(block);
+			return NULL;
+		}
+
+		while (count--) {
+			unsigned pte = offset / 65536;
+
+			INSTANCE_WR(pt, (pte * 2) + 0, offset | 1);
+			INSTANCE_WR(pt, (pte * 2) + 1, 0x00000000);
+			offset += 65536;
+		}
+	} else {
+		block->flags |= NOUVEAU_MEM_NOVM;
+	}	
+
+	if (flags&NOUVEAU_MEM_MAPPED)
+	{
+		struct drm_map_list *entry;
+		int ret = 0;
+		block->flags|=NOUVEAU_MEM_MAPPED;
+
+		if (type == NOUVEAU_MEM_AGP) {
+			if (dev_priv->gart_info.type != NOUVEAU_GART_SGDMA)
+			ret = drm_addmap(dev, block->start, block->size,
+					 _DRM_AGP, 0, &block->map);
+			else
+			ret = drm_addmap(dev, block->start, block->size,
+					 _DRM_SCATTER_GATHER, 0, &block->map);
+		}
+		else if (type == NOUVEAU_MEM_FB)
+			ret = drm_addmap(dev, block->start + dev_priv->fb_phys,
+					 block->size, _DRM_FRAME_BUFFER,
+					 0, &block->map);
+		else if (type == NOUVEAU_MEM_PCI)
+			ret = drm_addmap(dev, block->start, block->size,
+					 _DRM_SCATTER_GATHER, 0, &block->map);
+
+		if (ret) {
+			nouveau_mem_free_block(block);
+			return NULL;
+		}
+
+		entry = drm_find_matching_map(dev, block->map);
+		if (!entry) {
+			nouveau_mem_free_block(block);
+			return NULL;
+		}
+		block->map_handle = entry->user_token;
+	}
+
+	DRM_DEBUG("allocated %lld bytes at 0x%llx type=0x%08x\n", block->size, block->start, block->flags);
+	return block;
+}
+
+void nouveau_mem_free(struct drm_device* dev, struct mem_block* block)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("freeing 0x%llx type=0x%08x\n", block->start, block->flags);
+
+	if (block->flags&NOUVEAU_MEM_MAPPED)
+		drm_rmmap(dev, block->map);
+
+	/* G8x: Remove pages from vm */
+	if (block->flags & NOUVEAU_MEM_FB && dev_priv->card_type >= NV_50 &&
+	    !(block->flags & NOUVEAU_MEM_NOVM)) {
+		struct nouveau_gpuobj *pt = dev_priv->vm_vram_pt;
+		unsigned offset = block->start;
+		unsigned count = block->size / 65536;
+
+		if (!pt) {
+			DRM_ERROR("vm free without vm pt\n");
+			goto out_free;
+		}
+
+		while (count--) {
+			unsigned pte = offset / 65536;
+			INSTANCE_WR(pt, (pte * 2) + 0, 0);
+			INSTANCE_WR(pt, (pte * 2) + 1, 0);
+			offset += 65536;
+		}
+	}
+
+out_free:
+	nouveau_mem_free_block(block);
+}
+
+/*
+ * Ioctls
+ */
+
+int nouveau_ioctl_mem_alloc(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_nouveau_mem_alloc *alloc = data;
+	struct mem_block *block;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+
+	if (alloc->flags & NOUVEAU_MEM_INTERNAL)
+		return -EINVAL;
+
+	block=nouveau_mem_alloc(dev, alloc->alignment, alloc->size,
+				alloc->flags, file_priv);
+	if (!block)
+		return -ENOMEM;
+	alloc->map_handle=block->map_handle;
+	alloc->offset=block->start;
+	alloc->flags=block->flags;
+
+	return 0;
+}
+
+int nouveau_ioctl_mem_free(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_nouveau_mem_free *memfree = data;
+	struct mem_block *block;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+
+	block=NULL;
+	if (memfree->flags & NOUVEAU_MEM_FB)
+		block = find_block(dev_priv->fb_heap, memfree->offset);
+	else if (memfree->flags & NOUVEAU_MEM_AGP)
+		block = find_block(dev_priv->agp_heap, memfree->offset);
+	else if (memfree->flags & NOUVEAU_MEM_PCI)
+		block = find_block(dev_priv->pci_heap, memfree->offset);
+	if (!block)
+		return -EFAULT;
+	if (block->file_priv != file_priv)
+		return -EPERM;
+
+	nouveau_mem_free(dev, block);
+	return 0;
+}
diff --git a/drivers/char/drm/nouveau_notifier.c b/drivers/char/drm/nouveau_notifier.c
new file mode 100644
index 0000000..82c8ab7
--- /dev/null
+++ b/drivers/char/drm/nouveau_notifier.c
@@ -0,0 +1,165 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+int
+nouveau_notifier_init_channel(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	int flags, ret;
+
+	flags = (NOUVEAU_MEM_PCI | NOUVEAU_MEM_MAPPED |
+	         NOUVEAU_MEM_FB_ACCEPTABLE);
+
+	chan->notifier_block = nouveau_mem_alloc(dev, 0, PAGE_SIZE, flags,
+						 (struct drm_file *)-2);
+	if (!chan->notifier_block)
+		return -ENOMEM;
+	DRM_DEBUG("Allocated notifier block in 0x%08x\n",
+		  chan->notifier_block->flags);
+
+	ret = nouveau_mem_init_heap(&chan->notifier_heap,
+				    0, chan->notifier_block->size);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+void
+nouveau_notifier_takedown_channel(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+
+	if (chan->notifier_block) {
+		nouveau_mem_free(dev, chan->notifier_block);
+		chan->notifier_block = NULL;
+	}
+
+	nouveau_mem_takedown(&chan->notifier_heap);
+}
+
+static void
+nouveau_notifier_gpuobj_dtor(struct drm_device *dev,
+			     struct nouveau_gpuobj *gpuobj)
+{
+	DRM_DEBUG("\n");
+
+	if (gpuobj->priv)
+		nouveau_mem_free_block(gpuobj->priv);
+}
+
+int
+nouveau_notifier_alloc(struct nouveau_channel *chan, uint32_t handle,
+		       int count, uint32_t *b_offset)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *nobj = NULL;
+	struct mem_block *mem;
+	uint32_t offset;
+	int target, ret;
+
+	if (!chan->notifier_heap) {
+		DRM_ERROR("Channel %d doesn't have a notifier heap!\n",
+			  chan->id);
+		return -EINVAL;
+	}
+
+	mem = nouveau_mem_alloc_block(chan->notifier_heap, count*32, 0,
+				      (struct drm_file *)-2);
+	if (!mem) {
+		DRM_ERROR("Channel %d notifier block full\n", chan->id);
+		return -ENOMEM;
+	}
+	mem->flags = NOUVEAU_MEM_NOTIFIER;
+
+	offset = chan->notifier_block->start;
+	if (chan->notifier_block->flags & NOUVEAU_MEM_FB) {
+		target = NV_DMA_TARGET_VIDMEM;
+	} else
+	if (chan->notifier_block->flags & NOUVEAU_MEM_AGP) {
+		if (dev_priv->gart_info.type == NOUVEAU_GART_SGDMA &&
+		    dev_priv->card_type < NV_50) {
+			ret = nouveau_sgdma_get_page(dev, offset, &offset);
+			if (ret)
+				return ret;
+			target = NV_DMA_TARGET_PCI;
+		} else {
+			target = NV_DMA_TARGET_AGP;
+		}
+	} else
+	if (chan->notifier_block->flags & NOUVEAU_MEM_PCI) {
+		target = NV_DMA_TARGET_PCI_NONLINEAR;
+	} else {
+		DRM_ERROR("Bad DMA target, flags 0x%08x!\n",
+			  chan->notifier_block->flags);
+		return -EINVAL;
+	}
+	offset += mem->start;
+
+	if ((ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					  offset, mem->size,
+					  NV_DMA_ACCESS_RW, target, &nobj))) {
+		nouveau_mem_free_block(mem);
+		DRM_ERROR("Error creating notifier ctxdma: %d\n", ret);
+		return ret;
+	}
+	nobj->dtor   = nouveau_notifier_gpuobj_dtor;
+	nobj->priv   = mem;
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, chan, handle, nobj, NULL))) {
+		nouveau_gpuobj_del(dev, &nobj);
+		nouveau_mem_free_block(mem);
+		DRM_ERROR("Error referencing notifier ctxdma: %d\n", ret);
+		return ret;
+	}
+
+	*b_offset = mem->start;
+	return 0;
+}
+
+int
+nouveau_ioctl_notifier_alloc(struct drm_device *dev, void *data,
+			     struct drm_file *file_priv)
+{
+	struct drm_nouveau_notifierobj_alloc *na = data;
+	struct nouveau_channel *chan;
+	int ret;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+	NOUVEAU_GET_USER_CHANNEL_WITH_RETURN(na->channel, file_priv, chan);
+
+	ret = nouveau_notifier_alloc(chan, na->handle, na->count, &na->offset);
+	if (ret)
+		return ret;
+
+	return 0;
+}
diff --git a/drivers/char/drm/nouveau_object.c b/drivers/char/drm/nouveau_object.c
new file mode 100644
index 0000000..df534bd
--- /dev/null
+++ b/drivers/char/drm/nouveau_object.c
@@ -0,0 +1,1172 @@
+/*
+ * Copyright (C) 2006 Ben Skeggs.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/*
+ * Authors:
+ *   Ben Skeggs <darktama@iinet.net.au>
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+/* NVidia uses context objects to drive drawing operations.
+
+   Context objects can be selected into 8 subchannels in the FIFO,
+   and then used via DMA command buffers.
+
+   A context object is referenced by a user defined handle (CARD32). The HW
+   looks up graphics objects in a hash table in the instance RAM.
+
+   An entry in the hash table consists of 2 CARD32. The first CARD32 contains
+   the handle, the second one a bitfield, that contains the address of the
+   object in instance RAM.
+
+   The format of the second CARD32 seems to be:
+
+   NV4 to NV30:
+
+   15: 0  instance_addr >> 4
+   17:16  engine (here uses 1 = graphics)
+   28:24  channel id (here uses 0)
+   31	  valid (use 1)
+
+   NV40:
+
+   15: 0  instance_addr >> 4   (maybe 19-0)
+   21:20  engine (here uses 1 = graphics)
+   I'm unsure about the other bits, but using 0 seems to work.
+
+   The key into the hash table depends on the object handle and channel id and
+   is given as:
+*/
+static uint32_t
+nouveau_ramht_hash_handle(struct drm_device *dev, int channel, uint32_t handle)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+	uint32_t hash = 0;
+	int i;
+
+	DRM_DEBUG("ch%d handle=0x%08x\n", channel, handle);
+
+	for (i=32;i>0;i-=dev_priv->ramht_bits) {
+		hash ^= (handle & ((1 << dev_priv->ramht_bits) - 1));
+		handle >>= dev_priv->ramht_bits;
+	}
+	if (dev_priv->card_type < NV_50)
+		hash ^= channel << (dev_priv->ramht_bits - 4);
+	hash <<= 3;
+
+	DRM_DEBUG("hash=0x%08x\n", hash);
+	return hash;
+}
+
+static int
+nouveau_ramht_entry_valid(struct drm_device *dev, struct nouveau_gpuobj *ramht,
+			  uint32_t offset)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+	uint32_t ctx = INSTANCE_RD(ramht, (offset + 4)/4);
+
+	if (dev_priv->card_type < NV_40)
+		return ((ctx & NV_RAMHT_CONTEXT_VALID) != 0);
+	return (ctx != 0);
+}
+
+static int
+nouveau_ramht_insert(struct drm_device *dev, struct nouveau_gpuobj_ref *ref)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+	struct nouveau_channel *chan = dev_priv->fifos[ref->channel];
+	struct nouveau_gpuobj *ramht = chan->ramht ? chan->ramht->gpuobj : NULL;
+	struct nouveau_gpuobj *gpuobj = ref->gpuobj;
+	uint32_t ctx, co, ho;
+
+	if (!ramht) {
+		DRM_ERROR("No hash table!\n");
+		return -EINVAL;
+	}
+
+	if (dev_priv->card_type < NV_40) {
+		ctx = NV_RAMHT_CONTEXT_VALID | (ref->instance >> 4) |
+		      (ref->channel   << NV_RAMHT_CONTEXT_CHANNEL_SHIFT) |
+		      (gpuobj->engine << NV_RAMHT_CONTEXT_ENGINE_SHIFT);
+	} else
+	if (dev_priv->card_type < NV_50) {
+		ctx = (ref->instance >> 4) |
+		      (ref->channel   << NV40_RAMHT_CONTEXT_CHANNEL_SHIFT) |
+		      (gpuobj->engine << NV40_RAMHT_CONTEXT_ENGINE_SHIFT);
+	} else {
+		ctx = (ref->instance  >> 4) |
+		      (gpuobj->engine << NV40_RAMHT_CONTEXT_ENGINE_SHIFT);
+	}
+
+	co = ho = nouveau_ramht_hash_handle(dev, ref->channel, ref->handle);
+	do {
+		if (!nouveau_ramht_entry_valid(dev, ramht, co)) {
+			DRM_DEBUG("insert ch%d 0x%08x: h=0x%08x, c=0x%08x\n",
+				  ref->channel, co, ref->handle, ctx);
+			INSTANCE_WR(ramht, (co + 0)/4, ref->handle);
+			INSTANCE_WR(ramht, (co + 4)/4, ctx);
+
+			list_add_tail(&ref->list, &chan->ramht_refs);
+			return 0;
+		}
+		DRM_DEBUG("collision ch%d 0x%08x: h=0x%08x\n",
+			  ref->channel, co, INSTANCE_RD(ramht, co/4));
+
+		co += 8;
+		if (co >= dev_priv->ramht_size) {
+			DRM_INFO("no space left after collision\n");
+			co = 0;
+			/* exit as it seems to cause crash with nouveau_demo and
+			 * 0xdead0001 object */
+			break;
+		}
+	} while (co != ho);
+
+	DRM_ERROR("RAMHT space exhausted. ch=%d\n", ref->channel);
+	return -ENOMEM;
+}
+
+static void
+nouveau_ramht_remove(struct drm_device *dev, struct nouveau_gpuobj_ref *ref)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_channel *chan = dev_priv->fifos[ref->channel];
+	struct nouveau_gpuobj *ramht = chan->ramht ? chan->ramht->gpuobj : NULL;
+	uint32_t co, ho;
+
+	if (!ramht) {
+		DRM_ERROR("No hash table!\n");
+		return;
+	}
+
+	co = ho = nouveau_ramht_hash_handle(dev, ref->channel, ref->handle);
+	do {
+		if (nouveau_ramht_entry_valid(dev, ramht, co) &&
+		    (ref->handle == INSTANCE_RD(ramht, (co/4)))) {
+			DRM_DEBUG("remove ch%d 0x%08x: h=0x%08x, c=0x%08x\n",
+				  ref->channel, co, ref->handle,
+				  INSTANCE_RD(ramht, (co + 4)));
+			INSTANCE_WR(ramht, (co + 0)/4, 0x00000000);
+			INSTANCE_WR(ramht, (co + 4)/4, 0x00000000);
+
+			list_del(&ref->list);
+			return;
+		}
+
+		co += 8;
+		if (co >= dev_priv->ramht_size)
+			co = 0;
+	} while (co != ho);
+
+	DRM_ERROR("RAMHT entry not found. ch=%d, handle=0x%08x\n",
+		  ref->channel, ref->handle);
+}
+
+int
+nouveau_gpuobj_new(struct drm_device *dev, struct nouveau_channel *chan,
+		   int size, int align, uint32_t flags,
+		   struct nouveau_gpuobj **gpuobj_ret)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct nouveau_gpuobj *gpuobj;
+	struct mem_block *pramin = NULL;
+	int ret;
+
+	DRM_DEBUG("ch%d size=%d align=%d flags=0x%08x\n",
+		  chan ? chan->id : -1, size, align, flags);
+
+	if (!dev_priv || !gpuobj_ret || *gpuobj_ret != NULL)
+		return -EINVAL;
+
+	gpuobj = drm_calloc(1, sizeof(*gpuobj), DRM_MEM_DRIVER);
+	if (!gpuobj)
+		return -ENOMEM;
+	DRM_DEBUG("gpuobj %p\n", gpuobj);
+	gpuobj->flags = flags;
+	gpuobj->im_channel = chan ? chan->id : -1;
+
+	list_add_tail(&gpuobj->list, &dev_priv->gpuobj_list);
+
+	/* Choose between global instmem heap, and per-channel private
+	 * instmem heap.  On <NV50 allow requests for private instmem
+	 * to be satisfied from global heap if no per-channel area
+	 * available.
+	 */
+	if (chan) {
+		if (chan->ramin_heap) {
+			DRM_DEBUG("private heap\n");
+			pramin = chan->ramin_heap;
+		} else
+		if (dev_priv->card_type < NV_50) {
+			DRM_DEBUG("global heap fallback\n");
+			pramin = dev_priv->ramin_heap;
+		}
+	} else {
+		DRM_DEBUG("global heap\n");
+		pramin = dev_priv->ramin_heap;
+	}
+
+	if (!pramin) {
+		DRM_ERROR("No PRAMIN heap!\n");
+		return -EINVAL;
+	}
+
+	if (!chan && (ret = engine->instmem.populate(dev, gpuobj, &size))) {
+		nouveau_gpuobj_del(dev, &gpuobj);
+		return ret;
+	}
+
+	/* Allocate a chunk of the PRAMIN aperture */
+	gpuobj->im_pramin = nouveau_mem_alloc_block(pramin, size,
+						    drm_order(align),
+						    (struct drm_file *)-2);
+	if (!gpuobj->im_pramin) {
+		nouveau_gpuobj_del(dev, &gpuobj);
+		return -ENOMEM;
+	}
+	gpuobj->im_pramin->flags = NOUVEAU_MEM_INSTANCE;
+
+	if (!chan && (ret = engine->instmem.bind(dev, gpuobj))) {
+		nouveau_gpuobj_del(dev, &gpuobj);
+		return ret;
+	}
+
+	if (gpuobj->flags & NVOBJ_FLAG_ZERO_ALLOC) {
+		int i;
+
+		for (i = 0; i < gpuobj->im_pramin->size; i += 4)
+			INSTANCE_WR(gpuobj, i/4, 0);
+	}
+
+	*gpuobj_ret = gpuobj;
+	return 0;
+}
+
+int
+nouveau_gpuobj_early_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	INIT_LIST_HEAD(&dev_priv->gpuobj_list);
+
+	return 0;
+}
+
+int
+nouveau_gpuobj_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	DRM_DEBUG("\n");
+
+	if (dev_priv->card_type < NV_50) {
+		if ((ret = nouveau_gpuobj_new_fake(dev, dev_priv->ramht_offset,
+						   ~0, dev_priv->ramht_size,
+						   NVOBJ_FLAG_ZERO_ALLOC |
+						   NVOBJ_FLAG_ALLOW_NO_REFS,
+						   &dev_priv->ramht, NULL)))
+			return ret;
+	}
+
+	return 0;
+}
+
+void
+nouveau_gpuobj_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	nouveau_gpuobj_del(dev, &dev_priv->ramht);
+}
+
+void
+nouveau_gpuobj_late_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *gpuobj = NULL;
+	struct list_head *entry, *tmp;
+
+	DRM_DEBUG("\n");
+
+	list_for_each_safe(entry, tmp, &dev_priv->gpuobj_list) {
+		gpuobj = list_entry(entry, struct nouveau_gpuobj, list);
+
+		DRM_ERROR("gpuobj %p still exists at takedown, refs=%d\n",
+			  gpuobj, gpuobj->refcount);
+		gpuobj->refcount = 0;
+		nouveau_gpuobj_del(dev, &gpuobj);
+	}
+}
+
+int
+nouveau_gpuobj_del(struct drm_device *dev, struct nouveau_gpuobj **pgpuobj)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct nouveau_gpuobj *gpuobj;
+
+	DRM_DEBUG("gpuobj %p\n", pgpuobj ? *pgpuobj : NULL);
+
+	if (!dev_priv || !pgpuobj || !(*pgpuobj))
+		return -EINVAL;
+	gpuobj = *pgpuobj;
+
+	if (gpuobj->refcount != 0) {
+		DRM_ERROR("gpuobj refcount is %d\n", gpuobj->refcount);
+		return -EINVAL;
+	}
+
+	if (gpuobj->dtor)
+		gpuobj->dtor(dev, gpuobj);
+
+	if (gpuobj->im_backing) {
+		if (gpuobj->flags & NVOBJ_FLAG_FAKE)
+			drm_free(gpuobj->im_backing,
+				 sizeof(*gpuobj->im_backing), DRM_MEM_DRIVER);
+		else
+			engine->instmem.clear(dev, gpuobj);
+	}
+
+	if (gpuobj->im_pramin) {
+		if (gpuobj->flags & NVOBJ_FLAG_FAKE)
+			drm_free(gpuobj->im_pramin, sizeof(*gpuobj->im_pramin),
+				 DRM_MEM_DRIVER);
+		else
+			nouveau_mem_free_block(gpuobj->im_pramin);
+	}
+
+	list_del(&gpuobj->list);
+
+	*pgpuobj = NULL;
+	drm_free(gpuobj, sizeof(*gpuobj), DRM_MEM_DRIVER);
+	return 0;
+}
+
+static int
+nouveau_gpuobj_instance_get(struct drm_device *dev,
+			    struct nouveau_channel *chan,
+			    struct nouveau_gpuobj *gpuobj, uint32_t *inst)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *cpramin;
+
+	/* <NV50 use PRAMIN address everywhere */
+	if (dev_priv->card_type < NV_50) {
+		*inst = gpuobj->im_pramin->start;
+		return 0;
+	}
+
+	if (chan && gpuobj->im_channel != chan->id) {
+		DRM_ERROR("Channel mismatch: obj %d, ref %d\n",
+			  gpuobj->im_channel, chan->id);
+		return -EINVAL;
+	}
+
+	/* NV50 channel-local instance */
+	if (chan > 0) {
+		cpramin = chan->ramin->gpuobj;
+		*inst = gpuobj->im_pramin->start - cpramin->im_pramin->start;
+		return 0;
+	}
+
+	/* NV50 global (VRAM) instance */
+	if (gpuobj->im_channel < 0) {
+		/* ...from global heap */
+		if (!gpuobj->im_backing) {
+			DRM_ERROR("AII, no VRAM backing gpuobj\n");
+			return -EINVAL;
+		}
+		*inst = gpuobj->im_backing->start;
+		return 0;
+	} else {
+		/* ...from local heap */
+		cpramin = dev_priv->fifos[gpuobj->im_channel]->ramin->gpuobj;
+		*inst = cpramin->im_backing->start +
+			(gpuobj->im_pramin->start - cpramin->im_pramin->start);
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+int
+nouveau_gpuobj_ref_add(struct drm_device *dev, struct nouveau_channel *chan,
+		       uint32_t handle, struct nouveau_gpuobj *gpuobj,
+		       struct nouveau_gpuobj_ref **ref_ret)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj_ref *ref;
+	uint32_t instance;
+	int ret;
+
+	DRM_DEBUG("ch%d h=0x%08x gpuobj=%p\n",
+		  chan ? chan->id : -1, handle, gpuobj);
+
+	if (!dev_priv || !gpuobj || (ref_ret && *ref_ret != NULL))
+		return -EINVAL;
+
+	if (!chan && !ref_ret)
+		return -EINVAL;
+
+	ret = nouveau_gpuobj_instance_get(dev, chan, gpuobj, &instance);
+	if (ret)
+		return ret;
+
+	ref = drm_calloc(1, sizeof(*ref), DRM_MEM_DRIVER);
+	if (!ref)
+		return -ENOMEM;
+	ref->gpuobj   = gpuobj;
+	ref->channel  = chan ? chan->id : -1;
+	ref->instance = instance;
+
+	if (!ref_ret) {
+		ref->handle = handle;
+
+		ret = nouveau_ramht_insert(dev, ref);
+		if (ret) {
+			drm_free(ref, sizeof(*ref), DRM_MEM_DRIVER);
+			return ret;
+		}
+	} else {
+		ref->handle = ~0;
+		*ref_ret = ref;
+	}
+
+	ref->gpuobj->refcount++;
+	return 0;
+}
+
+int nouveau_gpuobj_ref_del(struct drm_device *dev, struct nouveau_gpuobj_ref **pref)
+{
+	struct nouveau_gpuobj_ref *ref;
+
+	DRM_DEBUG("ref %p\n", pref ? *pref : NULL);
+
+	if (!dev || !pref || *pref == NULL)
+		return -EINVAL;
+	ref = *pref;
+
+	if (ref->handle != ~0)
+		nouveau_ramht_remove(dev, ref);
+
+	if (ref->gpuobj) {
+		ref->gpuobj->refcount--;
+
+		if (ref->gpuobj->refcount == 0) {
+			if (!(ref->gpuobj->flags & NVOBJ_FLAG_ALLOW_NO_REFS))
+				nouveau_gpuobj_del(dev, &ref->gpuobj);
+		}
+	}
+
+	*pref = NULL;
+	drm_free(ref, sizeof(ref), DRM_MEM_DRIVER);
+	return 0;
+}
+
+int
+nouveau_gpuobj_new_ref(struct drm_device *dev,
+		       struct nouveau_channel *oc, struct nouveau_channel *rc,
+		       uint32_t handle, int size, int align, uint32_t flags,
+		       struct nouveau_gpuobj_ref **ref)
+{
+	struct nouveau_gpuobj *gpuobj = NULL;
+	int ret;
+
+	if ((ret = nouveau_gpuobj_new(dev, oc, size, align, flags, &gpuobj)))
+		return ret;
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, rc, handle, gpuobj, ref))) {
+		nouveau_gpuobj_del(dev, &gpuobj);
+		return ret;
+	}
+
+	return 0;
+}
+
+int
+nouveau_gpuobj_ref_find(struct nouveau_channel *chan, uint32_t handle,
+			struct nouveau_gpuobj_ref **ref_ret)
+{
+	struct nouveau_gpuobj_ref *ref;
+	struct list_head *entry, *tmp;
+
+	list_for_each_safe(entry, tmp, &chan->ramht_refs) {		
+		ref = list_entry(entry, struct nouveau_gpuobj_ref, list);
+
+		if (ref->handle == handle) {
+			if (ref_ret)
+				*ref_ret = ref;
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+int
+nouveau_gpuobj_new_fake(struct drm_device *dev, uint32_t p_offset,
+			uint32_t b_offset, uint32_t size,
+			uint32_t flags, struct nouveau_gpuobj **pgpuobj,
+			struct nouveau_gpuobj_ref **pref)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *gpuobj = NULL;
+	int i;
+
+	DRM_DEBUG("p_offset=0x%08x b_offset=0x%08x size=0x%08x flags=0x%08x\n",
+		  p_offset, b_offset, size, flags);
+
+	gpuobj = drm_calloc(1, sizeof(*gpuobj), DRM_MEM_DRIVER);
+	if (!gpuobj)
+		return -ENOMEM;
+	DRM_DEBUG("gpuobj %p\n", gpuobj);
+	gpuobj->im_channel = -1;
+	gpuobj->flags      = flags | NVOBJ_FLAG_FAKE;
+
+	list_add_tail(&gpuobj->list, &dev_priv->gpuobj_list);
+
+	if (p_offset != ~0) {
+		gpuobj->im_pramin = drm_calloc(1, sizeof(struct mem_block),
+					       DRM_MEM_DRIVER);
+		if (!gpuobj->im_pramin) {
+			nouveau_gpuobj_del(dev, &gpuobj);
+			return -ENOMEM;
+		}
+		gpuobj->im_pramin->start = p_offset;
+		gpuobj->im_pramin->size  = size;
+	}
+
+	if (b_offset != ~0) {
+		gpuobj->im_backing = drm_calloc(1, sizeof(struct mem_block),
+					       DRM_MEM_DRIVER);
+		if (!gpuobj->im_backing) {
+			nouveau_gpuobj_del(dev, &gpuobj);
+			return -ENOMEM;
+		}
+		gpuobj->im_backing->start = b_offset;
+		gpuobj->im_backing->size  = size;
+	}
+
+	if (gpuobj->flags & NVOBJ_FLAG_ZERO_ALLOC) {
+		for (i = 0; i < gpuobj->im_pramin->size; i += 4)
+			INSTANCE_WR(gpuobj, i/4, 0);
+	}
+
+	if (pref) {
+		if ((i = nouveau_gpuobj_ref_add(dev, NULL, 0, gpuobj, pref))) {
+			nouveau_gpuobj_del(dev, &gpuobj);
+			return i;
+		}
+	}
+
+	if (pgpuobj)
+		*pgpuobj = gpuobj;
+	return 0;
+}
+
+
+static int
+nouveau_gpuobj_class_instmem_size(struct drm_device *dev, int class)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/*XXX: dodgy hack for now */
+	if (dev_priv->card_type >= NV_50)
+		return 24;
+	if (dev_priv->card_type >= NV_40)
+		return 32;
+	return 16;
+}
+
+/*
+   DMA objects are used to reference a piece of memory in the
+   framebuffer, PCI or AGP address space. Each object is 16 bytes big
+   and looks as follows:
+   
+   entry[0]
+   11:0  class (seems like I can always use 0 here)
+   12    page table present?
+   13    page entry linear?
+   15:14 access: 0 rw, 1 ro, 2 wo
+   17:16 target: 0 NV memory, 1 NV memory tiled, 2 PCI, 3 AGP
+   31:20 dma adjust (bits 0-11 of the address)
+   entry[1]
+   dma limit (size of transfer)
+   entry[X]
+   1     0 readonly, 1 readwrite
+   31:12 dma frame address of the page (bits 12-31 of the address)
+   entry[N]
+   page table terminator, same value as the first pte, as does nvidia
+   rivatv uses 0xffffffff
+
+   Non linear page tables need a list of frame addresses afterwards,
+   the rivatv project has some info on this.
+
+   The method below creates a DMA object in instance RAM and returns a handle
+   to it that can be used to set up context objects.
+*/
+int
+nouveau_gpuobj_dma_new(struct nouveau_channel *chan, int class,
+		       uint64_t offset, uint64_t size, int access,
+		       int target, struct nouveau_gpuobj **gpuobj)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+	uint32_t is_scatter_gather = 0;
+	
+	/* Total number of pages covered by the request.
+	 */
+	const unsigned int page_count = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+
+
+	DRM_DEBUG("ch%d class=0x%04x offset=0x%llx size=0x%llx\n",
+		  chan->id, class, offset, size);
+	DRM_DEBUG("access=%d target=%d\n", access, target);
+
+	switch (target) {
+        case NV_DMA_TARGET_AGP:
+                 offset += dev_priv->gart_info.aper_base;
+                 break;
+        case NV_DMA_TARGET_PCI_NONLINEAR:
+                /*assume the "offset" is a virtual memory address*/
+                is_scatter_gather = 1;
+                /*put back the right value*/
+                target = NV_DMA_TARGET_PCI;
+                break;
+        default:
+                break;
+        }
+	
+	ret = nouveau_gpuobj_new(dev, chan,
+				 is_scatter_gather ? ((page_count << 2) + 12) : nouveau_gpuobj_class_instmem_size(dev, class),
+				 16,
+				 NVOBJ_FLAG_ZERO_ALLOC | NVOBJ_FLAG_ZERO_FREE,
+				 gpuobj);
+	if (ret) {
+		DRM_ERROR("Error creating gpuobj: %d\n", ret);
+		return ret;
+	}
+
+	if (dev_priv->card_type < NV_50) {
+		uint32_t frame, adjust, pte_flags = 0;
+		adjust = offset &  0x00000fff;
+		if (access != NV_DMA_ACCESS_RO)
+				pte_flags |= (1<<1);
+		
+		if ( ! is_scatter_gather ) 
+			{
+			frame  = offset & ~0x00000fff;
+			
+			INSTANCE_WR(*gpuobj, 0, ((1<<12) | (1<<13) |
+					(adjust << 20) |
+					 (access << 14) |
+					 (target << 16) |
+					  class));
+			INSTANCE_WR(*gpuobj, 1, size - 1);
+			INSTANCE_WR(*gpuobj, 2, frame | pte_flags);
+			INSTANCE_WR(*gpuobj, 3, frame | pte_flags);
+			}
+		else 
+			{
+			/* Intial page entry in the scatter-gather area that
+			 * corresponds to the base offset
+			 */
+			unsigned int idx = offset / PAGE_SIZE;
+
+			uint32_t instance_offset;
+			unsigned int i;
+
+			if ((idx + page_count) > dev->sg->pages) {
+				DRM_ERROR("Requested page range exceedes "
+					  "allocated scatter-gather range!");
+				return -E2BIG;
+			}
+
+			DRM_DEBUG("Creating PCI DMA object using virtual zone starting at %#llx, size %d\n", offset, (uint32_t)size);
+	                INSTANCE_WR(*gpuobj, 0, ((1<<12) | (0<<13) |
+                                (adjust << 20) |
+                                (access << 14) |
+                                (target << 16) |
+                                class));
+			INSTANCE_WR(*gpuobj, 1, (uint32_t) size-1);
+
+
+			/*write starting at the third dword*/
+			instance_offset = 2;
+ 
+			/*for each PAGE, get its bus address, fill in the page table entry, and advance*/
+			for (i = 0; i < page_count; i++) {
+				if (dev->sg->busaddr[idx] == 0) {
+					dev->sg->busaddr[idx] =
+						pci_map_page(dev->pdev,
+							     dev->sg->pagelist[idx],
+							     0,
+							     PAGE_SIZE,
+							     DMA_BIDIRECTIONAL);
+
+					if (dma_mapping_error(dev->sg->busaddr[idx])) {
+						return -ENOMEM;
+					}
+				}
+
+				frame = (uint32_t) dev->sg->busaddr[idx];
+				INSTANCE_WR(*gpuobj, instance_offset, 
+					    frame | pte_flags);
+ 
+				idx++;
+				instance_offset ++;
+ 			}
+			}
+	} else {
+		uint32_t flags0, flags5;
+
+		if (target == NV_DMA_TARGET_VIDMEM) {
+			flags0 = 0x00190000;
+			flags5 = 0x00010000;
+		} else {
+			flags0 = 0x7fc00000;
+			flags5 = 0x00080000;
+		}
+
+		INSTANCE_WR(*gpuobj, 0, flags0 | class);
+		INSTANCE_WR(*gpuobj, 1, offset + size - 1);
+		INSTANCE_WR(*gpuobj, 2, offset);
+		INSTANCE_WR(*gpuobj, 5, flags5);
+	}
+
+	(*gpuobj)->engine = NVOBJ_ENGINE_SW;
+	(*gpuobj)->class  = class;
+	return 0;
+}
+
+int
+nouveau_gpuobj_gart_dma_new(struct nouveau_channel *chan,
+			    uint64_t offset, uint64_t size, int access,
+			    struct nouveau_gpuobj **gpuobj,
+			    uint32_t *o_ret)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if (dev_priv->gart_info.type == NOUVEAU_GART_AGP ||
+	    (dev_priv->card_type >= NV_50 &&
+	     dev_priv->gart_info.type == NOUVEAU_GART_SGDMA)) {
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     offset, size, access,
+					     NV_DMA_TARGET_AGP, gpuobj);
+		if (o_ret)
+			*o_ret = 0;
+	} else
+	if (dev_priv->gart_info.type == NOUVEAU_GART_SGDMA) {
+		*gpuobj = dev_priv->gart_info.sg_ctxdma;
+		if (offset & ~0xffffffffULL) {
+			DRM_ERROR("obj offset exceeds 32-bits\n");
+			return -EINVAL;
+		}
+		if (o_ret)
+			*o_ret = (uint32_t)offset;
+		ret = (*gpuobj != NULL) ? 0 : -EINVAL;
+	} else {
+		DRM_ERROR("Invalid GART type %d\n", dev_priv->gart_info.type);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+/* Context objects in the instance RAM have the following structure.
+ * On NV40 they are 32 byte long, on NV30 and smaller 16 bytes.
+
+   NV4 - NV30:
+
+   entry[0]
+   11:0 class
+   12   chroma key enable
+   13   user clip enable
+   14   swizzle enable
+   17:15 patch config:
+       scrcopy_and, rop_and, blend_and, scrcopy, srccopy_pre, blend_pre
+   18   synchronize enable
+   19   endian: 1 big, 0 little
+   21:20 dither mode
+   23    single step enable
+   24    patch status: 0 invalid, 1 valid
+   25    context_surface 0: 1 valid
+   26    context surface 1: 1 valid
+   27    context pattern: 1 valid
+   28    context rop: 1 valid
+   29,30 context beta, beta4
+   entry[1]
+   7:0   mono format
+   15:8  color format
+   31:16 notify instance address
+   entry[2]
+   15:0  dma 0 instance address
+   31:16 dma 1 instance address
+   entry[3]
+   dma method traps
+
+   NV40:
+   No idea what the exact format is. Here's what can be deducted:
+
+   entry[0]:
+   11:0  class  (maybe uses more bits here?)
+   17    user clip enable
+   21:19 patch config 
+   25    patch status valid ?
+   entry[1]:
+   15:0  DMA notifier  (maybe 20:0)
+   entry[2]:
+   15:0  DMA 0 instance (maybe 20:0)
+   24    big endian
+   entry[3]:
+   15:0  DMA 1 instance (maybe 20:0)
+   entry[4]:
+   entry[5]:
+   set to 0?
+*/
+int
+nouveau_gpuobj_gr_new(struct nouveau_channel *chan, int class,
+		      struct nouveau_gpuobj **gpuobj)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	DRM_DEBUG("ch%d class=0x%04x\n", chan->id, class);
+
+	ret = nouveau_gpuobj_new(dev, chan,
+				 nouveau_gpuobj_class_instmem_size(dev, class),
+				 16,
+				 NVOBJ_FLAG_ZERO_ALLOC | NVOBJ_FLAG_ZERO_FREE,
+				 gpuobj);
+	if (ret) {
+		DRM_ERROR("Error creating gpuobj: %d\n", ret);
+		return ret;
+	}
+
+	if (dev_priv->card_type >= NV_50) {
+		INSTANCE_WR(*gpuobj, 0, class);
+		INSTANCE_WR(*gpuobj, 5, 0x00010000);
+	} else {
+	switch (class) {
+	case NV_CLASS_NULL:
+		INSTANCE_WR(*gpuobj, 0, 0x00001030);
+		INSTANCE_WR(*gpuobj, 1, 0xFFFFFFFF);
+		break;
+	default:
+		if (dev_priv->card_type >= NV_40) {
+			INSTANCE_WR(*gpuobj, 0, class);
+#ifdef __BIG_ENDIAN
+			INSTANCE_WR(*gpuobj, 2, 0x01000000);
+#endif
+		} else {
+#ifdef __BIG_ENDIAN
+			INSTANCE_WR(*gpuobj, 0, class | 0x00080000);
+#else
+			INSTANCE_WR(*gpuobj, 0, class);
+#endif
+		}
+	}
+	}
+
+	(*gpuobj)->engine = NVOBJ_ENGINE_GR;
+	(*gpuobj)->class  = class;
+	return 0;
+}
+
+static int
+nouveau_gpuobj_channel_init_pramin(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *pramin = NULL;
+	int size, base, ret;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	/* Base amount for object storage (4KiB enough?) */
+	size = 0x1000;
+	base = 0;
+
+	/* PGRAPH context */
+
+	if (dev_priv->card_type == NV_50) {
+		/* Various fixed table thingos */
+		size += 0x1400; /* mostly unknown stuff */
+		size += 0x4000; /* vm pd */
+		base  = 0x6000;
+		/* RAMHT, not sure about setting size yet, 32KiB to be safe */
+		size += 0x8000;
+		/* RAMFC */
+		size += 0x1000;
+		/* PGRAPH context */
+		size += 0x60000;
+	}
+
+	DRM_DEBUG("ch%d PRAMIN size: 0x%08x bytes, base alloc=0x%08x\n",
+		  chan->id, size, base);
+	ret = nouveau_gpuobj_new_ref(dev, NULL, NULL, 0, size, 0x1000, 0,
+				     &chan->ramin);
+	if (ret) {
+		DRM_ERROR("Error allocating channel PRAMIN: %d\n", ret);
+		return ret;
+	}
+	pramin = chan->ramin->gpuobj;
+
+	ret = nouveau_mem_init_heap(&chan->ramin_heap,
+				    pramin->im_pramin->start + base, size);
+	if (ret) {
+		DRM_ERROR("Error creating PRAMIN heap: %d\n", ret);
+		nouveau_gpuobj_ref_del(dev, &chan->ramin);
+		return ret;
+	}
+
+	return 0;
+}
+
+int
+nouveau_gpuobj_channel_init(struct nouveau_channel *chan,
+			    uint32_t vram_h, uint32_t tt_h)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *vram = NULL, *tt = NULL;
+	int ret, i;
+
+	INIT_LIST_HEAD(&chan->ramht_refs);
+
+	DRM_DEBUG("ch%d vram=0x%08x tt=0x%08x\n", chan->id, vram_h, tt_h);
+
+	/* Reserve a block of PRAMIN for the channel
+	 *XXX: maybe on <NV50 too at some point
+	 */
+	if (0 || dev_priv->card_type == NV_50) {
+		ret = nouveau_gpuobj_channel_init_pramin(chan);
+		if (ret)
+			return ret;
+	}
+
+	/* NV50 VM
+	 *  - Allocate per-channel page-directory
+	 *  - Point offset 0-512MiB at shared PCIEGART table
+	 *  - Point offset 512-1024MiB at shared VRAM table
+	 */
+	if (dev_priv->card_type >= NV_50) {
+		uint32_t vm_offset;
+
+		vm_offset = (dev_priv->chipset & 0xf0) == 0x50 ? 0x1400 : 0x200;
+		vm_offset += chan->ramin->gpuobj->im_pramin->start;
+		if ((ret = nouveau_gpuobj_new_fake(dev, vm_offset, ~0, 0x4000,
+						   0, &chan->vm_pd, NULL)))
+			return ret;
+		for (i=0; i<0x4000; i+=8) {
+			INSTANCE_WR(chan->vm_pd, (i+0)/4, 0x00000000);
+			INSTANCE_WR(chan->vm_pd, (i+4)/4, 0xdeadcafe);
+		}
+
+		if ((ret = nouveau_gpuobj_ref_add(dev, NULL, 0,
+						  dev_priv->gart_info.sg_ctxdma,
+						  &chan->vm_gart_pt)))
+			return ret;
+		INSTANCE_WR(chan->vm_pd, (0+0)/4,
+			    chan->vm_gart_pt->instance | 0x03);
+		INSTANCE_WR(chan->vm_pd, (0+4)/4, 0x00000000);
+
+		if ((ret = nouveau_gpuobj_ref_add(dev, NULL, 0,
+						  dev_priv->vm_vram_pt,
+						  &chan->vm_vram_pt)))
+			return ret;
+		INSTANCE_WR(chan->vm_pd, (8+0)/4,
+			    chan->vm_vram_pt->instance | 0x61);
+		INSTANCE_WR(chan->vm_pd, (8+4)/4, 0x00000000);
+	}
+
+	/* RAMHT */
+	if (dev_priv->card_type < NV_50) {
+		ret = nouveau_gpuobj_ref_add(dev, NULL, 0, dev_priv->ramht,
+					     &chan->ramht);
+		if (ret)
+			return ret;
+	} else {
+		ret = nouveau_gpuobj_new_ref(dev, chan, chan, 0,
+					     0x8000, 16,
+					     NVOBJ_FLAG_ZERO_ALLOC,
+					     &chan->ramht);
+		if (ret)
+			return ret;
+	}
+
+	/* VRAM ctxdma */
+	if (dev_priv->card_type >= NV_50) {
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     512*1024*1024,
+					     dev_priv->fb_available_size,
+					     NV_DMA_ACCESS_RW,
+					     NV_DMA_TARGET_AGP, &vram);
+		if (ret) {
+			DRM_ERROR("Error creating VRAM ctxdma: %d\n", ret);
+			return ret;
+		}
+	} else
+	if ((ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					  0, dev_priv->fb_available_size,
+					  NV_DMA_ACCESS_RW,
+					  NV_DMA_TARGET_VIDMEM, &vram))) {
+		DRM_ERROR("Error creating VRAM ctxdma: %d\n", ret);
+		return ret;
+	}
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, chan, vram_h, vram, NULL))) {
+		DRM_ERROR("Error referencing VRAM ctxdma: %d\n", ret);
+		return ret;
+	}
+
+	/* TT memory ctxdma */
+	if (dev_priv->gart_info.type != NOUVEAU_GART_NONE) {
+		ret = nouveau_gpuobj_gart_dma_new(chan, 0,
+						  dev_priv->gart_info.aper_size,
+						  NV_DMA_ACCESS_RW, &tt, NULL);
+	} else
+	if (dev_priv->pci_heap) {
+		ret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_IN_MEMORY,
+					     0, dev->sg->pages * PAGE_SIZE,
+					     NV_DMA_ACCESS_RW,
+					     NV_DMA_TARGET_PCI_NONLINEAR, &tt);
+	} else {
+		DRM_ERROR("Invalid GART type %d\n", dev_priv->gart_info.type);
+		ret = -EINVAL;
+	}
+
+	if (ret) {
+		DRM_ERROR("Error creating TT ctxdma: %d\n", ret);
+		return ret;
+	}
+
+	ret = nouveau_gpuobj_ref_add(dev, chan, tt_h, tt, NULL);
+	if (ret) {
+		DRM_ERROR("Error referencing TT ctxdma: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+void
+nouveau_gpuobj_channel_takedown(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct list_head *entry, *tmp;
+	struct nouveau_gpuobj_ref *ref;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	list_for_each_safe(entry, tmp, &chan->ramht_refs) {		
+		ref = list_entry(entry, struct nouveau_gpuobj_ref, list);
+
+		nouveau_gpuobj_ref_del(dev, &ref);
+	}
+
+	nouveau_gpuobj_ref_del(dev, &chan->ramht);
+
+	nouveau_gpuobj_del(dev, &chan->vm_pd);
+	nouveau_gpuobj_ref_del(dev, &chan->vm_gart_pt);
+	nouveau_gpuobj_ref_del(dev, &chan->vm_vram_pt);
+
+	if (chan->ramin_heap)
+		nouveau_mem_takedown(&chan->ramin_heap);
+	if (chan->ramin)
+		nouveau_gpuobj_ref_del(dev, &chan->ramin);
+
+}
+
+int nouveau_ioctl_grobj_alloc(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct nouveau_channel *chan;
+	struct drm_nouveau_grobj_alloc *init = data;
+	struct nouveau_gpuobj *gr = NULL;
+	int ret;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+	NOUVEAU_GET_USER_CHANNEL_WITH_RETURN(init->channel, file_priv, chan);
+
+	//FIXME: check args, only allow trusted objects to be created
+	
+	if (init->handle == ~0)
+		return -EINVAL;
+
+	if (nouveau_gpuobj_ref_find(chan, init->handle, NULL) == 0)
+		return -EEXIST;
+
+	ret = nouveau_gpuobj_gr_new(chan, init->class, &gr);
+	if (ret) {
+		DRM_ERROR("Error creating gr object: %d (%d/0x%08x)\n",
+			  ret, init->channel, init->handle);
+		return ret;
+	}
+
+	if ((ret = nouveau_gpuobj_ref_add(dev, chan, init->handle, gr, NULL))) {
+		DRM_ERROR("Error referencing gr object: %d (%d/0x%08x\n)",
+			  ret, init->channel, init->handle);
+		nouveau_gpuobj_del(dev, &gr);
+		return ret;
+	}
+
+	return 0;
+}
+
+int nouveau_ioctl_gpuobj_free(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct drm_nouveau_gpuobj_free *objfree = data;
+	struct nouveau_gpuobj_ref *ref;
+	struct nouveau_channel *chan;
+	int ret;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+	NOUVEAU_GET_USER_CHANNEL_WITH_RETURN(objfree->channel, file_priv, chan);
+
+	if ((ret = nouveau_gpuobj_ref_find(chan, objfree->handle, &ref)))
+		return ret;
+	nouveau_gpuobj_ref_del(dev, &ref);
+
+	return 0;
+}
+
diff --git a/drivers/char/drm/nouveau_reg.h b/drivers/char/drm/nouveau_reg.h
new file mode 100644
index 0000000..1ae0177
--- /dev/null
+++ b/drivers/char/drm/nouveau_reg.h
@@ -0,0 +1,593 @@
+
+
+#define NV03_BOOT_0                                        0x00100000
+#    define NV03_BOOT_0_RAM_AMOUNT                         0x00000003
+#    define NV03_BOOT_0_RAM_AMOUNT_8MB                     0x00000000
+#    define NV03_BOOT_0_RAM_AMOUNT_2MB                     0x00000001
+#    define NV03_BOOT_0_RAM_AMOUNT_4MB                     0x00000002
+#    define NV03_BOOT_0_RAM_AMOUNT_8MB_SDRAM               0x00000003
+#    define NV04_BOOT_0_RAM_AMOUNT_32MB                    0x00000000
+#    define NV04_BOOT_0_RAM_AMOUNT_4MB                     0x00000001
+#    define NV04_BOOT_0_RAM_AMOUNT_8MB                     0x00000002
+#    define NV04_BOOT_0_RAM_AMOUNT_16MB                    0x00000003
+
+#define NV04_FIFO_DATA                                     0x0010020c
+#    define NV10_FIFO_DATA_RAM_AMOUNT_MB_MASK              0xfff00000
+#    define NV10_FIFO_DATA_RAM_AMOUNT_MB_SHIFT             20
+
+#define NV_RAMIN                                           0x00700000
+
+#define NV_RAMHT_HANDLE_OFFSET                             0
+#define NV_RAMHT_CONTEXT_OFFSET                            4
+#    define NV_RAMHT_CONTEXT_VALID                         (1<<31)
+#    define NV_RAMHT_CONTEXT_CHANNEL_SHIFT                 24
+#    define NV_RAMHT_CONTEXT_ENGINE_SHIFT                  16
+#        define NV_RAMHT_CONTEXT_ENGINE_SOFTWARE           0
+#        define NV_RAMHT_CONTEXT_ENGINE_GRAPHICS           1
+#    define NV_RAMHT_CONTEXT_INSTANCE_SHIFT                0
+#    define NV40_RAMHT_CONTEXT_CHANNEL_SHIFT               23
+#    define NV40_RAMHT_CONTEXT_ENGINE_SHIFT                20
+#    define NV40_RAMHT_CONTEXT_INSTANCE_SHIFT              0
+
+/* DMA object defines */
+#define NV_DMA_ACCESS_RW 0
+#define NV_DMA_ACCESS_RO 1
+#define NV_DMA_ACCESS_WO 2
+#define NV_DMA_TARGET_VIDMEM 0
+#define NV_DMA_TARGET_PCI    2
+#define NV_DMA_TARGET_AGP    3
+/*The following is not a real value used by nvidia cards, it's changed by nouveau_object_dma_create*/
+#define NV_DMA_TARGET_PCI_NONLINEAR   8
+
+/* Some object classes we care about in the drm */
+#define NV_CLASS_DMA_FROM_MEMORY                           0x00000002
+#define NV_CLASS_DMA_TO_MEMORY                             0x00000003
+#define NV_CLASS_NULL                                      0x00000030
+#define NV_CLASS_DMA_IN_MEMORY                             0x0000003D
+
+#define NV03_USER(i)                             (0x00800000+(i*NV03_USER_SIZE))
+#define NV03_USER__SIZE                                                       16
+#define NV10_USER__SIZE                                                       32
+#define NV03_USER_SIZE                                                0x00010000
+#define NV03_USER_DMA_PUT(i)                     (0x00800040+(i*NV03_USER_SIZE))
+#define NV03_USER_DMA_PUT__SIZE                                               16
+#define NV10_USER_DMA_PUT__SIZE                                               32
+#define NV03_USER_DMA_GET(i)                     (0x00800044+(i*NV03_USER_SIZE))
+#define NV03_USER_DMA_GET__SIZE                                               16
+#define NV10_USER_DMA_GET__SIZE                                               32
+#define NV03_USER_REF_CNT(i)                     (0x00800048+(i*NV03_USER_SIZE))
+#define NV03_USER_REF_CNT__SIZE                                               16
+#define NV10_USER_REF_CNT__SIZE                                               32
+
+#define NV40_USER(i)                             (0x00c00000+(i*NV40_USER_SIZE))
+#define NV40_USER_SIZE                                                0x00001000
+#define NV40_USER_DMA_PUT(i)                     (0x00c00040+(i*NV40_USER_SIZE))
+#define NV40_USER_DMA_PUT__SIZE                                               32
+#define NV40_USER_DMA_GET(i)                     (0x00c00044+(i*NV40_USER_SIZE))
+#define NV40_USER_DMA_GET__SIZE                                               32
+#define NV40_USER_REF_CNT(i)                     (0x00c00048+(i*NV40_USER_SIZE))
+#define NV40_USER_REF_CNT__SIZE                                               32
+
+#define NV50_USER(i)                             (0x00c00000+(i*NV50_USER_SIZE))
+#define NV50_USER_SIZE                                                0x00002000
+#define NV50_USER_DMA_PUT(i)                     (0x00c00040+(i*NV50_USER_SIZE))
+#define NV50_USER_DMA_PUT__SIZE                                              128
+#define NV50_USER_DMA_GET(i)                     (0x00c00044+(i*NV50_USER_SIZE))
+#define NV50_USER_DMA_GET__SIZE                                              128
+/*XXX: I don't think this actually exists.. */
+#define NV50_USER_REF_CNT(i)                     (0x00c00048+(i*NV50_USER_SIZE))
+#define NV50_USER_REF_CNT__SIZE                                              128
+
+#define NV03_FIFO_SIZE                                     0x8000UL
+
+#define NV03_PMC_BOOT_0                                    0x00000000
+#define NV03_PMC_BOOT_1                                    0x00000004
+#define NV03_PMC_INTR_0                                    0x00000100
+#    define NV_PMC_INTR_0_PFIFO_PENDING                       (1<< 8)
+#    define NV_PMC_INTR_0_PGRAPH_PENDING                      (1<<12)
+#    define NV_PMC_INTR_0_NV50_I2C_PENDING                  (1<<21)
+#    define NV_PMC_INTR_0_CRTC0_PENDING                       (1<<24)
+#    define NV_PMC_INTR_0_CRTC1_PENDING                       (1<<25)
+#    define NV_PMC_INTR_0_NV50_DISPLAY_PENDING           (1<<26)
+#    define NV_PMC_INTR_0_CRTCn_PENDING                       (3<<24)
+#define NV03_PMC_INTR_EN_0                                 0x00000140
+#    define NV_PMC_INTR_EN_0_MASTER_ENABLE                    (1<< 0)
+#define NV03_PMC_ENABLE                                    0x00000200
+#    define NV_PMC_ENABLE_PFIFO                               (1<< 8)
+#    define NV_PMC_ENABLE_PGRAPH                              (1<<12)
+/* Disabling the below bit breaks newer (G7X only?) mobile chipsets,
+ * the card will hang early on in the X init process.
+ */
+#    define NV_PMC_ENABLE_UNK13                               (1<<13)
+#define NV40_PMC_1700                                      0x00001700
+#define NV40_PMC_1704                                      0x00001704
+#define NV40_PMC_1708                                      0x00001708
+#define NV40_PMC_170C                                      0x0000170C
+
+/* probably PMC ? */
+#define NV50_PUNK_BAR0_PRAMIN                              0x00001700
+#define NV50_PUNK_BAR_CFG_BASE                             0x00001704
+#define NV50_PUNK_BAR_CFG_BASE_VALID                          (1<<30)
+#define NV50_PUNK_BAR1_CTXDMA                              0x00001708
+#define NV50_PUNK_BAR1_CTXDMA_VALID                           (1<<31)
+#define NV50_PUNK_BAR3_CTXDMA                              0x0000170C
+#define NV50_PUNK_BAR3_CTXDMA_VALID                           (1<<31)
+#define NV50_PUNK_UNK1710                                  0x00001710
+
+#define NV04_PBUS_PCI_NV_1                                 0x00001804
+#define NV04_PBUS_PCI_NV_19                                0x0000184C
+
+#define NV04_PTIMER_INTR_0                                 0x00009100
+#define NV04_PTIMER_INTR_EN_0                              0x00009140
+#define NV04_PTIMER_NUMERATOR                              0x00009200
+#define NV04_PTIMER_DENOMINATOR                            0x00009210
+#define NV04_PTIMER_TIME_0                                 0x00009400
+#define NV04_PTIMER_TIME_1                                 0x00009410
+#define NV04_PTIMER_ALARM_0                                0x00009420
+
+#define NV50_I2C_CONTROLLER                           0x0000E054
+
+#define NV04_PFB_CFG0                                      0x00100200
+#define NV04_PFB_CFG1                                      0x00100204
+#define NV40_PFB_020C                                      0x0010020C
+#define NV10_PFB_TILE(i)                                   (0x00100240 + (i*16))
+#define NV10_PFB_TILE__SIZE                                8
+#define NV10_PFB_TLIMIT(i)                                 (0x00100244 + (i*16))
+#define NV10_PFB_TSIZE(i)                                  (0x00100248 + (i*16))
+#define NV10_PFB_TSTATUS(i)                                (0x0010024C + (i*16))
+#define NV10_PFB_CLOSE_PAGE2                               0x0010033C
+#define NV40_PFB_TILE(i)                                   (0x00100600 + (i*16))
+#define NV40_PFB_TILE__SIZE_0                              12
+#define NV40_PFB_TILE__SIZE_1                              15
+#define NV40_PFB_TLIMIT(i)                                 (0x00100604 + (i*16))
+#define NV40_PFB_TSIZE(i)                                  (0x00100608 + (i*16))
+#define NV40_PFB_TSTATUS(i)                                (0x0010060C + (i*16))
+#define NV40_PFB_UNK_800					0x00100800
+
+#define NV04_PGRAPH_DEBUG_0                                0x00400080
+#define NV04_PGRAPH_DEBUG_1                                0x00400084
+#define NV04_PGRAPH_DEBUG_2                                0x00400088
+#define NV04_PGRAPH_DEBUG_3                                0x0040008c
+#define NV10_PGRAPH_DEBUG_4                                0x00400090
+#define NV03_PGRAPH_INTR                                   0x00400100
+#define NV03_PGRAPH_NSTATUS                                0x00400104
+#    define NV04_PGRAPH_NSTATUS_STATE_IN_USE                  (1<<11)
+#    define NV04_PGRAPH_NSTATUS_INVALID_STATE                 (1<<12)
+#    define NV04_PGRAPH_NSTATUS_BAD_ARGUMENT                  (1<<13)
+#    define NV04_PGRAPH_NSTATUS_PROTECTION_FAULT              (1<<14)
+#    define NV10_PGRAPH_NSTATUS_STATE_IN_USE                  (1<<23)
+#    define NV10_PGRAPH_NSTATUS_INVALID_STATE                 (1<<24)
+#    define NV10_PGRAPH_NSTATUS_BAD_ARGUMENT                  (1<<25)
+#    define NV10_PGRAPH_NSTATUS_PROTECTION_FAULT              (1<<26)
+#define NV03_PGRAPH_NSOURCE                                0x00400108
+#    define NV03_PGRAPH_NSOURCE_NOTIFICATION                  (1<< 0)
+#    define NV03_PGRAPH_NSOURCE_DATA_ERROR                    (1<< 1)
+#    define NV03_PGRAPH_NSOURCE_PROTECTION_ERROR              (1<< 2)
+#    define NV03_PGRAPH_NSOURCE_RANGE_EXCEPTION               (1<< 3)
+#    define NV03_PGRAPH_NSOURCE_LIMIT_COLOR                   (1<< 4)
+#    define NV03_PGRAPH_NSOURCE_LIMIT_ZETA                    (1<< 5)
+#    define NV03_PGRAPH_NSOURCE_ILLEGAL_MTHD                  (1<< 6)
+#    define NV03_PGRAPH_NSOURCE_DMA_R_PROTECTION              (1<< 7)
+#    define NV03_PGRAPH_NSOURCE_DMA_W_PROTECTION              (1<< 8)
+#    define NV03_PGRAPH_NSOURCE_FORMAT_EXCEPTION              (1<< 9)
+#    define NV03_PGRAPH_NSOURCE_PATCH_EXCEPTION               (1<<10)
+#    define NV03_PGRAPH_NSOURCE_STATE_INVALID                 (1<<11)
+#    define NV03_PGRAPH_NSOURCE_DOUBLE_NOTIFY                 (1<<12)
+#    define NV03_PGRAPH_NSOURCE_NOTIFY_IN_USE                 (1<<13)
+#    define NV03_PGRAPH_NSOURCE_METHOD_CNT                    (1<<14)
+#    define NV03_PGRAPH_NSOURCE_BFR_NOTIFICATION              (1<<15)
+#    define NV03_PGRAPH_NSOURCE_DMA_VTX_PROTECTION            (1<<16)
+#    define NV03_PGRAPH_NSOURCE_DMA_WIDTH_A                   (1<<17)
+#    define NV03_PGRAPH_NSOURCE_DMA_WIDTH_B                   (1<<18)
+#define NV03_PGRAPH_INTR_EN                                0x00400140
+#define NV40_PGRAPH_INTR_EN                                0x0040013C
+#    define NV_PGRAPH_INTR_NOTIFY                             (1<< 0)
+#    define NV_PGRAPH_INTR_MISSING_HW                         (1<< 4)
+#    define NV_PGRAPH_INTR_CONTEXT_SWITCH                     (1<<12)
+#    define NV_PGRAPH_INTR_BUFFER_NOTIFY                      (1<<16)
+#    define NV_PGRAPH_INTR_ERROR                              (1<<20)
+#define NV10_PGRAPH_CTX_CONTROL                            0x00400144
+#define NV10_PGRAPH_CTX_USER                               0x00400148
+#define NV10_PGRAPH_CTX_SWITCH1                            0x0040014C
+#define NV10_PGRAPH_CTX_SWITCH2                            0x00400150
+#define NV10_PGRAPH_CTX_SWITCH3                            0x00400154
+#define NV10_PGRAPH_CTX_SWITCH4                            0x00400158
+#define NV10_PGRAPH_CTX_SWITCH5                            0x0040015C
+#define NV04_PGRAPH_CTX_SWITCH1                            0x00400160
+#define NV10_PGRAPH_CTX_CACHE1                             0x00400160
+#define NV04_PGRAPH_CTX_SWITCH2                            0x00400164
+#define NV04_PGRAPH_CTX_SWITCH3                            0x00400168
+#define NV04_PGRAPH_CTX_SWITCH4                            0x0040016C
+#define NV04_PGRAPH_CTX_CONTROL                            0x00400170
+#define NV04_PGRAPH_CTX_USER                               0x00400174
+#define NV04_PGRAPH_CTX_CACHE1                             0x00400180
+#define NV10_PGRAPH_CTX_CACHE2                             0x00400180
+#define NV03_PGRAPH_CTX_CONTROL                            0x00400190
+#define NV03_PGRAPH_CTX_USER                               0x00400194
+#define NV04_PGRAPH_CTX_CACHE2                             0x004001A0
+#define NV10_PGRAPH_CTX_CACHE3                             0x004001A0
+#define NV04_PGRAPH_CTX_CACHE3                             0x004001C0
+#define NV10_PGRAPH_CTX_CACHE4                             0x004001C0
+#define NV04_PGRAPH_CTX_CACHE4                             0x004001E0
+#define NV10_PGRAPH_CTX_CACHE5                             0x004001E0
+#define NV40_PGRAPH_CTXCTL_0304                            0x00400304
+#define NV40_PGRAPH_CTXCTL_0304_XFER_CTX                   0x00000001
+#define NV40_PGRAPH_CTXCTL_UCODE_STAT                      0x00400308
+#define NV40_PGRAPH_CTXCTL_UCODE_STAT_IP_MASK              0xff000000
+#define NV40_PGRAPH_CTXCTL_UCODE_STAT_IP_SHIFT                     24
+#define NV40_PGRAPH_CTXCTL_UCODE_STAT_OP_MASK              0x00ffffff
+#define NV40_PGRAPH_CTXCTL_0310                            0x00400310
+#define NV40_PGRAPH_CTXCTL_0310_XFER_SAVE                  0x00000020
+#define NV40_PGRAPH_CTXCTL_0310_XFER_LOAD                  0x00000040
+#define NV40_PGRAPH_CTXCTL_030C                            0x0040030c
+#define NV40_PGRAPH_CTXCTL_UCODE_INDEX                     0x00400324
+#define NV40_PGRAPH_CTXCTL_UCODE_DATA                      0x00400328
+#define NV40_PGRAPH_CTXCTL_CUR                             0x0040032c
+#define NV40_PGRAPH_CTXCTL_CUR_LOADED                      0x01000000
+#define NV40_PGRAPH_CTXCTL_CUR_INST_MASK                   0x000FFFFF
+#define NV03_PGRAPH_ABS_X_RAM                              0x00400400
+#define NV03_PGRAPH_ABS_Y_RAM                              0x00400480
+#define NV03_PGRAPH_X_MISC                                 0x00400500
+#define NV03_PGRAPH_Y_MISC                                 0x00400504
+#define NV04_PGRAPH_VALID1                                 0x00400508
+#define NV04_PGRAPH_SOURCE_COLOR                           0x0040050C
+#define NV04_PGRAPH_MISC24_0                               0x00400510
+#define NV03_PGRAPH_XY_LOGIC_MISC0                         0x00400514
+#define NV03_PGRAPH_XY_LOGIC_MISC1                         0x00400518
+#define NV03_PGRAPH_XY_LOGIC_MISC2                         0x0040051C
+#define NV03_PGRAPH_XY_LOGIC_MISC3                         0x00400520
+#define NV03_PGRAPH_CLIPX_0                                0x00400524
+#define NV03_PGRAPH_CLIPX_1                                0x00400528
+#define NV03_PGRAPH_CLIPY_0                                0x0040052C
+#define NV03_PGRAPH_CLIPY_1                                0x00400530
+#define NV03_PGRAPH_ABS_ICLIP_XMAX                         0x00400534
+#define NV03_PGRAPH_ABS_ICLIP_YMAX                         0x00400538
+#define NV03_PGRAPH_ABS_UCLIP_XMIN                         0x0040053C
+#define NV03_PGRAPH_ABS_UCLIP_YMIN                         0x00400540
+#define NV03_PGRAPH_ABS_UCLIP_XMAX                         0x00400544
+#define NV03_PGRAPH_ABS_UCLIP_YMAX                         0x00400548
+#define NV03_PGRAPH_ABS_UCLIPA_XMIN                        0x00400560
+#define NV03_PGRAPH_ABS_UCLIPA_YMIN                        0x00400564
+#define NV03_PGRAPH_ABS_UCLIPA_XMAX                        0x00400568
+#define NV03_PGRAPH_ABS_UCLIPA_YMAX                        0x0040056C
+#define NV04_PGRAPH_MISC24_1                               0x00400570
+#define NV04_PGRAPH_MISC24_2                               0x00400574
+#define NV04_PGRAPH_VALID2                                 0x00400578
+#define NV04_PGRAPH_PASSTHRU_0                             0x0040057C
+#define NV04_PGRAPH_PASSTHRU_1                             0x00400580
+#define NV04_PGRAPH_PASSTHRU_2                             0x00400584
+#define NV10_PGRAPH_DIMX_TEXTURE                           0x00400588
+#define NV10_PGRAPH_WDIMX_TEXTURE                          0x0040058C
+#define NV04_PGRAPH_COMBINE_0_ALPHA                        0x00400590
+#define NV04_PGRAPH_COMBINE_0_COLOR                        0x00400594
+#define NV04_PGRAPH_COMBINE_1_ALPHA                        0x00400598
+#define NV04_PGRAPH_COMBINE_1_COLOR                        0x0040059C
+#define NV04_PGRAPH_FORMAT_0                               0x004005A8
+#define NV04_PGRAPH_FORMAT_1                               0x004005AC
+#define NV04_PGRAPH_FILTER_0                               0x004005B0
+#define NV04_PGRAPH_FILTER_1                               0x004005B4
+#define NV03_PGRAPH_MONO_COLOR0                            0x00400600
+#define NV04_PGRAPH_ROP3                                   0x00400604
+#define NV04_PGRAPH_BETA_AND                               0x00400608
+#define NV04_PGRAPH_BETA_PREMULT                           0x0040060C
+#define NV04_PGRAPH_LIMIT_VIOL_PIX                         0x00400610
+#define NV04_PGRAPH_FORMATS                                0x00400618
+#define NV10_PGRAPH_DEBUG_2                                0x00400620
+#define NV04_PGRAPH_BOFFSET0                               0x00400640
+#define NV04_PGRAPH_BOFFSET1                               0x00400644
+#define NV04_PGRAPH_BOFFSET2                               0x00400648
+#define NV04_PGRAPH_BOFFSET3                               0x0040064C
+#define NV04_PGRAPH_BOFFSET4                               0x00400650
+#define NV04_PGRAPH_BOFFSET5                               0x00400654
+#define NV04_PGRAPH_BBASE0                                 0x00400658
+#define NV04_PGRAPH_BBASE1                                 0x0040065C
+#define NV04_PGRAPH_BBASE2                                 0x00400660
+#define NV04_PGRAPH_BBASE3                                 0x00400664
+#define NV04_PGRAPH_BBASE4                                 0x00400668
+#define NV04_PGRAPH_BBASE5                                 0x0040066C
+#define NV04_PGRAPH_BPITCH0                                0x00400670
+#define NV04_PGRAPH_BPITCH1                                0x00400674
+#define NV04_PGRAPH_BPITCH2                                0x00400678
+#define NV04_PGRAPH_BPITCH3                                0x0040067C
+#define NV04_PGRAPH_BPITCH4                                0x00400680
+#define NV04_PGRAPH_BLIMIT0                                0x00400684
+#define NV04_PGRAPH_BLIMIT1                                0x00400688
+#define NV04_PGRAPH_BLIMIT2                                0x0040068C
+#define NV04_PGRAPH_BLIMIT3                                0x00400690
+#define NV04_PGRAPH_BLIMIT4                                0x00400694
+#define NV04_PGRAPH_BLIMIT5                                0x00400698
+#define NV04_PGRAPH_BSWIZZLE2                              0x0040069C
+#define NV04_PGRAPH_BSWIZZLE5                              0x004006A0
+#define NV03_PGRAPH_STATUS                                 0x004006B0
+#define NV04_PGRAPH_STATUS                                 0x00400700
+#define NV04_PGRAPH_TRAPPED_ADDR                           0x00400704
+#define NV04_PGRAPH_TRAPPED_DATA                           0x00400708
+#define NV04_PGRAPH_SURFACE                                0x0040070C
+#define NV10_PGRAPH_TRAPPED_DATA_HIGH                      0x0040070C
+#define NV04_PGRAPH_STATE                                  0x00400710
+#define NV10_PGRAPH_SURFACE                                0x00400710
+#define NV04_PGRAPH_NOTIFY                                 0x00400714
+#define NV10_PGRAPH_STATE                                  0x00400714
+#define NV10_PGRAPH_NOTIFY                                 0x00400718
+
+#define NV04_PGRAPH_FIFO                                   0x00400720
+
+#define NV04_PGRAPH_BPIXEL                                 0x00400724
+#define NV10_PGRAPH_RDI_INDEX                              0x00400750
+#define NV04_PGRAPH_FFINTFC_ST2                            0x00400754
+#define NV10_PGRAPH_RDI_DATA                               0x00400754
+#define NV04_PGRAPH_DMA_PITCH                              0x00400760
+#define NV10_PGRAPH_FFINTFC_ST2                            0x00400764
+#define NV04_PGRAPH_DVD_COLORFMT                           0x00400764
+#define NV04_PGRAPH_SCALED_FORMAT                          0x00400768
+#define NV10_PGRAPH_DMA_PITCH                              0x00400770
+#define NV10_PGRAPH_DVD_COLORFMT                           0x00400774
+#define NV10_PGRAPH_SCALED_FORMAT                          0x00400778
+#define NV20_PGRAPH_CHANNEL_CTX_TABLE                      0x00400780
+#define NV20_PGRAPH_CHANNEL_CTX_POINTER                    0x00400784
+#define NV20_PGRAPH_CHANNEL_CTX_XFER                       0x00400788
+#define NV20_PGRAPH_CHANNEL_CTX_XFER_LOAD                  0x00000001
+#define NV20_PGRAPH_CHANNEL_CTX_XFER_SAVE                  0x00000002
+#define NV04_PGRAPH_PATT_COLOR0                            0x00400800
+#define NV04_PGRAPH_PATT_COLOR1                            0x00400804
+#define NV04_PGRAPH_PATTERN                                0x00400808
+#define NV04_PGRAPH_PATTERN_SHAPE                          0x00400810
+#define NV04_PGRAPH_CHROMA                                 0x00400814
+#define NV04_PGRAPH_CONTROL0                               0x00400818
+#define NV04_PGRAPH_CONTROL1                               0x0040081C
+#define NV04_PGRAPH_CONTROL2                               0x00400820
+#define NV04_PGRAPH_BLEND                                  0x00400824
+#define NV04_PGRAPH_STORED_FMT                             0x00400830
+#define NV04_PGRAPH_PATT_COLORRAM                          0x00400900
+#define NV40_PGRAPH_TILE0(i)                               (0x00400900 + (i*16))
+#define NV40_PGRAPH_TLIMIT0(i)                             (0x00400904 + (i*16))
+#define NV40_PGRAPH_TSIZE0(i)                              (0x00400908 + (i*16))
+#define NV40_PGRAPH_TSTATUS0(i)                            (0x0040090C + (i*16))
+#define NV10_PGRAPH_TILE(i)                                (0x00400B00 + (i*16))
+#define NV10_PGRAPH_TLIMIT(i)                              (0x00400B04 + (i*16))
+#define NV10_PGRAPH_TSIZE(i)                               (0x00400B08 + (i*16))
+#define NV10_PGRAPH_TSTATUS(i)                             (0x00400B0C + (i*16))
+#define NV04_PGRAPH_U_RAM                                  0x00400D00
+#define NV47_PGRAPH_TILE0(i)                               (0x00400D00 + (i*16))
+#define NV47_PGRAPH_TLIMIT0(i)                             (0x00400D04 + (i*16))
+#define NV47_PGRAPH_TSIZE0(i)                              (0x00400D08 + (i*16))
+#define NV47_PGRAPH_TSTATUS0(i)                            (0x00400D0C + (i*16))
+#define NV04_PGRAPH_V_RAM                                  0x00400D40
+#define NV04_PGRAPH_W_RAM                                  0x00400D80
+#define NV10_PGRAPH_COMBINER0_IN_ALPHA                     0x00400E40
+#define NV10_PGRAPH_COMBINER1_IN_ALPHA                     0x00400E44
+#define NV10_PGRAPH_COMBINER0_IN_RGB                       0x00400E48
+#define NV10_PGRAPH_COMBINER1_IN_RGB                       0x00400E4C
+#define NV10_PGRAPH_COMBINER_COLOR0                        0x00400E50
+#define NV10_PGRAPH_COMBINER_COLOR1                        0x00400E54
+#define NV10_PGRAPH_COMBINER0_OUT_ALPHA                    0x00400E58
+#define NV10_PGRAPH_COMBINER1_OUT_ALPHA                    0x00400E5C
+#define NV10_PGRAPH_COMBINER0_OUT_RGB                      0x00400E60
+#define NV10_PGRAPH_COMBINER1_OUT_RGB                      0x00400E64
+#define NV10_PGRAPH_COMBINER_FINAL0                        0x00400E68
+#define NV10_PGRAPH_COMBINER_FINAL1                        0x00400E6C
+#define NV10_PGRAPH_WINDOWCLIP_HORIZONTAL                  0x00400F00
+#define NV10_PGRAPH_WINDOWCLIP_VERTICAL                    0x00400F20
+#define NV10_PGRAPH_XFMODE0                                0x00400F40
+#define NV10_PGRAPH_XFMODE1                                0x00400F44
+#define NV10_PGRAPH_GLOBALSTATE0                           0x00400F48
+#define NV10_PGRAPH_GLOBALSTATE1                           0x00400F4C
+#define NV10_PGRAPH_PIPE_ADDRESS                           0x00400F50
+#define NV10_PGRAPH_PIPE_DATA                              0x00400F54
+#define NV04_PGRAPH_DMA_START_0                            0x00401000
+#define NV04_PGRAPH_DMA_START_1                            0x00401004
+#define NV04_PGRAPH_DMA_LENGTH                             0x00401008
+#define NV04_PGRAPH_DMA_MISC                               0x0040100C
+#define NV04_PGRAPH_DMA_DATA_0                             0x00401020
+#define NV04_PGRAPH_DMA_DATA_1                             0x00401024
+#define NV04_PGRAPH_DMA_RM                                 0x00401030
+#define NV04_PGRAPH_DMA_A_XLATE_INST                       0x00401040
+#define NV04_PGRAPH_DMA_A_CONTROL                          0x00401044
+#define NV04_PGRAPH_DMA_A_LIMIT                            0x00401048
+#define NV04_PGRAPH_DMA_A_TLB_PTE                          0x0040104C
+#define NV04_PGRAPH_DMA_A_TLB_TAG                          0x00401050
+#define NV04_PGRAPH_DMA_A_ADJ_OFFSET                       0x00401054
+#define NV04_PGRAPH_DMA_A_OFFSET                           0x00401058
+#define NV04_PGRAPH_DMA_A_SIZE                             0x0040105C
+#define NV04_PGRAPH_DMA_A_Y_SIZE                           0x00401060
+#define NV04_PGRAPH_DMA_B_XLATE_INST                       0x00401080
+#define NV04_PGRAPH_DMA_B_CONTROL                          0x00401084
+#define NV04_PGRAPH_DMA_B_LIMIT                            0x00401088
+#define NV04_PGRAPH_DMA_B_TLB_PTE                          0x0040108C
+#define NV04_PGRAPH_DMA_B_TLB_TAG                          0x00401090
+#define NV04_PGRAPH_DMA_B_ADJ_OFFSET                       0x00401094
+#define NV04_PGRAPH_DMA_B_OFFSET                           0x00401098
+#define NV04_PGRAPH_DMA_B_SIZE                             0x0040109C
+#define NV04_PGRAPH_DMA_B_Y_SIZE                           0x004010A0
+#define NV40_PGRAPH_TILE1(i)                               (0x00406900 + (i*16))
+#define NV40_PGRAPH_TLIMIT1(i)                             (0x00406904 + (i*16))
+#define NV40_PGRAPH_TSIZE1(i)                              (0x00406908 + (i*16))
+#define NV40_PGRAPH_TSTATUS1(i)                            (0x0040690C + (i*16))
+
+
+/* It's a guess that this works on NV03. Confirmed on NV04, though */
+#define NV04_PFIFO_DELAY_0                                 0x00002040
+#define NV04_PFIFO_DMA_TIMESLICE                           0x00002044
+#define NV04_PFIFO_NEXT_CHANNEL                            0x00002050
+#define NV03_PFIFO_INTR_0                                  0x00002100
+#define NV03_PFIFO_INTR_EN_0                               0x00002140
+#    define NV_PFIFO_INTR_CACHE_ERROR                         (1<< 0)
+#    define NV_PFIFO_INTR_RUNOUT                              (1<< 4)
+#    define NV_PFIFO_INTR_RUNOUT_OVERFLOW                     (1<< 8)
+#    define NV_PFIFO_INTR_DMA_PUSHER                          (1<<12)
+#    define NV_PFIFO_INTR_DMA_PT                              (1<<16)
+#    define NV_PFIFO_INTR_SEMAPHORE                           (1<<20)
+#    define NV_PFIFO_INTR_ACQUIRE_TIMEOUT                     (1<<24)
+#define NV03_PFIFO_RAMHT                                   0x00002210
+#define NV03_PFIFO_RAMFC                                   0x00002214
+#define NV03_PFIFO_RAMRO                                   0x00002218
+#define NV40_PFIFO_RAMFC                                   0x00002220
+#define NV03_PFIFO_CACHES                                  0x00002500
+#define NV04_PFIFO_MODE                                    0x00002504
+#define NV04_PFIFO_DMA                                     0x00002508
+#define NV04_PFIFO_SIZE                                    0x0000250c
+#define NV50_PFIFO_CTX_TABLE(c)                        (0x2600+(c)*4)
+#define NV50_PFIFO_CTX_TABLE__SIZE                                128
+#define NV50_PFIFO_CTX_TABLE_CHANNEL_ENABLED                  (1<<31)
+#define NV50_PFIFO_CTX_TABLE_UNK30_BAD                        (1<<30)
+#define NV50_PFIFO_CTX_TABLE_INSTANCE_MASK_G80             0x0FFFFFFF
+#define NV50_PFIFO_CTX_TABLE_INSTANCE_MASK_G84             0x00FFFFFF
+#define NV03_PFIFO_CACHE0_PUSH0                            0x00003000
+#define NV03_PFIFO_CACHE0_PULL0                            0x00003040
+#define NV04_PFIFO_CACHE0_PULL0                            0x00003050
+#define NV04_PFIFO_CACHE0_PULL1                            0x00003054
+#define NV03_PFIFO_CACHE1_PUSH0                            0x00003200
+#define NV03_PFIFO_CACHE1_PUSH1                            0x00003204
+#define NV03_PFIFO_CACHE1_PUSH1_DMA                            (1<<8)
+#define NV40_PFIFO_CACHE1_PUSH1_DMA                           (1<<16)
+#define NV03_PFIFO_CACHE1_PUSH1_CHID_MASK                  0x0000000f
+#define NV10_PFIFO_CACHE1_PUSH1_CHID_MASK                  0x0000001f
+#define NV50_PFIFO_CACHE1_PUSH1_CHID_MASK                  0x0000007f
+#define NV03_PFIFO_CACHE1_PUT                              0x00003210
+#define NV04_PFIFO_CACHE1_DMA_PUSH                         0x00003220
+#define NV04_PFIFO_CACHE1_DMA_FETCH                        0x00003224
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_8_BYTES         0x00000000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_16_BYTES        0x00000008
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_24_BYTES        0x00000010
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_32_BYTES        0x00000018
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_40_BYTES        0x00000020
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_48_BYTES        0x00000028
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_56_BYTES        0x00000030
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_64_BYTES        0x00000038
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_72_BYTES        0x00000040
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_80_BYTES        0x00000048
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_88_BYTES        0x00000050
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_96_BYTES        0x00000058
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_104_BYTES       0x00000060
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_112_BYTES       0x00000068
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_120_BYTES       0x00000070
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_128_BYTES       0x00000078
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_136_BYTES       0x00000080
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_144_BYTES       0x00000088
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_152_BYTES       0x00000090
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_160_BYTES       0x00000098
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_168_BYTES       0x000000A0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_176_BYTES       0x000000A8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_184_BYTES       0x000000B0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_192_BYTES       0x000000B8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_200_BYTES       0x000000C0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_208_BYTES       0x000000C8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_216_BYTES       0x000000D0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_224_BYTES       0x000000D8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_232_BYTES       0x000000E0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_240_BYTES       0x000000E8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_248_BYTES       0x000000F0
+#    define NV_PFIFO_CACHE1_DMA_FETCH_TRIG_256_BYTES       0x000000F8
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE                 0x0000E000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_32_BYTES        0x00000000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_64_BYTES        0x00002000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_96_BYTES        0x00004000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_128_BYTES       0x00006000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_160_BYTES       0x00008000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_192_BYTES       0x0000A000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_224_BYTES       0x0000C000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_SIZE_256_BYTES       0x0000E000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS             0x001F0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_0           0x00000000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_1           0x00010000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_2           0x00020000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_3           0x00030000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_4           0x00040000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_5           0x00050000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_6           0x00060000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_7           0x00070000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_8           0x00080000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_9           0x00090000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_10          0x000A0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_11          0x000B0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_12          0x000C0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_13          0x000D0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_14          0x000E0000
+#    define NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_15          0x000F0000
+#    define NV_PFIFO_CACHE1_ENDIAN                         0x80000000
+#    define NV_PFIFO_CACHE1_LITTLE_ENDIAN                  0x7FFFFFFF
+#    define NV_PFIFO_CACHE1_BIG_ENDIAN                     0x80000000
+#define NV04_PFIFO_CACHE1_DMA_STATE                        0x00003228
+#define NV04_PFIFO_CACHE1_DMA_INSTANCE                     0x0000322c
+#define NV04_PFIFO_CACHE1_DMA_CTL                          0x00003230
+#define NV04_PFIFO_CACHE1_DMA_PUT                          0x00003240
+#define NV04_PFIFO_CACHE1_DMA_GET                          0x00003244
+#define NV10_PFIFO_CACHE1_REF_CNT                          0x00003248
+#define NV10_PFIFO_CACHE1_DMA_SUBROUTINE                   0x0000324C
+#define NV03_PFIFO_CACHE1_PULL0                            0x00003240
+#define NV04_PFIFO_CACHE1_PULL0                            0x00003250
+#define NV03_PFIFO_CACHE1_PULL1                            0x00003250
+#define NV04_PFIFO_CACHE1_PULL1                            0x00003254
+#define NV04_PFIFO_CACHE1_HASH                             0x00003258
+#define NV10_PFIFO_CACHE1_ACQUIRE_TIMEOUT                  0x00003260
+#define NV10_PFIFO_CACHE1_ACQUIRE_TIMESTAMP                0x00003264
+#define NV10_PFIFO_CACHE1_ACQUIRE_VALUE                    0x00003268
+#define NV10_PFIFO_CACHE1_SEMAPHORE                        0x0000326C
+#define NV03_PFIFO_CACHE1_GET                              0x00003270
+#define NV04_PFIFO_CACHE1_ENGINE                           0x00003280
+#define NV04_PFIFO_CACHE1_DMA_DCOUNT                       0x000032A0
+#define NV40_PFIFO_GRCTX_INSTANCE                          0x000032E0
+#define NV40_PFIFO_UNK32E4                                 0x000032E4
+#define NV04_PFIFO_CACHE1_METHOD(i)                (0x00003800+(i*8))
+#define NV04_PFIFO_CACHE1_DATA(i)                  (0x00003804+(i*8))
+#define NV40_PFIFO_CACHE1_METHOD(i)                (0x00090000+(i*8))
+#define NV40_PFIFO_CACHE1_DATA(i)                  (0x00090004+(i*8))
+
+#define NV_CRTC0_INTSTAT                                   0x00600100
+#define NV_CRTC0_INTEN                                     0x00600140
+#define NV_CRTC1_INTSTAT                                   0x00602100
+#define NV_CRTC1_INTEN                                     0x00602140
+#    define NV_CRTC_INTR_VBLANK                                (1<<0)
+
+/* This name is a partial guess. */
+#define NV50_DISPLAY_SUPERVISOR                     0x00610024
+
+/* Fifo commands. These are not regs, neither masks */
+#define NV03_FIFO_CMD_JUMP                                 0x20000000
+#define NV03_FIFO_CMD_JUMP_OFFSET_MASK                     0x1ffffffc
+#define NV03_FIFO_CMD_REWIND                               (NV03_FIFO_CMD_JUMP | (0 & NV03_FIFO_CMD_JUMP_OFFSET_MASK))
+
+/* RAMFC offsets */
+#define NV04_RAMFC_DMA_PUT                                       0x00
+#define NV04_RAMFC_DMA_GET                                       0x04
+#define NV04_RAMFC_DMA_INSTANCE                                  0x08
+#define NV04_RAMFC_DMA_STATE                                     0x0C
+#define NV04_RAMFC_DMA_FETCH                                     0x10
+#define NV04_RAMFC_ENGINE                                        0x14
+#define NV04_RAMFC_PULL1_ENGINE                                  0x18
+
+#define NV10_RAMFC_DMA_PUT                                       0x00
+#define NV10_RAMFC_DMA_GET                                       0x04
+#define NV10_RAMFC_REF_CNT                                       0x08
+#define NV10_RAMFC_DMA_INSTANCE                                  0x0C
+#define NV10_RAMFC_DMA_STATE                                     0x10
+#define NV10_RAMFC_DMA_FETCH                                     0x14
+#define NV10_RAMFC_ENGINE                                        0x18
+#define NV10_RAMFC_PULL1_ENGINE                                  0x1C
+#define NV10_RAMFC_ACQUIRE_VALUE                                 0x20
+#define NV10_RAMFC_ACQUIRE_TIMESTAMP                             0x24
+#define NV10_RAMFC_ACQUIRE_TIMEOUT                               0x28
+#define NV10_RAMFC_SEMAPHORE                                     0x2C
+#define NV10_RAMFC_DMA_SUBROUTINE                                0x30
+
+#define NV40_RAMFC_DMA_PUT                                       0x00
+#define NV40_RAMFC_DMA_GET                                       0x04
+#define NV40_RAMFC_REF_CNT                                       0x08
+#define NV40_RAMFC_DMA_INSTANCE                                  0x0C
+#define NV40_RAMFC_DMA_DCOUNT /* ? */                            0x10
+#define NV40_RAMFC_DMA_STATE                                     0x14
+#define NV40_RAMFC_DMA_FETCH                                     0x18
+#define NV40_RAMFC_ENGINE                                        0x1C
+#define NV40_RAMFC_PULL1_ENGINE                                  0x20
+#define NV40_RAMFC_ACQUIRE_VALUE                                 0x24
+#define NV40_RAMFC_ACQUIRE_TIMESTAMP                             0x28
+#define NV40_RAMFC_ACQUIRE_TIMEOUT                               0x2C
+#define NV40_RAMFC_SEMAPHORE                                     0x30
+#define NV40_RAMFC_DMA_SUBROUTINE                                0x34
+#define NV40_RAMFC_GRCTX_INSTANCE /* guess */                    0x38
+#define NV40_RAMFC_DMA_TIMESLICE                                 0x3C
+#define NV40_RAMFC_UNK_40                                        0x40
+#define NV40_RAMFC_UNK_44                                        0x44
+#define NV40_RAMFC_UNK_48                                        0x48
+#define NV40_RAMFC_UNK_4C                                        0x4C
+#define NV40_RAMFC_UNK_50                                        0x50
diff --git a/drivers/char/drm/nouveau_sgdma.c b/drivers/char/drm/nouveau_sgdma.c
new file mode 100644
index 0000000..cc4d5a9
--- /dev/null
+++ b/drivers/char/drm/nouveau_sgdma.c
@@ -0,0 +1,341 @@
+#include "drmP.h"
+#include "nouveau_drv.h"
+
+#define NV_CTXDMA_PAGE_SHIFT 12
+#define NV_CTXDMA_PAGE_SIZE  (1 << NV_CTXDMA_PAGE_SHIFT)
+#define NV_CTXDMA_PAGE_MASK  (NV_CTXDMA_PAGE_SIZE - 1)
+
+struct nouveau_sgdma_be {
+	struct drm_ttm_backend backend;
+	struct drm_device *dev;
+
+	int         pages;
+	int         pages_populated;
+	dma_addr_t *pagelist;
+	int         is_bound;
+
+	unsigned int pte_start;
+};
+
+static int
+nouveau_sgdma_needs_ub_cache_adjust(struct drm_ttm_backend *be)
+{
+	return ((be->flags & DRM_BE_FLAG_BOUND_CACHED) ? 0 : 1);
+}
+
+static int
+nouveau_sgdma_populate(struct drm_ttm_backend *be, unsigned long num_pages,
+		       struct page **pages, struct page *dummy_read_page)
+{
+	struct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)be;
+	int p, d, o;
+
+	DRM_DEBUG("num_pages = %ld\n", num_pages);
+
+	if (nvbe->pagelist)
+		return -EINVAL;
+	nvbe->pages    = (num_pages << PAGE_SHIFT) >> NV_CTXDMA_PAGE_SHIFT;
+	nvbe->pagelist = drm_alloc(nvbe->pages*sizeof(dma_addr_t),
+				   DRM_MEM_PAGES);
+
+	nvbe->pages_populated = d = 0;
+	for (p = 0; p < num_pages; p++) {
+		for (o = 0; o < PAGE_SIZE; o += NV_CTXDMA_PAGE_SIZE) {
+			struct page *page = pages[p];
+			if (!page)
+				page = dummy_read_page;
+			nvbe->pagelist[d] = pci_map_page(nvbe->dev->pdev,
+							 page, o,
+							 NV_CTXDMA_PAGE_SIZE,
+							 PCI_DMA_BIDIRECTIONAL);
+			if (pci_dma_mapping_error(nvbe->pagelist[d])) {
+				be->func->clear(be);
+				DRM_ERROR("pci_map_page failed\n");
+				return -EINVAL;
+			}
+			nvbe->pages_populated = ++d;
+		}
+	}
+
+	return 0;
+}
+
+static void
+nouveau_sgdma_clear(struct drm_ttm_backend *be)
+{
+	struct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)be;
+	int d;
+
+	DRM_DEBUG("\n");
+
+	if (nvbe && nvbe->pagelist) {
+		if (nvbe->is_bound)
+			be->func->unbind(be);
+
+		for (d = 0; d < nvbe->pages_populated; d++) {
+			pci_unmap_page(nvbe->dev->pdev, nvbe->pagelist[d],
+				       NV_CTXDMA_PAGE_SIZE,
+				       PCI_DMA_BIDIRECTIONAL);
+		}
+		drm_free(nvbe->pagelist, nvbe->pages*sizeof(dma_addr_t),
+			 DRM_MEM_PAGES);
+	}
+}
+
+static int
+nouveau_sgdma_bind(struct drm_ttm_backend *be, struct drm_bo_mem_reg *mem)
+{
+	struct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)be;
+	struct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;
+	struct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;
+	uint64_t offset = (mem->mm_node->start << PAGE_SHIFT);
+	uint32_t i;
+
+	DRM_DEBUG("pg=0x%lx (0x%llx), cached=%d\n", mem->mm_node->start,
+		  offset, (mem->flags & DRM_BO_FLAG_CACHED) == 1);
+
+	if (offset & NV_CTXDMA_PAGE_MASK)
+		return -EINVAL;
+	nvbe->pte_start = (offset >> NV_CTXDMA_PAGE_SHIFT);
+	if (dev_priv->card_type < NV_50)
+		nvbe->pte_start += 2; /* skip ctxdma header */
+
+	for (i = nvbe->pte_start; i < nvbe->pte_start + nvbe->pages; i++) {
+		uint64_t pteval = nvbe->pagelist[i - nvbe->pte_start];
+
+		if (pteval & NV_CTXDMA_PAGE_MASK) {
+			DRM_ERROR("Bad pteval 0x%llx\n", pteval);
+			return -EINVAL;
+		}
+
+		if (dev_priv->card_type < NV_50) {
+			INSTANCE_WR(gpuobj, i, pteval | 3);
+		} else {
+			INSTANCE_WR(gpuobj, (i<<1)+0, pteval | 0x21);
+			INSTANCE_WR(gpuobj, (i<<1)+1, 0x00000000);
+		}
+	}
+
+	nvbe->is_bound  = 1;
+	return 0;
+}
+
+static int
+nouveau_sgdma_unbind(struct drm_ttm_backend *be)
+{
+	struct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)be;
+	struct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	if (nvbe->is_bound) {
+		struct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;
+		unsigned int pte;
+
+		pte = nvbe->pte_start;
+		while (pte < (nvbe->pte_start + nvbe->pages)) {
+			uint64_t pteval = dev_priv->gart_info.sg_dummy_bus;
+
+			if (dev_priv->card_type < NV_50) {
+				INSTANCE_WR(gpuobj, pte, pteval | 3);
+			} else {
+				INSTANCE_WR(gpuobj, (pte<<1)+0, pteval | 0x21);
+				INSTANCE_WR(gpuobj, (pte<<1)+1, 0x00000000);
+			}
+
+			pte++;
+		}
+
+		nvbe->is_bound = 0;
+	}
+
+	return 0;
+}
+
+static void
+nouveau_sgdma_destroy(struct drm_ttm_backend *be)
+{
+	DRM_DEBUG("\n");
+	if (be) {
+		struct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)be;
+		if (nvbe) {
+			if (nvbe->pagelist)
+				be->func->clear(be);
+			drm_ctl_free(nvbe, sizeof(*nvbe), DRM_MEM_TTM);
+		}
+	}
+}
+
+static struct drm_ttm_backend_func nouveau_sgdma_backend = {
+	.needs_ub_cache_adjust	= nouveau_sgdma_needs_ub_cache_adjust,
+	.populate		= nouveau_sgdma_populate,
+	.clear			= nouveau_sgdma_clear,
+	.bind			= nouveau_sgdma_bind,
+	.unbind			= nouveau_sgdma_unbind,
+	.destroy		= nouveau_sgdma_destroy
+};
+
+struct drm_ttm_backend *
+nouveau_sgdma_init_ttm(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_sgdma_be *nvbe;
+
+	if (!dev_priv->gart_info.sg_ctxdma)
+		return NULL;
+
+	nvbe = drm_ctl_calloc(1, sizeof(*nvbe), DRM_MEM_TTM);
+	if (!nvbe)
+		return NULL;
+
+	nvbe->dev = dev;
+
+	nvbe->backend.func	= &nouveau_sgdma_backend;
+
+	return &nvbe->backend;
+}
+
+int
+nouveau_sgdma_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *gpuobj = NULL;
+	uint32_t aper_size, obj_size;
+	int i, ret;
+
+	if (dev_priv->card_type < NV_50) {
+		aper_size = (64 * 1024 * 1024);
+		obj_size  = (aper_size >> NV_CTXDMA_PAGE_SHIFT) * 4;
+		obj_size += 8; /* ctxdma header */
+	} else {
+		/* 1 entire VM page table */
+		aper_size = (512 * 1024 * 1024);
+		obj_size  = (aper_size >> NV_CTXDMA_PAGE_SHIFT) * 8;
+	}
+
+	if ((ret = nouveau_gpuobj_new(dev, NULL, obj_size, 16,
+				      NVOBJ_FLAG_ALLOW_NO_REFS |
+				      NVOBJ_FLAG_ZERO_ALLOC |
+				      NVOBJ_FLAG_ZERO_FREE, &gpuobj)))  {
+		DRM_ERROR("Error creating sgdma object: %d\n", ret);
+		return ret;
+	}
+
+	dev_priv->gart_info.sg_dummy_page =
+		alloc_page(GFP_KERNEL|__GFP_DMA32);
+	SetPageLocked(dev_priv->gart_info.sg_dummy_page);
+	dev_priv->gart_info.sg_dummy_bus =
+		pci_map_page(dev->pdev, dev_priv->gart_info.sg_dummy_page, 0,
+			     PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+
+	if (dev_priv->card_type < NV_50) {
+		/* Maybe use NV_DMA_TARGET_AGP for PCIE? NVIDIA do this, and
+		 * confirmed to work on c51.  Perhaps means NV_DMA_TARGET_PCIE
+		 * on those cards? */
+		INSTANCE_WR(gpuobj, 0, NV_CLASS_DMA_IN_MEMORY |
+				       (1 << 12) /* PT present */ |
+				       (0 << 13) /* PT *not* linear */ |
+				       (NV_DMA_ACCESS_RW  << 14) |
+				       (NV_DMA_TARGET_PCI << 16));
+		INSTANCE_WR(gpuobj, 1, aper_size - 1);
+		for (i=2; i<2+(aper_size>>12); i++) {
+			INSTANCE_WR(gpuobj, i,
+				    dev_priv->gart_info.sg_dummy_bus | 3);
+		}
+	} else {
+		for (i=0; i<obj_size; i+=8) {
+			INSTANCE_WR(gpuobj, (i+0)/4,
+				    dev_priv->gart_info.sg_dummy_bus | 0x21);
+			INSTANCE_WR(gpuobj, (i+4)/4, 0);
+		}
+	}
+
+	dev_priv->gart_info.type      = NOUVEAU_GART_SGDMA;
+	dev_priv->gart_info.aper_base = 0;
+	dev_priv->gart_info.aper_size = aper_size;
+	dev_priv->gart_info.sg_ctxdma = gpuobj;
+	return 0;
+}
+
+void
+nouveau_sgdma_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	if (dev_priv->gart_info.sg_dummy_page) {
+		pci_unmap_page(dev->pdev, dev_priv->gart_info.sg_dummy_bus,
+			       NV_CTXDMA_PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+		unlock_page(dev_priv->gart_info.sg_dummy_page);
+		__free_page(dev_priv->gart_info.sg_dummy_page);
+		dev_priv->gart_info.sg_dummy_page = NULL;
+		dev_priv->gart_info.sg_dummy_bus = 0;
+	}
+
+	nouveau_gpuobj_del(dev, &dev_priv->gart_info.sg_ctxdma);
+}
+
+int
+nouveau_sgdma_nottm_hack_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_ttm_backend *be;
+	struct drm_scatter_gather sgreq;
+	struct drm_mm_node mm_node;
+	struct drm_bo_mem_reg mem;
+	int ret;
+
+	dev_priv->gart_info.sg_be = nouveau_sgdma_init_ttm(dev);
+	if (!dev_priv->gart_info.sg_be)
+		return -ENOMEM;
+	be = dev_priv->gart_info.sg_be;
+
+	/* Hack the aperture size down to the amount of system memory
+	 * we're going to bind into it.
+	 */
+	if (dev_priv->gart_info.aper_size > 32*1024*1024)
+		dev_priv->gart_info.aper_size = 32*1024*1024;
+
+	sgreq.size = dev_priv->gart_info.aper_size;
+	if ((ret = drm_sg_alloc(dev, &sgreq))) {
+		DRM_ERROR("drm_sg_alloc failed: %d\n", ret);
+		return ret;
+	}
+	dev_priv->gart_info.sg_handle = sgreq.handle;
+
+	if ((ret = be->func->populate(be, dev->sg->pages, dev->sg->pagelist, dev->bm.dummy_read_page))) {
+		DRM_ERROR("failed populate: %d\n", ret);
+		return ret;
+	}
+
+	mm_node.start = 0;
+	mem.mm_node = &mm_node;
+
+	if ((ret = be->func->bind(be, &mem))) {
+		DRM_ERROR("failed bind: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+void
+nouveau_sgdma_nottm_hack_takedown(struct drm_device *dev)
+{
+}
+
+int
+nouveau_sgdma_get_page(struct drm_device *dev, uint32_t offset, uint32_t *page)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;
+	int pte;
+
+	pte = (offset >> NV_CTXDMA_PAGE_SHIFT);
+	if (dev_priv->card_type < NV_50) {
+		*page = INSTANCE_RD(gpuobj, (pte + 2)) & ~NV_CTXDMA_PAGE_MASK;
+		return 0;
+	}
+
+	DRM_ERROR("Unimplemented on NV50\n");
+	return -EINVAL;
+}
diff --git a/drivers/char/drm/nouveau_state.c b/drivers/char/drm/nouveau_state.c
new file mode 100644
index 0000000..d273d4b
--- /dev/null
+++ b/drivers/char/drm/nouveau_state.c
@@ -0,0 +1,705 @@
+/*
+ * Copyright 2005 Stephane Marchesin
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "drm_sarea.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+static int nouveau_init_card_mappings(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	/* resource 0 is mmio regs */
+	/* resource 1 is linear FB */
+	/* resource 2 is RAMIN (mmio regs + 0x1000000) */
+	/* resource 6 is bios */
+
+	/* map the mmio regs */
+	ret = drm_addmap(dev, drm_get_resource_start(dev, 0),
+			      drm_get_resource_len(dev, 0),
+			      _DRM_REGISTERS, _DRM_READ_ONLY|_DRM_DRIVER, &dev_priv->mmio);
+	if (ret) {
+		DRM_ERROR("Unable to initialize the mmio mapping (%d). "
+			  "Please report your setup to " DRIVER_EMAIL "\n",
+			  ret);
+		return -EINVAL;
+	}
+	DRM_DEBUG("regs mapped ok at 0x%lx\n", dev_priv->mmio->offset);
+
+	/* map larger RAMIN aperture on NV40 cards */
+	dev_priv->ramin = NULL;
+	if (dev_priv->card_type >= NV_40) {
+		int ramin_resource = 2;
+		if (drm_get_resource_len(dev, ramin_resource) == 0)
+			ramin_resource = 3;
+
+		ret = drm_addmap(dev,
+				 drm_get_resource_start(dev, ramin_resource),
+				 drm_get_resource_len(dev, ramin_resource),
+				 _DRM_REGISTERS, _DRM_READ_ONLY|_DRM_DRIVER,
+				 &dev_priv->ramin);
+		if (ret) {
+			DRM_ERROR("Failed to init RAMIN mapping, "
+				  "limited instance memory available\n");
+			dev_priv->ramin = NULL;
+		}
+	}
+
+	/* On older cards (or if the above failed), create a map covering
+	 * the BAR0 PRAMIN aperture */
+	if (!dev_priv->ramin) {
+		ret = drm_addmap(dev,
+				 drm_get_resource_start(dev, 0) + NV_RAMIN,
+				 (1*1024*1024),
+				 _DRM_REGISTERS, _DRM_READ_ONLY|_DRM_DRIVER,
+				 &dev_priv->ramin);
+		if (ret) {
+			DRM_ERROR("Failed to map BAR0 PRAMIN: %d\n", ret);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int nouveau_stub_init(struct drm_device *dev) { return 0; }
+static void nouveau_stub_takedown(struct drm_device *dev) {}
+
+static int nouveau_init_engine_ptrs(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+
+	switch (dev_priv->chipset & 0xf0) {
+	case 0x00:
+		engine->instmem.init	= nv04_instmem_init;
+		engine->instmem.takedown= nv04_instmem_takedown;
+		engine->instmem.populate	= nv04_instmem_populate;
+		engine->instmem.clear		= nv04_instmem_clear;
+		engine->instmem.bind		= nv04_instmem_bind;
+		engine->instmem.unbind		= nv04_instmem_unbind;
+		engine->mc.init		= nv04_mc_init;
+		engine->mc.takedown	= nv04_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nv04_fb_init;
+		engine->fb.takedown	= nv04_fb_takedown;
+		engine->graph.init	= nv04_graph_init;
+		engine->graph.takedown	= nv04_graph_takedown;
+		engine->graph.create_context	= nv04_graph_create_context;
+		engine->graph.destroy_context	= nv04_graph_destroy_context;
+		engine->graph.load_context	= nv04_graph_load_context;
+		engine->graph.save_context	= nv04_graph_save_context;
+		engine->fifo.channels	= 16;
+		engine->fifo.init	= nouveau_fifo_init;
+		engine->fifo.takedown	= nouveau_stub_takedown;
+		engine->fifo.channel_id		= nv04_fifo_channel_id;
+		engine->fifo.create_context	= nv04_fifo_create_context;
+		engine->fifo.destroy_context	= nv04_fifo_destroy_context;
+		engine->fifo.load_context	= nv04_fifo_load_context;
+		engine->fifo.save_context	= nv04_fifo_save_context;
+		break;
+	case 0x10:
+		engine->instmem.init	= nv04_instmem_init;
+		engine->instmem.takedown= nv04_instmem_takedown;
+		engine->instmem.populate	= nv04_instmem_populate;
+		engine->instmem.clear		= nv04_instmem_clear;
+		engine->instmem.bind		= nv04_instmem_bind;
+		engine->instmem.unbind		= nv04_instmem_unbind;
+		engine->mc.init		= nv04_mc_init;
+		engine->mc.takedown	= nv04_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nv10_fb_init;
+		engine->fb.takedown	= nv10_fb_takedown;
+		engine->graph.init	= nv10_graph_init;
+		engine->graph.takedown	= nv10_graph_takedown;
+		engine->graph.create_context	= nv10_graph_create_context;
+		engine->graph.destroy_context	= nv10_graph_destroy_context;
+		engine->graph.load_context	= nv10_graph_load_context;
+		engine->graph.save_context	= nv10_graph_save_context;
+		engine->fifo.channels	= 32;
+		engine->fifo.init	= nouveau_fifo_init;
+		engine->fifo.takedown	= nouveau_stub_takedown;
+		engine->fifo.channel_id		= nv10_fifo_channel_id;
+		engine->fifo.create_context	= nv10_fifo_create_context;
+		engine->fifo.destroy_context	= nv10_fifo_destroy_context;
+		engine->fifo.load_context	= nv10_fifo_load_context;
+		engine->fifo.save_context	= nv10_fifo_save_context;
+		break;
+	case 0x20:
+		engine->instmem.init	= nv04_instmem_init;
+		engine->instmem.takedown= nv04_instmem_takedown;
+		engine->instmem.populate	= nv04_instmem_populate;
+		engine->instmem.clear		= nv04_instmem_clear;
+		engine->instmem.bind		= nv04_instmem_bind;
+		engine->instmem.unbind		= nv04_instmem_unbind;
+		engine->mc.init		= nv04_mc_init;
+		engine->mc.takedown	= nv04_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nv10_fb_init;
+		engine->fb.takedown	= nv10_fb_takedown;
+		engine->graph.init	= nv20_graph_init;
+		engine->graph.takedown	= nv20_graph_takedown;
+		engine->graph.create_context	= nv20_graph_create_context;
+		engine->graph.destroy_context	= nv20_graph_destroy_context;
+		engine->graph.load_context	= nv20_graph_load_context;
+		engine->graph.save_context	= nv20_graph_save_context;
+		engine->fifo.channels	= 32;
+		engine->fifo.init	= nouveau_fifo_init;
+		engine->fifo.takedown	= nouveau_stub_takedown;
+		engine->fifo.channel_id		= nv10_fifo_channel_id;
+		engine->fifo.create_context	= nv10_fifo_create_context;
+		engine->fifo.destroy_context	= nv10_fifo_destroy_context;
+		engine->fifo.load_context	= nv10_fifo_load_context;
+		engine->fifo.save_context	= nv10_fifo_save_context;
+		break;
+	case 0x30:
+		engine->instmem.init	= nv04_instmem_init;
+		engine->instmem.takedown= nv04_instmem_takedown;
+		engine->instmem.populate	= nv04_instmem_populate;
+		engine->instmem.clear		= nv04_instmem_clear;
+		engine->instmem.bind		= nv04_instmem_bind;
+		engine->instmem.unbind		= nv04_instmem_unbind;
+		engine->mc.init		= nv04_mc_init;
+		engine->mc.takedown	= nv04_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nv10_fb_init;
+		engine->fb.takedown	= nv10_fb_takedown;
+		engine->graph.init	= nv30_graph_init;
+		engine->graph.takedown	= nv20_graph_takedown;
+		engine->graph.create_context	= nv20_graph_create_context;
+		engine->graph.destroy_context	= nv20_graph_destroy_context;
+		engine->graph.load_context	= nv20_graph_load_context;
+		engine->graph.save_context	= nv20_graph_save_context;
+		engine->fifo.channels	= 32;
+		engine->fifo.init	= nouveau_fifo_init;
+		engine->fifo.takedown	= nouveau_stub_takedown;
+		engine->fifo.channel_id		= nv10_fifo_channel_id;
+		engine->fifo.create_context	= nv10_fifo_create_context;
+		engine->fifo.destroy_context	= nv10_fifo_destroy_context;
+		engine->fifo.load_context	= nv10_fifo_load_context;
+		engine->fifo.save_context	= nv10_fifo_save_context;
+		break;
+	case 0x40:
+	case 0x60:
+		engine->instmem.init	= nv04_instmem_init;
+		engine->instmem.takedown= nv04_instmem_takedown;
+		engine->instmem.populate	= nv04_instmem_populate;
+		engine->instmem.clear		= nv04_instmem_clear;
+		engine->instmem.bind		= nv04_instmem_bind;
+		engine->instmem.unbind		= nv04_instmem_unbind;
+		engine->mc.init		= nv40_mc_init;
+		engine->mc.takedown	= nv40_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nv40_fb_init;
+		engine->fb.takedown	= nv40_fb_takedown;
+		engine->graph.init	= nv40_graph_init;
+		engine->graph.takedown	= nv40_graph_takedown;
+		engine->graph.create_context	= nv40_graph_create_context;
+		engine->graph.destroy_context	= nv40_graph_destroy_context;
+		engine->graph.load_context	= nv40_graph_load_context;
+		engine->graph.save_context	= nv40_graph_save_context;
+		engine->fifo.channels	= 32;
+		engine->fifo.init	= nv40_fifo_init;
+		engine->fifo.takedown	= nouveau_stub_takedown;
+		engine->fifo.channel_id		= nv10_fifo_channel_id;
+		engine->fifo.create_context	= nv40_fifo_create_context;
+		engine->fifo.destroy_context	= nv40_fifo_destroy_context;
+		engine->fifo.load_context	= nv40_fifo_load_context;
+		engine->fifo.save_context	= nv40_fifo_save_context;
+		break;
+	case 0x50:
+	case 0x80: /* gotta love NVIDIA's consistency.. */
+		engine->instmem.init	= nv50_instmem_init;
+		engine->instmem.takedown= nv50_instmem_takedown;
+		engine->instmem.populate	= nv50_instmem_populate;
+		engine->instmem.clear		= nv50_instmem_clear;
+		engine->instmem.bind		= nv50_instmem_bind;
+		engine->instmem.unbind		= nv50_instmem_unbind;
+		engine->mc.init		= nv50_mc_init;
+		engine->mc.takedown	= nv50_mc_takedown;
+		engine->timer.init	= nv04_timer_init;
+		engine->timer.read	= nv04_timer_read;
+		engine->timer.takedown	= nv04_timer_takedown;
+		engine->fb.init		= nouveau_stub_init;
+		engine->fb.takedown	= nouveau_stub_takedown;
+		engine->graph.init	= nv50_graph_init;
+		engine->graph.takedown	= nv50_graph_takedown;
+		engine->graph.create_context	= nv50_graph_create_context;
+		engine->graph.destroy_context	= nv50_graph_destroy_context;
+		engine->graph.load_context	= nv50_graph_load_context;
+		engine->graph.save_context	= nv50_graph_save_context;
+		engine->fifo.channels	= 128;
+		engine->fifo.init	= nv50_fifo_init;
+		engine->fifo.takedown	= nv50_fifo_takedown;
+		engine->fifo.channel_id		= nv50_fifo_channel_id;
+		engine->fifo.create_context	= nv50_fifo_create_context;
+		engine->fifo.destroy_context	= nv50_fifo_destroy_context;
+		engine->fifo.load_context	= nv50_fifo_load_context;
+		engine->fifo.save_context	= nv50_fifo_save_context;
+		break;
+	default:
+		DRM_ERROR("NV%02x unsupported\n", dev_priv->chipset);
+		return 1;
+	}
+
+	return 0;
+}
+
+int
+nouveau_card_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine;
+	int ret;
+
+	DRM_DEBUG("prev state = %d\n", dev_priv->init_state);
+
+	if (dev_priv->init_state == NOUVEAU_CARD_INIT_DONE)
+		return 0;
+	dev_priv->ttm = 0;
+
+	/* Determine exact chipset we're running on */
+	if (dev_priv->card_type < NV_10)
+		dev_priv->chipset = dev_priv->card_type;
+	else
+		dev_priv->chipset =
+			(NV_READ(NV03_PMC_BOOT_0) & 0x0ff00000) >> 20;
+
+	/* Initialise internal driver API hooks */
+	ret = nouveau_init_engine_ptrs(dev);
+	if (ret) return ret;
+	engine = &dev_priv->Engine;
+	dev_priv->init_state = NOUVEAU_CARD_INIT_FAILED;
+
+	ret = nouveau_gpuobj_early_init(dev);
+	if (ret) return ret;
+
+	/* Initialise instance memory, must happen before mem_init so we
+	 * know exactly how much VRAM we're able to use for "normal"
+	 * purposes.
+	 */
+	ret = engine->instmem.init(dev);
+	if (ret) return ret;
+
+	/* Setup the memory manager */
+	if (dev_priv->ttm) {
+		ret = nouveau_mem_init_ttm(dev);
+		if (ret) return ret;
+	} else {
+		ret = nouveau_mem_init(dev);
+		if (ret) return ret;
+	}
+
+	ret = nouveau_gpuobj_init(dev);
+	if (ret) return ret;
+
+	/* Parse BIOS tables / Run init tables? */
+
+	/* PMC */
+	ret = engine->mc.init(dev);
+	if (ret) return ret;
+
+	/* PTIMER */
+	ret = engine->timer.init(dev);
+	if (ret) return ret;
+
+	/* PFB */
+	ret = engine->fb.init(dev);
+	if (ret) return ret;
+
+	/* PGRAPH */
+	ret = engine->graph.init(dev);
+	if (ret) return ret;
+
+	/* PFIFO */
+	ret = engine->fifo.init(dev);
+	if (ret) return ret;
+
+	/* this call irq_preinstall, register irq handler and
+	 * call irq_postinstall
+	 */
+	ret = drm_irq_install(dev);
+	if (ret) return ret;
+
+	/* what about PVIDEO/PCRTC/PRAMDAC etc? */
+
+	ret = nouveau_dma_channel_init(dev);
+	if (ret) return ret;
+
+	dev_priv->init_state = NOUVEAU_CARD_INIT_DONE;
+	return 0;
+}
+
+static void nouveau_card_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+
+	DRM_DEBUG("prev state = %d\n", dev_priv->init_state);
+
+	if (dev_priv->init_state != NOUVEAU_CARD_INIT_DOWN) {
+		nouveau_dma_channel_takedown(dev);
+
+		engine->fifo.takedown(dev);
+		engine->graph.takedown(dev);
+		engine->fb.takedown(dev);
+		engine->timer.takedown(dev);
+		engine->mc.takedown(dev);
+
+		nouveau_sgdma_nottm_hack_takedown(dev);
+		nouveau_sgdma_takedown(dev);
+
+		nouveau_gpuobj_takedown(dev);
+		nouveau_gpuobj_del(dev, &dev_priv->vm_vram_pt);
+
+		nouveau_mem_close(dev);
+		engine->instmem.takedown(dev);
+
+		drm_irq_uninstall(dev);
+
+		nouveau_gpuobj_late_takedown(dev);
+
+		dev_priv->init_state = NOUVEAU_CARD_INIT_DOWN;
+	}
+}
+
+/* here a client dies, release the stuff that was allocated for its
+ * file_priv */
+void nouveau_preclose(struct drm_device *dev, struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	nouveau_fifo_cleanup(dev, file_priv);
+	nouveau_mem_release(file_priv,dev_priv->fb_heap);
+	nouveau_mem_release(file_priv,dev_priv->agp_heap);
+	nouveau_mem_release(file_priv,dev_priv->pci_heap);
+}
+
+/* first module load, setup the mmio/fb mapping */
+int nouveau_firstopen(struct drm_device *dev)
+{
+#if defined(__powerpc__)
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct device_node *dn;
+#endif
+	int ret;
+
+	/* Map any PCI resources we need on the card */
+	ret = nouveau_init_card_mappings(dev);
+	if (ret) return ret;
+
+#if defined(__powerpc__)
+	/* Put the card in BE mode if it's not */
+	if (NV_READ(NV03_PMC_BOOT_1))
+		NV_WRITE(NV03_PMC_BOOT_1,0x00000001);
+
+	DRM_MEMORYBARRIER();
+#endif
+
+#if defined(__linux__) && defined(__powerpc__)
+	/* if we have an OF card, copy vbios to RAMIN */
+	dn = pci_device_to_OF_node(dev->pdev);
+	if (dn)
+	{
+		int size;
+		const uint32_t *bios = of_get_property(dn, "NVDA,BMP", &size);
+		if (bios)
+		{
+			int i;
+			for(i=0;i<size;i+=4)
+				NV_WI32(i, bios[i/4]);
+			DRM_INFO("OF bios successfully copied (%d bytes)\n",size);
+		}
+		else
+			DRM_INFO("Unable to get the OF bios\n");
+	}
+	else
+		DRM_INFO("Unable to get the OF node\n");
+#endif
+	return 0;
+}
+
+#define NV40_CHIPSET_MASK 0x00000baf
+#define NV44_CHIPSET_MASK 0x00005450
+
+int nouveau_load(struct drm_device *dev, unsigned long flags)
+{
+	struct drm_nouveau_private *dev_priv;
+	void __iomem *regs;
+	uint32_t reg0,reg1;
+	uint8_t architecture = 0;
+
+	dev_priv = drm_calloc(1, sizeof(*dev_priv), DRM_MEM_DRIVER);
+	if (!dev_priv)
+		return -ENOMEM;
+
+	dev_priv->flags = flags & NOUVEAU_FLAGS;
+	dev_priv->init_state = NOUVEAU_CARD_INIT_DOWN;
+
+	DRM_DEBUG("vendor: 0x%X device: 0x%X class: 0x%X\n", dev->pci_vendor, dev->pci_device, dev->pdev->class);
+
+	/* Time to determine the card architecture */
+	regs = ioremap_nocache(pci_resource_start(dev->pdev, 0), 0x8);
+	if (!regs) {
+		DRM_ERROR("Could not ioremap to determine register\n");
+		return -ENOMEM;
+	}
+
+	reg0 = readl(regs+NV03_PMC_BOOT_0);
+	reg1 = readl(regs+NV03_PMC_BOOT_1);
+#if defined(__powerpc__)
+	if (reg1)
+		reg0=___swab32(reg0);
+#endif
+
+	/* We're dealing with >=NV10 */
+	if ((reg0 & 0x0f000000) > 0 ) {
+		/* Bit 27-20 contain the architecture in hex */
+		architecture = (reg0 & 0xff00000) >> 20;
+	/* NV04 or NV05 */
+	} else if ((reg0 & 0xff00fff0) == 0x20004000) {
+		architecture = 0x04;
+	}
+
+	iounmap(regs);
+
+	if (architecture >= 0x80) {
+		dev_priv->card_type = NV_50;
+	} else if (architecture >= 0x60) {
+		/* FIXME we need to figure out who's who for NV6x */
+		dev_priv->card_type = NV_44;
+	} else if (architecture >= 0x50) {
+		dev_priv->card_type = NV_50;
+	} else if (architecture >= 0x40) {
+		uint8_t subarch = architecture & 0xf;
+		/* Selection criteria borrowed from NV40EXA */
+		if (NV40_CHIPSET_MASK & (1 << subarch)) {
+			dev_priv->card_type = NV_40;
+		} else if (NV44_CHIPSET_MASK & (1 << subarch)) {
+			dev_priv->card_type = NV_44;
+		} else {
+			dev_priv->card_type = NV_UNKNOWN;
+		}
+	} else if (architecture >= 0x30) {
+		dev_priv->card_type = NV_30;
+	} else if (architecture >= 0x20) {
+		dev_priv->card_type = NV_20;
+	} else if (architecture >= 0x17) {
+		dev_priv->card_type = NV_17;
+	} else if (architecture >= 0x11) {
+		dev_priv->card_type = NV_11;
+	} else if (architecture >= 0x10) {
+		dev_priv->card_type = NV_10;
+	} else if (architecture >= 0x04) {
+		dev_priv->card_type = NV_04;
+	} else {
+		dev_priv->card_type = NV_UNKNOWN;
+	}
+
+	DRM_INFO("Detected an NV%d generation card (0x%08x)\n", dev_priv->card_type,reg0);
+
+	if (dev_priv->card_type == NV_UNKNOWN) {
+		return -EINVAL;
+	}
+
+	/* Special flags */
+	if (dev->pci_device == 0x01a0) {
+		dev_priv->flags |= NV_NFORCE;
+	} else if (dev->pci_device == 0x01f0) {
+		dev_priv->flags |= NV_NFORCE2;
+	}
+
+	dev->dev_private = (void *)dev_priv;
+
+	return 0;
+}
+
+void nouveau_lastclose(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* In the case of an error dev_priv may not be be allocated yet */
+	if (dev_priv && dev_priv->card_type) {
+		nouveau_card_takedown(dev);
+
+		if(dev_priv->fb_mtrr>0)
+		{
+			drm_mtrr_del(dev_priv->fb_mtrr, drm_get_resource_start(dev, 1),nouveau_mem_fb_amount(dev), DRM_MTRR_WC);
+			dev_priv->fb_mtrr=0;
+		}
+
+		if (dev_priv->ramin) {
+			drm_rmmap(dev, dev_priv->ramin);
+			dev_priv->ramin = NULL;
+		}
+		if (dev_priv->mmio) {
+			drm_rmmap(dev, dev_priv->mmio);
+			dev_priv->mmio = NULL;
+		}
+	}
+}
+
+int nouveau_unload(struct drm_device *dev)
+{
+	drm_free(dev->dev_private, sizeof(*dev->dev_private), DRM_MEM_DRIVER);
+	dev->dev_private = NULL;
+	return 0;
+}
+
+int
+nouveau_ioctl_card_init(struct drm_device *dev, void *data,
+			struct drm_file *file_priv)
+{
+	return nouveau_card_init(dev);
+}
+
+int nouveau_ioctl_getparam(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_nouveau_getparam *getparam = data;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+
+	switch (getparam->param) {
+	case NOUVEAU_GETPARAM_CHIPSET_ID:
+		getparam->value = dev_priv->chipset;
+		break;
+	case NOUVEAU_GETPARAM_PCI_VENDOR:
+		getparam->value=dev->pci_vendor;
+		break;
+	case NOUVEAU_GETPARAM_PCI_DEVICE:
+		getparam->value=dev->pci_device;
+		break;
+	case NOUVEAU_GETPARAM_BUS_TYPE:
+		if (drm_device_is_agp(dev))
+			getparam->value=NV_AGP;
+		else if (drm_device_is_pcie(dev))
+			getparam->value=NV_PCIE;
+		else
+			getparam->value=NV_PCI;
+		break;
+	case NOUVEAU_GETPARAM_FB_PHYSICAL:
+		getparam->value=dev_priv->fb_phys;
+		break;
+	case NOUVEAU_GETPARAM_AGP_PHYSICAL:
+		getparam->value=dev_priv->gart_info.aper_base;
+		break;
+	case NOUVEAU_GETPARAM_PCI_PHYSICAL:
+		if ( dev -> sg )
+		  getparam->value=(unsigned long)(void *)dev->sg->virtual;
+		else
+		     {
+		     DRM_ERROR("Requested PCIGART address, while no PCIGART was created\n");
+		     return -EINVAL;
+		     }
+		break;
+	case NOUVEAU_GETPARAM_FB_SIZE:
+		getparam->value=dev_priv->fb_available_size;
+		break;
+	case NOUVEAU_GETPARAM_AGP_SIZE:
+		getparam->value=dev_priv->gart_info.aper_size;
+		break;
+	default:
+		DRM_ERROR("unknown parameter %lld\n", getparam->param);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int nouveau_ioctl_setparam(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct drm_nouveau_setparam *setparam = data;
+
+	NOUVEAU_CHECK_INITIALISED_WITH_RETURN;
+
+	switch (setparam->param) {
+	case NOUVEAU_SETPARAM_CMDBUF_LOCATION:
+		switch (setparam->value) {
+		case NOUVEAU_MEM_AGP:
+		case NOUVEAU_MEM_FB:
+		case NOUVEAU_MEM_PCI:
+		case NOUVEAU_MEM_AGP | NOUVEAU_MEM_PCI_ACCEPTABLE:
+			break;
+		default:
+			DRM_ERROR("invalid CMDBUF_LOCATION value=%lld\n",
+					setparam->value);
+			return -EINVAL;
+		}
+		dev_priv->config.cmdbuf.location = setparam->value;
+		break;
+	case NOUVEAU_SETPARAM_CMDBUF_SIZE:
+		dev_priv->config.cmdbuf.size = setparam->value;
+		break;
+	default:
+		DRM_ERROR("unknown parameter %lld\n", setparam->param);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* waits for idle */
+void nouveau_wait_for_idle(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv=dev->dev_private;
+	switch(dev_priv->card_type) {
+	case NV_50:
+		break;
+	default: {
+		/* This stuff is more or less a copy of what is seen
+		 * in nv28 kmmio dump.
+		 */
+		uint64_t started = dev_priv->Engine.timer.read(dev);
+		uint64_t stopped = started;
+		uint32_t status;
+		do {
+			uint32_t pmc_e = NV_READ(NV03_PMC_ENABLE);
+			(void)pmc_e;
+			status = NV_READ(NV04_PGRAPH_STATUS);
+			if (!status)
+				break;
+			stopped = dev_priv->Engine.timer.read(dev);
+		/* It'll never wrap anyway... */
+		} while (stopped - started < 1000000000ULL);
+		if (status)
+			DRM_ERROR("timed out with status 0x%08x\n",
+			          status);
+	}
+	}
+}
diff --git a/drivers/char/drm/nouveau_swmthd.c b/drivers/char/drm/nouveau_swmthd.c
new file mode 100644
index 0000000..c3666bf
--- /dev/null
+++ b/drivers/char/drm/nouveau_swmthd.c
@@ -0,0 +1,191 @@
+/*
+ * Copyright (C) 2007 Arthur Huillet.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/*
+ * Authors:
+ *   Arthur Huillet <arthur.huillet AT free DOT fr>
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_reg.h"
+
+/*TODO: add a "card_type" attribute*/
+typedef struct{
+	uint32_t oclass; /* object class for this software method */
+	uint32_t mthd; /* method number */
+	void (*method_code)(struct drm_device *dev, uint32_t oclass, uint32_t mthd); /* pointer to the function that does the work */
+ } nouveau_software_method_t;
+
+
+ /* This function handles the NV04 setcontext software methods.
+One function for all because they are very similar.*/
+static void nouveau_NV04_setcontext_sw_method(struct drm_device *dev, uint32_t oclass, uint32_t mthd) {
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst_loc = NV_READ(NV04_PGRAPH_CTX_SWITCH4) & 0xFFFF;
+	uint32_t value_to_set = 0, bit_to_set = 0;
+
+	switch ( oclass ) {
+		case 0x4a:
+			switch ( mthd ) {
+				case 0x188 :
+				case 0x18c :
+					bit_to_set = 0;
+					break;
+				case 0x198 :
+					bit_to_set = 1 << 24; /*PATCH_STATUS_VALID*/
+					break;
+				case 0x2fc :
+					bit_to_set = NV_READ(NV04_PGRAPH_TRAPPED_DATA) << 15; /*PATCH_CONFIG = NV04_PGRAPH_TRAPPED_DATA*/
+					break;
+				default : ;
+				};
+			break;
+		case 0x5c:
+			switch ( mthd ) {
+				case 0x184:
+					bit_to_set = 1 << 13; /*USER_CLIP_ENABLE*/
+					break;
+				case 0x188:
+				case 0x18c:
+					bit_to_set = 0;
+					break;
+				case 0x198:
+					bit_to_set = 1 << 24; /*PATCH_STATUS_VALID*/
+					break;
+				case 0x2fc :
+					bit_to_set = NV_READ(NV04_PGRAPH_TRAPPED_DATA) << 15; /*PATCH_CONFIG = NV04_PGRAPH_TRAPPED_DATA*/
+					break;
+			};
+			break;
+		case 0x5f:
+			switch ( mthd ) {
+				case 0x184 :
+					bit_to_set = 1 << 12; /*CHROMA_KEY_ENABLE*/
+					break;
+				case 0x188 :
+					bit_to_set = 1 << 13; /*USER_CLIP_ENABLE*/
+					break;
+				case 0x18c :
+				case 0x190 :
+					bit_to_set = 0;
+					break;
+				case 0x19c :
+					bit_to_set = 1 << 24; /*PATCH_STATUS_VALID*/
+					break;
+				case 0x2fc :
+					bit_to_set = NV_READ(NV04_PGRAPH_TRAPPED_DATA) << 15; /*PATCH_CONFIG = NV04_PGRAPH_TRAPPED_DATA*/
+					break;
+			};
+			break;
+		case 0x61:
+			switch ( mthd ) {
+				case 0x188 :
+					bit_to_set = 1 << 13; /*USER_CLIP_ENABLE*/
+					break;
+				case 0x18c :
+				case 0x190 :
+					bit_to_set = 0;
+					break;
+				case 0x19c :
+					bit_to_set = 1 << 24; /*PATCH_STATUS_VALID*/
+					break;
+				case 0x2fc :
+					bit_to_set = NV_READ(NV04_PGRAPH_TRAPPED_DATA) << 15; /*PATCH_CONFIG = NV04_PGRAPH_TRAPPED_DATA*/
+					break;
+			};
+			break;
+		case 0x77:
+			switch ( mthd ) {
+				case 0x198 :
+					bit_to_set = 1 << 24; /*PATCH_STATUS_VALID*/
+					break;
+				case 0x304 :
+					bit_to_set = NV_READ(NV04_PGRAPH_TRAPPED_DATA) << 15; //PATCH_CONFIG
+					break;
+			};
+			break;
+		default :;
+		};
+
+	value_to_set = (NV_READ(0x00700000 | inst_loc << 4))| bit_to_set;
+
+	/*RAMIN*/
+	nouveau_wait_for_idle(dev);
+	NV_WRITE(0x00700000 | inst_loc << 4, value_to_set);
+
+	/*DRM_DEBUG("CTX_SWITCH1 value is %#x\n", NV_READ(NV04_PGRAPH_CTX_SWITCH1));*/
+	NV_WRITE(NV04_PGRAPH_CTX_SWITCH1, value_to_set);
+
+	/*DRM_DEBUG("CTX_CACHE1 + xxx value is %#x\n", NV_READ(NV04_PGRAPH_CTX_CACHE1 + (((NV_READ(NV04_PGRAPH_TRAPPED_ADDR) >> 13) & 0x7) << 2)));*/
+	NV_WRITE(NV04_PGRAPH_CTX_CACHE1 + (((NV_READ(NV04_PGRAPH_TRAPPED_ADDR) >> 13) & 0x7) << 2), value_to_set);
+}
+
+ nouveau_software_method_t nouveau_sw_methods[] = {
+	/*NV04 context software methods*/
+	{ 0x4a, 0x188, nouveau_NV04_setcontext_sw_method },
+	{ 0x4a, 0x18c, nouveau_NV04_setcontext_sw_method },
+	{ 0x4a, 0x198, nouveau_NV04_setcontext_sw_method },
+	{ 0x4a, 0x2fc, nouveau_NV04_setcontext_sw_method },
+	{ 0x5c, 0x184, nouveau_NV04_setcontext_sw_method },
+	{ 0x5c, 0x188, nouveau_NV04_setcontext_sw_method },
+	{ 0x5c, 0x18c, nouveau_NV04_setcontext_sw_method },
+	{ 0x5c, 0x198, nouveau_NV04_setcontext_sw_method },
+	{ 0x5c, 0x2fc, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x184, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x188, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x18c, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x190, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x19c, nouveau_NV04_setcontext_sw_method },
+	{ 0x5f, 0x2fc, nouveau_NV04_setcontext_sw_method },
+	{ 0x61, 0x188, nouveau_NV04_setcontext_sw_method },
+	{ 0x61, 0x18c, nouveau_NV04_setcontext_sw_method },
+	{ 0x61, 0x190, nouveau_NV04_setcontext_sw_method },
+	{ 0x61, 0x19c, nouveau_NV04_setcontext_sw_method },
+	{ 0x61, 0x2fc, nouveau_NV04_setcontext_sw_method },
+	{ 0x77, 0x198, nouveau_NV04_setcontext_sw_method },
+	{ 0x77, 0x304, nouveau_NV04_setcontext_sw_method },
+	/*terminator*/
+	{ 0x0, 0x0, NULL, },
+ };
+
+ int nouveau_sw_method_execute(struct drm_device *dev, uint32_t oclass, uint32_t method) {
+	int i = 0;
+	while ( nouveau_sw_methods[ i ] . method_code != NULL )
+		{
+		if ( nouveau_sw_methods[ i ] . oclass == oclass && nouveau_sw_methods[ i ] . mthd == method )
+			{
+			nouveau_sw_methods[ i ] . method_code(dev, oclass, method);
+			return 0;
+			}
+		i ++;
+		}
+
+	 return 1;
+ }
diff --git a/drivers/char/drm/nouveau_swmthd.h b/drivers/char/drm/nouveau_swmthd.h
new file mode 100644
index 0000000..5b9409f
--- /dev/null
+++ b/drivers/char/drm/nouveau_swmthd.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright (C) 2007 Arthur Huillet.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/*
+ * Authors:
+ *   Arthur Huillet <arthur.huillet AT free DOT fr>
+ */
+
+int nouveau_sw_method_execute(struct drm_device *dev, uint32_t oclass, uint32_t method); /* execute the given software method, returns 0 on success */
diff --git a/drivers/char/drm/nv04_fb.c b/drivers/char/drm/nv04_fb.c
new file mode 100644
index 0000000..58a9247
--- /dev/null
+++ b/drivers/char/drm/nv04_fb.c
@@ -0,0 +1,23 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv04_fb_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* This is what the DDX did for NV_ARCH_04, but a mmio-trace shows
+	 * nvidia reading PFB_CFG_0, then writing back its original value.
+	 * (which was 0x701114 in this case)
+	 */
+	NV_WRITE(NV04_PFB_CFG0, 0x1114);
+
+	return 0;
+}
+
+void
+nv04_fb_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv04_fifo.c b/drivers/char/drm/nv04_fifo.c
new file mode 100644
index 0000000..230c8e7
--- /dev/null
+++ b/drivers/char/drm/nv04_fifo.c
@@ -0,0 +1,138 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+#define RAMFC_WR(offset,val) INSTANCE_WR(chan->ramfc->gpuobj, \
+					 NV04_RAMFC_##offset/4, (val))
+#define RAMFC_RD(offset)     INSTANCE_RD(chan->ramfc->gpuobj, \
+					 NV04_RAMFC_##offset/4)
+#define NV04_RAMFC(c) (dev_priv->ramfc_offset + ((c) * NV04_RAMFC__SIZE))
+#define NV04_RAMFC__SIZE 32
+
+int
+nv04_fifo_channel_id(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	return (NV_READ(NV03_PFIFO_CACHE1_PUSH1) &
+			NV03_PFIFO_CACHE1_PUSH1_CHID_MASK);
+}
+
+int
+nv04_fifo_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if ((ret = nouveau_gpuobj_new_fake(dev, NV04_RAMFC(chan->id), ~0,
+						NV04_RAMFC__SIZE,
+						NVOBJ_FLAG_ZERO_ALLOC |
+						NVOBJ_FLAG_ZERO_FREE,
+						NULL, &chan->ramfc)))
+		return ret;
+
+	/* Setup initial state */
+	RAMFC_WR(DMA_PUT, chan->pushbuf_base);
+	RAMFC_WR(DMA_GET, chan->pushbuf_base);
+	RAMFC_WR(DMA_INSTANCE, chan->pushbuf->instance >> 4);
+	RAMFC_WR(DMA_FETCH, (NV_PFIFO_CACHE1_DMA_FETCH_TRIG_128_BYTES |
+			     NV_PFIFO_CACHE1_DMA_FETCH_SIZE_128_BYTES |
+			     NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_8 |
+#ifdef __BIG_ENDIAN
+			     NV_PFIFO_CACHE1_BIG_ENDIAN |
+#endif
+			     0));
+
+	/* enable the fifo dma operation */
+	NV_WRITE(NV04_PFIFO_MODE,NV_READ(NV04_PFIFO_MODE) | (1<<chan->id));
+	return 0;
+}
+
+void
+nv04_fifo_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV04_PFIFO_MODE, NV_READ(NV04_PFIFO_MODE)&~(1<<chan->id));
+
+	nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+}
+
+int
+nv04_fifo_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH1,
+		 NV03_PFIFO_CACHE1_PUSH1_DMA | chan->id);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_GET, RAMFC_RD(DMA_GET));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUT, RAMFC_RD(DMA_PUT));
+
+	tmp = RAMFC_RD(DMA_INSTANCE);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_INSTANCE, tmp & 0xFFFF);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_DCOUNT, tmp >> 16);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_STATE, RAMFC_RD(DMA_STATE));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_FETCH, RAMFC_RD(DMA_FETCH));
+	NV_WRITE(NV04_PFIFO_CACHE1_ENGINE, RAMFC_RD(ENGINE));
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL1, RAMFC_RD(PULL1_ENGINE));
+
+	/* Reset NV04_PFIFO_CACHE1_DMA_CTL_AT_INFO to INVALID */
+	tmp = NV_READ(NV04_PFIFO_CACHE1_DMA_CTL) & ~(1<<31);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_CTL, tmp);
+
+	return 0;
+}
+
+int
+nv04_fifo_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	RAMFC_WR(DMA_PUT, NV04_PFIFO_CACHE1_DMA_PUT);
+	RAMFC_WR(DMA_GET, NV04_PFIFO_CACHE1_DMA_GET);
+
+	tmp  = NV_READ(NV04_PFIFO_CACHE1_DMA_DCOUNT) << 16;
+	tmp |= NV_READ(NV04_PFIFO_CACHE1_DMA_INSTANCE);
+	RAMFC_WR(DMA_INSTANCE, tmp);
+
+	RAMFC_WR(DMA_STATE, NV_READ(NV04_PFIFO_CACHE1_DMA_STATE));
+	RAMFC_WR(DMA_FETCH, NV_READ(NV04_PFIFO_CACHE1_DMA_FETCH));
+	RAMFC_WR(ENGINE, NV_READ(NV04_PFIFO_CACHE1_ENGINE));
+	RAMFC_WR(PULL1_ENGINE, NV_READ(NV04_PFIFO_CACHE1_PULL1));
+
+	return 0;
+}
diff --git a/drivers/char/drm/nv04_graph.c b/drivers/char/drm/nv04_graph.c
new file mode 100644
index 0000000..6caae25
--- /dev/null
+++ b/drivers/char/drm/nv04_graph.c
@@ -0,0 +1,516 @@
+/*
+ * Copyright 2007 Stephane Marchesin
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drm.h"
+#include "nouveau_drv.h"
+
+static uint32_t nv04_graph_ctx_regs [] = {
+	NV04_PGRAPH_CTX_SWITCH1,
+	NV04_PGRAPH_CTX_SWITCH2,
+	NV04_PGRAPH_CTX_SWITCH3,
+	NV04_PGRAPH_CTX_SWITCH4,
+	NV04_PGRAPH_CTX_CACHE1,
+	NV04_PGRAPH_CTX_CACHE2,
+	NV04_PGRAPH_CTX_CACHE3,
+	NV04_PGRAPH_CTX_CACHE4,
+	0x00400184,
+	0x004001a4,
+	0x004001c4,
+	0x004001e4,
+	0x00400188,
+	0x004001a8,
+	0x004001c8,
+	0x004001e8,
+	0x0040018c,
+	0x004001ac,
+	0x004001cc,
+	0x004001ec,
+	0x00400190,
+	0x004001b0,
+	0x004001d0,
+	0x004001f0,
+	0x00400194,
+	0x004001b4,
+	0x004001d4,
+	0x004001f4,
+	0x00400198,
+	0x004001b8,
+	0x004001d8,
+	0x004001f8,
+	0x0040019c,
+	0x004001bc,
+	0x004001dc,
+	0x004001fc,
+	0x00400174,
+	NV04_PGRAPH_DMA_START_0,
+	NV04_PGRAPH_DMA_START_1,
+	NV04_PGRAPH_DMA_LENGTH,
+	NV04_PGRAPH_DMA_MISC,
+	NV04_PGRAPH_DMA_PITCH,
+	NV04_PGRAPH_BOFFSET0,
+	NV04_PGRAPH_BBASE0,
+	NV04_PGRAPH_BLIMIT0,
+	NV04_PGRAPH_BOFFSET1,
+	NV04_PGRAPH_BBASE1,
+	NV04_PGRAPH_BLIMIT1,
+	NV04_PGRAPH_BOFFSET2,
+	NV04_PGRAPH_BBASE2,
+	NV04_PGRAPH_BLIMIT2,
+	NV04_PGRAPH_BOFFSET3,
+	NV04_PGRAPH_BBASE3,
+	NV04_PGRAPH_BLIMIT3,
+	NV04_PGRAPH_BOFFSET4,
+	NV04_PGRAPH_BBASE4,
+	NV04_PGRAPH_BLIMIT4,
+	NV04_PGRAPH_BOFFSET5,
+	NV04_PGRAPH_BBASE5,
+	NV04_PGRAPH_BLIMIT5,
+	NV04_PGRAPH_BPITCH0,
+	NV04_PGRAPH_BPITCH1,
+	NV04_PGRAPH_BPITCH2,
+	NV04_PGRAPH_BPITCH3,
+	NV04_PGRAPH_BPITCH4,
+	NV04_PGRAPH_SURFACE,
+	NV04_PGRAPH_STATE,
+	NV04_PGRAPH_BSWIZZLE2,
+	NV04_PGRAPH_BSWIZZLE5,
+	NV04_PGRAPH_BPIXEL,
+	NV04_PGRAPH_NOTIFY,
+	NV04_PGRAPH_PATT_COLOR0,
+	NV04_PGRAPH_PATT_COLOR1,
+	NV04_PGRAPH_PATT_COLORRAM+0x00,
+	NV04_PGRAPH_PATT_COLORRAM+0x01,
+	NV04_PGRAPH_PATT_COLORRAM+0x02,
+	NV04_PGRAPH_PATT_COLORRAM+0x03,
+	NV04_PGRAPH_PATT_COLORRAM+0x04,
+	NV04_PGRAPH_PATT_COLORRAM+0x05,
+	NV04_PGRAPH_PATT_COLORRAM+0x06,
+	NV04_PGRAPH_PATT_COLORRAM+0x07,
+	NV04_PGRAPH_PATT_COLORRAM+0x08,
+	NV04_PGRAPH_PATT_COLORRAM+0x09,
+	NV04_PGRAPH_PATT_COLORRAM+0x0A,
+	NV04_PGRAPH_PATT_COLORRAM+0x0B,
+	NV04_PGRAPH_PATT_COLORRAM+0x0C,
+	NV04_PGRAPH_PATT_COLORRAM+0x0D,
+	NV04_PGRAPH_PATT_COLORRAM+0x0E,
+	NV04_PGRAPH_PATT_COLORRAM+0x0F,
+	NV04_PGRAPH_PATT_COLORRAM+0x10,
+	NV04_PGRAPH_PATT_COLORRAM+0x11,
+	NV04_PGRAPH_PATT_COLORRAM+0x12,
+	NV04_PGRAPH_PATT_COLORRAM+0x13,
+	NV04_PGRAPH_PATT_COLORRAM+0x14,
+	NV04_PGRAPH_PATT_COLORRAM+0x15,
+	NV04_PGRAPH_PATT_COLORRAM+0x16,
+	NV04_PGRAPH_PATT_COLORRAM+0x17,
+	NV04_PGRAPH_PATT_COLORRAM+0x18,
+	NV04_PGRAPH_PATT_COLORRAM+0x19,
+	NV04_PGRAPH_PATT_COLORRAM+0x1A,
+	NV04_PGRAPH_PATT_COLORRAM+0x1B,
+	NV04_PGRAPH_PATT_COLORRAM+0x1C,
+	NV04_PGRAPH_PATT_COLORRAM+0x1D,
+	NV04_PGRAPH_PATT_COLORRAM+0x1E,
+	NV04_PGRAPH_PATT_COLORRAM+0x1F,
+	NV04_PGRAPH_PATT_COLORRAM+0x20,
+	NV04_PGRAPH_PATT_COLORRAM+0x21,
+	NV04_PGRAPH_PATT_COLORRAM+0x22,
+	NV04_PGRAPH_PATT_COLORRAM+0x23,
+	NV04_PGRAPH_PATT_COLORRAM+0x24,
+	NV04_PGRAPH_PATT_COLORRAM+0x25,
+	NV04_PGRAPH_PATT_COLORRAM+0x26,
+	NV04_PGRAPH_PATT_COLORRAM+0x27,
+	NV04_PGRAPH_PATT_COLORRAM+0x28,
+	NV04_PGRAPH_PATT_COLORRAM+0x29,
+	NV04_PGRAPH_PATT_COLORRAM+0x2A,
+	NV04_PGRAPH_PATT_COLORRAM+0x2B,
+	NV04_PGRAPH_PATT_COLORRAM+0x2C,
+	NV04_PGRAPH_PATT_COLORRAM+0x2D,
+	NV04_PGRAPH_PATT_COLORRAM+0x2E,
+	NV04_PGRAPH_PATT_COLORRAM+0x2F,
+	NV04_PGRAPH_PATT_COLORRAM+0x30,
+	NV04_PGRAPH_PATT_COLORRAM+0x31,
+	NV04_PGRAPH_PATT_COLORRAM+0x32,
+	NV04_PGRAPH_PATT_COLORRAM+0x33,
+	NV04_PGRAPH_PATT_COLORRAM+0x34,
+	NV04_PGRAPH_PATT_COLORRAM+0x35,
+	NV04_PGRAPH_PATT_COLORRAM+0x36,
+	NV04_PGRAPH_PATT_COLORRAM+0x37,
+	NV04_PGRAPH_PATT_COLORRAM+0x38,
+	NV04_PGRAPH_PATT_COLORRAM+0x39,
+	NV04_PGRAPH_PATT_COLORRAM+0x3A,
+	NV04_PGRAPH_PATT_COLORRAM+0x3B,
+	NV04_PGRAPH_PATT_COLORRAM+0x3C,
+	NV04_PGRAPH_PATT_COLORRAM+0x3D,
+	NV04_PGRAPH_PATT_COLORRAM+0x3E,
+	NV04_PGRAPH_PATT_COLORRAM+0x3F,
+	NV04_PGRAPH_PATTERN,
+	0x0040080c,
+	NV04_PGRAPH_PATTERN_SHAPE,
+	0x00400600,
+	NV04_PGRAPH_ROP3,
+	NV04_PGRAPH_CHROMA,
+	NV04_PGRAPH_BETA_AND,
+	NV04_PGRAPH_BETA_PREMULT,
+	NV04_PGRAPH_CONTROL0,
+	NV04_PGRAPH_CONTROL1,
+	NV04_PGRAPH_CONTROL2,
+	NV04_PGRAPH_BLEND,
+	NV04_PGRAPH_STORED_FMT,
+	NV04_PGRAPH_SOURCE_COLOR,
+	0x00400560,
+	0x00400568,
+	0x00400564,
+	0x0040056c,
+	0x00400400,
+	0x00400480,
+	0x00400404,
+	0x00400484,
+	0x00400408,
+	0x00400488,
+	0x0040040c,
+	0x0040048c,
+	0x00400410,
+	0x00400490,
+	0x00400414,
+	0x00400494,
+	0x00400418,
+	0x00400498,
+	0x0040041c,
+	0x0040049c,
+	0x00400420,
+	0x004004a0,
+	0x00400424,
+	0x004004a4,
+	0x00400428,
+	0x004004a8,
+	0x0040042c,
+	0x004004ac,
+	0x00400430,
+	0x004004b0,
+	0x00400434,
+	0x004004b4,
+	0x00400438,
+	0x004004b8,
+	0x0040043c,
+	0x004004bc,
+	0x00400440,
+	0x004004c0,
+	0x00400444,
+	0x004004c4,
+	0x00400448,
+	0x004004c8,
+	0x0040044c,
+	0x004004cc,
+	0x00400450,
+	0x004004d0,
+	0x00400454,
+	0x004004d4,
+	0x00400458,
+	0x004004d8,
+	0x0040045c,
+	0x004004dc,
+	0x00400460,
+	0x004004e0,
+	0x00400464,
+	0x004004e4,
+	0x00400468,
+	0x004004e8,
+	0x0040046c,
+	0x004004ec,
+	0x00400470,
+	0x004004f0,
+	0x00400474,
+	0x004004f4,
+	0x00400478,
+	0x004004f8,
+	0x0040047c,
+	0x004004fc,
+	0x0040053c,
+	0x00400544,
+	0x00400540,
+	0x00400548,
+	0x00400560,
+	0x00400568,
+	0x00400564,
+	0x0040056c,
+	0x00400534,
+	0x00400538,
+	0x00400514,
+	0x00400518,
+	0x0040051c,
+	0x00400520,
+	0x00400524,
+	0x00400528,
+	0x0040052c,
+	0x00400530,
+	0x00400d00,
+	0x00400d40,
+	0x00400d80,
+	0x00400d04,
+	0x00400d44,
+	0x00400d84,
+	0x00400d08,
+	0x00400d48,
+	0x00400d88,
+	0x00400d0c,
+	0x00400d4c,
+	0x00400d8c,
+	0x00400d10,
+	0x00400d50,
+	0x00400d90,
+	0x00400d14,
+	0x00400d54,
+	0x00400d94,
+	0x00400d18,
+	0x00400d58,
+	0x00400d98,
+	0x00400d1c,
+	0x00400d5c,
+	0x00400d9c,
+	0x00400d20,
+	0x00400d60,
+	0x00400da0,
+	0x00400d24,
+	0x00400d64,
+	0x00400da4,
+	0x00400d28,
+	0x00400d68,
+	0x00400da8,
+	0x00400d2c,
+	0x00400d6c,
+	0x00400dac,
+	0x00400d30,
+	0x00400d70,
+	0x00400db0,
+	0x00400d34,
+	0x00400d74,
+	0x00400db4,
+	0x00400d38,
+	0x00400d78,
+	0x00400db8,
+	0x00400d3c,
+	0x00400d7c,
+	0x00400dbc,
+	0x00400590,
+	0x00400594,
+	0x00400598,
+	0x0040059c,
+	0x004005a8,
+	0x004005ac,
+	0x004005b0,
+	0x004005b4,
+	0x004005c0,
+	0x004005c4,
+	0x004005c8,
+	0x004005cc,
+	0x004005d0,
+	0x004005d4,
+	0x004005d8,
+	0x004005dc,
+	0x004005e0,
+	NV04_PGRAPH_PASSTHRU_0,
+	NV04_PGRAPH_PASSTHRU_1,
+	NV04_PGRAPH_PASSTHRU_2,
+	NV04_PGRAPH_DVD_COLORFMT,
+	NV04_PGRAPH_SCALED_FORMAT,
+	NV04_PGRAPH_MISC24_0,
+	NV04_PGRAPH_MISC24_1,
+	NV04_PGRAPH_MISC24_2,
+	0x00400500,
+	0x00400504,
+	NV04_PGRAPH_VALID1,
+	NV04_PGRAPH_VALID2
+
+
+};
+
+struct graph_state {
+	int nv04[sizeof(nv04_graph_ctx_regs)/sizeof(nv04_graph_ctx_regs[0])];
+};
+
+void nouveau_nv04_context_switch(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct nouveau_channel *next, *last;
+	int chid;
+
+	if (!dev) {
+		DRM_DEBUG("Invalid drm_device\n");
+		return;
+	}
+	dev_priv = dev->dev_private;
+	if (!dev_priv) {
+		DRM_DEBUG("Invalid drm_nouveau_private\n");
+		return;
+	}
+	if (!dev_priv->fifos) {
+		DRM_DEBUG("Invalid drm_nouveau_private->fifos\n");
+		return;
+	}
+
+	chid = engine->fifo.channel_id(dev);
+	next = dev_priv->fifos[chid];
+
+	if (!next) {
+		DRM_DEBUG("Invalid next channel\n");
+		return;
+	}
+
+	chid = (NV_READ(NV04_PGRAPH_CTX_USER) >> 24) & (engine->fifo.channels - 1);
+	last = dev_priv->fifos[chid];
+
+	if (!last) {
+		DRM_DEBUG("WARNING: Invalid last channel, switch to %x\n",
+		          next->id);
+	} else {
+		DRM_INFO("NV: PGRAPH context switch interrupt channel %x -> %x\n",
+		         last->id, next->id);
+	}
+
+/*	NV_WRITE(NV03_PFIFO_CACHES, 0x0);
+	NV_WRITE(NV04_PFIFO_CACHE0_PULL0, 0x0);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x0);*/
+	NV_WRITE(NV04_PGRAPH_FIFO,0x0);
+
+	if (last)
+		nv04_graph_save_context(last);
+
+	nouveau_wait_for_idle(dev);
+
+	NV_WRITE(NV04_PGRAPH_CTX_CONTROL, 0x10000000);
+	NV_WRITE(NV04_PGRAPH_CTX_USER, (NV_READ(NV04_PGRAPH_CTX_USER) & 0xffffff) | (0x0f << 24));
+
+	nouveau_wait_for_idle(dev);
+
+	nv04_graph_load_context(next);
+
+	NV_WRITE(NV04_PGRAPH_CTX_CONTROL, 0x10010100);
+	NV_WRITE(NV04_PGRAPH_CTX_USER, next->id << 24);
+	NV_WRITE(NV04_PGRAPH_FFINTFC_ST2, NV_READ(NV04_PGRAPH_FFINTFC_ST2)&0x000FFFFF);
+
+/*	NV_WRITE(NV04_PGRAPH_FIFO,0x0);
+	NV_WRITE(NV04_PFIFO_CACHE0_PULL0, 0x0);
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL0, 0x1);
+	NV_WRITE(NV03_PFIFO_CACHES, 0x1);*/
+	NV_WRITE(NV04_PGRAPH_FIFO,0x1);
+}
+
+int nv04_graph_create_context(struct nouveau_channel *chan) {
+	struct graph_state* pgraph_ctx;
+	DRM_DEBUG("nv04_graph_context_create %d\n", chan->id);
+
+	chan->pgraph_ctx = pgraph_ctx = drm_calloc(1, sizeof(*pgraph_ctx),
+					      DRM_MEM_DRIVER);
+
+	if (pgraph_ctx == NULL)
+		return -ENOMEM;
+
+	//dev_priv->fifos[channel].pgraph_ctx_user = channel << 24;
+	pgraph_ctx->nv04[0] = 0x0001ffff;
+	/* is it really needed ??? */
+	//dev_priv->fifos[channel].pgraph_ctx[1] = NV_READ(NV_PGRAPH_DEBUG_4);
+	//dev_priv->fifos[channel].pgraph_ctx[2] = NV_READ(0x004006b0);
+
+	return 0;
+}
+
+void nv04_graph_destroy_context(struct nouveau_channel *chan)
+{
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+
+	drm_free(pgraph_ctx, sizeof(*pgraph_ctx), DRM_MEM_DRIVER);
+	chan->pgraph_ctx = NULL;
+}
+
+int nv04_graph_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	int i;
+
+	for (i = 0; i < sizeof(nv04_graph_ctx_regs)/sizeof(nv04_graph_ctx_regs[0]); i++)
+		NV_WRITE(nv04_graph_ctx_regs[i], pgraph_ctx->nv04[i]);
+
+	return 0;
+}
+
+int nv04_graph_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	int i;
+
+	for (i = 0; i < sizeof(nv04_graph_ctx_regs)/sizeof(nv04_graph_ctx_regs[0]); i++)
+		pgraph_ctx->nv04[i] = NV_READ(nv04_graph_ctx_regs[i]);
+
+	return 0;
+}
+
+int nv04_graph_init(struct drm_device *dev) {
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PGRAPH);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PGRAPH);
+
+	/* Enable PGRAPH interrupts */
+	NV_WRITE(NV03_PGRAPH_INTR, 0xFFFFFFFF);
+	NV_WRITE(NV03_PGRAPH_INTR_EN, 0xFFFFFFFF);
+
+	NV_WRITE(NV04_PGRAPH_VALID1, 0);
+	NV_WRITE(NV04_PGRAPH_VALID2, 0);
+	/*NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x000001FF);
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x001FFFFF);*/
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x1231c000);
+	/*1231C000 blob, 001 haiku*/
+	//*V_WRITE(NV04_PGRAPH_DEBUG_1, 0xf2d91100);*/
+	NV_WRITE(NV04_PGRAPH_DEBUG_1, 0x72111100);
+	/*0x72111100 blob , 01 haiku*/
+	/*NV_WRITE(NV04_PGRAPH_DEBUG_2, 0x11d5f870);*/
+	NV_WRITE(NV04_PGRAPH_DEBUG_2, 0x11d5f071);
+	/*haiku same*/
+
+	/*NV_WRITE(NV04_PGRAPH_DEBUG_3, 0xfad4ff31);*/
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, 0xf0d4ff31);
+	/*haiku and blob 10d4*/
+
+	NV_WRITE(NV04_PGRAPH_STATE        , 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_CTX_CONTROL  , 0x10010100);
+	NV_WRITE(NV04_PGRAPH_FIFO         , 0x00000001);
+
+	/* These don't belong here, they're part of a per-channel context */
+	NV_WRITE(NV04_PGRAPH_PATTERN_SHAPE, 0x00000000);
+	NV_WRITE(NV04_PGRAPH_BETA_AND     , 0xFFFFFFFF);
+
+	return 0;
+}
+
+void nv04_graph_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv04_instmem.c b/drivers/char/drm/nv04_instmem.c
new file mode 100644
index 0000000..804f9a7
--- /dev/null
+++ b/drivers/char/drm/nv04_instmem.c
@@ -0,0 +1,159 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+static void
+nv04_instmem_determine_amount(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	/* Figure out how much instance memory we need */
+	if (dev_priv->card_type >= NV_40) {
+		/* We'll want more instance memory than this on some NV4x cards.
+		 * There's a 16MB aperture to play with that maps onto the end
+		 * of vram.  For now, only reserve a small piece until we know
+		 * more about what each chipset requires.
+		 */
+		dev_priv->ramin_rsvd_vram = (1*1024* 1024);
+	} else {
+		/*XXX: what *are* the limits on <NV40 cards?, and does RAMIN
+		 *     exist in vram on those cards as well?
+		 */
+		dev_priv->ramin_rsvd_vram = (512*1024);
+	}
+	DRM_DEBUG("RAMIN size: %dKiB\n", dev_priv->ramin_rsvd_vram>>10);
+
+	/* Clear all of it, except the BIOS image that's in the first 64KiB */
+	for (i=(64*1024); i<dev_priv->ramin_rsvd_vram; i+=4)
+		NV_WI32(i, 0x00000000);
+}
+
+static void
+nv04_instmem_configure_fixed_tables(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+
+	/* FIFO hash table (RAMHT)
+	 *   use 4k hash table at RAMIN+0x10000
+	 *   TODO: extend the hash table
+	 */
+	dev_priv->ramht_offset = 0x10000;
+	dev_priv->ramht_bits   = 9;
+	dev_priv->ramht_size   = (1 << dev_priv->ramht_bits);
+	DRM_DEBUG("RAMHT offset=0x%x, size=%d\n", dev_priv->ramht_offset,
+						  dev_priv->ramht_size);
+
+	/* FIFO runout table (RAMRO) - 512k at 0x11200 */
+	dev_priv->ramro_offset = 0x11200;
+	dev_priv->ramro_size   = 512;
+	DRM_DEBUG("RAMRO offset=0x%x, size=%d\n", dev_priv->ramro_offset,
+						  dev_priv->ramro_size);
+
+	/* FIFO context table (RAMFC)
+	 *   NV40  : Not sure exactly how to position RAMFC on some cards,
+	 *           0x30002 seems to position it at RAMIN+0x20000 on these
+	 *           cards.  RAMFC is 4kb (32 fifos, 128byte entries).
+	 *   Others: Position RAMFC at RAMIN+0x11400
+	 */
+	switch(dev_priv->card_type)
+	{
+		case NV_40:
+		case NV_44:
+			dev_priv->ramfc_offset = 0x20000;
+			dev_priv->ramfc_size   = engine->fifo.channels *
+						 nouveau_fifo_ctx_size(dev);
+			break;
+		case NV_30:
+		case NV_20:
+		case NV_17:
+		case NV_11:
+		case NV_10:
+		case NV_04:
+		default:
+			dev_priv->ramfc_offset = 0x11400;
+			dev_priv->ramfc_size   = engine->fifo.channels *
+						 nouveau_fifo_ctx_size(dev);
+			break;
+	}
+	DRM_DEBUG("RAMFC offset=0x%x, size=%d\n", dev_priv->ramfc_offset,
+						  dev_priv->ramfc_size);
+}
+
+int nv04_instmem_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t offset;
+	int ret = 0;
+
+	nv04_instmem_determine_amount(dev);
+	nv04_instmem_configure_fixed_tables(dev);
+
+	/* Create a heap to manage RAMIN allocations, we don't allocate
+	 * the space that was reserved for RAMHT/FC/RO.
+	 */
+	offset = dev_priv->ramfc_offset + dev_priv->ramfc_size;
+
+	/* On my NV4E, there's *something* clobbering the 16KiB just after
+	 * where we setup these fixed tables.  No idea what it is just yet,
+	 * so reserve this space on all NV4X cards for now.
+	 */
+	if (dev_priv->card_type >= NV_40)
+		offset += 16*1024;
+
+	ret = nouveau_mem_init_heap(&dev_priv->ramin_heap,
+				    offset, dev_priv->ramin_rsvd_vram - offset);
+	if (ret) {
+		dev_priv->ramin_heap = NULL;
+		DRM_ERROR("Failed to init RAMIN heap\n");
+	}
+
+	return ret;
+}
+
+void
+nv04_instmem_takedown(struct drm_device *dev)
+{
+}
+
+int
+nv04_instmem_populate(struct drm_device *dev, struct nouveau_gpuobj *gpuobj, uint32_t *sz)
+{
+	if (gpuobj->im_backing)
+		return -EINVAL;
+
+	return 0;
+}
+
+void
+nv04_instmem_clear(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	if (gpuobj && gpuobj->im_backing) {
+		if (gpuobj->im_bound)
+			dev_priv->Engine.instmem.unbind(dev, gpuobj);
+		gpuobj->im_backing = NULL;
+	}
+}
+
+int
+nv04_instmem_bind(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	if (!gpuobj->im_pramin || gpuobj->im_bound)
+		return -EINVAL;
+
+	gpuobj->im_bound = 1;
+	return 0;
+}
+
+int
+nv04_instmem_unbind(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	if (gpuobj->im_bound == 0)
+		return -EINVAL;
+
+	gpuobj->im_bound = 0;
+	return 0;
+}
diff --git a/drivers/char/drm/nv04_mc.c b/drivers/char/drm/nv04_mc.c
new file mode 100644
index 0000000..24c1f7b
--- /dev/null
+++ b/drivers/char/drm/nv04_mc.c
@@ -0,0 +1,22 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv04_mc_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	/* Power up everything, resetting each individual unit will
+	 * be done later if needed.
+	 */
+	NV_WRITE(NV03_PMC_ENABLE, 0xFFFFFFFF);
+
+	return 0;
+}
+
+void
+nv04_mc_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv04_timer.c b/drivers/char/drm/nv04_timer.c
new file mode 100644
index 0000000..616f197
--- /dev/null
+++ b/drivers/char/drm/nv04_timer.c
@@ -0,0 +1,53 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv04_timer_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV04_PTIMER_INTR_EN_0, 0x00000000);
+	NV_WRITE(NV04_PTIMER_INTR_0, 0xFFFFFFFF);
+
+	/* Just use the pre-existing values when possible for now; these regs
+	 * are not written in nv (driver writer missed a /4 on the address), and
+	 * writing 8 and 3 to the correct regs breaks the timings on the LVDS
+	 * hardware sequencing microcode.
+	 * A correct solution (involving calculations with the GPU PLL) can
+	 * be done when kernel modesetting lands
+	 */
+	if (!NV_READ(NV04_PTIMER_NUMERATOR) || !NV_READ(NV04_PTIMER_DENOMINATOR)) {
+		NV_WRITE(NV04_PTIMER_NUMERATOR, 0x00000008);
+		NV_WRITE(NV04_PTIMER_DENOMINATOR, 0x00000003);
+	}
+
+	return 0;
+}
+
+uint64_t
+nv04_timer_read(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t low;
+	/* From kmmio dumps on nv28 this looks like how the blob does this.
+	 * It reads the high dword twice, before and after.
+	 * The only explanation seems to be that the 64-bit timer counter
+	 * advances between high and low dword reads and may corrupt the
+	 * result. Not confirmed.
+	 */
+	uint32_t high2 = NV_READ(NV04_PTIMER_TIME_1);
+	uint32_t high1;
+	do {
+		high1 = high2;
+		low = NV_READ(NV04_PTIMER_TIME_0);
+		high2 = NV_READ(NV04_PTIMER_TIME_1);
+	} while(high1 != high2);
+	return (((uint64_t)high2) << 32) | (uint64_t)low;
+}
+
+void
+nv04_timer_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv10_fb.c b/drivers/char/drm/nv10_fb.c
new file mode 100644
index 0000000..6e0773a
--- /dev/null
+++ b/drivers/char/drm/nv10_fb.c
@@ -0,0 +1,25 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv10_fb_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t fb_bar_size;
+	int i;
+
+	fb_bar_size = drm_get_resource_len(dev, 0) - 1;
+	for (i=0; i<NV10_PFB_TILE__SIZE; i++) {
+		NV_WRITE(NV10_PFB_TILE(i), 0);
+		NV_WRITE(NV10_PFB_TLIMIT(i), fb_bar_size);
+	}
+
+	return 0;
+}
+
+void
+nv10_fb_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv10_fifo.c b/drivers/char/drm/nv10_fifo.c
new file mode 100644
index 0000000..6d50b6c
--- /dev/null
+++ b/drivers/char/drm/nv10_fifo.c
@@ -0,0 +1,169 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+
+#define RAMFC_WR(offset,val) INSTANCE_WR(chan->ramfc->gpuobj, \
+					 NV10_RAMFC_##offset/4, (val))
+#define RAMFC_RD(offset)     INSTANCE_RD(chan->ramfc->gpuobj, \
+					 NV10_RAMFC_##offset/4)
+#define NV10_RAMFC(c) (dev_priv->ramfc_offset + ((c) * NV10_RAMFC__SIZE))
+#define NV10_RAMFC__SIZE ((dev_priv->chipset) >= 0x17 ? 64 : 32)
+
+int
+nv10_fifo_channel_id(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	return (NV_READ(NV03_PFIFO_CACHE1_PUSH1) &
+			NV10_PFIFO_CACHE1_PUSH1_CHID_MASK);
+}
+
+int
+nv10_fifo_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if ((ret = nouveau_gpuobj_new_fake(dev, NV10_RAMFC(chan->id), ~0,
+						NV10_RAMFC__SIZE,
+						NVOBJ_FLAG_ZERO_ALLOC |
+						NVOBJ_FLAG_ZERO_FREE,
+						NULL, &chan->ramfc)))
+		return ret;
+
+	/* Fill entries that are seen filled in dumps of nvidia driver just
+	 * after channel's is put into DMA mode
+	 */
+	RAMFC_WR(DMA_PUT       , chan->pushbuf_base);
+	RAMFC_WR(DMA_GET       , chan->pushbuf_base);
+	RAMFC_WR(DMA_INSTANCE  , chan->pushbuf->instance >> 4);
+	RAMFC_WR(DMA_FETCH     , NV_PFIFO_CACHE1_DMA_FETCH_TRIG_128_BYTES |
+				 NV_PFIFO_CACHE1_DMA_FETCH_SIZE_128_BYTES |
+				 NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_8 |
+#ifdef __BIG_ENDIAN
+				 NV_PFIFO_CACHE1_BIG_ENDIAN |
+#endif
+				 0);
+
+	/* enable the fifo dma operation */
+	NV_WRITE(NV04_PFIFO_MODE,NV_READ(NV04_PFIFO_MODE)|(1<<chan->id));
+	return 0;
+}
+
+void
+nv10_fifo_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV04_PFIFO_MODE, NV_READ(NV04_PFIFO_MODE)&~(1<<chan->id));
+
+	nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+}
+
+int
+nv10_fifo_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH1,
+		 NV03_PFIFO_CACHE1_PUSH1_DMA | chan->id);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_GET          , RAMFC_RD(DMA_GET));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUT          , RAMFC_RD(DMA_PUT));
+	NV_WRITE(NV10_PFIFO_CACHE1_REF_CNT          , RAMFC_RD(REF_CNT));
+
+	tmp = RAMFC_RD(DMA_INSTANCE);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_INSTANCE     , tmp & 0xFFFF);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_DCOUNT       , tmp >> 16);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_STATE        , RAMFC_RD(DMA_STATE));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_FETCH        , RAMFC_RD(DMA_FETCH));
+	NV_WRITE(NV04_PFIFO_CACHE1_ENGINE           , RAMFC_RD(ENGINE));
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL1            , RAMFC_RD(PULL1_ENGINE));
+
+	if (dev_priv->chipset >= 0x17) {
+		NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_VALUE,
+			 RAMFC_RD(ACQUIRE_VALUE));
+		NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_TIMESTAMP,
+			 RAMFC_RD(ACQUIRE_TIMESTAMP));
+		NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_TIMEOUT,
+			 RAMFC_RD(ACQUIRE_TIMEOUT));
+		NV_WRITE(NV10_PFIFO_CACHE1_SEMAPHORE,
+			 RAMFC_RD(SEMAPHORE));
+		NV_WRITE(NV10_PFIFO_CACHE1_DMA_SUBROUTINE,
+			 RAMFC_RD(DMA_SUBROUTINE));
+	}
+
+	/* Reset NV04_PFIFO_CACHE1_DMA_CTL_AT_INFO to INVALID */
+	tmp = NV_READ(NV04_PFIFO_CACHE1_DMA_CTL) & ~(1<<31);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_CTL, tmp);
+
+	return 0;
+}
+
+int
+nv10_fifo_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	RAMFC_WR(DMA_PUT          , NV_READ(NV04_PFIFO_CACHE1_DMA_PUT));
+	RAMFC_WR(DMA_GET          , NV_READ(NV04_PFIFO_CACHE1_DMA_GET));
+	RAMFC_WR(REF_CNT          , NV_READ(NV10_PFIFO_CACHE1_REF_CNT));
+
+	tmp  = NV_READ(NV04_PFIFO_CACHE1_DMA_INSTANCE) & 0xFFFF;
+	tmp |= (NV_READ(NV04_PFIFO_CACHE1_DMA_DCOUNT) << 16);
+	RAMFC_WR(DMA_INSTANCE     , tmp);
+
+	RAMFC_WR(DMA_STATE        , NV_READ(NV04_PFIFO_CACHE1_DMA_STATE));
+	RAMFC_WR(DMA_FETCH	  , NV_READ(NV04_PFIFO_CACHE1_DMA_FETCH));
+	RAMFC_WR(ENGINE           , NV_READ(NV04_PFIFO_CACHE1_ENGINE));
+	RAMFC_WR(PULL1_ENGINE     , NV_READ(NV04_PFIFO_CACHE1_PULL1));
+
+	if (dev_priv->chipset >= 0x17) {
+		RAMFC_WR(ACQUIRE_VALUE,
+			 NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_VALUE));
+		RAMFC_WR(ACQUIRE_TIMESTAMP,
+			 NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_TIMESTAMP));
+		RAMFC_WR(ACQUIRE_TIMEOUT,
+			 NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_TIMEOUT));
+		RAMFC_WR(SEMAPHORE,
+			 NV_READ(NV10_PFIFO_CACHE1_SEMAPHORE));
+		RAMFC_WR(DMA_SUBROUTINE,
+			 NV_READ(NV04_PFIFO_CACHE1_DMA_GET));
+	}
+
+	return 0;
+}
diff --git a/drivers/char/drm/nv10_graph.c b/drivers/char/drm/nv10_graph.c
new file mode 100644
index 0000000..9bf6c7e
--- /dev/null
+++ b/drivers/char/drm/nv10_graph.c
@@ -0,0 +1,871 @@
+/*
+ * Copyright 2007 Matthieu CASTET <castet.matthieu@free.fr>
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * PRECISION INSIGHT AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drm.h"
+#include "nouveau_drv.h"
+
+#define NV10_FIFO_NUMBER 32
+
+struct pipe_state {
+	uint32_t pipe_0x0000[0x040/4];
+	uint32_t pipe_0x0040[0x010/4];
+	uint32_t pipe_0x0200[0x0c0/4];
+	uint32_t pipe_0x4400[0x080/4];
+	uint32_t pipe_0x6400[0x3b0/4];
+	uint32_t pipe_0x6800[0x2f0/4];
+	uint32_t pipe_0x6c00[0x030/4];
+	uint32_t pipe_0x7000[0x130/4];
+	uint32_t pipe_0x7400[0x0c0/4];
+	uint32_t pipe_0x7800[0x0c0/4];
+};
+
+static int nv10_graph_ctx_regs [] = {
+NV10_PGRAPH_CTX_SWITCH1,
+NV10_PGRAPH_CTX_SWITCH2,
+NV10_PGRAPH_CTX_SWITCH3,
+NV10_PGRAPH_CTX_SWITCH4,
+NV10_PGRAPH_CTX_SWITCH5,
+NV10_PGRAPH_CTX_CACHE1,	/* 8 values from 0x400160 to 0x40017c */
+NV10_PGRAPH_CTX_CACHE2,	/* 8 values from 0x400180 to 0x40019c */
+NV10_PGRAPH_CTX_CACHE3,	/* 8 values from 0x4001a0 to 0x4001bc */
+NV10_PGRAPH_CTX_CACHE4,	/* 8 values from 0x4001c0 to 0x4001dc */
+NV10_PGRAPH_CTX_CACHE5,	/* 8 values from 0x4001e0 to 0x4001fc */
+0x00400164,
+0x00400184,
+0x004001a4,
+0x004001c4,
+0x004001e4,
+0x00400168,
+0x00400188,
+0x004001a8,
+0x004001c8,
+0x004001e8,
+0x0040016c,
+0x0040018c,
+0x004001ac,
+0x004001cc,
+0x004001ec,
+0x00400170,
+0x00400190,
+0x004001b0,
+0x004001d0,
+0x004001f0,
+0x00400174,
+0x00400194,
+0x004001b4,
+0x004001d4,
+0x004001f4,
+0x00400178,
+0x00400198,
+0x004001b8,
+0x004001d8,
+0x004001f8,
+0x0040017c,
+0x0040019c,
+0x004001bc,
+0x004001dc,
+0x004001fc,
+NV10_PGRAPH_CTX_USER,
+NV04_PGRAPH_DMA_START_0,
+NV04_PGRAPH_DMA_START_1,
+NV04_PGRAPH_DMA_LENGTH,
+NV04_PGRAPH_DMA_MISC,
+NV10_PGRAPH_DMA_PITCH,
+NV04_PGRAPH_BOFFSET0,
+NV04_PGRAPH_BBASE0,
+NV04_PGRAPH_BLIMIT0,
+NV04_PGRAPH_BOFFSET1,
+NV04_PGRAPH_BBASE1,
+NV04_PGRAPH_BLIMIT1,
+NV04_PGRAPH_BOFFSET2,
+NV04_PGRAPH_BBASE2,
+NV04_PGRAPH_BLIMIT2,
+NV04_PGRAPH_BOFFSET3,
+NV04_PGRAPH_BBASE3,
+NV04_PGRAPH_BLIMIT3,
+NV04_PGRAPH_BOFFSET4,
+NV04_PGRAPH_BBASE4,
+NV04_PGRAPH_BLIMIT4,
+NV04_PGRAPH_BOFFSET5,
+NV04_PGRAPH_BBASE5,
+NV04_PGRAPH_BLIMIT5,
+NV04_PGRAPH_BPITCH0,
+NV04_PGRAPH_BPITCH1,
+NV04_PGRAPH_BPITCH2,
+NV04_PGRAPH_BPITCH3,
+NV04_PGRAPH_BPITCH4,
+NV10_PGRAPH_SURFACE,
+NV10_PGRAPH_STATE,
+NV04_PGRAPH_BSWIZZLE2,
+NV04_PGRAPH_BSWIZZLE5,
+NV04_PGRAPH_BPIXEL,
+NV10_PGRAPH_NOTIFY,
+NV04_PGRAPH_PATT_COLOR0,
+NV04_PGRAPH_PATT_COLOR1,
+NV04_PGRAPH_PATT_COLORRAM, /* 64 values from 0x400900 to 0x4009fc */
+0x00400904,
+0x00400908,
+0x0040090c,
+0x00400910,
+0x00400914,
+0x00400918,
+0x0040091c,
+0x00400920,
+0x00400924,
+0x00400928,
+0x0040092c,
+0x00400930,
+0x00400934,
+0x00400938,
+0x0040093c,
+0x00400940,
+0x00400944,
+0x00400948,
+0x0040094c,
+0x00400950,
+0x00400954,
+0x00400958,
+0x0040095c,
+0x00400960,
+0x00400964,
+0x00400968,
+0x0040096c,
+0x00400970,
+0x00400974,
+0x00400978,
+0x0040097c,
+0x00400980,
+0x00400984,
+0x00400988,
+0x0040098c,
+0x00400990,
+0x00400994,
+0x00400998,
+0x0040099c,
+0x004009a0,
+0x004009a4,
+0x004009a8,
+0x004009ac,
+0x004009b0,
+0x004009b4,
+0x004009b8,
+0x004009bc,
+0x004009c0,
+0x004009c4,
+0x004009c8,
+0x004009cc,
+0x004009d0,
+0x004009d4,
+0x004009d8,
+0x004009dc,
+0x004009e0,
+0x004009e4,
+0x004009e8,
+0x004009ec,
+0x004009f0,
+0x004009f4,
+0x004009f8,
+0x004009fc,
+NV04_PGRAPH_PATTERN,	/* 2 values from 0x400808 to 0x40080c */
+0x0040080c,
+NV04_PGRAPH_PATTERN_SHAPE,
+NV03_PGRAPH_MONO_COLOR0,
+NV04_PGRAPH_ROP3,
+NV04_PGRAPH_CHROMA,
+NV04_PGRAPH_BETA_AND,
+NV04_PGRAPH_BETA_PREMULT,
+0x00400e70,
+0x00400e74,
+0x00400e78,
+0x00400e7c,
+0x00400e80,
+0x00400e84,
+0x00400e88,
+0x00400e8c,
+0x00400ea0,
+0x00400ea4,
+0x00400ea8,
+0x00400e90,
+0x00400e94,
+0x00400e98,
+0x00400e9c,
+NV10_PGRAPH_WINDOWCLIP_HORIZONTAL, /* 8 values from 0x400f00 to 0x400f1c */
+NV10_PGRAPH_WINDOWCLIP_VERTICAL,   /* 8 values from 0x400f20 to 0x400f3c */
+0x00400f04,
+0x00400f24,
+0x00400f08,
+0x00400f28,
+0x00400f0c,
+0x00400f2c,
+0x00400f10,
+0x00400f30,
+0x00400f14,
+0x00400f34,
+0x00400f18,
+0x00400f38,
+0x00400f1c,
+0x00400f3c,
+NV10_PGRAPH_XFMODE0,
+NV10_PGRAPH_XFMODE1,
+NV10_PGRAPH_GLOBALSTATE0,
+NV10_PGRAPH_GLOBALSTATE1,
+NV04_PGRAPH_STORED_FMT,
+NV04_PGRAPH_SOURCE_COLOR,
+NV03_PGRAPH_ABS_X_RAM,	/* 32 values from 0x400400 to 0x40047c */
+NV03_PGRAPH_ABS_Y_RAM,	/* 32 values from 0x400480 to 0x4004fc */
+0x00400404,
+0x00400484,
+0x00400408,
+0x00400488,
+0x0040040c,
+0x0040048c,
+0x00400410,
+0x00400490,
+0x00400414,
+0x00400494,
+0x00400418,
+0x00400498,
+0x0040041c,
+0x0040049c,
+0x00400420,
+0x004004a0,
+0x00400424,
+0x004004a4,
+0x00400428,
+0x004004a8,
+0x0040042c,
+0x004004ac,
+0x00400430,
+0x004004b0,
+0x00400434,
+0x004004b4,
+0x00400438,
+0x004004b8,
+0x0040043c,
+0x004004bc,
+0x00400440,
+0x004004c0,
+0x00400444,
+0x004004c4,
+0x00400448,
+0x004004c8,
+0x0040044c,
+0x004004cc,
+0x00400450,
+0x004004d0,
+0x00400454,
+0x004004d4,
+0x00400458,
+0x004004d8,
+0x0040045c,
+0x004004dc,
+0x00400460,
+0x004004e0,
+0x00400464,
+0x004004e4,
+0x00400468,
+0x004004e8,
+0x0040046c,
+0x004004ec,
+0x00400470,
+0x004004f0,
+0x00400474,
+0x004004f4,
+0x00400478,
+0x004004f8,
+0x0040047c,
+0x004004fc,
+NV03_PGRAPH_ABS_UCLIP_XMIN,
+NV03_PGRAPH_ABS_UCLIP_XMAX,
+NV03_PGRAPH_ABS_UCLIP_YMIN,
+NV03_PGRAPH_ABS_UCLIP_YMAX,
+0x00400550,
+0x00400558,
+0x00400554,
+0x0040055c,
+NV03_PGRAPH_ABS_UCLIPA_XMIN,
+NV03_PGRAPH_ABS_UCLIPA_XMAX,
+NV03_PGRAPH_ABS_UCLIPA_YMIN,
+NV03_PGRAPH_ABS_UCLIPA_YMAX,
+NV03_PGRAPH_ABS_ICLIP_XMAX,
+NV03_PGRAPH_ABS_ICLIP_YMAX,
+NV03_PGRAPH_XY_LOGIC_MISC0,
+NV03_PGRAPH_XY_LOGIC_MISC1,
+NV03_PGRAPH_XY_LOGIC_MISC2,
+NV03_PGRAPH_XY_LOGIC_MISC3,
+NV03_PGRAPH_CLIPX_0,
+NV03_PGRAPH_CLIPX_1,
+NV03_PGRAPH_CLIPY_0,
+NV03_PGRAPH_CLIPY_1,
+NV10_PGRAPH_COMBINER0_IN_ALPHA,
+NV10_PGRAPH_COMBINER1_IN_ALPHA,
+NV10_PGRAPH_COMBINER0_IN_RGB,
+NV10_PGRAPH_COMBINER1_IN_RGB,
+NV10_PGRAPH_COMBINER_COLOR0,
+NV10_PGRAPH_COMBINER_COLOR1,
+NV10_PGRAPH_COMBINER0_OUT_ALPHA,
+NV10_PGRAPH_COMBINER1_OUT_ALPHA,
+NV10_PGRAPH_COMBINER0_OUT_RGB,
+NV10_PGRAPH_COMBINER1_OUT_RGB,
+NV10_PGRAPH_COMBINER_FINAL0,
+NV10_PGRAPH_COMBINER_FINAL1,
+0x00400e00,
+0x00400e04,
+0x00400e08,
+0x00400e0c,
+0x00400e10,
+0x00400e14,
+0x00400e18,
+0x00400e1c,
+0x00400e20,
+0x00400e24,
+0x00400e28,
+0x00400e2c,
+0x00400e30,
+0x00400e34,
+0x00400e38,
+0x00400e3c,
+NV04_PGRAPH_PASSTHRU_0,
+NV04_PGRAPH_PASSTHRU_1,
+NV04_PGRAPH_PASSTHRU_2,
+NV10_PGRAPH_DIMX_TEXTURE,
+NV10_PGRAPH_WDIMX_TEXTURE,
+NV10_PGRAPH_DVD_COLORFMT,
+NV10_PGRAPH_SCALED_FORMAT,
+NV04_PGRAPH_MISC24_0,
+NV04_PGRAPH_MISC24_1,
+NV04_PGRAPH_MISC24_2,
+NV03_PGRAPH_X_MISC,
+NV03_PGRAPH_Y_MISC,
+NV04_PGRAPH_VALID1,
+NV04_PGRAPH_VALID2,
+};
+
+static int nv17_graph_ctx_regs [] = {
+NV10_PGRAPH_DEBUG_4,
+0x004006b0,
+0x00400eac,
+0x00400eb0,
+0x00400eb4,
+0x00400eb8,
+0x00400ebc,
+0x00400ec0,
+0x00400ec4,
+0x00400ec8,
+0x00400ecc,
+0x00400ed0,
+0x00400ed4,
+0x00400ed8,
+0x00400edc,
+0x00400ee0,
+0x00400a00,
+0x00400a04,
+};
+
+struct graph_state {
+	int nv10[sizeof(nv10_graph_ctx_regs)/sizeof(nv10_graph_ctx_regs[0])];
+	int nv17[sizeof(nv17_graph_ctx_regs)/sizeof(nv17_graph_ctx_regs[0])];
+	struct pipe_state pipe_state;
+};
+
+static void nv10_graph_save_pipe(struct nouveau_channel *chan) {
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	struct pipe_state *fifo_pipe_state = &pgraph_ctx->pipe_state;
+	int i;
+#define PIPE_SAVE(addr) \
+	do { \
+		NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, addr); \
+		for (i=0; i < sizeof(fifo_pipe_state->pipe_##addr)/sizeof(fifo_pipe_state->pipe_##addr[0]); i++) \
+			fifo_pipe_state->pipe_##addr[i] = NV_READ(NV10_PGRAPH_PIPE_DATA); \
+	} while (0)
+
+	PIPE_SAVE(0x4400);
+	PIPE_SAVE(0x0200);
+	PIPE_SAVE(0x6400);
+	PIPE_SAVE(0x6800);
+	PIPE_SAVE(0x6c00);
+	PIPE_SAVE(0x7000);
+	PIPE_SAVE(0x7400);
+	PIPE_SAVE(0x7800);
+	PIPE_SAVE(0x0040);
+	PIPE_SAVE(0x0000);
+
+#undef PIPE_SAVE
+}
+
+static void nv10_graph_load_pipe(struct nouveau_channel *chan) {
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	struct pipe_state *fifo_pipe_state = &pgraph_ctx->pipe_state;
+	int i;
+	uint32_t xfmode0, xfmode1;
+#define PIPE_RESTORE(addr) \
+	do { \
+		NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, addr); \
+		for (i=0; i < sizeof(fifo_pipe_state->pipe_##addr)/sizeof(fifo_pipe_state->pipe_##addr[0]); i++) \
+			NV_WRITE(NV10_PGRAPH_PIPE_DATA, fifo_pipe_state->pipe_##addr[i]); \
+	} while (0)
+
+
+	nouveau_wait_for_idle(dev);
+	/* XXX check haiku comments */
+	xfmode0 = NV_READ(NV10_PGRAPH_XFMODE0);
+	xfmode1 = NV_READ(NV10_PGRAPH_XFMODE1);
+	NV_WRITE(NV10_PGRAPH_XFMODE0, 0x10000000);
+	NV_WRITE(NV10_PGRAPH_XFMODE1, 0x00000000);
+	NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, 0x000064c0);
+	for (i = 0; i < 4; i++)
+		NV_WRITE(NV10_PGRAPH_PIPE_DATA, 0x3f800000);
+	for (i = 0; i < 4; i++)
+		NV_WRITE(NV10_PGRAPH_PIPE_DATA, 0x00000000);
+
+	NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, 0x00006ab0);
+	for (i = 0; i < 3; i++)
+		NV_WRITE(NV10_PGRAPH_PIPE_DATA, 0x3f800000);
+
+	NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, 0x00006a80);
+	for (i = 0; i < 3; i++)
+		NV_WRITE(NV10_PGRAPH_PIPE_DATA, 0x00000000);
+
+	NV_WRITE(NV10_PGRAPH_PIPE_ADDRESS, 0x00000040);
+	NV_WRITE(NV10_PGRAPH_PIPE_DATA, 0x00000008);
+
+
+	PIPE_RESTORE(0x0200);
+	nouveau_wait_for_idle(dev);
+
+	/* restore XFMODE */
+	NV_WRITE(NV10_PGRAPH_XFMODE0, xfmode0);
+	NV_WRITE(NV10_PGRAPH_XFMODE1, xfmode1);
+	PIPE_RESTORE(0x6400);
+	PIPE_RESTORE(0x6800);
+	PIPE_RESTORE(0x6c00);
+	PIPE_RESTORE(0x7000);
+	PIPE_RESTORE(0x7400);
+	PIPE_RESTORE(0x7800);
+	PIPE_RESTORE(0x4400);
+	PIPE_RESTORE(0x0000);
+	PIPE_RESTORE(0x0040);
+	nouveau_wait_for_idle(dev);
+
+#undef PIPE_RESTORE
+}
+
+static void nv10_graph_create_pipe(struct nouveau_channel *chan) {
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	struct pipe_state *fifo_pipe_state = &pgraph_ctx->pipe_state;
+	uint32_t *fifo_pipe_state_addr;
+	int i;
+#define PIPE_INIT(addr) \
+	do { \
+		fifo_pipe_state_addr = fifo_pipe_state->pipe_##addr; \
+	} while (0)
+#define PIPE_INIT_END(addr) \
+	do { \
+		if (fifo_pipe_state_addr != \
+				sizeof(fifo_pipe_state->pipe_##addr)/sizeof(fifo_pipe_state->pipe_##addr[0]) + fifo_pipe_state->pipe_##addr) \
+			DRM_ERROR("incomplete pipe init for 0x%x :  %p/%p\n", addr, fifo_pipe_state_addr, \
+					sizeof(fifo_pipe_state->pipe_##addr)/sizeof(fifo_pipe_state->pipe_##addr[0]) + fifo_pipe_state->pipe_##addr); \
+	} while (0)
+#define NV_WRITE_PIPE_INIT(value) *(fifo_pipe_state_addr++) = value
+
+	PIPE_INIT(0x0200);
+	for (i = 0; i < 48; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x0200);
+
+	PIPE_INIT(0x6400);
+	for (i = 0; i < 211; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x40000000);
+	NV_WRITE_PIPE_INIT(0x40000000);
+	NV_WRITE_PIPE_INIT(0x40000000);
+	NV_WRITE_PIPE_INIT(0x40000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f000000);
+	NV_WRITE_PIPE_INIT(0x3f000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	PIPE_INIT_END(0x6400);
+
+	PIPE_INIT(0x6800);
+	for (i = 0; i < 162; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x3f800000);
+	for (i = 0; i < 25; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x6800);
+
+	PIPE_INIT(0x6c00);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0xbf800000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x6c00);
+
+	PIPE_INIT(0x7000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x00000000);
+	NV_WRITE_PIPE_INIT(0x7149f2ca);
+	for (i = 0; i < 35; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x7000);
+
+	PIPE_INIT(0x7400);
+	for (i = 0; i < 48; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x7400);
+
+	PIPE_INIT(0x7800);
+	for (i = 0; i < 48; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x7800);
+
+	PIPE_INIT(0x4400);
+	for (i = 0; i < 32; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x4400);
+
+	PIPE_INIT(0x0000);
+	for (i = 0; i < 16; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x0000);
+
+	PIPE_INIT(0x0040);
+	for (i = 0; i < 4; i++)
+		NV_WRITE_PIPE_INIT(0x00000000);
+	PIPE_INIT_END(0x0040);
+
+#undef PIPE_INIT
+#undef PIPE_INIT_END
+#undef NV_WRITE_PIPE_INIT
+}
+
+static int nv10_graph_ctx_regs_find_offset(struct drm_device *dev, int reg)
+{
+	int i;
+	for (i = 0; i < sizeof(nv10_graph_ctx_regs)/sizeof(nv10_graph_ctx_regs[0]); i++) {
+		if (nv10_graph_ctx_regs[i] == reg)
+			return i;
+	}
+	DRM_ERROR("unknow offset nv10_ctx_regs %d\n", reg);
+	return -1;
+}
+
+static int nv17_graph_ctx_regs_find_offset(struct drm_device *dev, int reg)
+{
+	int i;
+	for (i = 0; i < sizeof(nv17_graph_ctx_regs)/sizeof(nv17_graph_ctx_regs[0]); i++) {
+		if (nv17_graph_ctx_regs[i] == reg)
+			return i;
+	}
+	DRM_ERROR("unknow offset nv17_ctx_regs %d\n", reg);
+	return -1;
+}
+
+int nv10_graph_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	int i;
+
+	for (i = 0; i < sizeof(nv10_graph_ctx_regs)/sizeof(nv10_graph_ctx_regs[0]); i++)
+		NV_WRITE(nv10_graph_ctx_regs[i], pgraph_ctx->nv10[i]);
+	if (dev_priv->chipset>=0x17) {
+		for (i = 0; i < sizeof(nv17_graph_ctx_regs)/sizeof(nv17_graph_ctx_regs[0]); i++)
+			NV_WRITE(nv17_graph_ctx_regs[i], pgraph_ctx->nv17[i]);
+	}
+
+	nv10_graph_load_pipe(chan);
+
+	return 0;
+}
+
+int nv10_graph_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	int i;
+
+	for (i = 0; i < sizeof(nv10_graph_ctx_regs)/sizeof(nv10_graph_ctx_regs[0]); i++)
+		pgraph_ctx->nv10[i] = NV_READ(nv10_graph_ctx_regs[i]);
+	if (dev_priv->chipset>=0x17) {
+		for (i = 0; i < sizeof(nv17_graph_ctx_regs)/sizeof(nv17_graph_ctx_regs[0]); i++)
+			pgraph_ctx->nv17[i] = NV_READ(nv17_graph_ctx_regs[i]);
+	}
+
+	nv10_graph_save_pipe(chan);
+
+	return 0;
+}
+
+void nouveau_nv10_context_switch(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv;
+	struct nouveau_engine *engine;
+	struct nouveau_channel *next, *last;
+	int chid;
+
+	if (!dev) {
+		DRM_DEBUG("Invalid drm_device\n");
+		return;
+	}
+	dev_priv = dev->dev_private;
+	if (!dev_priv) {
+		DRM_DEBUG("Invalid drm_nouveau_private\n");
+		return;
+	}
+	if (!dev_priv->fifos) {
+		DRM_DEBUG("Invalid drm_nouveau_private->fifos\n");
+		return;
+	}
+	engine = &dev_priv->Engine;
+
+	chid = (NV_READ(NV04_PGRAPH_TRAPPED_ADDR) >> 20) &
+		(engine->fifo.channels - 1);
+	next = dev_priv->fifos[chid];
+
+	if (!next) {
+		DRM_ERROR("Invalid next channel\n");
+		return;
+	}
+
+	chid = (NV_READ(NV10_PGRAPH_CTX_USER) >> 24) & 
+		(engine->fifo.channels - 1);
+	last = dev_priv->fifos[chid];
+
+	if (!last) {
+		DRM_INFO("WARNING: Invalid last channel, switch to %x\n",
+		          next->id);
+	} else {
+		DRM_DEBUG("NV: PGRAPH context switch interrupt channel %x -> %x\n",
+		         last->id, next->id);
+	}
+
+	NV_WRITE(NV04_PGRAPH_FIFO,0x0);
+	if (last) {
+		nouveau_wait_for_idle(dev);
+		nv10_graph_save_context(last);
+	}
+
+	nouveau_wait_for_idle(dev);
+
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10000000);
+
+	nouveau_wait_for_idle(dev);
+
+	nv10_graph_load_context(next);
+
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10010100);
+	NV_WRITE(NV10_PGRAPH_FFINTFC_ST2, NV_READ(NV10_PGRAPH_FFINTFC_ST2)&0xCFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_FIFO,0x1);
+}
+
+#define NV_WRITE_CTX(reg, val) do { \
+	int offset = nv10_graph_ctx_regs_find_offset(dev, reg); \
+	if (offset > 0) \
+		pgraph_ctx->nv10[offset] = val; \
+	} while (0)
+
+#define NV17_WRITE_CTX(reg, val) do { \
+	int offset = nv17_graph_ctx_regs_find_offset(dev, reg); \
+	if (offset > 0) \
+		pgraph_ctx->nv17[offset] = val; \
+	} while (0)
+
+int nv10_graph_create_context(struct nouveau_channel *chan) {
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct graph_state* pgraph_ctx;
+
+	DRM_DEBUG("nv10_graph_context_create %d\n", chan->id);
+
+	chan->pgraph_ctx = pgraph_ctx = drm_calloc(1, sizeof(*pgraph_ctx),
+					      DRM_MEM_DRIVER);
+
+	if (pgraph_ctx == NULL)
+		return -ENOMEM;
+
+	/* mmio trace suggest that should be done in ddx with methods/objects */
+
+	NV_WRITE_CTX(0x00400e88, 0x08000000);
+	NV_WRITE_CTX(0x00400e9c, 0x4b7fffff);
+	NV_WRITE_CTX(NV03_PGRAPH_XY_LOGIC_MISC0, 0x0001ffff);
+	NV_WRITE_CTX(0x00400e10, 0x00001000);
+	NV_WRITE_CTX(0x00400e14, 0x00001000);
+	NV_WRITE_CTX(0x00400e30, 0x00080008);
+	NV_WRITE_CTX(0x00400e34, 0x00080008);
+	if (dev_priv->chipset>=0x17) {
+		/* is it really needed ??? */
+		NV17_WRITE_CTX(NV10_PGRAPH_DEBUG_4, NV_READ(NV10_PGRAPH_DEBUG_4));
+		NV17_WRITE_CTX(0x004006b0, NV_READ(0x004006b0));
+		NV17_WRITE_CTX(0x00400eac, 0x0fff0000);
+		NV17_WRITE_CTX(0x00400eb0, 0x0fff0000);
+		NV17_WRITE_CTX(0x00400ec0, 0x00000080);
+		NV17_WRITE_CTX(0x00400ed0, 0x00000080);
+	}
+	NV_WRITE_CTX(NV10_PGRAPH_CTX_USER, chan->id << 24);
+
+	nv10_graph_create_pipe(chan);
+	return 0;
+}
+
+void nv10_graph_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct graph_state* pgraph_ctx = chan->pgraph_ctx;
+	int chid;
+
+	drm_free(pgraph_ctx, sizeof(*pgraph_ctx), DRM_MEM_DRIVER);
+	chan->pgraph_ctx = NULL;
+
+	chid = (NV_READ(NV10_PGRAPH_CTX_USER) >> 24) & (engine->fifo.channels - 1);
+
+	/* This code seems to corrupt the 3D pipe, but blob seems to do similar things ????
+	 */
+	if (chid == chan->id) {
+		DRM_INFO("cleanning a channel with graph in current context\n");
+	}
+}
+
+int nv10_graph_init(struct drm_device *dev) {
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PGRAPH);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PGRAPH);
+
+	NV_WRITE(NV03_PGRAPH_INTR   , 0xFFFFFFFF);
+	NV_WRITE(NV03_PGRAPH_INTR_EN, 0xFFFFFFFF);
+
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x00000000);
+	NV_WRITE(NV04_PGRAPH_DEBUG_1, 0x00118700);
+	//NV_WRITE(NV04_PGRAPH_DEBUG_2, 0x24E00810); /* 0x25f92ad9 */
+	NV_WRITE(NV04_PGRAPH_DEBUG_2, 0x25f92ad9);
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, 0x55DE0830 |
+				      (1<<29) |
+				      (1<<31));
+	if (dev_priv->chipset>=0x17) {
+		NV_WRITE(NV10_PGRAPH_DEBUG_4, 0x1f000000);
+		NV_WRITE(0x004006b0, 0x40000020);
+	}
+	else
+		NV_WRITE(NV10_PGRAPH_DEBUG_4, 0x00000000);
+
+	/* copy tile info from PFB */
+	for (i=0; i<NV10_PFB_TILE__SIZE; i++) {
+		NV_WRITE(NV10_PGRAPH_TILE(i), NV_READ(NV10_PFB_TILE(i)));
+		NV_WRITE(NV10_PGRAPH_TLIMIT(i), NV_READ(NV10_PFB_TLIMIT(i)));
+		NV_WRITE(NV10_PGRAPH_TSIZE(i), NV_READ(NV10_PFB_TSIZE(i)));
+		NV_WRITE(NV10_PGRAPH_TSTATUS(i), NV_READ(NV10_PFB_TSTATUS(i)));
+	}
+
+	NV_WRITE(NV10_PGRAPH_CTX_SWITCH1, 0x00000000);
+	NV_WRITE(NV10_PGRAPH_CTX_SWITCH2, 0x00000000);
+	NV_WRITE(NV10_PGRAPH_CTX_SWITCH3, 0x00000000);
+	NV_WRITE(NV10_PGRAPH_CTX_SWITCH4, 0x00000000);
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10010100);
+	NV_WRITE(NV10_PGRAPH_STATE      , 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_FIFO       , 0x00000001);
+
+	return 0;
+}
+
+void nv10_graph_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv20_graph.c b/drivers/char/drm/nv20_graph.c
new file mode 100644
index 0000000..d171113
--- /dev/null
+++ b/drivers/char/drm/nv20_graph.c
@@ -0,0 +1,889 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+/*
+ * NV20
+ * -----
+ * There are 3 families :
+ * NV20 is 0x10de:0x020*
+ * NV25/28 is 0x10de:0x025* / 0x10de:0x028*
+ * NV2A is 0x10de:0x02A0
+ *
+ * NV30
+ * -----
+ * There are 3 families :
+ * NV30/31 is 0x10de:0x030* / 0x10de:0x031*
+ * NV34 is 0x10de:0x032*
+ * NV35/36 is 0x10de:0x033* / 0x10de:0x034*
+ *
+ * Not seen in the wild, no dumps (probably NV35) :
+ * NV37 is 0x10de:0x00fc, 0x10de:0x00fd
+ * NV38 is 0x10de:0x0333, 0x10de:0x00fe
+ *
+ */
+
+#define NV20_GRCTX_SIZE (3580*4)
+#define NV25_GRCTX_SIZE (3529*4)
+#define NV2A_GRCTX_SIZE (3500*4)
+
+#define NV30_31_GRCTX_SIZE (24392)
+#define NV34_GRCTX_SIZE    (18140)
+#define NV35_36_GRCTX_SIZE (22396)
+
+static void nv20_graph_context_init(struct drm_device *dev,
+                                    struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+/*
+write32 #1 block at +0x00740adc NV_PRAMIN+0x40adc of 3369 (0xd29) elements:
++0x00740adc: ffff0000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740afc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b1c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b3c: 00000000 0fff0000 0fff0000 00000000 00000000 00000000 00000000 00000000
++0x00740b5c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b7c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b9c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740bbc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740bdc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740bfc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+
++0x00740c1c: 00000101 00000000 00000000 00000000 00000000 00000111 00000000 00000000
++0x00740c3c: 00000000 00000000 00000000 44400000 00000000 00000000 00000000 00000000
++0x00740c5c: 00000000 00000000 00000000 00000000 00000000 00000000 00030303 00030303
++0x00740c7c: 00030303 00030303 00000000 00000000 00000000 00000000 00080000 00080000
++0x00740c9c: 00080000 00080000 00000000 00000000 01012000 01012000 01012000 01012000
++0x00740cbc: 000105b8 000105b8 000105b8 000105b8 00080008 00080008 00080008 00080008
++0x00740cdc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740cfc: 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000
++0x00740d1c: 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000
++0x00740d3c: 00000000 00000000 4b7fffff 00000000 00000000 00000000 00000000 00000000
+
++0x00740d5c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740d7c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740d9c: 00000001 00000000 00004000 00000000 00000000 00000001 00000000 00040000
++0x00740dbc: 00010000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740ddc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+...
+*/
+	INSTANCE_WR(ctx, (0x33c/4)+0, 0xffff0000);
+	INSTANCE_WR(ctx, (0x33c/4)+25, 0x0fff0000);
+	INSTANCE_WR(ctx, (0x33c/4)+26, 0x0fff0000);
+	INSTANCE_WR(ctx, (0x33c/4)+80, 0x00000101);
+	INSTANCE_WR(ctx, (0x33c/4)+85, 0x00000111);
+	INSTANCE_WR(ctx, (0x33c/4)+91, 0x44400000);
+	for (i = 0; i < 4; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+102+i, 0x00030303);
+	for (i = 0; i < 4; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+110+i, 0x00080000);
+	for (i = 0; i < 4; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+116+i, 0x01012000);
+	for (i = 0; i < 4; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+120+i, 0x000105b8);
+	for (i = 0; i < 4; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+124+i, 0x00080008);
+	for (i = 0; i < 16; ++i)
+		INSTANCE_WR(ctx, (0x33c/4)+136+i, 0x07ff0000);
+	INSTANCE_WR(ctx, (0x33c/4)+154, 0x4b7fffff);
+	INSTANCE_WR(ctx, (0x33c/4)+176, 0x00000001);
+	INSTANCE_WR(ctx, (0x33c/4)+178, 0x00004000);
+	INSTANCE_WR(ctx, (0x33c/4)+181, 0x00000001);
+	INSTANCE_WR(ctx, (0x33c/4)+183, 0x00040000);
+	INSTANCE_WR(ctx, (0x33c/4)+184, 0x00010000);
+
+/*
+...
++0x0074239c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x007423bc: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x007423dc: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x007423fc: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
+...
++0x00742bdc: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742bfc: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742c1c: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742c3c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+...
+*/
+	for (i = 0; i < 0x880; i += 0x10) {
+		INSTANCE_WR(ctx, ((0x1c1c + i)/4)+0, 0x10700ff9);
+		INSTANCE_WR(ctx, ((0x1c1c + i)/4)+1, 0x0436086c);
+		INSTANCE_WR(ctx, ((0x1c1c + i)/4)+2, 0x000c001b);
+	}
+
+/*
+write32 #1 block at +0x00742fbc NV_PRAMIN+0x42fbc of 4 (0x4) elements:
++0x00742fbc: 3f800000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x281c/4), 0x3f800000);
+
+/*
+write32 #1 block at +0x00742ffc NV_PRAMIN+0x42ffc of 12 (0xc) elements:
++0x00742ffc: 40000000 3f800000 3f000000 00000000 40000000 3f800000 00000000 bf800000
++0x0074301c: 00000000 bf800000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x285c/4)+0, 0x40000000);
+	INSTANCE_WR(ctx, (0x285c/4)+1, 0x3f800000);
+	INSTANCE_WR(ctx, (0x285c/4)+2, 0x3f000000);
+	INSTANCE_WR(ctx, (0x285c/4)+4, 0x40000000);
+	INSTANCE_WR(ctx, (0x285c/4)+5, 0x3f800000);
+	INSTANCE_WR(ctx, (0x285c/4)+7, 0xbf800000);
+	INSTANCE_WR(ctx, (0x285c/4)+9, 0xbf800000);
+
+/*
+write32 #1 block at +0x00742fcc NV_PRAMIN+0x42fcc of 4 (0x4) elements:
++0x00742fcc: 00000000 3f800000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x282c/4)+1, 0x3f800000);
+
+/*
+write32 #1 block at +0x0074302c NV_PRAMIN+0x4302c of 4 (0x4) elements:
++0x0074302c: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x00743c9c NV_PRAMIN+0x43c9c of 4 (0x4) elements:
++0x00743c9c: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x00743c3c NV_PRAMIN+0x43c3c of 8 (0x8) elements:
++0x00743c3c: 00000000 00000000 000fe000 00000000 00000000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x349c/4)+2, 0x000fe000);
+
+/*
+write32 #1 block at +0x00743c6c NV_PRAMIN+0x43c6c of 4 (0x4) elements:
++0x00743c6c: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x00743ccc NV_PRAMIN+0x43ccc of 4 (0x4) elements:
++0x00743ccc: 00000000 000003f8 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x352c/4)+1, 0x000003f8);
+
+/* write32 #1 NV_PRAMIN+0x43ce0 <- 0x002fe000 */
+	INSTANCE_WR(ctx, 0x3540/4, 0x002fe000);
+
+/*
+write32 #1 block at +0x00743cfc NV_PRAMIN+0x43cfc of 8 (0x8) elements:
++0x00743cfc: 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c
+*/
+	for (i = 0; i < 8; ++i)
+		INSTANCE_WR(ctx, (0x355c/4)+i, 0x001c527c);
+}
+
+static void nv2a_graph_context_init(struct drm_device *dev,
+                                    struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x33c/4, 0xffff0000);
+	for(i = 0x3a0; i< 0x3a8; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x47c/4, 0x00000101);
+	INSTANCE_WR(ctx, 0x490/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x4a8/4, 0x44400000);
+	for(i = 0x4d4; i< 0x4e4; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00030303);
+	for(i = 0x4f4; i< 0x504; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080000);
+	for(i = 0x50c; i< 0x51c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for(i = 0x51c; i< 0x52c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x000105b8);
+	for(i = 0x52c; i< 0x53c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for(i = 0x55c; i< 0x59c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x5a4/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x5fc/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x604/4, 0x00004000);
+	INSTANCE_WR(ctx, 0x610/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x618/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x61c/4, 0x00010000);
+
+	for (i=0x1a9c; i <= 0x22fc/4; i += 32) {
+		INSTANCE_WR(ctx, i/4    , 0x10700ff9);
+		INSTANCE_WR(ctx, i/4 + 1, 0x0436086c);
+		INSTANCE_WR(ctx, i/4 + 2, 0x000c001b);
+	}
+
+	INSTANCE_WR(ctx, 0x269c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x26b0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x26dc/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x26e0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x26e4/4, 0x3f000000);
+	INSTANCE_WR(ctx, 0x26ec/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x26f0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x26f8/4, 0xbf800000);
+	INSTANCE_WR(ctx, 0x2700/4, 0xbf800000);
+	INSTANCE_WR(ctx, 0x3024/4, 0x000fe000);
+	INSTANCE_WR(ctx, 0x30a0/4, 0x000003f8);
+	INSTANCE_WR(ctx, 0x33fc/4, 0x002fe000);
+	for(i = 0x341c; i< 0x343c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x001c527c);
+}
+
+static void nv25_graph_context_init(struct drm_device *dev,
+                                    struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+/*
+write32 #1 block at +0x00740a7c NV_PRAMIN.GRCTX0+0x35c of 173 (0xad) elements:
++0x00740a7c: ffff0000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740a9c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740abc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740adc: 00000000 0fff0000 0fff0000 00000000 00000000 00000000 00000000 00000000
++0x00740afc: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b1c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b3c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b5c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+
++0x00740b7c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740b9c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740bbc: 00000101 00000000 00000000 00000000 00000000 00000111 00000000 00000000
++0x00740bdc: 00000000 00000000 00000000 00000080 ffff0000 00000001 00000000 00000000
++0x00740bfc: 00000000 00000000 44400000 00000000 00000000 00000000 00000000 00000000
++0x00740c1c: 4b800000 00000000 00000000 00000000 00000000 00030303 00030303 00030303
++0x00740c3c: 00030303 00000000 00000000 00000000 00000000 00080000 00080000 00080000
++0x00740c5c: 00080000 00000000 00000000 01012000 01012000 01012000 01012000 000105b8
+
++0x00740c7c: 000105b8 000105b8 000105b8 00080008 00080008 00080008 00080008 00000000
++0x00740c9c: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 07ff0000
++0x00740cbc: 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000
++0x00740cdc: 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 07ff0000 00000000
++0x00740cfc: 00000000 4b7fffff 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740d1c: 00000000 00000000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x35c/4)+0, 0xffff0000);
+	INSTANCE_WR(ctx, (0x35c/4)+25, 0x0fff0000);
+	INSTANCE_WR(ctx, (0x35c/4)+26, 0x0fff0000);
+	INSTANCE_WR(ctx, (0x35c/4)+80, 0x00000101);
+	INSTANCE_WR(ctx, (0x35c/4)+85, 0x00000111);
+	INSTANCE_WR(ctx, (0x35c/4)+91, 0x00000080);
+	INSTANCE_WR(ctx, (0x35c/4)+92, 0xffff0000);
+	INSTANCE_WR(ctx, (0x35c/4)+93, 0x00000001);
+	INSTANCE_WR(ctx, (0x35c/4)+98, 0x44400000);
+	INSTANCE_WR(ctx, (0x35c/4)+104, 0x4b800000);
+	INSTANCE_WR(ctx, (0x35c/4)+109, 0x00030303);
+	INSTANCE_WR(ctx, (0x35c/4)+110, 0x00030303);
+	INSTANCE_WR(ctx, (0x35c/4)+111, 0x00030303);
+	INSTANCE_WR(ctx, (0x35c/4)+112, 0x00030303);
+	INSTANCE_WR(ctx, (0x35c/4)+117, 0x00080000);
+	INSTANCE_WR(ctx, (0x35c/4)+118, 0x00080000);
+	INSTANCE_WR(ctx, (0x35c/4)+119, 0x00080000);
+	INSTANCE_WR(ctx, (0x35c/4)+120, 0x00080000);
+	INSTANCE_WR(ctx, (0x35c/4)+123, 0x01012000);
+	INSTANCE_WR(ctx, (0x35c/4)+124, 0x01012000);
+	INSTANCE_WR(ctx, (0x35c/4)+125, 0x01012000);
+	INSTANCE_WR(ctx, (0x35c/4)+126, 0x01012000);
+	INSTANCE_WR(ctx, (0x35c/4)+127, 0x000105b8);
+	INSTANCE_WR(ctx, (0x35c/4)+128, 0x000105b8);
+	INSTANCE_WR(ctx, (0x35c/4)+129, 0x000105b8);
+	INSTANCE_WR(ctx, (0x35c/4)+130, 0x000105b8);
+	INSTANCE_WR(ctx, (0x35c/4)+131, 0x00080008);
+	INSTANCE_WR(ctx, (0x35c/4)+132, 0x00080008);
+	INSTANCE_WR(ctx, (0x35c/4)+133, 0x00080008);
+	INSTANCE_WR(ctx, (0x35c/4)+134, 0x00080008);
+	for (i=0; i<16; ++i)
+		INSTANCE_WR(ctx, (0x35c/4)+143+i, 0x07ff0000);
+	INSTANCE_WR(ctx, (0x35c/4)+161, 0x4b7fffff);
+
+/*
+write32 #1 block at +0x00740d34 NV_PRAMIN.GRCTX0+0x614 of 3136 (0xc40) elements:
++0x00740d34: 00000000 00000000 00000000 00000080 30201000 70605040 b0a09080 f0e0d0c0
++0x00740d54: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00740d74: 00000000 00000000 00000000 00000000 00000001 00000000 00004000 00000000
++0x00740d94: 00000000 00000001 00000000 00040000 00010000 00000000 00000000 00000000
++0x00740db4: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+...
++0x00742214: 00000000 00000000 00000000 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742234: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742254: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742274: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
+...
++0x00742a34: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742a54: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742a74: 10700ff9 0436086c 000c001b 00000000 10700ff9 0436086c 000c001b 00000000
++0x00742a94: 10700ff9 0436086c 000c001b 00000000 00000000 00000000 00000000 00000000
++0x00742ab4: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
++0x00742ad4: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x614/4)+3, 0x00000080);
+	INSTANCE_WR(ctx, (0x614/4)+4, 0x30201000);
+	INSTANCE_WR(ctx, (0x614/4)+5, 0x70605040);
+	INSTANCE_WR(ctx, (0x614/4)+6, 0xb0a09080);
+	INSTANCE_WR(ctx, (0x614/4)+7, 0xf0e0d0c0);
+	INSTANCE_WR(ctx, (0x614/4)+20, 0x00000001);
+	INSTANCE_WR(ctx, (0x614/4)+22, 0x00004000);
+	INSTANCE_WR(ctx, (0x614/4)+25, 0x00000001);
+	INSTANCE_WR(ctx, (0x614/4)+27, 0x00040000);
+	INSTANCE_WR(ctx, (0x614/4)+28, 0x00010000);
+	for (i=0; i < 0x880/4; i+=4) {
+		INSTANCE_WR(ctx, (0x1b04/4)+i+0, 0x10700ff9);
+		INSTANCE_WR(ctx, (0x1b04/4)+i+1, 0x0436086c);
+		INSTANCE_WR(ctx, (0x1b04/4)+i+2, 0x000c001b);
+	}
+
+/*
+write32 #1 block at +0x00742e24 NV_PRAMIN.GRCTX0+0x2704 of 4 (0x4) elements:
++0x00742e24: 3f800000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x2704/4), 0x3f800000);
+
+/*
+write32 #1 block at +0x00742e64 NV_PRAMIN.GRCTX0+0x2744 of 12 (0xc) elements:
++0x00742e64: 40000000 3f800000 3f000000 00000000 40000000 3f800000 00000000 bf800000
++0x00742e84: 00000000 bf800000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x2744/4)+0, 0x40000000);
+	INSTANCE_WR(ctx, (0x2744/4)+1, 0x3f800000);
+	INSTANCE_WR(ctx, (0x2744/4)+2, 0x3f000000);
+	INSTANCE_WR(ctx, (0x2744/4)+4, 0x40000000);
+	INSTANCE_WR(ctx, (0x2744/4)+5, 0x3f800000);
+	INSTANCE_WR(ctx, (0x2744/4)+7, 0xbf800000);
+	INSTANCE_WR(ctx, (0x2744/4)+9, 0xbf800000);
+
+/*
+write32 #1 block at +0x00742e34 NV_PRAMIN.GRCTX0+0x2714 of 4 (0x4) elements:
++0x00742e34: 00000000 3f800000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x2714/4)+1, 0x3f800000);
+
+/*
+write32 #1 block at +0x00742e94 NV_PRAMIN.GRCTX0+0x2774 of 4 (0x4) elements:
++0x00742e94: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x00743804 NV_PRAMIN.GRCTX0+0x30e4 of 4 (0x4) elements:
++0x00743804: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x007437a4 NV_PRAMIN.GRCTX0+0x3084 of 8 (0x8) elements:
++0x007437a4: 00000000 00000000 000fe000 00000000 00000000 00000000 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x3084/4)+2, 0x000fe000);
+
+/*
+write32 #1 block at +0x007437d4 NV_PRAMIN.GRCTX0+0x30b4 of 4 (0x4) elements:
++0x007437d4: 00000000 00000000 00000000 00000000
+write32 #1 block at +0x00743824 NV_PRAMIN.GRCTX0+0x3104 of 4 (0x4) elements:
++0x00743824: 00000000 000003f8 00000000 00000000
+*/
+	INSTANCE_WR(ctx, (0x3104/4)+1, 0x000003f8);
+
+/* write32 #1 NV_PRAMIN.GRCTX0+0x3468 <- 0x002fe000 */
+	INSTANCE_WR(ctx, 0x3468/4, 0x002fe000);
+
+/*
+write32 #1 block at +0x00743ba4 NV_PRAMIN.GRCTX0+0x3484 of 8 (0x8) elements:
++0x00743ba4: 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c 001c527c
+*/
+	for (i=0; i<8; ++i)
+		INSTANCE_WR(ctx, (0x3484/4)+i, 0x001c527c);
+}
+
+static void nv30_31_graph_context_init(struct drm_device *dev,
+                                       struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x410/4, 0x00000101);
+	INSTANCE_WR(ctx, 0x424/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x428/4, 0x00000060);
+	INSTANCE_WR(ctx, 0x444/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x448/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x44c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x460/4, 0x44400000);
+	INSTANCE_WR(ctx, 0x48c/4, 0xffff0000);
+	for(i = 0x4e0; i< 0x4e8; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x4ec/4, 0x00011100);
+	for(i = 0x508; i< 0x548; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x550/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x58c/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x590/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x594/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x598/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x59c/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x5b0/4, 0xb0000000);
+	for(i = 0x600; i< 0x640; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00010588);
+	for(i = 0x640; i< 0x680; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00030303);
+	for(i = 0x6c0; i< 0x700; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0008aae4);
+	for(i = 0x700; i< 0x740; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for(i = 0x740; i< 0x780; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x85c/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x860/4, 0x00010000);
+	for(i = 0x864; i< 0x874; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00040004);
+	for(i = 0x1f18; i<= 0x3088 ; i+= 16) {
+		INSTANCE_WR(ctx, i/4 + 0, 0x10700ff9);
+		INSTANCE_WR(ctx, i/4 + 1, 0x0436086c);
+		INSTANCE_WR(ctx, i/4 + 2, 0x000c001b);
+	}
+	for(i = 0x30b8; i< 0x30c8; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x344c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3808/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x381c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3848/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x384c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3850/4, 0x3f000000);
+	INSTANCE_WR(ctx, 0x3858/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x385c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3864/4, 0xbf800000);
+	INSTANCE_WR(ctx, 0x386c/4, 0xbf800000);
+}
+
+static void nv34_graph_context_init(struct drm_device *dev,
+                                    struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x40c/4, 0x01000101);
+	INSTANCE_WR(ctx, 0x420/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x424/4, 0x00000060);
+	INSTANCE_WR(ctx, 0x440/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x444/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x448/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x45c/4, 0x44400000);
+	INSTANCE_WR(ctx, 0x480/4, 0xffff0000);
+	for(i = 0x4d4; i< 0x4dc; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x4e0/4, 0x00011100);
+	for(i = 0x4fc; i< 0x53c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x544/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x57c/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x580/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x584/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x588/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x58c/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x5a0/4, 0xb0000000);
+	for(i = 0x5f0; i< 0x630; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00010588);
+	for(i = 0x630; i< 0x670; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00030303);
+	for(i = 0x6b0; i< 0x6f0; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0008aae4);
+	for(i = 0x6f0; i< 0x730; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for(i = 0x730; i< 0x770; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x850/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x854/4, 0x00010000);
+	for(i = 0x858; i< 0x868; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00040004);
+	for(i = 0x15ac; i<= 0x271c ; i+= 16) {
+		INSTANCE_WR(ctx, i/4 + 0, 0x10700ff9);
+		INSTANCE_WR(ctx, i/4 + 1, 0x0436086c);
+		INSTANCE_WR(ctx, i/4 + 2, 0x000c001b);
+	}
+	for(i = 0x274c; i< 0x275c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x2ae0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x2e9c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x2eb0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x2edc/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x2ee0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x2ee4/4, 0x3f000000);
+	INSTANCE_WR(ctx, 0x2eec/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x2ef0/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x2ef8/4, 0xbf800000);
+	INSTANCE_WR(ctx, 0x2f00/4, 0xbf800000);
+}
+
+static void nv35_36_graph_context_init(struct drm_device *dev,
+                                       struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x40c/4, 0x00000101);
+	INSTANCE_WR(ctx, 0x420/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x424/4, 0x00000060);
+	INSTANCE_WR(ctx, 0x440/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x444/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x448/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x45c/4, 0x44400000);
+	INSTANCE_WR(ctx, 0x488/4, 0xffff0000);
+	for(i = 0x4dc; i< 0x4e4; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x4e8/4, 0x00011100);
+	for(i = 0x504; i< 0x544; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x54c/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x588/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x58c/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x590/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x594/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x598/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x5ac/4, 0xb0000000);
+	for(i = 0x604; i< 0x644; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00010588);
+	for(i = 0x644; i< 0x684; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00030303);
+	for(i = 0x6c4; i< 0x704; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0008aae4);
+	for(i = 0x704; i< 0x744; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for(i = 0x744; i< 0x784; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x860/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x864/4, 0x00010000);
+	for(i = 0x868; i< 0x878; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00040004);
+	for(i = 0x1f1c; i<= 0x308c ; i+= 16) {
+		INSTANCE_WR(ctx, i/4 + 0, 0x10700ff9);
+		INSTANCE_WR(ctx, i/4 + 1, 0x0436086c);
+		INSTANCE_WR(ctx, i/4 + 2, 0x000c001b);
+	}
+	for(i = 0x30bc; i< 0x30cc; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x3450/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x380c/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3820/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x384c/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x3850/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3854/4, 0x3f000000);
+	INSTANCE_WR(ctx, 0x385c/4, 0x40000000);
+	INSTANCE_WR(ctx, 0x3860/4, 0x3f800000);
+	INSTANCE_WR(ctx, 0x3868/4, 0xbf800000);
+	INSTANCE_WR(ctx, 0x3870/4, 0xbf800000);
+}
+
+int nv20_graph_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	void (*ctx_init)(struct drm_device *, struct nouveau_gpuobj *);
+	unsigned int ctx_size;
+	unsigned int idoffs = 0x28/4;
+	int ret;
+
+	switch (dev_priv->chipset) {
+	case 0x20:
+		ctx_size = NV20_GRCTX_SIZE;
+		ctx_init = nv20_graph_context_init;
+		idoffs = 0;
+		break;
+	case 0x25:
+	case 0x28:
+		ctx_size = NV25_GRCTX_SIZE;
+		ctx_init = nv25_graph_context_init;
+		break;
+	case 0x2a:
+		ctx_size = NV2A_GRCTX_SIZE;
+		ctx_init = nv2a_graph_context_init;
+		idoffs = 0;
+		break;
+	case 0x30:
+	case 0x31:
+		ctx_size = NV30_31_GRCTX_SIZE;
+		ctx_init = nv30_31_graph_context_init;
+		break;
+	case 0x34:
+		ctx_size = NV34_GRCTX_SIZE;
+		ctx_init = nv34_graph_context_init;
+		break;
+	case 0x35:
+	case 0x36:
+		ctx_size = NV35_36_GRCTX_SIZE;
+		ctx_init = nv35_36_graph_context_init;
+		break;
+	default:
+		ctx_size = 0;
+		ctx_init = nv35_36_graph_context_init;
+		DRM_ERROR("Please contact the devs if you want your NV%x"
+		          " card to work\n", dev_priv->chipset);
+		return -ENOSYS;
+		break;
+	}
+
+	if ((ret = nouveau_gpuobj_new_ref(dev, chan, NULL, 0, ctx_size, 16,
+					  NVOBJ_FLAG_ZERO_ALLOC,
+					  &chan->ramin_grctx)))
+		return ret;
+
+	/* Initialise default context values */
+	ctx_init(dev, chan->ramin_grctx->gpuobj);
+
+	/* nv20: INSTANCE_WR(chan->ramin_grctx->gpuobj, 10, chan->id<<24); */
+	INSTANCE_WR(chan->ramin_grctx->gpuobj, idoffs, (chan->id<<24)|0x1);
+	                                                     /* CTX_USER */
+
+	INSTANCE_WR(dev_priv->ctx_table->gpuobj, chan->id,
+			chan->ramin_grctx->instance >> 4);
+
+	return 0;
+}
+
+void nv20_graph_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	if (chan->ramin_grctx)
+		nouveau_gpuobj_ref_del(dev, &chan->ramin_grctx);
+
+	INSTANCE_WR(dev_priv->ctx_table->gpuobj, chan->id, 0);
+}
+
+int nv20_graph_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst;
+
+	if (!chan->ramin_grctx)
+		return -EINVAL;
+	inst = chan->ramin_grctx->instance >> 4;
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_XFER,
+		 NV20_PGRAPH_CHANNEL_CTX_XFER_LOAD);
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10010100);
+
+	nouveau_wait_for_idle(dev);
+	return 0;
+}
+
+int nv20_graph_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst;
+
+	if (!chan->ramin_grctx)
+		return -EINVAL;
+	inst = chan->ramin_grctx->instance >> 4;
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_XFER,
+		 NV20_PGRAPH_CHANNEL_CTX_XFER_SAVE);
+
+	nouveau_wait_for_idle(dev);
+	return 0;
+}
+
+static void nv20_graph_rdi(struct drm_device *dev) {
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i, writecount = 32;
+	uint32_t rdi_index = 0x2c80000;
+
+	if (dev_priv->chipset == 0x20) {
+		rdi_index = 0x3d0000;
+		writecount = 15;
+	}
+
+	NV_WRITE(NV10_PGRAPH_RDI_INDEX, rdi_index);
+	for (i = 0; i < writecount; i++)
+		NV_WRITE(NV10_PGRAPH_RDI_DATA, 0);
+
+	nouveau_wait_for_idle(dev);
+}
+
+int nv20_graph_init(struct drm_device *dev) {
+	struct drm_nouveau_private *dev_priv =
+		(struct drm_nouveau_private *)dev->dev_private;
+	uint32_t tmp, vramsz;
+	int ret, i;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PGRAPH);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PGRAPH);
+
+	/* Create Context Pointer Table */
+	dev_priv->ctx_table_size = 32 * 4;
+	if ((ret = nouveau_gpuobj_new_ref(dev, NULL, NULL, 0,
+					  dev_priv->ctx_table_size, 16,
+					  NVOBJ_FLAG_ZERO_ALLOC,
+					  &dev_priv->ctx_table)))
+		return ret;
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_TABLE,
+		 dev_priv->ctx_table->instance >> 4);
+
+	nv20_graph_rdi(dev);
+
+	NV_WRITE(NV03_PGRAPH_INTR   , 0xFFFFFFFF);
+	NV_WRITE(NV03_PGRAPH_INTR_EN, 0xFFFFFFFF);
+
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x00000000);
+	NV_WRITE(NV04_PGRAPH_DEBUG_1, 0x00118700);
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, 0xF3CE0475); /* 0x4 = auto ctx switch */
+	NV_WRITE(NV10_PGRAPH_DEBUG_4, 0x00000000);
+	NV_WRITE(0x40009C           , 0x00000040);
+
+	if (dev_priv->chipset >= 0x25) {
+		NV_WRITE(0x400890, 0x00080000);
+		NV_WRITE(0x400610, 0x304B1FB6);
+		NV_WRITE(0x400B80, 0x18B82880);
+		NV_WRITE(0x400B84, 0x44000000);
+		NV_WRITE(0x400098, 0x40000080);
+		NV_WRITE(0x400B88, 0x000000ff);
+	} else {
+		NV_WRITE(0x400880, 0x00080000); /* 0x0008c7df */
+		NV_WRITE(0x400094, 0x00000005);
+		NV_WRITE(0x400B80, 0x45CAA208); /* 0x45eae20e */
+		NV_WRITE(0x400B84, 0x24000000);
+		NV_WRITE(0x400098, 0x00000040);
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00E00038);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00000030);
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00E10038);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00000030);
+	}
+
+	/* copy tile info from PFB */
+	for (i = 0; i < NV10_PFB_TILE__SIZE; i++) {
+		NV_WRITE(0x00400904 + i*0x10, NV_READ(NV10_PFB_TLIMIT(i)));
+			/* which is NV40_PGRAPH_TLIMIT0(i) ?? */
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0030+i*4);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA, NV_READ(NV10_PFB_TLIMIT(i)));
+		NV_WRITE(0x00400908 + i*0x10, NV_READ(NV10_PFB_TSIZE(i)));
+			/* which is NV40_PGRAPH_TSIZE0(i) ?? */
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0050+i*4);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA, NV_READ(NV10_PFB_TSIZE(i)));
+		NV_WRITE(0x00400900 + i*0x10, NV_READ(NV10_PFB_TILE(i)));
+			/* which is NV40_PGRAPH_TILE0(i) ?? */
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0010+i*4);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA, NV_READ(NV10_PFB_TILE(i)));
+	}
+	for (i = 0; i < 8; i++) {
+		NV_WRITE(0x400980+i*4, NV_READ(0x100300+i*4));
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0090+i*4);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA, NV_READ(0x100300+i*4));
+	}
+	NV_WRITE(0x4009a0, NV_READ(0x100324));
+	NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA000C);
+	NV_WRITE(NV10_PGRAPH_RDI_DATA, NV_READ(0x100324));
+
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10000100);
+	NV_WRITE(NV10_PGRAPH_STATE      , 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_FIFO       , 0x00000001);
+
+	tmp = NV_READ(NV10_PGRAPH_SURFACE) & 0x0007ff00;
+	NV_WRITE(NV10_PGRAPH_SURFACE, tmp);
+	tmp = NV_READ(NV10_PGRAPH_SURFACE) | 0x00020100;
+	NV_WRITE(NV10_PGRAPH_SURFACE, tmp);
+
+	/* begin RAM config */
+	vramsz = drm_get_resource_len(dev, 0) - 1;
+	NV_WRITE(0x4009A4, NV_READ(NV04_PFB_CFG0));
+	NV_WRITE(0x4009A8, NV_READ(NV04_PFB_CFG1));
+	NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0000);
+	NV_WRITE(NV10_PGRAPH_RDI_DATA , NV_READ(NV04_PFB_CFG0));
+	NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0004);
+	NV_WRITE(NV10_PGRAPH_RDI_DATA , NV_READ(NV04_PFB_CFG1));
+	NV_WRITE(0x400820, 0);
+	NV_WRITE(0x400824, 0);
+	NV_WRITE(0x400864, vramsz-1);
+	NV_WRITE(0x400868, vramsz-1);
+
+	/* interesting.. the below overwrites some of the tile setup above.. */
+	NV_WRITE(0x400B20, 0x00000000);
+	NV_WRITE(0x400B04, 0xFFFFFFFF);
+
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_XMIN, 0);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_YMIN, 0);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_XMAX, 0x7fff);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_YMAX, 0x7fff);
+
+	return 0;
+}
+
+void nv20_graph_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	nouveau_gpuobj_ref_del(dev, &dev_priv->ctx_table);
+}
+
+int nv30_graph_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+//	uint32_t vramsz, tmp;
+	int ret, i;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PGRAPH);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PGRAPH);
+
+	/* Create Context Pointer Table */
+	dev_priv->ctx_table_size = 32 * 4;
+	if ((ret = nouveau_gpuobj_new_ref(dev, NULL, NULL, 0,
+					  dev_priv->ctx_table_size, 16,
+					  NVOBJ_FLAG_ZERO_ALLOC,
+					  &dev_priv->ctx_table)))
+		return ret;
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_TABLE,
+			dev_priv->ctx_table->instance >> 4);
+
+	NV_WRITE(NV03_PGRAPH_INTR   , 0xFFFFFFFF);
+	NV_WRITE(NV03_PGRAPH_INTR_EN, 0xFFFFFFFF);
+
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x00000000);
+	NV_WRITE(NV04_PGRAPH_DEBUG_1, 0x401287c0);
+	NV_WRITE(0x400890, 0x01b463ff);
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, 0xf2de0475);
+	NV_WRITE(NV10_PGRAPH_DEBUG_4, 0x00008000);
+	NV_WRITE(NV04_PGRAPH_LIMIT_VIOL_PIX, 0xf04bdff6);
+	NV_WRITE(0x400B80, 0x1003d888);
+	NV_WRITE(0x400B84, 0x0c000000);
+	NV_WRITE(0x400098, 0x00000000);
+	NV_WRITE(0x40009C, 0x0005ad00);
+	NV_WRITE(0x400B88, 0x62ff00ff); // suspiciously like PGRAPH_DEBUG_2
+	NV_WRITE(0x4000a0, 0x00000000);
+	NV_WRITE(0x4000a4, 0x00000008);
+	NV_WRITE(0x4008a8, 0xb784a400);
+	NV_WRITE(0x400ba0, 0x002f8685);
+	NV_WRITE(0x400ba4, 0x00231f3f);
+	NV_WRITE(0x4008a4, 0x40000020);
+
+	if (dev_priv->chipset == 0x34) {
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0004);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00200201);
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0008);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00000008);
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00EA0000);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00000032);
+		NV_WRITE(NV10_PGRAPH_RDI_INDEX, 0x00E00004);
+		NV_WRITE(NV10_PGRAPH_RDI_DATA , 0x00000002);
+	}
+
+	NV_WRITE(0x4000c0, 0x00000016);
+
+	/* copy tile info from PFB */
+	for (i = 0; i < NV10_PFB_TILE__SIZE; i++) {
+		NV_WRITE(0x00400904 + i*0x10, NV_READ(NV10_PFB_TLIMIT(i)));
+			/* which is NV40_PGRAPH_TLIMIT0(i) ?? */
+		NV_WRITE(0x00400908 + i*0x10, NV_READ(NV10_PFB_TSIZE(i)));
+			/* which is NV40_PGRAPH_TSIZE0(i) ?? */
+		NV_WRITE(0x00400900 + i*0x10, NV_READ(NV10_PFB_TILE(i)));
+			/* which is NV40_PGRAPH_TILE0(i) ?? */
+	}
+
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10000100);
+	NV_WRITE(NV10_PGRAPH_STATE      , 0xFFFFFFFF);
+	NV_WRITE(0x0040075c             , 0x00000001);
+	NV_WRITE(NV04_PGRAPH_FIFO       , 0x00000001);
+
+	/* begin RAM config */
+//	vramsz = drm_get_resource_len(dev, 0) - 1;
+	NV_WRITE(0x4009A4, NV_READ(NV04_PFB_CFG0));
+	NV_WRITE(0x4009A8, NV_READ(NV04_PFB_CFG1));
+	if (dev_priv->chipset != 0x34) {
+		NV_WRITE(0x400750, 0x00EA0000);
+		NV_WRITE(0x400754, NV_READ(NV04_PFB_CFG0));
+		NV_WRITE(0x400750, 0x00EA0004);
+		NV_WRITE(0x400754, NV_READ(NV04_PFB_CFG1));
+	}
+
+
+	return 0;
+}
diff --git a/drivers/char/drm/nv40_fb.c b/drivers/char/drm/nv40_fb.c
new file mode 100644
index 0000000..ae784cb
--- /dev/null
+++ b/drivers/char/drm/nv40_fb.c
@@ -0,0 +1,62 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv40_fb_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t fb_bar_size, tmp;
+	int num_tiles;
+	int i;
+
+	/* This is strictly a NV4x register (don't know about NV5x). */
+	/* The blob sets these to all kinds of values, and they mess up our setup. */
+	/* I got value 0x52802 instead. For some cards the blob even sets it back to 0x1. */
+	/* Note: the blob doesn't read this value, so i'm pretty sure this is safe for all cards. */
+	/* Any idea what this is? */
+	NV_WRITE(NV40_PFB_UNK_800, 0x1);
+
+	switch (dev_priv->chipset) {
+	case 0x40:
+	case 0x45:
+		tmp = NV_READ(NV10_PFB_CLOSE_PAGE2);
+		NV_WRITE(NV10_PFB_CLOSE_PAGE2, tmp & ~(1<<15));
+		num_tiles = NV10_PFB_TILE__SIZE;
+		break;
+	case 0x46: /* G72 */
+	case 0x47: /* G70 */
+	case 0x49: /* G71 */
+	case 0x4b: /* G73 */
+	case 0x4c: /* C51 (G7X version) */
+		num_tiles = NV40_PFB_TILE__SIZE_1;
+		break;
+	default:
+		num_tiles = NV40_PFB_TILE__SIZE_0;
+		break;
+	}
+
+	fb_bar_size = drm_get_resource_len(dev, 0) - 1;
+	switch (dev_priv->chipset) {
+	case 0x40:
+		for (i=0; i<num_tiles; i++) {
+			NV_WRITE(NV10_PFB_TILE(i), 0);
+			NV_WRITE(NV10_PFB_TLIMIT(i), fb_bar_size);
+		}
+		break;
+	default:
+		for (i=0; i<num_tiles; i++) {
+			NV_WRITE(NV40_PFB_TILE(i), 0);
+			NV_WRITE(NV40_PFB_TLIMIT(i), fb_bar_size);
+		}
+		break;
+	}
+
+	return 0;
+}
+
+void
+nv40_fb_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv40_fifo.c b/drivers/char/drm/nv40_fifo.c
new file mode 100644
index 0000000..fc38bb9
--- /dev/null
+++ b/drivers/char/drm/nv40_fifo.c
@@ -0,0 +1,199 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+
+#define RAMFC_WR(offset,val) INSTANCE_WR(chan->ramfc->gpuobj, \
+					 NV40_RAMFC_##offset/4, (val))
+#define RAMFC_RD(offset)     INSTANCE_RD(chan->ramfc->gpuobj, \
+					 NV40_RAMFC_##offset/4)
+#define NV40_RAMFC(c) (dev_priv->ramfc_offset + ((c)*NV40_RAMFC__SIZE))
+#define NV40_RAMFC__SIZE 128
+
+int
+nv40_fifo_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if ((ret = nouveau_gpuobj_new_fake(dev, NV40_RAMFC(chan->id), ~0,
+						NV40_RAMFC__SIZE,
+						NVOBJ_FLAG_ZERO_ALLOC |
+						NVOBJ_FLAG_ZERO_FREE,
+						NULL, &chan->ramfc)))
+		return ret;
+
+	/* Fill entries that are seen filled in dumps of nvidia driver just
+	 * after channel's is put into DMA mode
+	 */
+	RAMFC_WR(DMA_PUT       , chan->pushbuf_base);
+	RAMFC_WR(DMA_GET       , chan->pushbuf_base);
+	RAMFC_WR(DMA_INSTANCE  , chan->pushbuf->instance >> 4);
+	RAMFC_WR(DMA_FETCH     , NV_PFIFO_CACHE1_DMA_FETCH_TRIG_128_BYTES |
+				 NV_PFIFO_CACHE1_DMA_FETCH_SIZE_128_BYTES |
+				 NV_PFIFO_CACHE1_DMA_FETCH_MAX_REQS_8 |
+#ifdef __BIG_ENDIAN
+				 NV_PFIFO_CACHE1_BIG_ENDIAN |
+#endif
+				 0x30000000 /* no idea.. */);
+	RAMFC_WR(DMA_SUBROUTINE, 0);
+	RAMFC_WR(GRCTX_INSTANCE, chan->ramin_grctx->instance >> 4);
+	RAMFC_WR(DMA_TIMESLICE , 0x0001FFFF);
+
+	/* enable the fifo dma operation */
+	NV_WRITE(NV04_PFIFO_MODE,NV_READ(NV04_PFIFO_MODE)|(1<<chan->id));
+	return 0;
+}
+
+void
+nv40_fifo_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV04_PFIFO_MODE, NV_READ(NV04_PFIFO_MODE)&~(1<<chan->id));
+
+	if (chan->ramfc)
+		nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+}
+
+int
+nv40_fifo_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp, tmp2;
+
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_GET          , RAMFC_RD(DMA_GET));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_PUT          , RAMFC_RD(DMA_PUT));
+	NV_WRITE(NV10_PFIFO_CACHE1_REF_CNT          , RAMFC_RD(REF_CNT));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_INSTANCE     , RAMFC_RD(DMA_INSTANCE));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_DCOUNT       , RAMFC_RD(DMA_DCOUNT));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_STATE        , RAMFC_RD(DMA_STATE));
+
+	/* No idea what 0x2058 is.. */
+	tmp   = RAMFC_RD(DMA_FETCH);
+	tmp2  = NV_READ(0x2058) & 0xFFF;
+	tmp2 |= (tmp & 0x30000000);
+	NV_WRITE(0x2058, tmp2);
+	tmp  &= ~0x30000000;
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_FETCH        , tmp);
+
+	NV_WRITE(NV04_PFIFO_CACHE1_ENGINE           , RAMFC_RD(ENGINE));
+	NV_WRITE(NV04_PFIFO_CACHE1_PULL1            , RAMFC_RD(PULL1_ENGINE));
+	NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_VALUE    , RAMFC_RD(ACQUIRE_VALUE));
+	NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_TIMESTAMP, RAMFC_RD(ACQUIRE_TIMESTAMP));
+	NV_WRITE(NV10_PFIFO_CACHE1_ACQUIRE_TIMEOUT  , RAMFC_RD(ACQUIRE_TIMEOUT));
+	NV_WRITE(NV10_PFIFO_CACHE1_SEMAPHORE        , RAMFC_RD(SEMAPHORE));
+	NV_WRITE(NV10_PFIFO_CACHE1_DMA_SUBROUTINE   , RAMFC_RD(DMA_SUBROUTINE));
+	NV_WRITE(NV40_PFIFO_GRCTX_INSTANCE          , RAMFC_RD(GRCTX_INSTANCE));
+	NV_WRITE(0x32e4, RAMFC_RD(UNK_40));
+	/* NVIDIA does this next line twice... */
+	NV_WRITE(0x32e8, RAMFC_RD(UNK_44));
+	NV_WRITE(0x2088, RAMFC_RD(UNK_4C));
+	NV_WRITE(0x3300, RAMFC_RD(UNK_50));
+
+	/* not sure what part is PUT, and which is GET.. never seen a non-zero
+	 * value appear in a mmio-trace yet..
+	 */
+
+	/* Don't clobber the TIMEOUT_ENABLED flag when restoring from RAMFC */
+	tmp  = NV_READ(NV04_PFIFO_DMA_TIMESLICE) & ~0x1FFFF;
+	tmp |= RAMFC_RD(DMA_TIMESLICE) & 0x1FFFF;
+	NV_WRITE(NV04_PFIFO_DMA_TIMESLICE, tmp);
+
+	/* Set channel active, and in DMA mode */
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH1,
+		 NV03_PFIFO_CACHE1_PUSH1_DMA | chan->id);
+
+	/* Reset DMA_CTL_AT_INFO to INVALID */
+	tmp = NV_READ(NV04_PFIFO_CACHE1_DMA_CTL) & ~(1<<31);
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_CTL, tmp);
+
+	return 0;
+}
+
+int
+nv40_fifo_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	RAMFC_WR(DMA_PUT          , NV_READ(NV04_PFIFO_CACHE1_DMA_PUT));
+	RAMFC_WR(DMA_GET          , NV_READ(NV04_PFIFO_CACHE1_DMA_GET));
+	RAMFC_WR(REF_CNT          , NV_READ(NV10_PFIFO_CACHE1_REF_CNT));
+	RAMFC_WR(DMA_INSTANCE     , NV_READ(NV04_PFIFO_CACHE1_DMA_INSTANCE));
+	RAMFC_WR(DMA_DCOUNT       , NV_READ(NV04_PFIFO_CACHE1_DMA_DCOUNT));
+	RAMFC_WR(DMA_STATE        , NV_READ(NV04_PFIFO_CACHE1_DMA_STATE));
+
+	tmp  = NV_READ(NV04_PFIFO_CACHE1_DMA_FETCH);
+	tmp |= NV_READ(0x2058) & 0x30000000;
+	RAMFC_WR(DMA_FETCH	  , tmp);
+
+	RAMFC_WR(ENGINE           , NV_READ(NV04_PFIFO_CACHE1_ENGINE));
+	RAMFC_WR(PULL1_ENGINE     , NV_READ(NV04_PFIFO_CACHE1_PULL1));
+	RAMFC_WR(ACQUIRE_VALUE    , NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_VALUE));
+	tmp = NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_TIMESTAMP);
+	RAMFC_WR(ACQUIRE_TIMESTAMP, tmp);
+	RAMFC_WR(ACQUIRE_TIMEOUT  , NV_READ(NV10_PFIFO_CACHE1_ACQUIRE_TIMEOUT));
+	RAMFC_WR(SEMAPHORE        , NV_READ(NV10_PFIFO_CACHE1_SEMAPHORE));
+
+	/* NVIDIA read 0x3228 first, then write DMA_GET here.. maybe something
+	 * more involved depending on the value of 0x3228?
+	 */
+	RAMFC_WR(DMA_SUBROUTINE   , NV_READ(NV04_PFIFO_CACHE1_DMA_GET));
+
+	RAMFC_WR(GRCTX_INSTANCE   , NV_READ(NV40_PFIFO_GRCTX_INSTANCE));
+
+	/* No idea what the below is for exactly, ripped from a mmio-trace */
+	RAMFC_WR(UNK_40           , NV_READ(NV40_PFIFO_UNK32E4));
+
+	/* NVIDIA do this next line twice.. bug? */
+	RAMFC_WR(UNK_44           , NV_READ(0x32e8));
+	RAMFC_WR(UNK_4C           , NV_READ(0x2088));
+	RAMFC_WR(UNK_50           , NV_READ(0x3300));
+
+
+	return 0;
+}
+
+int
+nv40_fifo_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if ((ret = nouveau_fifo_init(dev)))
+		return ret;
+
+	NV_WRITE(NV04_PFIFO_DMA_TIMESLICE, 0x2101ffff);
+	return 0;
+}
diff --git a/drivers/char/drm/nv40_graph.c b/drivers/char/drm/nv40_graph.c
new file mode 100644
index 0000000..2540fc5
--- /dev/null
+++ b/drivers/char/drm/nv40_graph.c
@@ -0,0 +1,1918 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+/*TODO: deciper what each offset in the context represents. The below
+ *      contexts are taken from dumps just after the 3D object is
+ *      created.
+ */
+static void
+nv40_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	/* Always has the "instance address" of itself at offset 0 */
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	/* unknown */
+	INSTANCE_WR(ctx, 0x00024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x0016c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00170/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00174/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00180/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00184/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00188/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0018c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0019c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001a0/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001b0/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001c0/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0039c/4, 0x00000010);
+	INSTANCE_WR(ctx, 0x00480/4, 0x00000100);
+	INSTANCE_WR(ctx, 0x00494/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00498/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x004b4/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x004b8/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x004bc/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x004d0/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x004ec/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x004fc/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00504/4, 0x00011100);
+	for (i=0x00520; i<=0x0055c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00568/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x00594/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x00598/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x0059c/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x005a0/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x005b4/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x005cc/4, 0x00000004);
+	INSTANCE_WR(ctx, 0x005d8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0060c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00610/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00614/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00618/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00628/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0062c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00630/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00640/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x0067c/4, 0x00ffff00);
+	/* 0x680-0x6BC - NV30_TCL_PRIMITIVE_3D_TX_ADDRESS_UNIT(0-15) */
+	/* 0x6C0-0x6FC - NV30_TCL_PRIMITIVE_3D_TX_FORMAT_UNIT(0-15) */
+	for (i=0x006C0; i<=0x006fc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	/* 0x700-0x73C - NV30_TCL_PRIMITIVE_3D_TX_WRAP_UNIT(0-15) */
+	for (i=0x00700; i<=0x0073c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	/* 0x740-0x77C - NV30_TCL_PRIMITIVE_3D_TX_ENABLE_UNIT(0-15) */
+	/* 0x780-0x7BC - NV30_TCL_PRIMITIVE_3D_TX_SWIZZLE_UNIT(0-15) */
+	for (i=0x00780; i<=0x007bc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	/* 0x7C0-0x7FC - NV30_TCL_PRIMITIVE_3D_TX_FILTER_UNIT(0-15) */
+	for (i=0x007c0; i<=0x007fc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	/* 0x800-0x83C - NV30_TCL_PRIMITIVE_3D_TX_XY_DIM_UNIT(0-15) */
+	for (i=0x00800; i<=0x0083c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	/* 0x840-0x87C - NV30_TCL_PRIMITIVE_3D_TX_UNK07_UNIT(0-15) */
+	/* 0x880-0x8BC - NV30_TCL_PRIMITIVE_3D_TX_DEPTH_UNIT(0-15) */
+	for (i=0x00880; i<=0x008bc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	/* unknown */
+	for (i=0x00910; i<=0x0091c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x00920; i<=0x0092c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x00940; i<=0x0094c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x00960; i<=0x0096c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x00980/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x009b4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x009c0/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x009c4/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x009c8/4, 0x60103f00);
+	INSTANCE_WR(ctx, 0x009d4/4, 0x00020000);
+	INSTANCE_WR(ctx, 0x00a08/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x00aac/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00af0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00af8/4, 0x80800001);
+	INSTANCE_WR(ctx, 0x00bcc/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00bf8/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00bfc/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c00/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c04/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c08/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c0c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c44/4, 0x00000001);
+	for (i=0x03008; i<=0x03080; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x05288; i<=0x08570; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x08628; i<=0x08e18; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x0bd28; i<=0x0f010; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0f0c8; i<=0x0f8b8; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x127c8; i<=0x15ab0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x15b68; i<=0x16358; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x19268; i<=0x1c550; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x1c608; i<=0x1cdf8; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x1fd08; i<=0x22ff0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x230a8; i<=0x23898; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x267a8; i<=0x29a90; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x29b48; i<=0x2a338; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv41_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00000024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0000011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00000120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00000128/4, 0x02008821);
+	for (i = 0x00000178; i <= 0x00000180; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00000188/4, 0x00000040);
+	for (i = 0x00000194; i <= 0x000001b0; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x000001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00000340/4, 0x00040000);
+	for (i = 0x00000350; i <= 0x0000035c; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00000388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0000039c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x000003cc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x000003d0/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x000003ec/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x000003f0/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x000003f4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000408/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00000418/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00000424/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00000428/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00000430/4, 0x00011100);
+	for (i = 0x0000044c; i <= 0x00000488; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00000494/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x000004bc/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x000004c0/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x000004c4/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x000004c8/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x000004dc/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x000004f8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0000052c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00000530/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00000534/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00000538/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00000548/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0000054c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00000550/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000560/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x00000598/4, 0x00ffff00);
+	for (i = 0x000005dc; i <= 0x00000618; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i = 0x0000061c; i <= 0x00000658; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i = 0x0000069c; i <= 0x000006d8; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i = 0x000006dc; i <= 0x00000718; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i = 0x0000071c; i <= 0x00000758; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i = 0x0000079c; i <= 0x000007d8; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i = 0x0000082c; i <= 0x00000838; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i = 0x0000083c; i <= 0x00000848; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i = 0x0000085c; i <= 0x00000868; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i = 0x0000087c; i <= 0x00000888; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x0000089c/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x000008d0/4, 0x00000021);
+	INSTANCE_WR(ctx, 0x000008d4/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x000008e0/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x000008e4/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x000008e8/4, 0x20103f00);
+	INSTANCE_WR(ctx, 0x000008f4/4, 0x00020000);
+	INSTANCE_WR(ctx, 0x0000092c/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x000009b8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x000009fc/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00000a04/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00000a08/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00000aac/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00000ab8/4, 0x0000ffff);
+	for (i = 0x00000ad4; i <= 0x00000ae4; i += 4)
+		INSTANCE_WR(ctx, i/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00000ae8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000b20/4, 0x00000001);
+	for (i = 0x00002ee8; i <= 0x00002f60; i += 8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i = 0x00005168; i <= 0x00007358; i += 24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i = 0x00007368; i <= 0x00007758; i += 16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i = 0x0000a068; i <= 0x0000c258; i += 24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i = 0x0000c268; i <= 0x0000c658; i += 16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i = 0x0000ef68; i <= 0x00011158; i += 24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i = 0x00011168; i <= 0x00011558; i += 16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i = 0x00013e68; i <= 0x00016058; i += 24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i = 0x00016068; i <= 0x00016458; i += 16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+};
+
+static void
+nv43_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00178/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00180/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00188/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00194/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00198/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0019c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001a0/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001a4/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001a8/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001ac/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001b0/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0039c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x003cc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003d0/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x003ec/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x003f0/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00408/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00418/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00424/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00428/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00430/4, 0x00011100);
+	for (i=0x0044c; i<=0x00488; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00494/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x004bc/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x004c0/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x004c4/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x004c8/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x004dc/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0052c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00530/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00534/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00538/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00548/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0054c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00550/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00560/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x00598/4, 0x00ffff00);
+	for (i=0x005dc; i<=0x00618; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x0061c; i<=0x00658; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x0069c; i<=0x006d8; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x006dc; i<=0x00718; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x0071c; i<=0x00758; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x0079c; i<=0x007d8; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x0082c; i<=0x00838; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x0083c; i<=0x00848; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x0085c; i<=0x00868; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x0087c; i<=0x00888; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x0089c/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x008d0/4, 0x00000021);
+	INSTANCE_WR(ctx, 0x008d4/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x008e0/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x008e4/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x008e8/4, 0x0c103f00);
+	INSTANCE_WR(ctx, 0x008f4/4, 0x00020000);
+	INSTANCE_WR(ctx, 0x0092c/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x009b8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x009fc/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00a04/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00a08/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00a8c/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00a98/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00ab4/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00ab8/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00abc/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00ac0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00af8/4, 0x00000001);
+	for (i=0x02ec0; i<=0x02f38; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x04c80; i<=0x06e70; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x06e80; i<=0x07270; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x096c0; i<=0x0b8b0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0b8c0; i<=0x0bcb0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x0e100; i<=0x102f0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x10300; i<=0x106f0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+};
+
+static void
+nv46_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00040/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00044/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0004c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00138/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x0013c/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00144/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00174/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00178/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00180/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00184/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00188/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0018c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00190/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00194/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00198/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0019c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001a4/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001ec/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x0036c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00370/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00374/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00378/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003a4/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x003b8/4, 0x00003010);
+	INSTANCE_WR(ctx, 0x003dc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003e0/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003e4/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003e8/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003ec/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003f0/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003f8/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003fc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00400/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00404/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00408/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0040c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00410/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00414/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00418/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004b0/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004b4/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x004d0/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x004d4/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x004d8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x004ec/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x004fc/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00500/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00504/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00508/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0050c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00510/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00514/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00518/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0051c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00520/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00524/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00528/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0052c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00530/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00534/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00538/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0053c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00550/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00554/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x0055c/4, 0x00011100);
+	for (i=0x00578; i<0x005b4; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c0/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x005e8/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x005ec/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x005f0/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x005f4/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x00608/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x00624/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00658/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x0065c/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00660/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00664/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00674/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00678/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x0067c/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0068c/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x006c8/4, 0x00ffff00);
+	for (i=0x0070c; i<=0x00748; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x0074c; i<=0x00788; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x007cc; i<=0x00808; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x0080c; i<=0x00848; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x0084c; i<=0x00888; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x008cc; i<=0x00908; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x0095c; i<=0x00968; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x0096c; i<=0x00978; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x0098c; i<=0x00998; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x009ac; i<=0x009b8; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x009cc/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x00a00/4, 0x00000421);
+	INSTANCE_WR(ctx, 0x00a04/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x00a08/4, 0x00011001);
+	INSTANCE_WR(ctx, 0x00a14/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x00a18/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x00a1c/4, 0x0c103f00);
+	INSTANCE_WR(ctx, 0x00a28/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00a60/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x00aec/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00b30/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00b38/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00b3c/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00bc0/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00bcc/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00be8/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00bec/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00bf0/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00bf4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00c2c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00c30/4, 0x08e00001);
+	INSTANCE_WR(ctx, 0x00c34/4, 0x000e3000);
+	for (i=0x017f8; i<=0x01870; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x035b8; i<=0x057a8; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x057b8; i<=0x05ba8; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x07f38; i<=0x0a128; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0a138; i<=0x0a528; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x0c8b8; i<=0x0eaa8; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0eab8; i<=0x0eea8; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+/* This may only work on 7800 AGP cards, will include a warning */
+static void
+nv47_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00000024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0000011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00000120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00000128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00000178/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0000017c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00000180/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00000188/4, 0x00000040);
+	for (i=0x00000194; i<=0x000001b0; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x000001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00000340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00000350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00000354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00000358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0000035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00000388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0000039c/4, 0x00001010);
+	for (i=0x000003c0; i<=0x000003fc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00000454/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00000458/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x00000474/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x00000478/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x0000047c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000490/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x000004a0/4, 0xffff0000);
+	for (i=0x000004a4; i<=0x000004e0; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x000004f4/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x000004f8/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00000500/4, 0x00011100);
+	for (i=0x0000051c; i<=0x00000558; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00000564/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x0000058c/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x00000590/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x00000594/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x00000598/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x000005ac/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x000005c8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x000005fc/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00000600/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00000604/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00000608/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00000618/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0000061c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00000620/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00000630/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x0000066c/4, 0x00ffff00);
+	for (i=0x000006b0; i<=0x000006ec; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x000006f0; i<=0x0000072c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x00000770; i<=0x000007ac; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x000007b0; i<=0x000007ec; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x000007f0; i<=0x0000082c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x00000870; i<=0x000008ac; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	INSTANCE_WR(ctx, 0x00000900/4, 0x0001bc80);
+	INSTANCE_WR(ctx, 0x00000904/4, 0x0001bc80);
+	INSTANCE_WR(ctx, 0x00000908/4, 0x0001bc80);
+	INSTANCE_WR(ctx, 0x0000090c/4, 0x0001bc80);
+	INSTANCE_WR(ctx, 0x00000910/4, 0x00000202);
+	INSTANCE_WR(ctx, 0x00000914/4, 0x00000202);
+	INSTANCE_WR(ctx, 0x00000918/4, 0x00000202);
+	INSTANCE_WR(ctx, 0x0000091c/4, 0x00000202);
+	for (i=0x00000930; i<=0x0000095c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x00000970/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x000009a4/4, 0x00000021);
+	INSTANCE_WR(ctx, 0x000009a8/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x000009b4/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x000009b8/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x000009bc/4, 0x40103f00);
+	INSTANCE_WR(ctx, 0x000009c8/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00000a00/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x00000a8c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000ad0/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00000adc/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00000ae0/4, 0x00888001);
+	for (i=0x00000b10; i<=0x00000b8c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00000bb4/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00000bc0/4, 0x0000ffff);
+	for (i=0x00000bdc; i<=0x00000bf8; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00000bfc/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000c34/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00000c38/4, 0x08e00001);
+	INSTANCE_WR(ctx, 0x00000c3c/4, 0x000e3000);
+	for (i=0x00003000; i<=0x00003078; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x00004dc0; i<=0x00006fb0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x00006fc0; i<=0x000073b0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x00009800; i<=0x0000b9f0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0000ba00; i<=0x00010430; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x00010440; i<=0x00010830; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x00012c80; i<=0x00014e70; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x00014e80; i<=0x00015270; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x000176c0; i<=0x000198b0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x000198c0; i<=0x00019cb0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x0001c100; i<=0x0001e2f0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0001e300; i<=0x0001e6f0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv49_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00004/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00008/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x0000c/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00010/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00014/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00018/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x0001c/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00020/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x000c4/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x000c8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x000d0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x001bc/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x001c0/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x001c8/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00218/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0021c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00220/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00228/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00234/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00238/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0023c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00240/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00244/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00248/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0024c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00250/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00270/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x003e0/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x003f0/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003f8/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003fc/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00428/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0043c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x00460/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00464/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00468/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0046c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00470/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00474/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00478/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0047c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00480/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00484/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00488/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0048c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00490/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00494/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00498/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0049c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004f4/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x00514/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x00518/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x0051c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00530/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00540/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00544/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00548/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0054c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00550/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00554/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00558/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0055c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00560/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00564/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00568/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0056c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00570/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00574/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00578/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0057c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00580/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00594/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00598/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x005a0/4, 0x00011100);
+	INSTANCE_WR(ctx, 0x005bc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005cc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005dc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005ec/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00604/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x0062c/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x00630/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x00634/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x00638/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x0064c/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x00668/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0069c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x006a0/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x006a4/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x006a8/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x006b8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x006bc/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x006c0/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x006d0/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x0070c/4, 0x00ffff00);
+	for (i=0x00750; i<=0x0078c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x00790; i<=0x007cc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x00810; i<=0x0084c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x00850; i<=0x0088c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x00890; i<=0x008cc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x00910; i<=0x0094c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x009a0; i<=0x009ac; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x009b0; i<=0x009bc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x009d0; i<=0x009dc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x009f0; i<=0x009fc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x00a10/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x00a44/4, 0x00000421);
+	INSTANCE_WR(ctx, 0x00a48/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x00a54/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x00a58/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x00a5c/4, 0x20103f00);
+	INSTANCE_WR(ctx, 0x00a68/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00aa0/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x00b2c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00b70/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00b7c/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00b80/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00bb0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bb4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bb8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bbc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bcc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bdc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bec/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bfc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c00/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c04/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c08/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c0c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c10/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c14/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c18/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c1c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c20/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c24/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c28/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c2c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c54/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00c60/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00c7c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c80/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c84/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c88/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c8c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c90/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c94/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c98/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c9c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00cd4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00cd8/4, 0x08e00001);
+	INSTANCE_WR(ctx, 0x00cdc/4, 0x000e3000);
+	for(i=0x030a0; i<=0x03118; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x098a0; i<=0x0ba90; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x0baa0; i<=0x0be90; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x0e2e0; i<=0x0fff0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x10008; i<=0x104d0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x104e0; i<=0x108d0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x12d20; i<=0x14f10; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x14f20; i<=0x15310; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x17760; i<=0x19950; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x19960; i<=0x19d50; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x1c1a0; i<=0x1e390; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x1e3a0; i<=0x1e790; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x20be0; i<=0x22dd0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x22de0; i<=0x231d0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv4a_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00158/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0015c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00160/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00164/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00168/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0016c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00170/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00174/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00178/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00180/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00188/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0039c/4, 0x00003010);
+	INSTANCE_WR(ctx, 0x003cc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003d0/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x003ec/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x003f0/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00408/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00418/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00424/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00428/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00430/4, 0x00011100);
+	for (i=0x0044c; i<=0x00488; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00494/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x004bc/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x004c0/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x004c4/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x004c8/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x004dc/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0052c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00530/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00534/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00538/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00548/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0054c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00550/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0055c/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x00594/4, 0x00ffff00);
+	for (i=0x005d8; i<=0x00614; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x00618; i<=0x00654; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x00698; i<=0x006d4; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x006d8; i<=0x00714; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x00718; i<=0x00754; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x00798; i<=0x007d4; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x00828; i<=0x00834; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x00838; i<=0x00844; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x00858; i<=0x00864; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x00878; i<=0x00884; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x00898/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x008cc/4, 0x00000021);
+	INSTANCE_WR(ctx, 0x008d0/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x008d4/4, 0x00011001);
+	INSTANCE_WR(ctx, 0x008e0/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x008e4/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x008e8/4, 0x0c103f00);
+	INSTANCE_WR(ctx, 0x008f4/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x0092c/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x009b8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x009fc/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00a04/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00a08/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00a8c/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00a98/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00ab4/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00ab8/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00abc/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00ac0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00af8/4, 0x00000001);
+	for (i=0x016c0; i<=0x01738; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x03840; i<=0x05670; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x05680; i<=0x05a70; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x07e00; i<=0x09ff0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0a000; i<=0x0a3f0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x0c780; i<=0x0e970; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x0e980; i<=0x0ed70; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv4b_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00004/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00008/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x0000c/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00010/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00014/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00018/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x0001c/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x00020/4, 0x0000c040);
+	INSTANCE_WR(ctx, 0x000c4/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x000c8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x000d0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x001bc/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x001c0/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x001c8/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00218/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0021c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00220/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00228/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00234/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00238/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0023c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00240/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00244/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00248/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x0024c/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00250/4, 0x80000000);
+	INSTANCE_WR(ctx, 0x00270/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x003e0/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x003f0/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003f8/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x003fc/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00428/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0043c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x00460/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00464/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00468/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0046c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00470/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00474/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00478/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0047c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00480/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00484/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00488/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0048c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00490/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00494/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x00498/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x0049c/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004f4/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x00514/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x00518/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x0051c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00530/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00540/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00544/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00548/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0054c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00550/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00554/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00558/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0055c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00560/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00564/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00568/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0056c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00570/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00574/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00578/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x0057c/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00580/4, 0x88888888);
+	INSTANCE_WR(ctx, 0x00594/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00598/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x005a0/4, 0x00011100);
+	INSTANCE_WR(ctx, 0x005bc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005c8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005cc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005d8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005dc/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005e8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005ec/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f0/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f4/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x005f8/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00604/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x0062c/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x00630/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x00634/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x00638/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x0064c/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x00668/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0069c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x006a0/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x006a4/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x006a8/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x006b8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x006bc/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x006c0/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x006d0/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x0070c/4, 0x00ffff00);
+	for (i=0x00750; i<=0x0078c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x00790; i<=0x007cc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x00810; i<=0x0084c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x00850; i<=0x0088c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x00890; i<=0x008cc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x00910; i<=0x0094c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x009a0; i<=0x009ac; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x009b0; i<=0x009bc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x009d0; i<=0x009dc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x009f0; i<=0x009fc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x00a10/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x00a44/4, 0x00000421);
+	INSTANCE_WR(ctx, 0x00a48/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x00a54/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x00a58/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x00a5c/4, 0x20103f00);
+	INSTANCE_WR(ctx, 0x00a68/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00aa0/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x00b2c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00b70/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00b7c/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00b80/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00bb0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bb4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bb8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bbc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bc8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bcc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bd8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bdc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00be8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bec/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf0/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf4/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bf8/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00bfc/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c00/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c04/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c08/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c0c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c10/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c14/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c18/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c1c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c20/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c24/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c28/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c2c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00c54/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00c60/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00c7c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c80/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c84/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c88/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c8c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c90/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c94/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c98/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00c9c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00cd4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00cd8/4, 0x08e00001);
+	INSTANCE_WR(ctx, 0x00cdc/4, 0x000e3000);
+	for(i=0x030a0; i<=0x03118; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x098a0; i<=0x0ba90; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x0baa0; i<=0x0be90; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x0e2e0; i<=0x0fff0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x10008; i<=0x104d0; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x104e0; i<=0x108d0; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x12d20; i<=0x14f10; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x14f20; i<=0x15310; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for(i=0x17760; i<=0x19950; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for(i=0x19960; i<=0x19d50; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv4c_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00158/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0015c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00160/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00164/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00168/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0016c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00170/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00174/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00178/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00180/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00188/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0039c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x003d0/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003d4/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x003f0/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x003f4/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x003f8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0040c/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x0041c/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00428/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x0042c/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00434/4, 0x00011100);
+	for (i=0x00450; i<0x0048c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00498/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x004c0/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x004c4/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x004c8/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x004cc/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x004e0/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x004fc/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00530/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00534/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00538/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x0053c/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x0054c/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x00550/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00554/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00564/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x0059c/4, 0x00ffff00);
+	for (i=0x005e0; i<=0x0061c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x00620; i<=0x0065c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x006a0; i<=0x006dc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x006e0; i<=0x0071c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x00720; i<=0x0075c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x007a0; i<=0x007dc; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x00830; i<=0x0083c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x00840; i<=0x0084c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x00860; i<=0x0086c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x00880; i<=0x0088c; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x008a0/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x008d4/4, 0x00000020);
+	INSTANCE_WR(ctx, 0x008d8/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x008dc/4, 0x00011001);
+	INSTANCE_WR(ctx, 0x008e8/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x008ec/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x008f0/4, 0x0c103f00);
+	INSTANCE_WR(ctx, 0x008fc/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00934/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x009c0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00a04/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00a0c/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00a10/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00a74/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00a80/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00a9c/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00aa0/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00ad8/4, 0x00000001);
+	for (i=0x016a0; i<0x01718; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x03460; i<0x05650; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x05660; i<0x05a50; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+static void
+nv4e_graph_context_init(struct drm_device *dev, struct nouveau_gpuobj *ctx)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i;
+
+	INSTANCE_WR(ctx, 0x00000/4, ctx->im_pramin->start);
+	INSTANCE_WR(ctx, 0x00024/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00028/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00030/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0011c/4, 0x20010001);
+	INSTANCE_WR(ctx, 0x00120/4, 0x0f73ef00);
+	INSTANCE_WR(ctx, 0x00128/4, 0x02008821);
+	INSTANCE_WR(ctx, 0x00158/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0015c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00160/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00164/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00168/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x0016c/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00170/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00174/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00178/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x0017c/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00180/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x00188/4, 0x00000040);
+	INSTANCE_WR(ctx, 0x001d0/4, 0x0b0b0b0c);
+	INSTANCE_WR(ctx, 0x00340/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x00350/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00354/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00358/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x0035c/4, 0x55555555);
+	INSTANCE_WR(ctx, 0x00388/4, 0x00000008);
+	INSTANCE_WR(ctx, 0x0039c/4, 0x00001010);
+	INSTANCE_WR(ctx, 0x003cc/4, 0x00000111);
+	INSTANCE_WR(ctx, 0x003d0/4, 0x00080060);
+	INSTANCE_WR(ctx, 0x003ec/4, 0x00000080);
+	INSTANCE_WR(ctx, 0x003f0/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x003f4/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00408/4, 0x46400000);
+	INSTANCE_WR(ctx, 0x00418/4, 0xffff0000);
+	INSTANCE_WR(ctx, 0x00424/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00428/4, 0x0fff0000);
+	INSTANCE_WR(ctx, 0x00430/4, 0x00011100);
+	for (i=0x0044c; i<=0x00488; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x07ff0000);
+	INSTANCE_WR(ctx, 0x00494/4, 0x4b7fffff);
+	INSTANCE_WR(ctx, 0x004bc/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x004c0/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x004c4/4, 0xb8a89888);
+	INSTANCE_WR(ctx, 0x004c8/4, 0xf8e8d8c8);
+	INSTANCE_WR(ctx, 0x004dc/4, 0x40100000);
+	INSTANCE_WR(ctx, 0x004f8/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0052c/4, 0x435185d6);
+	INSTANCE_WR(ctx, 0x00530/4, 0x2155b699);
+	INSTANCE_WR(ctx, 0x00534/4, 0xfedcba98);
+	INSTANCE_WR(ctx, 0x00538/4, 0x00000098);
+	INSTANCE_WR(ctx, 0x00548/4, 0xffffffff);
+	INSTANCE_WR(ctx, 0x0054c/4, 0x00ff7000);
+	INSTANCE_WR(ctx, 0x00550/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x0055c/4, 0x00ff0000);
+	INSTANCE_WR(ctx, 0x00594/4, 0x00ffff00);
+	for (i=0x005d8; i<=0x00614; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00018488);
+	for (i=0x00618; i<=0x00654; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00028202);
+	for (i=0x00698; i<=0x006d4; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0000aae4);
+	for (i=0x006d8; i<=0x00714; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x01012000);
+	for (i=0x00718; i<=0x00754; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	for (i=0x00798; i<=0x007d4; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00100008);
+	for (i=0x00828; i<=0x00834; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x0001bc80);
+	for (i=0x00838; i<=0x00844; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000202);
+	for (i=0x00858; i<=0x00864; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00000008);
+	for (i=0x00878; i<=0x00884; i+=4)
+		INSTANCE_WR(ctx, i/4, 0x00080008);
+	INSTANCE_WR(ctx, 0x00898/4, 0x00000002);
+	INSTANCE_WR(ctx, 0x008cc/4, 0x00000020);
+	INSTANCE_WR(ctx, 0x008d0/4, 0x030c30c3);
+	INSTANCE_WR(ctx, 0x008d4/4, 0x00011001);
+	INSTANCE_WR(ctx, 0x008e0/4, 0x3e020200);
+	INSTANCE_WR(ctx, 0x008e4/4, 0x00ffffff);
+	INSTANCE_WR(ctx, 0x008e8/4, 0x0c103f00);
+	INSTANCE_WR(ctx, 0x008f4/4, 0x00040000);
+	INSTANCE_WR(ctx, 0x0092c/4, 0x00008100);
+	INSTANCE_WR(ctx, 0x009b8/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x009fc/4, 0x00001001);
+	INSTANCE_WR(ctx, 0x00a04/4, 0x00000003);
+	INSTANCE_WR(ctx, 0x00a08/4, 0x00888001);
+	INSTANCE_WR(ctx, 0x00a6c/4, 0x00000005);
+	INSTANCE_WR(ctx, 0x00a78/4, 0x0000ffff);
+	INSTANCE_WR(ctx, 0x00a94/4, 0x00005555);
+	INSTANCE_WR(ctx, 0x00a98/4, 0x00000001);
+	INSTANCE_WR(ctx, 0x00aa4/4, 0x00000001);
+	for (i=0x01668; i<=0x016e0; i+=8)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+	for (i=0x03428; i<=0x05618; i+=24)
+		INSTANCE_WR(ctx, i/4, 0x00000001);
+	for (i=0x05628; i<=0x05a18; i+=16)
+		INSTANCE_WR(ctx, i/4, 0x3f800000);
+}
+
+int
+nv40_graph_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	void (*ctx_init)(struct drm_device *, struct nouveau_gpuobj *);
+	int ret;
+
+	/* These functions populate the graphics context with a whole heap
+	 * of default state.  All these functions are very similar, with
+	 * a minimal amount of chipset-specific changes.  However, as we're
+	 * currently dependant on the context programs used by the NVIDIA
+	 * binary driver these functions must match the layout expected by
+	 * them.  Hopefully at some point this will all change.
+	 */
+	switch (dev_priv->chipset) {
+	case 0x40:
+		ctx_init = nv40_graph_context_init;
+		break;
+	case 0x41:
+	case 0x42:
+		ctx_init = nv41_graph_context_init;
+		break;
+	case 0x43:
+		ctx_init = nv43_graph_context_init;
+		break;
+	case 0x46:
+		ctx_init = nv46_graph_context_init;
+		break;
+	case 0x47:
+		ctx_init = nv47_graph_context_init;
+		break;
+	case 0x49:
+		ctx_init = nv49_graph_context_init;
+		break;
+	case 0x44:
+	case 0x4a:
+		ctx_init = nv4a_graph_context_init;
+		break;
+	case 0x4b:
+		ctx_init = nv4b_graph_context_init;
+		break;
+	case 0x4c:
+	case 0x67:
+		ctx_init = nv4c_graph_context_init;
+		break;
+	case 0x4e:
+		ctx_init = nv4e_graph_context_init;
+		break;
+	default:
+		ctx_init = nv40_graph_context_init;
+		break;
+	}
+
+	/* Allocate a 175KiB block of PRAMIN to store the context.  This
+	 * is massive overkill for a lot of chipsets, but it should be safe
+	 * until we're able to implement this properly (will happen at more
+	 * or less the same time we're able to write our own context programs.
+	 */
+	if ((ret = nouveau_gpuobj_new_ref(dev, chan, NULL, 0, 175*1024, 16,
+					  NVOBJ_FLAG_ZERO_ALLOC,
+					  &chan->ramin_grctx)))
+		return ret;
+
+	/* Initialise default context values */
+	ctx_init(dev, chan->ramin_grctx->gpuobj);
+
+	return 0;
+}
+
+void
+nv40_graph_destroy_context(struct nouveau_channel *chan)
+{
+	nouveau_gpuobj_ref_del(chan->dev, &chan->ramin_grctx);
+}
+
+static int
+nv40_graph_transfer_context(struct drm_device *dev, uint32_t inst, int save)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t old_cp, tv = 1000, tmp;
+	int i;
+
+	old_cp = NV_READ(NV20_PGRAPH_CHANNEL_CTX_POINTER);
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+
+	tmp  = NV_READ(NV40_PGRAPH_CTXCTL_0310);
+	tmp |= save ? NV40_PGRAPH_CTXCTL_0310_XFER_SAVE :
+		      NV40_PGRAPH_CTXCTL_0310_XFER_LOAD;
+	NV_WRITE(NV40_PGRAPH_CTXCTL_0310, tmp);
+
+	tmp  = NV_READ(NV40_PGRAPH_CTXCTL_0304);
+	tmp |= NV40_PGRAPH_CTXCTL_0304_XFER_CTX;
+	NV_WRITE(NV40_PGRAPH_CTXCTL_0304, tmp);
+
+	for (i = 0; i < tv; i++) {
+		if (NV_READ(NV40_PGRAPH_CTXCTL_030C) == 0)
+			break;
+	}
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, old_cp);
+
+	if (i == tv) {
+		uint32_t ucstat = NV_READ(NV40_PGRAPH_CTXCTL_UCODE_STAT);
+		DRM_ERROR("Failed: Instance=0x%08x Save=%d\n", inst, save);
+		DRM_ERROR("IP: 0x%02x, Opcode: 0x%08x\n",
+			  ucstat >> NV40_PGRAPH_CTXCTL_UCODE_STAT_IP_SHIFT,
+			  ucstat  & NV40_PGRAPH_CTXCTL_UCODE_STAT_OP_MASK);
+		DRM_ERROR("0x40030C = 0x%08x\n",
+			  NV_READ(NV40_PGRAPH_CTXCTL_030C));
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+/* Save current context (from PGRAPH) into the channel's context
+ *XXX: fails sometimes, not sure why..
+ */
+int
+nv40_graph_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	uint32_t inst;
+
+	if (!chan->ramin_grctx)
+		return -EINVAL;
+	inst = chan->ramin_grctx->instance >> 4;
+
+	return nv40_graph_transfer_context(dev, inst, 1);
+}
+
+/* Restore the context for a specific channel into PGRAPH
+ * XXX: fails sometimes.. not sure why
+ */
+int
+nv40_graph_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst;
+	int ret;
+
+	if (!chan->ramin_grctx)
+		return -EINVAL;
+	inst = chan->ramin_grctx->instance >> 4;
+
+	ret = nv40_graph_transfer_context(dev, inst, 0);
+	if (ret)
+		return ret;
+
+	/* 0x40032C, no idea of it's exact function.  Could simply be a
+	 * record of the currently active PGRAPH context.  It's currently
+	 * unknown as to what bit 24 does.  The nv ddx has it set, so we will
+	 * set it here too.
+	 */
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+	NV_WRITE(NV40_PGRAPH_CTXCTL_CUR,
+		 (inst & NV40_PGRAPH_CTXCTL_CUR_INST_MASK) |
+		  NV40_PGRAPH_CTXCTL_CUR_LOADED);
+	/* 0x32E0 records the instance address of the active FIFO's PGRAPH
+	 * context.  If at any time this doesn't match 0x40032C, you will
+	 * recieve PGRAPH_INTR_CONTEXT_SWITCH
+	 */
+	NV_WRITE(NV40_PFIFO_GRCTX_INSTANCE, inst);
+	return 0;
+}
+
+/* These blocks of "magic numbers" are actually a microcode that the GPU uses
+ * to control how graphics contexts get saved and restored between PRAMIN
+ * and PGRAPH during a context switch.  We're currently using values seen
+ * in mmio-traces of the binary driver.
+ */
+static uint32_t nv40_ctx_prog[] = {
+	/*(DEBLOBBED)*/
+	~0
+};
+
+static uint32_t nv41_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static uint32_t nv43_ctx_prog[] = {
+	/*(DEBLOBBED)*/
+	~0
+};
+
+static uint32_t nv44_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static uint32_t nv46_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static uint32_t nv47_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+//this is used for nv49 and nv4b
+static uint32_t nv49_4b_ctx_prog[] ={
+	/*(DEBLOBBED)*/ ~0
+};
+
+
+static uint32_t nv4a_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static uint32_t nv4c_ctx_prog[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static uint32_t nv4e_ctx_prog[] = {
+	/*(DEBLOBBED)*/
+	~0
+};
+
+/*
+ * G70		0x47
+ * G71		0x49
+ * NV45		0x48
+ * G72[M]	0x46
+ * G73		0x4b
+ * C51_G7X	0x4c
+ * C51		0x4e
+ */
+int
+nv40_graph_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv =
+		(struct drm_nouveau_private *)dev->dev_private;
+	uint32_t *ctx_prog;
+	uint32_t vramsz, tmp;
+	int i, j;
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) &
+			~NV_PMC_ENABLE_PGRAPH);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |
+			 NV_PMC_ENABLE_PGRAPH);
+
+	switch (dev_priv->chipset) {
+	case 0x40: ctx_prog = nv40_ctx_prog; break;
+	case 0x41:
+	case 0x42: ctx_prog = nv41_ctx_prog; break;
+	case 0x43: ctx_prog = nv43_ctx_prog; break;
+	case 0x44: ctx_prog = nv44_ctx_prog; break;
+	case 0x46: ctx_prog = nv46_ctx_prog; break;
+	case 0x47: ctx_prog = nv47_ctx_prog; break;
+	case 0x49: ctx_prog = nv49_4b_ctx_prog; break;
+	case 0x4a: ctx_prog = nv4a_ctx_prog; break;
+	case 0x4b: ctx_prog = nv49_4b_ctx_prog; break;
+	case 0x4c:
+	case 0x67: ctx_prog = nv4c_ctx_prog; break;
+	case 0x4e: ctx_prog = nv4e_ctx_prog; break;
+	default:
+		DRM_ERROR("Context program for 0x%02x unavailable\n",
+			  dev_priv->chipset);
+		ctx_prog = NULL;
+		break;
+	}
+
+	/* Load the context program onto the card */
+	if (ctx_prog) {
+		DRM_DEBUG("Loading context program\n");
+		i = 0;
+
+		NV_WRITE(NV40_PGRAPH_CTXCTL_UCODE_INDEX, 0);
+		while (ctx_prog[i] != ~0) {
+			NV_WRITE(NV40_PGRAPH_CTXCTL_UCODE_DATA, ctx_prog[i]);
+			i++;
+		}
+	}
+
+	/* No context present currently */
+	NV_WRITE(NV40_PGRAPH_CTXCTL_CUR, 0x00000000);
+
+	NV_WRITE(NV03_PGRAPH_INTR   , 0xFFFFFFFF);
+	NV_WRITE(NV40_PGRAPH_INTR_EN, 0xFFFFFFFF);
+
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_DEBUG_0, 0x00000000);
+	NV_WRITE(NV04_PGRAPH_DEBUG_1, 0x401287c0);
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, 0xe0de8055);
+	NV_WRITE(NV10_PGRAPH_DEBUG_4, 0x00008000);
+	NV_WRITE(NV04_PGRAPH_LIMIT_VIOL_PIX, 0x00be3c5f);
+
+	NV_WRITE(NV10_PGRAPH_CTX_CONTROL, 0x10010100);
+	NV_WRITE(NV10_PGRAPH_STATE      , 0xFFFFFFFF);
+	NV_WRITE(NV04_PGRAPH_FIFO       , 0x00000001);
+
+	j = NV_READ(0x1540) & 0xff;
+	if (j) {
+		for (i=0; !(j&1); j>>=1, i++);
+		NV_WRITE(0x405000, i);
+	}
+
+	if (dev_priv->chipset == 0x40) {
+		NV_WRITE(0x4009b0, 0x83280fff);
+		NV_WRITE(0x4009b4, 0x000000a0);
+	} else {
+		NV_WRITE(0x400820, 0x83280eff);
+		NV_WRITE(0x400824, 0x000000a0);
+	}
+
+	switch (dev_priv->chipset) {
+	case 0x40:
+	case 0x45:
+		NV_WRITE(0x4009b8, 0x0078e366);
+		NV_WRITE(0x4009bc, 0x0000014c);
+		break;
+	case 0x41:
+	case 0x42: /* pciid also 0x00Cx */
+//	case 0x0120: //XXX (pciid)
+		NV_WRITE(0x400828, 0x007596ff);
+		NV_WRITE(0x40082c, 0x00000108);
+		break;
+	case 0x43:
+		NV_WRITE(0x400828, 0x0072cb77);
+		NV_WRITE(0x40082c, 0x00000108);
+		break;
+	case 0x44:
+	case 0x46: /* G72 */
+	case 0x4a:
+	case 0x4c: /* G7x-based C51 */
+	case 0x4e:
+		NV_WRITE(0x400860, 0);
+		NV_WRITE(0x400864, 0);
+		break;
+	case 0x47: /* G70 */
+	case 0x49: /* G71 */
+	case 0x4b: /* G73 */
+		NV_WRITE(0x400828, 0x07830610);
+		NV_WRITE(0x40082c, 0x0000016A);
+		break;
+	default:
+		break;
+	}
+
+	NV_WRITE(0x400b38, 0x2ffff800);
+	NV_WRITE(0x400b3c, 0x00006000);
+
+	/* copy tile info from PFB */
+	switch (dev_priv->chipset) {
+	case 0x40: /* vanilla NV40 */
+		for (i=0; i<NV10_PFB_TILE__SIZE; i++) {
+			tmp = NV_READ(NV10_PFB_TILE(i));
+			NV_WRITE(NV40_PGRAPH_TILE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TILE1(i), tmp);
+			tmp = NV_READ(NV10_PFB_TLIMIT(i));
+			NV_WRITE(NV40_PGRAPH_TLIMIT0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TLIMIT1(i), tmp);
+			tmp = NV_READ(NV10_PFB_TSIZE(i));
+			NV_WRITE(NV40_PGRAPH_TSIZE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSIZE1(i), tmp);
+			tmp = NV_READ(NV10_PFB_TSTATUS(i));
+			NV_WRITE(NV40_PGRAPH_TSTATUS0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSTATUS1(i), tmp);
+		}
+		break;
+	case 0x44:
+	case 0x4a:
+	case 0x4e: /* NV44-based cores don't have 0x406900? */
+		for (i=0; i<NV40_PFB_TILE__SIZE_0; i++) {
+			tmp = NV_READ(NV40_PFB_TILE(i));
+			NV_WRITE(NV40_PGRAPH_TILE0(i), tmp);
+			tmp = NV_READ(NV40_PFB_TLIMIT(i));
+			NV_WRITE(NV40_PGRAPH_TLIMIT0(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSIZE(i));
+			NV_WRITE(NV40_PGRAPH_TSIZE0(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSTATUS(i));
+			NV_WRITE(NV40_PGRAPH_TSTATUS0(i), tmp);
+		}
+		break;
+	case 0x46:
+	case 0x47:
+	case 0x49:
+	case 0x4b: /* G7X-based cores */
+		for (i=0; i<NV40_PFB_TILE__SIZE_1; i++) {
+			tmp = NV_READ(NV40_PFB_TILE(i));
+			NV_WRITE(NV47_PGRAPH_TILE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TILE1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TLIMIT(i));
+			NV_WRITE(NV47_PGRAPH_TLIMIT0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TLIMIT1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSIZE(i));
+			NV_WRITE(NV47_PGRAPH_TSIZE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSIZE1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSTATUS(i));
+			NV_WRITE(NV47_PGRAPH_TSTATUS0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSTATUS1(i), tmp);
+		}
+		break;
+	default: /* everything else */
+		for (i=0; i<NV40_PFB_TILE__SIZE_0; i++) {
+			tmp = NV_READ(NV40_PFB_TILE(i));
+			NV_WRITE(NV40_PGRAPH_TILE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TILE1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TLIMIT(i));
+			NV_WRITE(NV40_PGRAPH_TLIMIT0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TLIMIT1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSIZE(i));
+			NV_WRITE(NV40_PGRAPH_TSIZE0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSIZE1(i), tmp);
+			tmp = NV_READ(NV40_PFB_TSTATUS(i));
+			NV_WRITE(NV40_PGRAPH_TSTATUS0(i), tmp);
+			NV_WRITE(NV40_PGRAPH_TSTATUS1(i), tmp);
+		}
+		break;
+	}
+
+	/* begin RAM config */
+	vramsz = drm_get_resource_len(dev, 0) - 1;
+	switch (dev_priv->chipset) {
+	case 0x40:
+		NV_WRITE(0x4009A4, NV_READ(NV04_PFB_CFG0));
+		NV_WRITE(0x4009A8, NV_READ(NV04_PFB_CFG1));
+		NV_WRITE(0x4069A4, NV_READ(NV04_PFB_CFG0));
+		NV_WRITE(0x4069A8, NV_READ(NV04_PFB_CFG1));
+		NV_WRITE(0x400820, 0);
+		NV_WRITE(0x400824, 0);
+		NV_WRITE(0x400864, vramsz);
+		NV_WRITE(0x400868, vramsz);
+		break;
+	default:
+		switch (dev_priv->chipset) {
+		case 0x46:
+		case 0x47:
+		case 0x49:
+		case 0x4b:
+			NV_WRITE(0x400DF0, NV_READ(NV04_PFB_CFG0));
+			NV_WRITE(0x400DF4, NV_READ(NV04_PFB_CFG1));
+			break;
+		default:
+			NV_WRITE(0x4009F0, NV_READ(NV04_PFB_CFG0));
+			NV_WRITE(0x4009F4, NV_READ(NV04_PFB_CFG1));
+			break;
+		}
+		NV_WRITE(0x4069F0, NV_READ(NV04_PFB_CFG0));
+		NV_WRITE(0x4069F4, NV_READ(NV04_PFB_CFG1));
+		NV_WRITE(0x400840, 0);
+		NV_WRITE(0x400844, 0);
+		NV_WRITE(0x4008A0, vramsz);
+		NV_WRITE(0x4008A4, vramsz);
+		break;
+	}
+
+	/* per-context state, doesn't belong here */
+	NV_WRITE(0x400B20, 0x00000000);
+	NV_WRITE(0x400B04, 0xFFFFFFFF);
+
+	tmp = NV_READ(NV10_PGRAPH_SURFACE) & 0x0007ff00;
+	NV_WRITE(NV10_PGRAPH_SURFACE, tmp);
+	tmp = NV_READ(NV10_PGRAPH_SURFACE) | 0x00020100;
+	NV_WRITE(NV10_PGRAPH_SURFACE, tmp);
+
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_XMIN, 0);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_YMIN, 0);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_XMAX, 0x7fff);
+	NV_WRITE(NV03_PGRAPH_ABS_UCLIP_YMAX, 0x7fff);
+
+	return 0;
+}
+
+void nv40_graph_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv40_mc.c b/drivers/char/drm/nv40_mc.c
new file mode 100644
index 0000000..ead6f87
--- /dev/null
+++ b/drivers/char/drm/nv40_mc.c
@@ -0,0 +1,38 @@
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+#include "nouveau_drm.h"
+
+int
+nv40_mc_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	/* Power up everything, resetting each individual unit will
+	 * be done later if needed.
+	 */
+	NV_WRITE(NV03_PMC_ENABLE, 0xFFFFFFFF);
+
+	switch (dev_priv->chipset) {
+	case 0x44:
+	case 0x46: /* G72 */
+	case 0x4e:
+	case 0x4c: /* C51_G7X */
+		tmp = NV_READ(NV40_PFB_020C);
+		NV_WRITE(NV40_PMC_1700, tmp);
+		NV_WRITE(NV40_PMC_1704, 0);
+		NV_WRITE(NV40_PMC_1708, 0);
+		NV_WRITE(NV40_PMC_170C, tmp);
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+void
+nv40_mc_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/nv50_fifo.c b/drivers/char/drm/nv50_fifo.c
new file mode 100644
index 0000000..edf4edb
--- /dev/null
+++ b/drivers/char/drm/nv50_fifo.c
@@ -0,0 +1,339 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+struct nv50_fifo_priv {
+	struct nouveau_gpuobj_ref *thingo[2];
+	int cur_thingo;
+};
+
+#define IS_G80 ((dev_priv->chipset & 0xf0) == 0x50)
+
+static void
+nv50_fifo_init_thingo(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nv50_fifo_priv *priv = dev_priv->Engine.fifo.priv;
+	struct nouveau_gpuobj_ref *cur;
+	int i, nr;
+
+	DRM_DEBUG("\n");
+
+	cur = priv->thingo[priv->cur_thingo];
+	priv->cur_thingo = !priv->cur_thingo;
+
+	/* We never schedule channel 0 or 127 */
+	for (i = 1, nr = 0; i < 127; i++) {
+		if (dev_priv->fifos[i]) {
+			INSTANCE_WR(cur->gpuobj, nr++, i);
+		}
+	}
+	NV_WRITE(0x32f4, cur->instance >> 12);
+	NV_WRITE(0x32ec, nr);
+	NV_WRITE(0x2500, 0x101);
+}
+
+static int
+nv50_fifo_channel_enable(struct drm_device *dev, int channel, int nt)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_channel *chan = dev_priv->fifos[channel];
+	uint32_t inst;
+
+	DRM_DEBUG("ch%d\n", channel);
+
+	if (!chan->ramfc)
+		return -EINVAL;
+
+	if (IS_G80) inst = chan->ramfc->instance >> 12;
+	else        inst = chan->ramfc->instance >> 8;
+	NV_WRITE(NV50_PFIFO_CTX_TABLE(channel),
+		 inst | NV50_PFIFO_CTX_TABLE_CHANNEL_ENABLED);
+
+	if (!nt) nv50_fifo_init_thingo(dev);
+	return 0;
+}
+
+static void
+nv50_fifo_channel_disable(struct drm_device *dev, int channel, int nt)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst;
+
+	DRM_DEBUG("ch%d, nt=%d\n", channel, nt);
+
+	if (IS_G80) inst = NV50_PFIFO_CTX_TABLE_INSTANCE_MASK_G80;
+	else        inst = NV50_PFIFO_CTX_TABLE_INSTANCE_MASK_G84;
+	NV_WRITE(NV50_PFIFO_CTX_TABLE(channel), inst);
+
+	if (!nt) nv50_fifo_init_thingo(dev);
+}
+
+static void
+nv50_fifo_init_reset(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t pmc_e = NV_PMC_ENABLE_PFIFO;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) & ~pmc_e);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |  pmc_e);
+}
+
+static void
+nv50_fifo_init_intr(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(NV03_PFIFO_INTR_0, 0xFFFFFFFF);
+	NV_WRITE(NV03_PFIFO_INTR_EN_0, 0xFFFFFFFF);
+}
+
+static void
+nv50_fifo_init_context_table(struct drm_device *dev)
+{
+	int i;
+
+	DRM_DEBUG("\n");
+
+	for (i = 0; i < NV50_PFIFO_CTX_TABLE__SIZE; i++)
+		nv50_fifo_channel_disable(dev, i, 1);
+	nv50_fifo_init_thingo(dev);
+}
+
+static void
+nv50_fifo_init_regs__nv(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(0x250c, 0x6f3cfc34);
+}
+
+static void
+nv50_fifo_init_regs(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(0x2500, 0);
+	NV_WRITE(0x3250, 0);
+	NV_WRITE(0x3220, 0);
+	NV_WRITE(0x3204, 0);
+	NV_WRITE(0x3210, 0);
+	NV_WRITE(0x3270, 0);
+
+	/* Enable dummy channels setup by nv50_instmem.c */
+	nv50_fifo_channel_enable(dev, 0, 1);
+	nv50_fifo_channel_enable(dev, 127, 1);
+}
+
+int
+nv50_fifo_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nv50_fifo_priv *priv;
+	int ret;
+
+	DRM_DEBUG("\n");
+
+	priv = drm_calloc(1, sizeof(*priv), DRM_MEM_DRIVER);
+	if (!priv)
+		return -ENOMEM;
+	dev_priv->Engine.fifo.priv = priv;
+
+	nv50_fifo_init_reset(dev);
+	nv50_fifo_init_intr(dev);
+
+	ret = nouveau_gpuobj_new_ref(dev, NULL, NULL, 0, 128*4, 0x1000,
+				     NVOBJ_FLAG_ZERO_ALLOC, &priv->thingo[0]);
+	if (ret) {
+		DRM_ERROR("error creating thingo0: %d\n", ret);
+		return ret;
+	}
+
+	ret = nouveau_gpuobj_new_ref(dev, NULL, NULL, 0, 128*4, 0x1000,
+				     NVOBJ_FLAG_ZERO_ALLOC, &priv->thingo[1]);
+	if (ret) {
+		DRM_ERROR("error creating thingo1: %d\n", ret);
+		return ret;
+	}
+
+	nv50_fifo_init_context_table(dev);
+	nv50_fifo_init_regs__nv(dev);
+	nv50_fifo_init_regs(dev);
+
+	return 0;
+}
+
+void
+nv50_fifo_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nv50_fifo_priv *priv = dev_priv->Engine.fifo.priv;
+
+	DRM_DEBUG("\n");
+
+	if (!priv)
+		return;
+
+	nouveau_gpuobj_ref_del(dev, &priv->thingo[0]);
+	nouveau_gpuobj_ref_del(dev, &priv->thingo[1]);
+
+	dev_priv->Engine.fifo.priv = NULL;
+	drm_free(priv, sizeof(*priv), DRM_MEM_DRIVER);
+}
+
+int
+nv50_fifo_channel_id(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	return (NV_READ(NV03_PFIFO_CACHE1_PUSH1) &
+			NV50_PFIFO_CACHE1_PUSH1_CHID_MASK);
+}
+
+int
+nv50_fifo_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *ramfc = NULL;
+	int ret;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	if (IS_G80) {
+		uint32_t ramfc_offset = chan->ramin->gpuobj->im_pramin->start;
+		uint32_t vram_offset = chan->ramin->gpuobj->im_backing->start;
+		ret = nouveau_gpuobj_new_fake(dev, ramfc_offset, vram_offset,
+					      0x100, NVOBJ_FLAG_ZERO_ALLOC |
+					      NVOBJ_FLAG_ZERO_FREE, &ramfc,
+					      &chan->ramfc);
+		if (ret)
+			return ret;
+	} else {
+		ret = nouveau_gpuobj_new_ref(dev, chan, NULL, 0, 0x100, 256,
+					     NVOBJ_FLAG_ZERO_ALLOC |
+					     NVOBJ_FLAG_ZERO_FREE,
+					     &chan->ramfc);
+		if (ret)
+			return ret;
+		ramfc = chan->ramfc->gpuobj;
+	}
+
+	INSTANCE_WR(ramfc, 0x48/4, chan->pushbuf->instance >> 4);
+	INSTANCE_WR(ramfc, 0x80/4, (0xc << 24) | (chan->ramht->instance >> 4));
+	INSTANCE_WR(ramfc, 0x3c/4, 0x000f0078); /* fetch? */
+	INSTANCE_WR(ramfc, 0x44/4, 0x2101ffff);
+	INSTANCE_WR(ramfc, 0x60/4, 0x7fffffff);
+	INSTANCE_WR(ramfc, 0x10/4, 0x00000000);
+	INSTANCE_WR(ramfc, 0x08/4, 0x00000000);
+	INSTANCE_WR(ramfc, 0x40/4, 0x00000000);
+	INSTANCE_WR(ramfc, 0x50/4, 0x2039b2e0);
+	INSTANCE_WR(ramfc, 0x54/4, 0x000f0000);
+	INSTANCE_WR(ramfc, 0x7c/4, 0x30000001);
+	INSTANCE_WR(ramfc, 0x78/4, 0x00000000);
+	INSTANCE_WR(ramfc, 0x4c/4, chan->pushbuf_mem->size - 1);
+
+	if (!IS_G80) {
+		INSTANCE_WR(chan->ramin->gpuobj, 0, chan->id);
+		INSTANCE_WR(chan->ramin->gpuobj, 1, chan->ramfc->instance);
+
+		INSTANCE_WR(ramfc, 0x88/4, 0x3d520); /* some vram addy >> 10 */
+		INSTANCE_WR(ramfc, 0x98/4, chan->ramin->instance >> 12);
+	}
+
+	ret = nv50_fifo_channel_enable(dev, chan->id, 0);
+	if (ret) {
+		DRM_ERROR("error enabling ch%d: %d\n", chan->id, ret);
+		nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+		return ret;
+	}
+
+	return 0;
+}
+
+void
+nv50_fifo_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	nv50_fifo_channel_disable(dev, chan->id, 0);
+
+	/* Dummy channel, also used on ch 127 */
+	if (chan->id == 0)
+		nv50_fifo_channel_disable(dev, 127, 0);
+
+	nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+}
+
+int
+nv50_fifo_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *ramfc = chan->ramfc->gpuobj;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	/*XXX: incomplete, only touches the regs that NV does */
+
+	NV_WRITE(0x3244, 0);
+	NV_WRITE(0x3240, 0);
+
+	NV_WRITE(0x3224, INSTANCE_RD(ramfc, 0x3c/4));
+	NV_WRITE(NV04_PFIFO_CACHE1_DMA_INSTANCE, INSTANCE_RD(ramfc, 0x48/4));
+	NV_WRITE(0x3234, INSTANCE_RD(ramfc, 0x4c/4));
+	NV_WRITE(0x3254, 1);
+	NV_WRITE(NV03_PFIFO_RAMHT, INSTANCE_RD(ramfc, 0x80/4));
+
+	if (!IS_G80) {
+		NV_WRITE(0x340c, INSTANCE_RD(ramfc, 0x88/4));
+		NV_WRITE(0x3410, INSTANCE_RD(ramfc, 0x98/4));
+	}
+
+	NV_WRITE(NV03_PFIFO_CACHE1_PUSH1, chan->id | (1<<16));
+	return 0;
+}
+
+int
+nv50_fifo_save_context(struct nouveau_channel *chan)
+{
+	DRM_DEBUG("ch%d\n", chan->id);
+	DRM_ERROR("stub!\n");
+	return 0;
+}
diff --git a/drivers/char/drm/nv50_graph.c b/drivers/char/drm/nv50_graph.c
new file mode 100644
index 0000000..55df5fc
--- /dev/null
+++ b/drivers/char/drm/nv50_graph.c
@@ -0,0 +1,960 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+#define IS_G80 ((dev_priv->chipset & 0xf0) == 0x50)
+
+static void
+nv50_graph_init_reset(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t pmc_e = NV_PMC_ENABLE_PGRAPH | (1 << 21);
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) & ~pmc_e);
+	NV_WRITE(NV03_PMC_ENABLE, NV_READ(NV03_PMC_ENABLE) |  pmc_e);
+}
+
+static void
+nv50_graph_init_intr(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+	NV_WRITE(NV03_PGRAPH_INTR, 0xffffffff);
+	NV_WRITE(0x400138, 0xffffffff);
+	NV_WRITE(NV40_PGRAPH_INTR_EN, 0xffffffff);
+}
+
+static void
+nv50_graph_init_regs__nv(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(0x400804, 0xc0000000);
+	NV_WRITE(0x406800, 0xc0000000);
+	NV_WRITE(0x400c04, 0xc0000000);
+	NV_WRITE(0x401804, 0xc0000000);
+	NV_WRITE(0x405018, 0xc0000000);
+	NV_WRITE(0x402000, 0xc0000000);
+
+	NV_WRITE(0x400108, 0xffffffff);
+
+	NV_WRITE(0x400824, 0x00004000);
+	NV_WRITE(0x400500, 0x00010001);
+}
+
+static void
+nv50_graph_init_regs(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	DRM_DEBUG("\n");
+
+	NV_WRITE(NV04_PGRAPH_DEBUG_3, (1<<2) /* HW_CONTEXT_SWITCH_ENABLED */);
+}
+
+static uint32_t nv84_ctx_voodoo[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+ 
+static uint32_t nv86_ctx_voodoo[] = {
+	/*(DEBLOBBED)*/ ~0
+};
+
+static int
+nv50_graph_init_ctxctl(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t *voodoo = NULL;
+
+	DRM_DEBUG("\n");
+
+	switch (dev_priv->chipset) {
+	case 0x84:
+		voodoo = nv84_ctx_voodoo;
+		break;
+	case 0x86:
+		voodoo = nv86_ctx_voodoo;
+		break;
+	default:
+		DRM_ERROR("no voodoo for chipset NV%02x\n", dev_priv->chipset);
+		return -EINVAL;
+	}
+
+	NV_WRITE(NV40_PGRAPH_CTXCTL_UCODE_INDEX, 0);
+	while (*voodoo != ~0) {
+		NV_WRITE(NV40_PGRAPH_CTXCTL_UCODE_DATA, *voodoo);
+		voodoo++;
+	}
+
+	NV_WRITE(0x400320, 4);
+	NV_WRITE(NV40_PGRAPH_CTXCTL_CUR, 0);
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, 0);
+
+	return 0;
+}
+
+int
+nv50_graph_init(struct drm_device *dev)
+{
+	int ret;
+
+	DRM_DEBUG("\n");
+
+	nv50_graph_init_reset(dev);
+	nv50_graph_init_intr(dev);
+	nv50_graph_init_regs__nv(dev);
+	nv50_graph_init_regs(dev);
+
+	ret = nv50_graph_init_ctxctl(dev);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+void
+nv50_graph_takedown(struct drm_device *dev)
+{
+	DRM_DEBUG("\n");
+}
+
+static void
+nv86_graph_init_ctxvals(struct drm_device *dev, struct nouveau_gpuobj_ref *ref)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_gpuobj *ctx = ref->gpuobj;
+
+	INSTANCE_WR(ctx, 0x0/4, 0x1);
+	INSTANCE_WR(ctx, 0x10C/4, 0x30);
+	INSTANCE_WR(ctx, 0x1D4/4, 0x3);
+	INSTANCE_WR(ctx, 0x1D8/4, 0x1000);
+	INSTANCE_WR(ctx, 0x218/4, 0xFE0C);
+	INSTANCE_WR(ctx, 0x22C/4, 0x1000);
+	INSTANCE_WR(ctx, 0x258/4, 0x187);
+	INSTANCE_WR(ctx, 0x26C/4, 0x1018);
+	INSTANCE_WR(ctx, 0x270/4, 0xFF);
+	INSTANCE_WR(ctx, 0x2AC/4, 0x4);
+	INSTANCE_WR(ctx, 0x2B0/4, 0x44D00DF);
+	INSTANCE_WR(ctx, 0x2B8/4, 0x600);
+	INSTANCE_WR(ctx, 0x2D0/4, 0x1000000);
+	INSTANCE_WR(ctx, 0x2D4/4, 0xFF);
+	INSTANCE_WR(ctx, 0x2DC/4, 0x400);
+	INSTANCE_WR(ctx, 0x2F4/4, 0x1);
+	INSTANCE_WR(ctx, 0x2F8/4, 0x80);
+	INSTANCE_WR(ctx, 0x2FC/4, 0x4);
+	INSTANCE_WR(ctx, 0x318/4, 0x2);
+	INSTANCE_WR(ctx, 0x31C/4, 0x1);
+	INSTANCE_WR(ctx, 0x328/4, 0x1);
+	INSTANCE_WR(ctx, 0x32C/4, 0x100);
+	INSTANCE_WR(ctx, 0x344/4, 0x2);
+	INSTANCE_WR(ctx, 0x348/4, 0x1);
+	INSTANCE_WR(ctx, 0x34C/4, 0x1);
+	INSTANCE_WR(ctx, 0x35C/4, 0x1);
+	INSTANCE_WR(ctx, 0x360/4, 0x3FFFFF);
+	INSTANCE_WR(ctx, 0x364/4, 0x1FFF);
+	INSTANCE_WR(ctx, 0x36C/4, 0x1);
+	INSTANCE_WR(ctx, 0x370/4, 0x1);
+	INSTANCE_WR(ctx, 0x378/4, 0x1);
+	INSTANCE_WR(ctx, 0x37C/4, 0x1);
+	INSTANCE_WR(ctx, 0x380/4, 0x1);
+	INSTANCE_WR(ctx, 0x384/4, 0x4);
+	INSTANCE_WR(ctx, 0x388/4, 0x1);
+	INSTANCE_WR(ctx, 0x38C/4, 0x1);
+	INSTANCE_WR(ctx, 0x390/4, 0x1);
+	INSTANCE_WR(ctx, 0x394/4, 0x7);
+	INSTANCE_WR(ctx, 0x398/4, 0x1);
+	INSTANCE_WR(ctx, 0x39C/4, 0x7);
+	INSTANCE_WR(ctx, 0x3A0/4, 0x1);
+	INSTANCE_WR(ctx, 0x3A4/4, 0x1);
+	INSTANCE_WR(ctx, 0x3A8/4, 0x1);
+	INSTANCE_WR(ctx, 0x3BC/4, 0x1);
+	INSTANCE_WR(ctx, 0x3C0/4, 0x100);
+	INSTANCE_WR(ctx, 0x3C8/4, 0x1);
+	INSTANCE_WR(ctx, 0x3D4/4, 0x100);
+	INSTANCE_WR(ctx, 0x3D8/4, 0x1);
+	INSTANCE_WR(ctx, 0x3DC/4, 0x100);
+	INSTANCE_WR(ctx, 0x3E4/4, 0x1);
+	INSTANCE_WR(ctx, 0x3F0/4, 0x100);
+	INSTANCE_WR(ctx, 0x404/4, 0x4);
+	INSTANCE_WR(ctx, 0x408/4, 0x70);
+	INSTANCE_WR(ctx, 0x40C/4, 0x80);
+	INSTANCE_WR(ctx, 0x420/4, 0xC);
+	INSTANCE_WR(ctx, 0x428/4, 0x8);
+	INSTANCE_WR(ctx, 0x42C/4, 0x14);
+	INSTANCE_WR(ctx, 0x434/4, 0x29);
+	INSTANCE_WR(ctx, 0x438/4, 0x27);
+	INSTANCE_WR(ctx, 0x43C/4, 0x26);
+	INSTANCE_WR(ctx, 0x440/4, 0x8);
+	INSTANCE_WR(ctx, 0x444/4, 0x4);
+	INSTANCE_WR(ctx, 0x448/4, 0x27);
+	INSTANCE_WR(ctx, 0x454/4, 0x1);
+	INSTANCE_WR(ctx, 0x458/4, 0x2);
+	INSTANCE_WR(ctx, 0x45C/4, 0x3);
+	INSTANCE_WR(ctx, 0x460/4, 0x4);
+	INSTANCE_WR(ctx, 0x464/4, 0x5);
+	INSTANCE_WR(ctx, 0x468/4, 0x6);
+	INSTANCE_WR(ctx, 0x46C/4, 0x7);
+	INSTANCE_WR(ctx, 0x470/4, 0x1);
+	INSTANCE_WR(ctx, 0x4B4/4, 0xCF);
+	INSTANCE_WR(ctx, 0x4E4/4, 0x80);
+	INSTANCE_WR(ctx, 0x4E8/4, 0x4);
+	INSTANCE_WR(ctx, 0x4EC/4, 0x4);
+	INSTANCE_WR(ctx, 0x4F0/4, 0x3);
+	INSTANCE_WR(ctx, 0x4F4/4, 0x1);
+	INSTANCE_WR(ctx, 0x500/4, 0x12);
+	INSTANCE_WR(ctx, 0x504/4, 0x10);
+	INSTANCE_WR(ctx, 0x508/4, 0xC);
+	INSTANCE_WR(ctx, 0x50C/4, 0x1);
+	INSTANCE_WR(ctx, 0x51C/4, 0x4);
+	INSTANCE_WR(ctx, 0x520/4, 0x2);
+	INSTANCE_WR(ctx, 0x524/4, 0x4);
+	INSTANCE_WR(ctx, 0x530/4, 0x3FFFFF);
+	INSTANCE_WR(ctx, 0x534/4, 0x1FFF);
+	INSTANCE_WR(ctx, 0x55C/4, 0x4);
+	INSTANCE_WR(ctx, 0x560/4, 0x14);
+	INSTANCE_WR(ctx, 0x564/4, 0x1);
+	INSTANCE_WR(ctx, 0x570/4, 0x2);
+	INSTANCE_WR(ctx, 0x57C/4, 0x1);
+	INSTANCE_WR(ctx, 0x584/4, 0x2);
+	INSTANCE_WR(ctx, 0x588/4, 0x1000);
+	INSTANCE_WR(ctx, 0x58C/4, 0xE00);
+	INSTANCE_WR(ctx, 0x590/4, 0x1000);
+	INSTANCE_WR(ctx, 0x594/4, 0x1E00);
+	INSTANCE_WR(ctx, 0x59C/4, 0x1);
+	INSTANCE_WR(ctx, 0x5A0/4, 0x1);
+	INSTANCE_WR(ctx, 0x5A4/4, 0x1);
+	INSTANCE_WR(ctx, 0x5A8/4, 0x1);
+	INSTANCE_WR(ctx, 0x5AC/4, 0x1);
+	INSTANCE_WR(ctx, 0x5BC/4, 0x200);
+	INSTANCE_WR(ctx, 0x5C4/4, 0x1);
+	INSTANCE_WR(ctx, 0x5C8/4, 0x70);
+	INSTANCE_WR(ctx, 0x5CC/4, 0x80);
+	INSTANCE_WR(ctx, 0x5D8/4, 0x1);
+	INSTANCE_WR(ctx, 0x5DC/4, 0x70);
+	INSTANCE_WR(ctx, 0x5E0/4, 0x80);
+	INSTANCE_WR(ctx, 0x5F0/4, 0x1);
+	INSTANCE_WR(ctx, 0x5F4/4, 0xCF);
+	INSTANCE_WR(ctx, 0x5FC/4, 0x1);
+	INSTANCE_WR(ctx, 0x60C/4, 0xCF);
+	INSTANCE_WR(ctx, 0x614/4, 0x2);
+	INSTANCE_WR(ctx, 0x61C/4, 0x1);
+	INSTANCE_WR(ctx, 0x624/4, 0x1);
+	INSTANCE_WR(ctx, 0x62C/4, 0xCF);
+	INSTANCE_WR(ctx, 0x630/4, 0xCF);
+	INSTANCE_WR(ctx, 0x634/4, 0x1);
+	INSTANCE_WR(ctx, 0x63C/4, 0xF80);
+	INSTANCE_WR(ctx, 0x684/4, 0x7F0080);
+	INSTANCE_WR(ctx, 0x6C0/4, 0x7F0080);
+	INSTANCE_WR(ctx, 0x6E4/4, 0x3B74F821);
+	INSTANCE_WR(ctx, 0x6E8/4, 0x89058001);
+	INSTANCE_WR(ctx, 0x6F0/4, 0x1000);
+	INSTANCE_WR(ctx, 0x6F4/4, 0x1F);
+	INSTANCE_WR(ctx, 0x6F8/4, 0x27C10FA);
+	INSTANCE_WR(ctx, 0x6FC/4, 0x400000C0);
+	INSTANCE_WR(ctx, 0x700/4, 0xB7892080);
+	INSTANCE_WR(ctx, 0x70C/4, 0x3B74F821);
+	INSTANCE_WR(ctx, 0x710/4, 0x89058001);
+	INSTANCE_WR(ctx, 0x718/4, 0x1000);
+	INSTANCE_WR(ctx, 0x71C/4, 0x1F);
+	INSTANCE_WR(ctx, 0x720/4, 0x27C10FA);
+	INSTANCE_WR(ctx, 0x724/4, 0x400000C0);
+	INSTANCE_WR(ctx, 0x728/4, 0xB7892080);
+	INSTANCE_WR(ctx, 0x734/4, 0x10040);
+	INSTANCE_WR(ctx, 0x73C/4, 0x22);
+	INSTANCE_WR(ctx, 0x748/4, 0x10040);
+	INSTANCE_WR(ctx, 0x74C/4, 0x22);
+	INSTANCE_WR(ctx, 0x764/4, 0x1800000);
+	INSTANCE_WR(ctx, 0x768/4, 0x160000);
+	INSTANCE_WR(ctx, 0x76C/4, 0x1800000);
+	INSTANCE_WR(ctx, 0x77C/4, 0x3FFFF);
+	INSTANCE_WR(ctx, 0x780/4, 0x8C0000);
+	INSTANCE_WR(ctx, 0x7A4/4, 0x10401);
+	INSTANCE_WR(ctx, 0x7AC/4, 0x78);
+	INSTANCE_WR(ctx, 0x7B4/4, 0xBF);
+	INSTANCE_WR(ctx, 0x7BC/4, 0x1210);
+	INSTANCE_WR(ctx, 0x7C0/4, 0x8000080);
+	INSTANCE_WR(ctx, 0x7E4/4, 0x1800000);
+	INSTANCE_WR(ctx, 0x7E8/4, 0x160000);
+	INSTANCE_WR(ctx, 0x7EC/4, 0x1800000);
+	INSTANCE_WR(ctx, 0x7FC/4, 0x3FFFF);
+	INSTANCE_WR(ctx, 0x800/4, 0x8C0000);
+	INSTANCE_WR(ctx, 0x824/4, 0x10401);
+	INSTANCE_WR(ctx, 0x82C/4, 0x78);
+	INSTANCE_WR(ctx, 0x834/4, 0xBF);
+	INSTANCE_WR(ctx, 0x83C/4, 0x1210);
+	INSTANCE_WR(ctx, 0x840/4, 0x8000080);
+	INSTANCE_WR(ctx, 0x868/4, 0x27070);
+	INSTANCE_WR(ctx, 0x874/4, 0x3FFFFFF);
+	INSTANCE_WR(ctx, 0x88C/4, 0x120407);
+	INSTANCE_WR(ctx, 0x890/4, 0x5091507);
+	INSTANCE_WR(ctx, 0x894/4, 0x5010202);
+	INSTANCE_WR(ctx, 0x898/4, 0x30201);
+	INSTANCE_WR(ctx, 0x8B4/4, 0x40);
+	INSTANCE_WR(ctx, 0x8B8/4, 0xD0C0B0A);
+	INSTANCE_WR(ctx, 0x8BC/4, 0x141210);
+	INSTANCE_WR(ctx, 0x8C0/4, 0x1F0);
+	INSTANCE_WR(ctx, 0x8C4/4, 0x1);
+	INSTANCE_WR(ctx, 0x8C8/4, 0x3);
+	INSTANCE_WR(ctx, 0x8D4/4, 0x39E00);
+	INSTANCE_WR(ctx, 0x8D8/4, 0x100);
+	INSTANCE_WR(ctx, 0x8DC/4, 0x3800);
+	INSTANCE_WR(ctx, 0x8E0/4, 0x404040);
+	INSTANCE_WR(ctx, 0x8E4/4, 0xFF0A);
+	INSTANCE_WR(ctx, 0x8EC/4, 0x77F005);
+	INSTANCE_WR(ctx, 0x8F0/4, 0x3F7FFF);
+	INSTANCE_WR(ctx, 0x7BA0/4, 0x21);
+	INSTANCE_WR(ctx, 0x7BC0/4, 0x1);
+	INSTANCE_WR(ctx, 0x7BE0/4, 0x2);
+	INSTANCE_WR(ctx, 0x7C00/4, 0x100);
+	INSTANCE_WR(ctx, 0x7C20/4, 0x100);
+	INSTANCE_WR(ctx, 0x7C40/4, 0x1);
+	INSTANCE_WR(ctx, 0x7CA0/4, 0x1);
+	INSTANCE_WR(ctx, 0x7CC0/4, 0x2);
+	INSTANCE_WR(ctx, 0x7CE0/4, 0x100);
+	INSTANCE_WR(ctx, 0x7D00/4, 0x100);
+	INSTANCE_WR(ctx, 0x7D20/4, 0x1);
+	INSTANCE_WR(ctx, 0x11640/4, 0x4);
+	INSTANCE_WR(ctx, 0x11660/4, 0x4);
+	INSTANCE_WR(ctx, 0x49FE0/4, 0x4);
+	INSTANCE_WR(ctx, 0x4A000/4, 0x4);
+	INSTANCE_WR(ctx, 0x4A020/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x4A040/4, 0x3);
+	INSTANCE_WR(ctx, 0x4A080/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x4A0C0/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x4A0E0/4, 0x1);
+	INSTANCE_WR(ctx, 0x4A100/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x4A160/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x4A180/4, 0x27);
+	INSTANCE_WR(ctx, 0x4A1E0/4, 0x1);
+	INSTANCE_WR(ctx, 0x51A20/4, 0x1);
+	INSTANCE_WR(ctx, 0x51D00/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x51EA0/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x51EC0/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x51F00/4, 0x80);
+	INSTANCE_WR(ctx, 0x51F80/4, 0x80);
+	INSTANCE_WR(ctx, 0x51FC0/4, 0x3F);
+	INSTANCE_WR(ctx, 0x52120/4, 0x2);
+	INSTANCE_WR(ctx, 0x52140/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x52160/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x52280/4, 0x4);
+	INSTANCE_WR(ctx, 0x52300/4, 0x4);
+	INSTANCE_WR(ctx, 0x52540/4, 0x1);
+	INSTANCE_WR(ctx, 0x52560/4, 0x1001);
+	INSTANCE_WR(ctx, 0x52580/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x525A0/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x525C0/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x525E0/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x52A00/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52A20/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52A40/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52A60/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52A80/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52AA0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52AC0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52AE0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52B00/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52B20/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52B40/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52B60/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52B80/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52BA0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52BC0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52BE0/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x52C00/4, 0x10);
+	INSTANCE_WR(ctx, 0x52C60/4, 0x3);
+	INSTANCE_WR(ctx, 0xA84/4, 0xF);
+	INSTANCE_WR(ctx, 0xB24/4, 0x20);
+	INSTANCE_WR(ctx, 0xD04/4, 0x1A);
+	INSTANCE_WR(ctx, 0xEC4/4, 0x4);
+	INSTANCE_WR(ctx, 0xEE4/4, 0x4);
+	INSTANCE_WR(ctx, 0xF24/4, 0x4);
+	INSTANCE_WR(ctx, 0xF44/4, 0x8);
+	INSTANCE_WR(ctx, 0xF84/4, 0x7FF);
+	INSTANCE_WR(ctx, 0x1124/4, 0xF);
+	INSTANCE_WR(ctx, 0x3604/4, 0xF);
+	INSTANCE_WR(ctx, 0x3644/4, 0x1);
+	INSTANCE_WR(ctx, 0x41A4/4, 0xF);
+	INSTANCE_WR(ctx, 0x14844/4, 0xF);
+	INSTANCE_WR(ctx, 0x14AE4/4, 0x1);
+	INSTANCE_WR(ctx, 0x14B04/4, 0x100);
+	INSTANCE_WR(ctx, 0x14B24/4, 0x100);
+	INSTANCE_WR(ctx, 0x14B44/4, 0x11);
+	INSTANCE_WR(ctx, 0x14B84/4, 0x8);
+	INSTANCE_WR(ctx, 0x14C44/4, 0x1);
+	INSTANCE_WR(ctx, 0x14C84/4, 0x1);
+	INSTANCE_WR(ctx, 0x14CA4/4, 0x1);
+	INSTANCE_WR(ctx, 0x14CC4/4, 0x1);
+	INSTANCE_WR(ctx, 0x14CE4/4, 0xCF);
+	INSTANCE_WR(ctx, 0x14D04/4, 0x2);
+	INSTANCE_WR(ctx, 0x14DE4/4, 0x1);
+	INSTANCE_WR(ctx, 0x14E24/4, 0x1);
+	INSTANCE_WR(ctx, 0x14E44/4, 0x1);
+	INSTANCE_WR(ctx, 0x14E64/4, 0x1);
+	INSTANCE_WR(ctx, 0x14F04/4, 0x4);
+	INSTANCE_WR(ctx, 0x14F44/4, 0x1);
+	INSTANCE_WR(ctx, 0x14F64/4, 0x15);
+	INSTANCE_WR(ctx, 0x14FE4/4, 0x4444480);
+	INSTANCE_WR(ctx, 0x15764/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x15804/4, 0x100);
+	INSTANCE_WR(ctx, 0x15864/4, 0x10001);
+	INSTANCE_WR(ctx, 0x158A4/4, 0x10001);
+	INSTANCE_WR(ctx, 0x158C4/4, 0x1);
+	INSTANCE_WR(ctx, 0x158E4/4, 0x10001);
+	INSTANCE_WR(ctx, 0x15904/4, 0x1);
+	INSTANCE_WR(ctx, 0x15924/4, 0x4);
+	INSTANCE_WR(ctx, 0x15944/4, 0x2);
+	INSTANCE_WR(ctx, 0x166C4/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x166E4/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x16784/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x16904/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x16924/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x15948/4, 0x3FFFFF);
+	INSTANCE_WR(ctx, 0x159A8/4, 0x1FFF);
+	INSTANCE_WR(ctx, 0x15B88/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x15C68/4, 0x4);
+	INSTANCE_WR(ctx, 0x15C88/4, 0x1A);
+	INSTANCE_WR(ctx, 0x15CE8/4, 0x1);
+	INSTANCE_WR(ctx, 0x15F48/4, 0xFFFF00);
+	INSTANCE_WR(ctx, 0x16028/4, 0xF);
+	INSTANCE_WR(ctx, 0x16128/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x16148/4, 0x11);
+	INSTANCE_WR(ctx, 0x16348/4, 0x4);
+	INSTANCE_WR(ctx, 0x163E8/4, 0x2);
+	INSTANCE_WR(ctx, 0x16408/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x16428/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x164A8/4, 0x5);
+	INSTANCE_WR(ctx, 0x164C8/4, 0x52);
+	INSTANCE_WR(ctx, 0x16568/4, 0x1);
+	INSTANCE_WR(ctx, 0x16788/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x167A8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x167C8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x167E8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16808/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16828/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16848/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16868/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16888/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x168A8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x168C8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x168E8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16908/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16928/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16948/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16968/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16988/4, 0x10);
+	INSTANCE_WR(ctx, 0x16E68/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x16E88/4, 0x5);
+	INSTANCE_WR(ctx, 0x16EE8/4, 0x1);
+	INSTANCE_WR(ctx, 0x16F28/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x16F48/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x16F68/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x16F88/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x16FA8/4, 0x3);
+	INSTANCE_WR(ctx, 0x173A8/4, 0xFFFF00);
+	INSTANCE_WR(ctx, 0x173C8/4, 0x1A);
+	INSTANCE_WR(ctx, 0x17408/4, 0x3);
+	INSTANCE_WR(ctx, 0x178E8/4, 0x102);
+	INSTANCE_WR(ctx, 0x17928/4, 0x4);
+	INSTANCE_WR(ctx, 0x17948/4, 0x4);
+	INSTANCE_WR(ctx, 0x17968/4, 0x4);
+	INSTANCE_WR(ctx, 0x17988/4, 0x4);
+	INSTANCE_WR(ctx, 0x179A8/4, 0x4);
+	INSTANCE_WR(ctx, 0x179C8/4, 0x4);
+	INSTANCE_WR(ctx, 0x17A08/4, 0x7FF);
+	INSTANCE_WR(ctx, 0x17A48/4, 0x102);
+	INSTANCE_WR(ctx, 0x17B88/4, 0x4);
+	INSTANCE_WR(ctx, 0x17BA8/4, 0x4);
+	INSTANCE_WR(ctx, 0x17BC8/4, 0x4);
+	INSTANCE_WR(ctx, 0x17BE8/4, 0x4);
+	INSTANCE_WR(ctx, 0x18228/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x18288/4, 0x804);
+	INSTANCE_WR(ctx, 0x182C8/4, 0x4);
+	INSTANCE_WR(ctx, 0x182E8/4, 0x4);
+	INSTANCE_WR(ctx, 0x18308/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x18348/4, 0x4);
+	INSTANCE_WR(ctx, 0x18368/4, 0x4);
+	INSTANCE_WR(ctx, 0x183A8/4, 0x10);
+	INSTANCE_WR(ctx, 0x18448/4, 0x804);
+	INSTANCE_WR(ctx, 0x18468/4, 0x1);
+	INSTANCE_WR(ctx, 0x18488/4, 0x1A);
+	INSTANCE_WR(ctx, 0x184A8/4, 0x7F);
+	INSTANCE_WR(ctx, 0x184E8/4, 0x1);
+	INSTANCE_WR(ctx, 0x18508/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x18548/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x18568/4, 0x4);
+	INSTANCE_WR(ctx, 0x18588/4, 0x4);
+	INSTANCE_WR(ctx, 0x185C8/4, 0x10);
+	INSTANCE_WR(ctx, 0x18648/4, 0x1);
+	INSTANCE_WR(ctx, 0x18668/4, 0x8100C12);
+	INSTANCE_WR(ctx, 0x18748/4, 0x7FF);
+	INSTANCE_WR(ctx, 0x18768/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x18E88/4, 0x1);
+	INSTANCE_WR(ctx, 0x18EE8/4, 0x10);
+	INSTANCE_WR(ctx, 0x19608/4, 0x88);
+	INSTANCE_WR(ctx, 0x19628/4, 0x88);
+	INSTANCE_WR(ctx, 0x19688/4, 0x4);
+	INSTANCE_WR(ctx, 0x19968/4, 0x26);
+	INSTANCE_WR(ctx, 0x199C8/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x19A48/4, 0x1A);
+	INSTANCE_WR(ctx, 0x19A68/4, 0x10);
+	INSTANCE_WR(ctx, 0x19F88/4, 0x52);
+	INSTANCE_WR(ctx, 0x19FC8/4, 0x26);
+	INSTANCE_WR(ctx, 0x1A008/4, 0x4);
+	INSTANCE_WR(ctx, 0x1A028/4, 0x4);
+	INSTANCE_WR(ctx, 0x1A068/4, 0x1A);
+	INSTANCE_WR(ctx, 0x1A0C8/4, 0xFFFF00);
+	INSTANCE_WR(ctx, 0x1A108/4, 0x4);
+	INSTANCE_WR(ctx, 0x1A128/4, 0x4);
+	INSTANCE_WR(ctx, 0x1A168/4, 0x80);
+	INSTANCE_WR(ctx, 0x1A188/4, 0x4);
+	INSTANCE_WR(ctx, 0x1A1A8/4, 0x80C14);
+	INSTANCE_WR(ctx, 0x1A1E8/4, 0x7FF);
+	INSTANCE_WR(ctx, 0x24A48/4, 0x4);
+	INSTANCE_WR(ctx, 0x24A68/4, 0x4);
+	INSTANCE_WR(ctx, 0x24AA8/4, 0x80);
+	INSTANCE_WR(ctx, 0x24AC8/4, 0x4);
+	INSTANCE_WR(ctx, 0x24AE8/4, 0x1);
+	INSTANCE_WR(ctx, 0x24B28/4, 0x27);
+	INSTANCE_WR(ctx, 0x24B68/4, 0x26);
+	INSTANCE_WR(ctx, 0x24BE8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24C08/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24C28/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24C48/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24C68/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24C88/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24CA8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24CC8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24CE8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24D08/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24D28/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24D48/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24D68/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24D88/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24DA8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x24DC8/4, 0x4000000);
+	INSTANCE_WR(ctx, 0x25268/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x25288/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x252E8/4, 0x1FE21);
+	INSTANCE_WR(ctx, 0xB0C/4, 0x2);
+	INSTANCE_WR(ctx, 0xB4C/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0xCEC/4, 0x1);
+	INSTANCE_WR(ctx, 0xD0C/4, 0x10);
+	INSTANCE_WR(ctx, 0xD6C/4, 0x1);
+	INSTANCE_WR(ctx, 0xE0C/4, 0x4);
+	INSTANCE_WR(ctx, 0xE2C/4, 0x400);
+	INSTANCE_WR(ctx, 0xE4C/4, 0x300);
+	INSTANCE_WR(ctx, 0xE6C/4, 0x1001);
+	INSTANCE_WR(ctx, 0xE8C/4, 0x15);
+	INSTANCE_WR(ctx, 0xF4C/4, 0x2);
+	INSTANCE_WR(ctx, 0x106C/4, 0x1);
+	INSTANCE_WR(ctx, 0x108C/4, 0x10);
+	INSTANCE_WR(ctx, 0x10CC/4, 0x1);
+	INSTANCE_WR(ctx, 0x134C/4, 0x10);
+	INSTANCE_WR(ctx, 0x156C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x158C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x15AC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x15CC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x15EC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x160C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x162C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x164C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x166C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x168C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16AC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16CC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x16EC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x170C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x172C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x174C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x1A8C/4, 0x10);
+	INSTANCE_WR(ctx, 0x1ACC/4, 0x3F);
+	INSTANCE_WR(ctx, 0x1BAC/4, 0x1);
+	INSTANCE_WR(ctx, 0x1BEC/4, 0x1);
+	INSTANCE_WR(ctx, 0x1C2C/4, 0x1);
+	INSTANCE_WR(ctx, 0x1DCC/4, 0x11);
+	INSTANCE_WR(ctx, 0x1ECC/4, 0xF);
+	INSTANCE_WR(ctx, 0x1FCC/4, 0x11);
+	INSTANCE_WR(ctx, 0x20AC/4, 0x1);
+	INSTANCE_WR(ctx, 0x20CC/4, 0x1);
+	INSTANCE_WR(ctx, 0x20EC/4, 0x1);
+	INSTANCE_WR(ctx, 0x210C/4, 0x2);
+	INSTANCE_WR(ctx, 0x212C/4, 0x1);
+	INSTANCE_WR(ctx, 0x214C/4, 0x2);
+	INSTANCE_WR(ctx, 0x216C/4, 0x1);
+	INSTANCE_WR(ctx, 0x21AC/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x21EC/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x24AC/4, 0x1);
+	INSTANCE_WR(ctx, 0x24CC/4, 0x2);
+	INSTANCE_WR(ctx, 0x24EC/4, 0x1);
+	INSTANCE_WR(ctx, 0x250C/4, 0x1);
+	INSTANCE_WR(ctx, 0x252C/4, 0x2);
+	INSTANCE_WR(ctx, 0x254C/4, 0x1);
+	INSTANCE_WR(ctx, 0x256C/4, 0x1);
+	INSTANCE_WR(ctx, 0x25EC/4, 0x11);
+	INSTANCE_WR(ctx, 0x260C/4, 0x1);
+	INSTANCE_WR(ctx, 0x328C/4, 0x2);
+	INSTANCE_WR(ctx, 0x32CC/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x346C/4, 0x1);
+	INSTANCE_WR(ctx, 0x348C/4, 0x10);
+	INSTANCE_WR(ctx, 0x34EC/4, 0x1);
+	INSTANCE_WR(ctx, 0x358C/4, 0x4);
+	INSTANCE_WR(ctx, 0x35AC/4, 0x400);
+	INSTANCE_WR(ctx, 0x35CC/4, 0x300);
+	INSTANCE_WR(ctx, 0x35EC/4, 0x1001);
+	INSTANCE_WR(ctx, 0x360C/4, 0x15);
+	INSTANCE_WR(ctx, 0x36CC/4, 0x2);
+	INSTANCE_WR(ctx, 0x37EC/4, 0x1);
+	INSTANCE_WR(ctx, 0x380C/4, 0x10);
+	INSTANCE_WR(ctx, 0x384C/4, 0x1);
+	INSTANCE_WR(ctx, 0x3ACC/4, 0x10);
+	INSTANCE_WR(ctx, 0x3CEC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3D0C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3D2C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3D4C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3D6C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3D8C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3DAC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3DCC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3DEC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3E0C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3E2C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3E4C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3E6C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3E8C/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3EAC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x3ECC/4, 0x3F800000);
+	INSTANCE_WR(ctx, 0x420C/4, 0x10);
+	INSTANCE_WR(ctx, 0x424C/4, 0x3F);
+	INSTANCE_WR(ctx, 0x432C/4, 0x1);
+	INSTANCE_WR(ctx, 0x436C/4, 0x1);
+	INSTANCE_WR(ctx, 0x43AC/4, 0x1);
+	INSTANCE_WR(ctx, 0x454C/4, 0x11);
+	INSTANCE_WR(ctx, 0x464C/4, 0xF);
+	INSTANCE_WR(ctx, 0x474C/4, 0x11);
+	INSTANCE_WR(ctx, 0x482C/4, 0x1);
+	INSTANCE_WR(ctx, 0x484C/4, 0x1);
+	INSTANCE_WR(ctx, 0x486C/4, 0x1);
+	INSTANCE_WR(ctx, 0x488C/4, 0x2);
+	INSTANCE_WR(ctx, 0x48AC/4, 0x1);
+	INSTANCE_WR(ctx, 0x48CC/4, 0x2);
+	INSTANCE_WR(ctx, 0x48EC/4, 0x1);
+	INSTANCE_WR(ctx, 0x492C/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x496C/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x4C2C/4, 0x1);
+	INSTANCE_WR(ctx, 0x4C4C/4, 0x2);
+	INSTANCE_WR(ctx, 0x4C6C/4, 0x1);
+	INSTANCE_WR(ctx, 0x4C8C/4, 0x1);
+	INSTANCE_WR(ctx, 0x4CAC/4, 0x2);
+	INSTANCE_WR(ctx, 0x4CCC/4, 0x1);
+	INSTANCE_WR(ctx, 0x4CEC/4, 0x1);
+	INSTANCE_WR(ctx, 0x4D6C/4, 0x11);
+	INSTANCE_WR(ctx, 0x4D8C/4, 0x1);
+	INSTANCE_WR(ctx, 0xA30/4, 0x4);
+	INSTANCE_WR(ctx, 0xCF0/4, 0x4);
+	INSTANCE_WR(ctx, 0xD10/4, 0x4);
+	INSTANCE_WR(ctx, 0xD30/4, 0x608080);
+	INSTANCE_WR(ctx, 0xDD0/4, 0x4);
+	INSTANCE_WR(ctx, 0xE30/4, 0x4);
+	INSTANCE_WR(ctx, 0xE50/4, 0x4);
+	INSTANCE_WR(ctx, 0xE70/4, 0x80);
+	INSTANCE_WR(ctx, 0xE90/4, 0x1E00);
+	INSTANCE_WR(ctx, 0xEB0/4, 0x4);
+	INSTANCE_WR(ctx, 0x1350/4, 0x4);
+	INSTANCE_WR(ctx, 0x1370/4, 0x80);
+	INSTANCE_WR(ctx, 0x1390/4, 0x4);
+	INSTANCE_WR(ctx, 0x13B0/4, 0x3020100);
+	INSTANCE_WR(ctx, 0x13D0/4, 0x3);
+	INSTANCE_WR(ctx, 0x13F0/4, 0x1E00);
+	INSTANCE_WR(ctx, 0x1410/4, 0x4);
+	INSTANCE_WR(ctx, 0x14B0/4, 0x4);
+	INSTANCE_WR(ctx, 0x14D0/4, 0x3);
+	INSTANCE_WR(ctx, 0x1550/4, 0x4);
+	INSTANCE_WR(ctx, 0x159F0/4, 0x4);
+	INSTANCE_WR(ctx, 0x15A10/4, 0x3);
+	INSTANCE_WR(ctx, 0x15C50/4, 0xF);
+	INSTANCE_WR(ctx, 0x15DD0/4, 0x4);
+	INSTANCE_WR(ctx, 0x15DF0/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x15E10/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x15E30/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x15E50/4, 0xFFFF);
+	INSTANCE_WR(ctx, 0x15F70/4, 0x1);
+	INSTANCE_WR(ctx, 0x15FF0/4, 0x1);
+	INSTANCE_WR(ctx, 0x160B0/4, 0x1);
+	INSTANCE_WR(ctx, 0x16250/4, 0x1);
+	INSTANCE_WR(ctx, 0x16270/4, 0x1);
+	INSTANCE_WR(ctx, 0x16290/4, 0x2);
+	INSTANCE_WR(ctx, 0x162B0/4, 0x1);
+	INSTANCE_WR(ctx, 0x162D0/4, 0x1);
+	INSTANCE_WR(ctx, 0x162F0/4, 0x2);
+	INSTANCE_WR(ctx, 0x16310/4, 0x1);
+	INSTANCE_WR(ctx, 0x16350/4, 0x11);
+	INSTANCE_WR(ctx, 0x16450/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x164B0/4, 0x4);
+	INSTANCE_WR(ctx, 0x16530/4, 0x11);
+	INSTANCE_WR(ctx, 0x16550/4, 0x1);
+	INSTANCE_WR(ctx, 0x16590/4, 0xCF);
+	INSTANCE_WR(ctx, 0x165B0/4, 0xCF);
+	INSTANCE_WR(ctx, 0x165D0/4, 0xCF);
+	INSTANCE_WR(ctx, 0x16730/4, 0x1);
+	INSTANCE_WR(ctx, 0x16750/4, 0x1);
+	INSTANCE_WR(ctx, 0x16770/4, 0x2);
+	INSTANCE_WR(ctx, 0x16790/4, 0x1);
+	INSTANCE_WR(ctx, 0x167B0/4, 0x1);
+	INSTANCE_WR(ctx, 0x167D0/4, 0x2);
+	INSTANCE_WR(ctx, 0x167F0/4, 0x1);
+	INSTANCE_WR(ctx, 0x16830/4, 0x1);
+	INSTANCE_WR(ctx, 0x16850/4, 0x1);
+	INSTANCE_WR(ctx, 0x16870/4, 0x1);
+	INSTANCE_WR(ctx, 0x16890/4, 0x1);
+	INSTANCE_WR(ctx, 0x168B0/4, 0x1);
+	INSTANCE_WR(ctx, 0x168D0/4, 0x1);
+	INSTANCE_WR(ctx, 0x168F0/4, 0x1);
+	INSTANCE_WR(ctx, 0x16910/4, 0x1);
+	INSTANCE_WR(ctx, 0x16930/4, 0x11);
+	INSTANCE_WR(ctx, 0x16A30/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x16A50/4, 0xF);
+	INSTANCE_WR(ctx, 0x16B50/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x16BB0/4, 0x11);
+	INSTANCE_WR(ctx, 0x16BD0/4, 0x1);
+	INSTANCE_WR(ctx, 0x16C50/4, 0x4);
+	INSTANCE_WR(ctx, 0x16D10/4, 0x1);
+	INSTANCE_WR(ctx, 0x16DB0/4, 0x11);
+	INSTANCE_WR(ctx, 0x16EB0/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x16F30/4, 0x11);
+	INSTANCE_WR(ctx, 0x16F50/4, 0x1);
+	INSTANCE_WR(ctx, 0x16F90/4, 0x1);
+	INSTANCE_WR(ctx, 0x16FD0/4, 0x1);
+	INSTANCE_WR(ctx, 0x17010/4, 0x7FF);
+	INSTANCE_WR(ctx, 0x17050/4, 0x1);
+	INSTANCE_WR(ctx, 0x17090/4, 0x1);
+	INSTANCE_WR(ctx, 0x175F0/4, 0x8);
+	INSTANCE_WR(ctx, 0x17610/4, 0x8);
+	INSTANCE_WR(ctx, 0x17630/4, 0x8);
+	INSTANCE_WR(ctx, 0x17650/4, 0x8);
+	INSTANCE_WR(ctx, 0x17670/4, 0x8);
+	INSTANCE_WR(ctx, 0x17690/4, 0x8);
+	INSTANCE_WR(ctx, 0x176B0/4, 0x8);
+	INSTANCE_WR(ctx, 0x176D0/4, 0x8);
+	INSTANCE_WR(ctx, 0x176F0/4, 0x11);
+	INSTANCE_WR(ctx, 0x177F0/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x17810/4, 0x400);
+	INSTANCE_WR(ctx, 0x17830/4, 0x400);
+	INSTANCE_WR(ctx, 0x17850/4, 0x400);
+	INSTANCE_WR(ctx, 0x17870/4, 0x400);
+	INSTANCE_WR(ctx, 0x17890/4, 0x400);
+	INSTANCE_WR(ctx, 0x178B0/4, 0x400);
+	INSTANCE_WR(ctx, 0x178D0/4, 0x400);
+	INSTANCE_WR(ctx, 0x178F0/4, 0x400);
+	INSTANCE_WR(ctx, 0x17910/4, 0x300);
+	INSTANCE_WR(ctx, 0x17930/4, 0x300);
+	INSTANCE_WR(ctx, 0x17950/4, 0x300);
+	INSTANCE_WR(ctx, 0x17970/4, 0x300);
+	INSTANCE_WR(ctx, 0x17990/4, 0x300);
+	INSTANCE_WR(ctx, 0x179B0/4, 0x300);
+	INSTANCE_WR(ctx, 0x179D0/4, 0x300);
+	INSTANCE_WR(ctx, 0x179F0/4, 0x300);
+	INSTANCE_WR(ctx, 0x17A10/4, 0x1);
+	INSTANCE_WR(ctx, 0x17A30/4, 0xF);
+	INSTANCE_WR(ctx, 0x17B30/4, 0x20);
+	INSTANCE_WR(ctx, 0x17B50/4, 0x11);
+	INSTANCE_WR(ctx, 0x17B70/4, 0x100);
+	INSTANCE_WR(ctx, 0x17BB0/4, 0x1);
+	INSTANCE_WR(ctx, 0x17C10/4, 0x40);
+	INSTANCE_WR(ctx, 0x17C30/4, 0x100);
+	INSTANCE_WR(ctx, 0x17C70/4, 0x3);
+	INSTANCE_WR(ctx, 0x17D10/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x17D90/4, 0x2);
+	INSTANCE_WR(ctx, 0x17DB0/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x17EF0/4, 0x1);
+	INSTANCE_WR(ctx, 0x17F90/4, 0x4);
+	INSTANCE_WR(ctx, 0x17FD0/4, 0x1);
+	INSTANCE_WR(ctx, 0x17FF0/4, 0x400);
+	INSTANCE_WR(ctx, 0x18010/4, 0x300);
+	INSTANCE_WR(ctx, 0x18030/4, 0x1001);
+	INSTANCE_WR(ctx, 0x180B0/4, 0x11);
+	INSTANCE_WR(ctx, 0x181B0/4, 0xFAC6881);
+	INSTANCE_WR(ctx, 0x181D0/4, 0xF);
+	INSTANCE_WR(ctx, 0x184D0/4, 0x1FFE67);
+	INSTANCE_WR(ctx, 0x18550/4, 0x11);
+	INSTANCE_WR(ctx, 0x185B0/4, 0x4);
+	INSTANCE_WR(ctx, 0x185F0/4, 0x1);
+	INSTANCE_WR(ctx, 0x18610/4, 0x1);
+	INSTANCE_WR(ctx, 0x18690/4, 0x1);
+	INSTANCE_WR(ctx, 0x18730/4, 0x1);
+	INSTANCE_WR(ctx, 0x18770/4, 0x1);
+	INSTANCE_WR(ctx, 0x187F0/4, 0x2A712488);
+	INSTANCE_WR(ctx, 0x18830/4, 0x4085C000);
+	INSTANCE_WR(ctx, 0x18850/4, 0x40);
+	INSTANCE_WR(ctx, 0x18870/4, 0x100);
+	INSTANCE_WR(ctx, 0x18890/4, 0x10100);
+	INSTANCE_WR(ctx, 0x188B0/4, 0x2800000);
+	INSTANCE_WR(ctx, 0x18B10/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x18B30/4, 0x4E3BFDF);
+	INSTANCE_WR(ctx, 0x18B50/4, 0x1);
+	INSTANCE_WR(ctx, 0x18B90/4, 0xFFFF00);
+	INSTANCE_WR(ctx, 0x18BB0/4, 0x1);
+	INSTANCE_WR(ctx, 0x18C10/4, 0xFFFF00);
+	INSTANCE_WR(ctx, 0x18D30/4, 0x1);
+	INSTANCE_WR(ctx, 0x18D70/4, 0x1);
+	INSTANCE_WR(ctx, 0x18D90/4, 0x30201000);
+	INSTANCE_WR(ctx, 0x18DB0/4, 0x70605040);
+	INSTANCE_WR(ctx, 0x18DD0/4, 0xB8A89888);
+	INSTANCE_WR(ctx, 0x18DF0/4, 0xF8E8D8C8);
+	INSTANCE_WR(ctx, 0x18E30/4, 0x1A);
+}
+
+
+int
+nv50_graph_create_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_engine *engine = &dev_priv->Engine;
+	struct nouveau_gpuobj *ramin = chan->ramin->gpuobj;
+	int grctx_size = 0x60000, hdr;
+	int ret;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	ret = nouveau_gpuobj_new_ref(dev, chan, NULL, 0, grctx_size, 0x1000,
+				     NVOBJ_FLAG_ZERO_ALLOC |
+				     NVOBJ_FLAG_ZERO_FREE, &chan->ramin_grctx);
+	if (ret)
+		return ret;
+
+	hdr = IS_G80 ? 0x200 : 0x20;
+	INSTANCE_WR(ramin, (hdr + 0x00)/4, 0x00190002);
+	INSTANCE_WR(ramin, (hdr + 0x04)/4, chan->ramin_grctx->instance +
+					   grctx_size - 1);
+	INSTANCE_WR(ramin, (hdr + 0x08)/4, chan->ramin_grctx->instance);
+	INSTANCE_WR(ramin, (hdr + 0x0c)/4, 0);
+	INSTANCE_WR(ramin, (hdr + 0x10)/4, 0);
+	INSTANCE_WR(ramin, (hdr + 0x14)/4, 0x00010000);
+
+	switch (dev_priv->chipset) {
+		case 0x86:
+			nv86_graph_init_ctxvals(dev, chan->ramin_grctx);
+			break;
+		default:
+			ret = engine->graph.load_context(chan);
+			if (ret) {
+				DRM_ERROR("Error hacking up initial context: %d\n", ret);
+				return ret;
+			}
+			break;
+	}
+
+	INSTANCE_WR(chan->ramin_grctx->gpuobj, 0x00000/4,
+		    chan->ramin->instance >> 12);
+	INSTANCE_WR(chan->ramin_grctx->gpuobj, 0x0011c/4, 0x00000002);
+
+	return 0;
+}
+
+void
+nv50_graph_destroy_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	int i, hdr;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	hdr = IS_G80 ? 0x200 : 0x20;
+	for (i=hdr; i<hdr+24; i+=4)
+		INSTANCE_WR(chan->ramin->gpuobj, i/4, 0);
+
+	nouveau_gpuobj_ref_del(dev, &chan->ramin_grctx);
+}
+
+static int
+nv50_graph_transfer_context(struct drm_device *dev, uint32_t inst, int save)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t old_cp, tv = 20000;
+	int i;
+
+	DRM_DEBUG("inst=0x%08x, save=%d\n", inst, save);
+
+	old_cp = NV_READ(NV20_PGRAPH_CHANNEL_CTX_POINTER);
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+	NV_WRITE(0x400824, NV_READ(0x400824) |
+		 (save ? NV40_PGRAPH_CTXCTL_0310_XFER_SAVE :
+			 NV40_PGRAPH_CTXCTL_0310_XFER_LOAD));
+	NV_WRITE(NV40_PGRAPH_CTXCTL_0304, NV40_PGRAPH_CTXCTL_0304_XFER_CTX);
+
+	for (i = 0; i < tv; i++) {
+		if (NV_READ(NV40_PGRAPH_CTXCTL_030C) == 0)
+			break;
+	}
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, old_cp);
+
+	if (i == tv) {
+		DRM_ERROR("failed: inst=0x%08x save=%d\n", inst, save);
+		DRM_ERROR("0x40030C = 0x%08x\n",
+			  NV_READ(NV40_PGRAPH_CTXCTL_030C));
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+int
+nv50_graph_load_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	uint32_t inst = chan->ramin->instance >> 12;
+	int ret; (void)ret;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+
+	NV_WRITE(NV20_PGRAPH_CHANNEL_CTX_POINTER, inst);
+	NV_WRITE(0x400320, 4);
+	NV_WRITE(NV40_PGRAPH_CTXCTL_CUR, inst | (1<<31));
+
+	return 0;
+}
+
+int
+nv50_graph_save_context(struct nouveau_channel *chan)
+{
+	struct drm_device *dev = chan->dev;
+	uint32_t inst = chan->ramin->instance >> 12;
+
+	DRM_DEBUG("ch%d\n", chan->id);
+
+	return nv50_graph_transfer_context(dev, inst, 1);
+}
diff --git a/drivers/char/drm/nv50_instmem.c b/drivers/char/drm/nv50_instmem.c
new file mode 100644
index 0000000..b7a51f0
--- /dev/null
+++ b/drivers/char/drm/nv50_instmem.c
@@ -0,0 +1,324 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ *
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+typedef struct {
+	uint32_t save1700[5]; /* 0x1700->0x1710 */
+
+	struct nouveau_gpuobj_ref *pramin_pt;
+	struct nouveau_gpuobj_ref *pramin_bar;
+} nv50_instmem_priv;
+
+#define NV50_INSTMEM_PAGE_SHIFT 12
+#define NV50_INSTMEM_PAGE_SIZE  (1 << NV50_INSTMEM_PAGE_SHIFT)
+#define NV50_INSTMEM_PT_SIZE(a)	(((a) >> 12) << 3)
+
+/*NOTE: - Assumes 0x1700 already covers the correct MiB of PRAMIN
+ */
+#define BAR0_WI32(g,o,v) do {                                     \
+	uint32_t offset;                                          \
+	if ((g)->im_backing) {                                    \
+		offset = (g)->im_backing->start;                  \
+	} else {                                                  \
+		offset  = chan->ramin->gpuobj->im_backing->start; \
+		offset += (g)->im_pramin->start;                  \
+	}                                                         \
+	offset += (o);                                            \
+	NV_WRITE(NV_RAMIN + (offset & 0xfffff), (v));             \
+} while(0)
+
+int
+nv50_instmem_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	struct nouveau_channel *chan;
+	uint32_t c_offset, c_size, c_ramfc, c_vmpd, c_base, pt_size;
+	nv50_instmem_priv *priv;
+	int ret, i;
+	uint32_t v;
+
+	priv = drm_calloc(1, sizeof(*priv), DRM_MEM_DRIVER);
+	if (!priv)
+		return -ENOMEM;
+	dev_priv->Engine.instmem.priv = priv;
+
+	/* Save state, will restore at takedown. */
+	for (i = 0x1700; i <= 0x1710; i+=4)
+		priv->save1700[(i-0x1700)/4] = NV_READ(i);
+
+	/* Reserve the last MiB of VRAM, we should probably try to avoid
+	 * setting up the below tables over the top of the VBIOS image at
+	 * some point.
+	 */
+	dev_priv->ramin_rsvd_vram = 1 << 20;
+	c_offset = nouveau_mem_fb_amount(dev) - dev_priv->ramin_rsvd_vram;
+	c_size   = 128 << 10;
+	c_vmpd   = ((dev_priv->chipset & 0xf0) == 0x50) ? 0x1400 : 0x200;
+	c_ramfc  = ((dev_priv->chipset & 0xf0) == 0x50) ? 0x0 : 0x20;
+	c_base   = c_vmpd + 0x4000;
+	pt_size  = NV50_INSTMEM_PT_SIZE(dev_priv->ramin->size);
+
+	DRM_DEBUG(" Rsvd VRAM base: 0x%08x\n", c_offset);
+	DRM_DEBUG("    VBIOS image: 0x%08x\n", (NV_READ(0x619f04)&~0xff)<<8);
+	DRM_DEBUG("  Aperture size: %d MiB\n",
+		  (uint32_t)dev_priv->ramin->size >> 20);
+	DRM_DEBUG("        PT size: %d KiB\n", pt_size >> 10);
+
+	NV_WRITE(NV50_PUNK_BAR0_PRAMIN, (c_offset >> 16));
+
+	/* Create a fake channel, and use it as our "dummy" channels 0/127.
+	 * The main reason for creating a channel is so we can use the gpuobj
+	 * code.  However, it's probably worth noting that NVIDIA also setup
+	 * their channels 0/127 with the same values they configure here.
+	 * So, there may be some other reason for doing this.
+	 *
+	 * Have to create the entire channel manually, as the real channel
+	 * creation code assumes we have PRAMIN access, and we don't until
+	 * we're done here.
+	 */
+	chan = drm_calloc(1, sizeof(*chan), DRM_MEM_DRIVER);
+	if (!chan)
+		return -ENOMEM;
+	chan->id = 0;
+	chan->dev = dev;
+	chan->file_priv = (struct drm_file *)-2;
+	dev_priv->fifos[0] = dev_priv->fifos[127] = chan;
+
+	/* Channel's PRAMIN object + heap */
+	if ((ret = nouveau_gpuobj_new_fake(dev, 0, c_offset, 128<<10, 0,
+					   NULL, &chan->ramin)))
+		return ret;
+
+	if (nouveau_mem_init_heap(&chan->ramin_heap, c_base, c_size - c_base))
+		return -ENOMEM;
+
+	/* RAMFC + zero channel's PRAMIN up to start of VM pagedir */
+	if ((ret = nouveau_gpuobj_new_fake(dev, c_ramfc, c_offset + c_ramfc,
+					   0x4000, 0, NULL, &chan->ramfc)))
+		return ret;
+
+	for (i = 0; i < c_vmpd; i += 4)
+		BAR0_WI32(chan->ramin->gpuobj, i, 0);
+
+	/* VM page directory */
+	if ((ret = nouveau_gpuobj_new_fake(dev, c_vmpd, c_offset + c_vmpd,
+					   0x4000, 0, &chan->vm_pd, NULL)))
+		return ret;
+	for (i = 0; i < 0x4000; i += 8) {
+		BAR0_WI32(chan->vm_pd, i + 0x00, 0x00000000);
+		BAR0_WI32(chan->vm_pd, i + 0x04, 0x00000000);
+	}
+
+	/* PRAMIN page table, cheat and map into VM at 0x0000000000.
+	 * We map the entire fake channel into the start of the PRAMIN BAR
+	 */
+	if ((ret = nouveau_gpuobj_new_ref(dev, chan, NULL, 0, pt_size, 0x1000,
+					  0, &priv->pramin_pt)))
+		return ret;
+
+	for (i = 0, v = c_offset; i < pt_size; i+=8, v+=0x1000) {
+		if (v < (c_offset + c_size))
+			BAR0_WI32(priv->pramin_pt->gpuobj, i + 0, v | 1);
+		else
+			BAR0_WI32(priv->pramin_pt->gpuobj, i + 0, 0x00000009);
+		BAR0_WI32(priv->pramin_pt->gpuobj, i + 4, 0x00000000);
+	}
+
+	BAR0_WI32(chan->vm_pd, 0x00, priv->pramin_pt->instance | 0x63);
+	BAR0_WI32(chan->vm_pd, 0x04, 0x00000000);
+
+	/* DMA object for PRAMIN BAR */
+	if ((ret = nouveau_gpuobj_new_ref(dev, chan, chan, 0, 6*4, 16, 0,
+					  &priv->pramin_bar)))
+		return ret;
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x00, 0x7fc00000);
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x04, dev_priv->ramin->size - 1);
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x08, 0x00000000);
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x0c, 0x00000000);
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x10, 0x00000000);
+	BAR0_WI32(priv->pramin_bar->gpuobj, 0x14, 0x00000000);
+
+	/* Poke the relevant regs, and pray it works :) */
+	NV_WRITE(NV50_PUNK_BAR_CFG_BASE, (chan->ramin->instance >> 12));
+	NV_WRITE(NV50_PUNK_UNK1710, 0);
+	NV_WRITE(NV50_PUNK_BAR_CFG_BASE, (chan->ramin->instance >> 12) |
+					 NV50_PUNK_BAR_CFG_BASE_VALID);
+	NV_WRITE(NV50_PUNK_BAR1_CTXDMA, 0);
+	NV_WRITE(NV50_PUNK_BAR3_CTXDMA, (priv->pramin_bar->instance >> 4) |
+					NV50_PUNK_BAR3_CTXDMA_VALID);
+
+	/* Assume that praying isn't enough, check that we can re-read the
+	 * entire fake channel back from the PRAMIN BAR */
+	for (i = 0; i < c_size; i+=4) {
+		if (NV_READ(NV_RAMIN + i) != NV_RI32(i)) {
+			DRM_ERROR("Error reading back PRAMIN at 0x%08x\n", i);
+			return -EINVAL;
+		}
+	}
+
+	/* Global PRAMIN heap */
+	if (nouveau_mem_init_heap(&dev_priv->ramin_heap,
+				  c_size, dev_priv->ramin->size - c_size)) {
+		dev_priv->ramin_heap = NULL;
+		DRM_ERROR("Failed to init RAMIN heap\n");
+	}
+
+	/*XXX: incorrect, but needed to make hash func "work" */
+	dev_priv->ramht_offset = 0x10000;
+	dev_priv->ramht_bits   = 9;
+	dev_priv->ramht_size   = (1 << dev_priv->ramht_bits);
+	return 0;
+}
+
+void
+nv50_instmem_takedown(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	nv50_instmem_priv *priv = dev_priv->Engine.instmem.priv;
+	struct nouveau_channel *chan = dev_priv->fifos[0];
+	int i;
+
+	DRM_DEBUG("\n");
+
+	if (!priv)
+		return;
+
+	/* Restore state from before init */
+	for (i = 0x1700; i <= 0x1710; i+=4)
+		NV_WRITE(i, priv->save1700[(i-0x1700)/4]);
+
+	nouveau_gpuobj_ref_del(dev, &priv->pramin_bar);
+	nouveau_gpuobj_ref_del(dev, &priv->pramin_pt);
+
+	/* Destroy dummy channel */
+	if (chan) {
+		nouveau_gpuobj_del(dev, &chan->vm_pd);
+		nouveau_gpuobj_ref_del(dev, &chan->ramfc);
+		nouveau_gpuobj_ref_del(dev, &chan->ramin);
+		nouveau_mem_takedown(&chan->ramin_heap);
+
+		dev_priv->fifos[0] = dev_priv->fifos[127] = NULL;
+		drm_free(chan, sizeof(*chan), DRM_MEM_DRIVER);
+	}
+
+	dev_priv->Engine.instmem.priv = NULL;
+	drm_free(priv, sizeof(*priv), DRM_MEM_DRIVER);
+}
+
+int
+nv50_instmem_populate(struct drm_device *dev, struct nouveau_gpuobj *gpuobj, uint32_t *sz)
+{
+	if (gpuobj->im_backing)
+		return -EINVAL;
+
+	*sz = (*sz + (NV50_INSTMEM_PAGE_SIZE-1)) & ~(NV50_INSTMEM_PAGE_SIZE-1);
+	if (*sz == 0)
+		return -EINVAL;
+
+	gpuobj->im_backing = nouveau_mem_alloc(dev, NV50_INSTMEM_PAGE_SIZE,
+					       *sz, NOUVEAU_MEM_FB |
+					       NOUVEAU_MEM_NOVM,
+					       (struct drm_file *)-2);
+	if (!gpuobj->im_backing) {
+		DRM_ERROR("Couldn't allocate vram to back PRAMIN pages\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+void
+nv50_instmem_clear(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	if (gpuobj && gpuobj->im_backing) {
+		if (gpuobj->im_bound)
+			dev_priv->Engine.instmem.unbind(dev, gpuobj);
+		nouveau_mem_free(dev, gpuobj->im_backing);
+		gpuobj->im_backing = NULL;
+	}
+}
+
+int
+nv50_instmem_bind(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	nv50_instmem_priv *priv = dev_priv->Engine.instmem.priv;
+	uint32_t pte, pte_end, vram;
+
+	if (!gpuobj->im_backing || !gpuobj->im_pramin || gpuobj->im_bound)
+		return -EINVAL;
+
+	DRM_DEBUG("st=0x%0llx sz=0x%0llx\n",
+		  gpuobj->im_pramin->start, gpuobj->im_pramin->size);
+
+	pte     = (gpuobj->im_pramin->start >> 12) << 3;
+	pte_end = ((gpuobj->im_pramin->size >> 12) << 3) + pte;
+	vram    = gpuobj->im_backing->start;
+
+	DRM_DEBUG("pramin=0x%llx, pte=%d, pte_end=%d\n",
+		  gpuobj->im_pramin->start, pte, pte_end);
+	DRM_DEBUG("first vram page: 0x%llx\n",
+		  gpuobj->im_backing->start);
+
+	while (pte < pte_end) {
+		INSTANCE_WR(priv->pramin_pt->gpuobj, (pte + 0)/4, vram | 1);
+		INSTANCE_WR(priv->pramin_pt->gpuobj, (pte + 4)/4, 0x00000000);
+
+		pte += 8;
+		vram += NV50_INSTMEM_PAGE_SIZE;
+	}
+
+	gpuobj->im_bound = 1;
+	return 0;
+}
+
+int
+nv50_instmem_unbind(struct drm_device *dev, struct nouveau_gpuobj *gpuobj)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+	nv50_instmem_priv *priv = dev_priv->Engine.instmem.priv;
+	uint32_t pte, pte_end;
+
+	if (gpuobj->im_bound == 0)
+		return -EINVAL;
+
+	pte     = (gpuobj->im_pramin->start >> 12) << 3;
+	pte_end = ((gpuobj->im_pramin->size >> 12) << 3) + pte;
+	while (pte < pte_end) {
+		INSTANCE_WR(priv->pramin_pt->gpuobj, (pte + 0)/4, 0x00000009);
+		INSTANCE_WR(priv->pramin_pt->gpuobj, (pte + 4)/4, 0x00000000);
+		pte += 8;
+	}
+
+	gpuobj->im_bound = 0;
+	return 0;
+}
diff --git a/drivers/char/drm/nv50_mc.c b/drivers/char/drm/nv50_mc.c
new file mode 100644
index 0000000..b111826
--- /dev/null
+++ b/drivers/char/drm/nv50_mc.c
@@ -0,0 +1,43 @@
+/*
+ * Copyright (C) 2007 Ben Skeggs.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sublicense, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial
+ * portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
+ * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+ * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "nouveau_drv.h"
+
+int
+nv50_mc_init(struct drm_device *dev)
+{
+	struct drm_nouveau_private *dev_priv = dev->dev_private;
+
+	NV_WRITE(NV03_PMC_ENABLE, 0xFFFFFFFF);
+
+	return 0;
+}
+
+void nv50_mc_takedown(struct drm_device *dev)
+{
+}
diff --git a/drivers/char/drm/radeon_cp.c b/drivers/char/drm/radeon_cp.c
index e53158f..6ec5d98 100644
--- a/drivers/char/drm/radeon_cp.c
+++ b/drivers/char/drm/radeon_cp.c
@@ -651,15 +651,17 @@ static void radeon_cp_init_ring_buffer(struct drm_device * dev,
 	tmp = RADEON_READ(RADEON_BUS_CNTL) & ~RADEON_BUS_MASTER_DIS;
 	RADEON_WRITE(RADEON_BUS_CNTL, tmp);
 
-	dev_priv->sarea_priv->last_frame = dev_priv->scratch[0] = 0;
-	RADEON_WRITE(RADEON_LAST_FRAME_REG, dev_priv->sarea_priv->last_frame);
+	if (dev_priv->sarea_priv) {
+		dev_priv->sarea_priv->last_frame = dev_priv->scratch[0] = 0;
+		RADEON_WRITE(RADEON_LAST_FRAME_REG, dev_priv->sarea_priv->last_frame);
 
-	dev_priv->sarea_priv->last_dispatch = dev_priv->scratch[1] = 0;
-	RADEON_WRITE(RADEON_LAST_DISPATCH_REG,
-		     dev_priv->sarea_priv->last_dispatch);
+		dev_priv->sarea_priv->last_dispatch = dev_priv->scratch[1] = 0;
+		RADEON_WRITE(RADEON_LAST_DISPATCH_REG,
+			     dev_priv->sarea_priv->last_dispatch);
 
-	dev_priv->sarea_priv->last_clear = dev_priv->scratch[2] = 0;
-	RADEON_WRITE(RADEON_LAST_CLEAR_REG, dev_priv->sarea_priv->last_clear);
+		dev_priv->sarea_priv->last_clear = dev_priv->scratch[2] = 0;
+		RADEON_WRITE(RADEON_LAST_CLEAR_REG, dev_priv->sarea_priv->last_clear);
+	}
 
 	radeon_do_wait_for_idle(dev_priv);
 
@@ -850,10 +852,10 @@ static void radeon_set_pcigart(drm_radeon_private_t * dev_priv, int on)
 	}
 }
 
-static int radeon_do_init_cp(struct drm_device * dev, drm_radeon_init_t * init)
+static int radeon_do_init_cp(struct drm_device * dev, drm_radeon_init_t * init, struct drm_file *file_priv)
 {
 	drm_radeon_private_t *dev_priv = dev->dev_private;
-
+	struct drm_radeon_master_private *master_priv = file_priv->master->driver_priv;
 	DRM_DEBUG("\n");
 
 	/* if we require new memory map but we don't have it fail */
@@ -977,8 +979,8 @@ static int radeon_do_init_cp(struct drm_device * dev, drm_radeon_init_t * init)
 	dev_priv->buffers_offset = init->buffers_offset;
 	dev_priv->gart_textures_offset = init->gart_textures_offset;
 
-	dev_priv->sarea = drm_getsarea(dev);
-	if (!dev_priv->sarea) {
+	master_priv->sarea = drm_getsarea(dev);
+	if (!master_priv->sarea) {
 		DRM_ERROR("could not find sarea!\n");
 		radeon_do_cleanup_cp(dev);
 		return -EINVAL;
@@ -1015,7 +1017,7 @@ static int radeon_do_init_cp(struct drm_device * dev, drm_radeon_init_t * init)
 	}
 
 	dev_priv->sarea_priv =
-	    (drm_radeon_sarea_t *) ((u8 *) dev_priv->sarea->handle +
+	    (drm_radeon_sarea_t *) ((u8 *) master_priv->sarea->handle +
 				    init->sarea_priv_offset);
 
 #if __OS_HAS_AGP
@@ -1308,7 +1310,7 @@ int radeon_cp_init(struct drm_device *dev, void *data, struct drm_file *file_pri
 	case RADEON_INIT_CP:
 	case RADEON_INIT_R200_CP:
 	case RADEON_INIT_R300_CP:
-		return radeon_do_init_cp(dev, init);
+		return radeon_do_init_cp(dev, init, file_priv);
 	case RADEON_CLEANUP_CP:
 		return radeon_do_cleanup_cp(dev);
 	}
@@ -1731,6 +1733,9 @@ int radeon_driver_load(struct drm_device *dev, unsigned long flags)
 
 	DRM_DEBUG("%s card detected\n",
 		  ((dev_priv->flags & RADEON_IS_AGP) ? "AGP" : (((dev_priv->flags & RADEON_IS_PCIE) ? "PCIE" : "PCI"))));
+	ret = drm_addmap(dev, drm_get_resource_start(dev, 2),
+			 drm_get_resource_len(dev, 2), _DRM_REGISTERS,
+			 _DRM_READ_ONLY|_DRM_DRIVER, &dev_priv->mmio);
 	return ret;
 }
 
@@ -1745,12 +1750,6 @@ int radeon_driver_firstopen(struct drm_device *dev)
 
 	dev_priv->gart_info.table_size = RADEON_PCIGART_TABLE_SIZE;
 
-	ret = drm_addmap(dev, drm_get_resource_start(dev, 2),
-			 drm_get_resource_len(dev, 2), _DRM_REGISTERS,
-			 _DRM_READ_ONLY, &dev_priv->mmio);
-	if (ret != 0)
-		return ret;
-
 	dev_priv->fb_aper_offset = drm_get_resource_start(dev, 0);
 	ret = drm_addmap(dev, dev_priv->fb_aper_offset,
 			 drm_get_resource_len(dev, 0), _DRM_FRAME_BUFFER,
@@ -1766,6 +1765,7 @@ int radeon_driver_unload(struct drm_device *dev)
 	drm_radeon_private_t *dev_priv = dev->dev_private;
 
 	DRM_DEBUG("\n");
+	drm_rmmap(dev, dev_priv->mmio);
 	drm_free(dev_priv, sizeof(*dev_priv), DRM_MEM_DRIVER);
 
 	dev->dev_private = NULL;
diff --git a/drivers/char/drm/radeon_drv.c b/drivers/char/drm/radeon_drv.c
index 349ac3d..52b7c46 100644
--- a/drivers/char/drm/radeon_drv.c
+++ b/drivers/char/drm/radeon_drv.c
@@ -81,6 +81,8 @@ static struct drm_driver driver = {
 	.get_reg_ofs = drm_core_get_reg_ofs,
 	.ioctls = radeon_ioctls,
 	.dma_ioctl = radeon_cp_buffers,
+	.master_create = radeon_master_create,
+	.master_destroy = radeon_master_destroy,
 	.fops = {
 		 .owner = THIS_MODULE,
 		 .open = drm_open,
diff --git a/drivers/char/drm/radeon_drv.h b/drivers/char/drm/radeon_drv.h
index 3f0eca9..2e5ffb7 100644
--- a/drivers/char/drm/radeon_drv.h
+++ b/drivers/char/drm/radeon_drv.h
@@ -280,8 +280,6 @@ typedef struct drm_radeon_private {
 	unsigned long buffers_offset;
 	unsigned long gart_textures_offset;
 
-	drm_local_map_t *sarea;
-	drm_local_map_t *mmio;
 	drm_local_map_t *cp_ring;
 	drm_local_map_t *ring_rptr;
 	drm_local_map_t *gart_textures;
@@ -311,8 +309,13 @@ typedef struct drm_radeon_private {
 	unsigned long fb_aper_offset;
 
 	int num_gb_pipes;
+	drm_local_map_t *mmio;
 } drm_radeon_private_t;
 
+struct drm_radeon_master_private {
+	drm_local_map_t *sarea;
+};
+
 typedef struct drm_radeon_buf_priv {
 	u32 age;
 } drm_radeon_buf_priv_t;
@@ -400,6 +403,9 @@ extern int radeon_driver_open(struct drm_device * dev, struct drm_file * filp_pr
 extern long radeon_compat_ioctl(struct file *filp, unsigned int cmd,
 				unsigned long arg);
 
+extern int radeon_master_create(struct drm_device *dev, struct drm_master *master);
+extern void radeon_master_destroy(struct drm_device *dev, struct drm_master *master);
+
 /* r300_cmdbuf.c */
 extern void r300_init_reg_flags(struct drm_device *dev);
 
diff --git a/drivers/char/drm/radeon_state.c b/drivers/char/drm/radeon_state.c
index 11c146b..822a858 100644
--- a/drivers/char/drm/radeon_state.c
+++ b/drivers/char/drm/radeon_state.c
@@ -1415,10 +1415,11 @@ static void radeon_cp_dispatch_swap(struct drm_device * dev)
 	ADVANCE_RING();
 }
 
-static void radeon_cp_dispatch_flip(struct drm_device * dev)
+static void radeon_cp_dispatch_flip(struct drm_device * dev, struct drm_master *master)
 {
 	drm_radeon_private_t *dev_priv = dev->dev_private;
-	struct drm_sarea *sarea = (struct drm_sarea *) dev_priv->sarea->handle;
+	struct drm_radeon_master_private *master_priv = master->driver_priv;
+	struct drm_sarea *sarea = (struct drm_sarea *) master_priv->sarea->handle;
 	int offset = (dev_priv->sarea_priv->pfCurrentPage == 1)
 	    ? dev_priv->front_offset : dev_priv->back_offset;
 	RING_LOCALS;
@@ -2174,7 +2175,7 @@ static int radeon_cp_flip(struct drm_device *dev, void *data, struct drm_file *f
 	if (!dev_priv->page_flipping)
 		radeon_do_init_pageflip(dev);
 
-	radeon_cp_dispatch_flip(dev);
+	radeon_cp_dispatch_flip(dev, file_priv->master);
 
 	COMMIT_RING();
 	return 0;
@@ -3020,7 +3021,7 @@ static int radeon_cp_getparam(struct drm_device *dev, void *data, struct drm_fil
 		 */
 	case RADEON_PARAM_SAREA_HANDLE:
 		/* The lock is the first dword in the sarea. */
-		value = (long)dev->lock.hw_lock;
+		value = (long)dev->primary->master->lock.hw_lock;
 		break;
 #endif
 	case RADEON_PARAM_GART_TEX_HANDLE:
@@ -3078,12 +3079,14 @@ static int radeon_cp_setparam(struct drm_device *dev, void *data, struct drm_fil
 			DRM_DEBUG("color tiling disabled\n");
 			dev_priv->front_pitch_offset &= ~RADEON_DST_TILE_MACRO;
 			dev_priv->back_pitch_offset &= ~RADEON_DST_TILE_MACRO;
-			dev_priv->sarea_priv->tiling_enabled = 0;
+			if (dev_priv->sarea_priv)
+				dev_priv->sarea_priv->tiling_enabled = 0;
 		} else if (sp->value == 1) {
 			DRM_DEBUG("color tiling enabled\n");
 			dev_priv->front_pitch_offset |= RADEON_DST_TILE_MACRO;
 			dev_priv->back_pitch_offset |= RADEON_DST_TILE_MACRO;
-			dev_priv->sarea_priv->tiling_enabled = 1;
+			if (dev_priv->sarea_priv)
+				dev_priv->sarea_priv->tiling_enabled = 1;
 		}
 		break;
 	case RADEON_SETPARAM_PCIGART_LOCATION:
@@ -3099,7 +3102,8 @@ static int radeon_cp_setparam(struct drm_device *dev, void *data, struct drm_fil
 			dev_priv->gart_info.table_size = RADEON_PCIGART_TABLE_SIZE;
 		break;
 	case RADEON_SETPARAM_VBLANK_CRTC:
-		return radeon_vblank_crtc_set(dev, sp->value);
+		if (dev_priv)
+			return radeon_vblank_crtc_set(dev, sp->value);
 		break;
 	default:
 		DRM_DEBUG("Invalid parameter %d\n", sp->param);
@@ -3129,15 +3133,42 @@ void radeon_driver_preclose(struct drm_device *dev, struct drm_file *file_priv)
 
 void radeon_driver_lastclose(struct drm_device *dev)
 {
-	if (dev->dev_private) {
-		drm_radeon_private_t *dev_priv = dev->dev_private;
+	radeon_do_release(dev);
+}
 
-		if (dev_priv->sarea_priv &&
-		    dev_priv->sarea_priv->pfCurrentPage != 0)
-			radeon_cp_dispatch_flip(dev);
-	}
+int radeon_master_create(struct drm_device *dev, struct drm_master *master)
+{
+	/* radeon doesn't support multi-master yet so just workaround */
+	struct drm_radeon_master_private *master_priv;
+
+	master_priv = drm_calloc(1, sizeof(*master_priv), DRM_MEM_DRIVER);
+	if (!master_priv)
+		return -ENOMEM;
+	
+	master->driver_priv = master_priv;
+	return 0;
+}
+
+void radeon_master_destroy(struct drm_device *dev, struct drm_master *master)
+{
+	struct drm_radeon_master_private *master_priv = master->driver_priv;
+	drm_radeon_private_t *dev_priv = dev->dev_private;
+
+	if (!master_priv)
+		return;
+
+	if (dev_priv->sarea_priv &&
+	    dev_priv->sarea_priv->pfCurrentPage != 0)
+		radeon_cp_dispatch_flip(dev, master);
+
+	dev_priv->sarea_priv = NULL;
+
+	if (master_priv->sarea)
+		drm_rmmap(dev, master_priv->sarea);
+
+	drm_free(master_priv, sizeof(*master_priv), DRM_MEM_DRIVER);
+	master->driver_priv = NULL;
 
-	radeon_do_release(dev);
 }
 
 int radeon_driver_open(struct drm_device *dev, struct drm_file *file_priv)
